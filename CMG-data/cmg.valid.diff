mmm include / linux / kernel . h <nl> ppp include / linux / kernel . h <nl> static inline char * pack_hex_byte ( char * buf , u8 byte ) <nl> /* <nl> * swap - swap value of @ a and @ b <nl> */ <nl> -# define swap ( a , b ) ({ typeof ( a ) __tmp = ( a ); ( a ) = ( b ); ( b ) = __tmp ; }) <nl> +# define swap ( a , b ) \ <nl> + do { typeof ( a ) __tmp = ( a ); ( a ) = ( b ); ( b ) = __tmp ; } while ( 0 ) <nl>  <nl> /** <nl> * container_of - cast a member of a structure out to the containing structure
mmm arch / arm / mach - berlin / platsmp . c <nl> ppp arch / arm / mach - berlin / platsmp . c <nl> # include < asm / smp_plat . h > <nl> # include < asm / smp_scu . h > <nl>  <nl> -# define CPU_RESET 0x00 <nl> +/* <nl> + * There are two reset registers , one with self - clearing ( SC ) <nl> + * reset and one with non - self - clearing reset ( NON_SC ). <nl> + */ <nl> +# define CPU_RESET_SC 0x00 <nl> +# define CPU_RESET_NON_SC 0x20 <nl>  <nl> # define RESET_VECT 0x00 <nl> # define SW_RESET_ADDR 0x94 <nl> static inline void berlin_perform_reset_cpu ( unsigned int cpu ) <nl> { <nl> u32 val ; <nl>  <nl> - val = readl ( cpu_ctrl + CPU_RESET ); <nl> + val = readl ( cpu_ctrl + CPU_RESET_NON_SC ); <nl> + val &= ~ BIT ( cpu_logical_map ( cpu )); <nl> + writel ( val , cpu_ctrl + CPU_RESET_NON_SC ); <nl> val |= BIT ( cpu_logical_map ( cpu )); <nl> - writel ( val , cpu_ctrl + CPU_RESET ); <nl> + writel ( val , cpu_ctrl + CPU_RESET_NON_SC ); <nl> } <nl>  <nl> static int berlin_boot_secondary ( unsigned int cpu , struct task_struct * idle )
mmm sound / core / control . c <nl> ppp sound / core / control . c <nl> static bool snd_ctl_remove_numid_conflict ( struct snd_card * card , <nl> { <nl> struct snd_kcontrol * kctl ; <nl>  <nl> + /* Make sure that the ids assigned to the control do not wrap around */ <nl> + if ( card -> last_numid >= UINT_MAX - count ) <nl> + card -> last_numid = 0 ; <nl> + <nl> list_for_each_entry ( kctl , & card -> controls , list ) { <nl> if ( kctl -> id . numid < card -> last_numid + 1 + count && <nl> kctl -> id . numid + kctl -> count > card -> last_numid + 1 ) {
mmm drivers / net / tulip / uli526x . c <nl> ppp drivers / net / tulip / uli526x . c <nl> static void uli526x_rx_packet ( struct net_device * dev , struct uli526x_board_info <nl>  <nl> if ( !( rdes0 & 0x8000 ) || <nl> (( db -> cr6_data & CR6_PM ) && ( rxlen > 6 )) ) { <nl> + struct sk_buff * new_skb = NULL ; <nl> + <nl> skb = rxptr -> rx_skb_ptr ; <nl>  <nl> /* Good packet , send to upper layer */ <nl> /* Shorst packet used new SKB */ <nl> - if ( ( rxlen < RX_COPY_SIZE ) && <nl> - ( ( skb = dev_alloc_skb ( rxlen + 2 ) ) <nl> - != NULL ) ) { <nl> + if (( rxlen < RX_COPY_SIZE ) && <nl> + (( new_skb = dev_alloc_skb ( rxlen + 2 ) != NULL ))) { <nl> + skb = new_skb ; <nl> /* size less than COPY_SIZE , allocate a rxlen SKB */ <nl> skb_reserve ( skb , 2 ); /* 16byte align */ <nl> memcpy ( skb_put ( skb , rxlen ),
mmm net / ipv4 / tcp . c <nl> ppp net / ipv4 / tcp . c <nl> int tcp_sendmsg ( struct sock * sk , struct msghdr * msg , size_t size ) <nl>  <nl> if (! skb_can_coalesce ( skb , i , pfrag -> page , <nl> pfrag -> offset )) { <nl> - if ( i == sysctl_max_skb_frags || ! sg ) { <nl> + if ( i >= sysctl_max_skb_frags || ! sg ) { <nl> tcp_mark_push ( tp , skb ); <nl> goto new_segment ; <nl> }
mmm arch / blackfin / mach - bf609 / clock . c <nl> ppp arch / blackfin / mach - bf609 / clock . c <nl> EXPORT_SYMBOL ( clk_enable ); <nl>  <nl> void clk_disable ( struct clk * clk ) <nl> { <nl> + if (! clk ) <nl> + return ; <nl> + <nl> if ( clk -> ops && clk -> ops -> disable ) <nl> clk -> ops -> disable ( clk ); <nl> }
mmm drivers / edac / edac_mc . c <nl> ppp drivers / edac / edac_mc . c <nl> void edac_mc_free ( struct mem_ctl_info * mci ) <nl> debugf1 ("% s ()\ n ", __func__ ); <nl>  <nl> edac_mc_unregister_sysfs_main_kobj ( mci ); <nl> + <nl> + /* free the mci instance memory here */ <nl> + kfree ( mci ); <nl> } <nl> EXPORT_SYMBOL_GPL ( edac_mc_free ); <nl> mmm drivers / edac / i7core_edac . c <nl> ppp drivers / edac / i7core_edac . c <nl> void edac_mc_free ( struct mem_ctl_info * mci ) <nl> debugf1 ("% s ()\ n ", __func__ ); <nl>  <nl> edac_mc_unregister_sysfs_main_kobj ( mci ); <nl> + <nl> + /* free the mci instance memory here */ <nl> + kfree ( mci ); <nl> } <nl> EXPORT_SYMBOL_GPL ( edac_mc_free ); <nl>  <nl> static void __devexit i7core_remove ( struct pci_dev * pdev ) <nl> /* Remove MC sysfs nodes */ <nl> edac_mc_del_mc (& i7core_dev -> pdev [ 0 ]-> dev ); <nl>  <nl> - /* Free data */ <nl> - debugf1 ("% s : free structs \ n "); <nl> + debugf1 ("% s : free mci struct \ n ", mci -> ctl_name ); <nl> kfree ( mci -> ctl_name ); <nl> edac_mc_free ( mci ); <nl> mmm drivers / edac / edac_mc_sysfs . c <nl> ppp drivers / edac / edac_mc_sysfs . c <nl> void edac_mc_free ( struct mem_ctl_info * mci ) <nl> debugf1 ("% s ()\ n ", __func__ ); <nl>  <nl> edac_mc_unregister_sysfs_main_kobj ( mci ); <nl> + <nl> + /* free the mci instance memory here */ <nl> + kfree ( mci ); <nl> } <nl> EXPORT_SYMBOL_GPL ( edac_mc_free ); <nl>  <nl> static void __devexit i7core_remove ( struct pci_dev * pdev ) <nl> /* Remove MC sysfs nodes */ <nl> edac_mc_del_mc (& i7core_dev -> pdev [ 0 ]-> dev ); <nl>  <nl> - /* Free data */ <nl> - debugf1 ("% s : free structs \ n "); <nl> + debugf1 ("% s : free mci struct \ n ", mci -> ctl_name ); <nl> kfree ( mci -> ctl_name ); <nl> edac_mc_free ( mci ); <nl>  <nl> static void edac_mci_control_release ( struct kobject * kobj ) <nl>  <nl> /* decrement the module ref count */ <nl> module_put ( mci -> owner ); <nl> - <nl> - /* free the mci instance memory here */ <nl> - kfree ( mci ); <nl> } <nl>  <nl> static struct kobj_type ktype_mci = {
mmm net / ipv6 / exthdrs_core . c <nl> ppp net / ipv6 / exthdrs_core . c <nl> int ipv6_find_hdr ( const struct sk_buff * skb , unsigned int * offset , <nl> found = ( nexthdr == target ); <nl>  <nl> if ((! ipv6_ext_hdr ( nexthdr )) || nexthdr == NEXTHDR_NONE ) { <nl> - if ( target < 0 ) <nl> + if ( target < 0 || found ) <nl> break ; <nl> return - ENOENT ; <nl> }
mmm arch / mips / include / asm / pgtable . h <nl> ppp arch / mips / include / asm / pgtable . h <nl> static inline struct page * pmd_page ( pmd_t pmd ) <nl>  <nl> static inline pmd_t pmd_modify ( pmd_t pmd , pgprot_t newprot ) <nl> { <nl> - pmd_val ( pmd ) = ( pmd_val ( pmd ) & _PAGE_CHG_MASK ) | <nl> + pmd_val ( pmd ) = ( pmd_val ( pmd ) & ( _PAGE_CHG_MASK | _PAGE_HUGE )) | <nl> ( pgprot_val ( newprot ) & ~ _PAGE_CHG_MASK ); <nl> return pmd ; <nl> }
mmm net / sctp / socket . c <nl> ppp net / sctp / socket . c <nl> static int sctp_getsockopt_disable_fragments ( struct sock * sk , int len , <nl> static int sctp_getsockopt_events ( struct sock * sk , int len , char __user * optval , <nl> int __user * optlen ) <nl> { <nl> - if ( len < sizeof ( struct sctp_event_subscribe )) <nl> + if ( len <= 0 ) <nl> return - EINVAL ; <nl> - len = sizeof ( struct sctp_event_subscribe ); <nl> + if ( len > sizeof ( struct sctp_event_subscribe )) <nl> + len = sizeof ( struct sctp_event_subscribe ); <nl> if ( put_user ( len , optlen )) <nl> return - EFAULT ; <nl> if ( copy_to_user ( optval , & sctp_sk ( sk )-> subscribe , len ))
mmm drivers / misc / eeprom / idt_89hpesx . c <nl> ppp drivers / misc / eeprom / idt_89hpesx . c <nl> static ssize_t idt_dbgfs_csr_write ( struct file * filep , const char __user * ubuf , <nl> csraddr_len = colon_ch - buf ; <nl> csraddr_str = <nl> kmalloc ( sizeof ( char )*( csraddr_len + 1 ), GFP_KERNEL ); <nl> - if ( csraddr_str == NULL ) <nl> - return - ENOMEM ; <nl> + if ( csraddr_str == NULL ) { <nl> + ret = - ENOMEM ; <nl> + goto free_buf ; <nl> + } <nl> /* Copy the register address to the substring buffer */ <nl> strncpy ( csraddr_str , buf , csraddr_len ); <nl> csraddr_str [ csraddr_len ] = '\ 0 ';
mmm drivers / staging / dgap / dgap . c <nl> ppp drivers / staging / dgap / dgap . c <nl> static int dgap_init_one ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl> { <nl> int rc ; <nl>  <nl> + if ( dgap_NumBoards >= MAXBOARDS ) <nl> + return - EPERM ; <nl> + <nl> /* wake up and enable device */ <nl> rc = pci_enable_device ( pdev ); <nl> 
mmm fs / overlayfs / inode . c <nl> ppp fs / overlayfs / inode . c <nl> int ovl_setattr ( struct dentry * dentry , struct iattr * attr ) <nl> if ( err ) <nl> goto out ; <nl>  <nl> - upperdentry = ovl_dentry_upper ( dentry ); <nl> - if ( upperdentry ) { <nl> + err = ovl_copy_up ( dentry ); <nl> + if (! err ) { <nl> + upperdentry = ovl_dentry_upper ( dentry ); <nl> + <nl> mutex_lock (& upperdentry -> d_inode -> i_mutex ); <nl> err = notify_change ( upperdentry , attr , NULL ); <nl> mutex_unlock (& upperdentry -> d_inode -> i_mutex ); <nl> - } else { <nl> - err = ovl_copy_up_last ( dentry , attr , false ); <nl> } <nl> ovl_drop_write ( dentry ); <nl> out :
mmm drivers / input / touchscreen / eeti_ts . c <nl> ppp drivers / input / touchscreen / eeti_ts . c <nl> static int eeti_ts_probe ( struct i2c_client * client , <nl> priv = kzalloc ( sizeof (* priv ), GFP_KERNEL ); <nl> if (! priv ) { <nl> dev_err (& client -> dev , " failed to allocate driver data \ n "); <nl> - goto err0 ; <nl> + return - ENOMEM ; <nl> } <nl>  <nl> mutex_init (& priv -> mutex ); <nl> input = input_allocate_device (); <nl> - <nl> if (! input ) { <nl> dev_err (& client -> dev , " Failed to allocate input device .\ n "); <nl> goto err1 ; <nl> static int eeti_ts_probe ( struct i2c_client * client , <nl> err1 : <nl> input_free_device ( input ); <nl> kfree ( priv ); <nl> - err0 : <nl> return err ; <nl> } <nl> 
mmm drivers / block / drbd / drbd_receiver . c <nl> ppp drivers / block / drbd / drbd_receiver . c <nl> static int drbd_asb_recover_0p ( struct drbd_conf * mdev ) __must_hold ( local ) <nl> break ; <nl> } <nl> /* Else fall through to one of the other strategies ... */ <nl> - dev_warn ( DEV , " Discard younger / older primary did not found a decision \ n " <nl> + dev_warn ( DEV , " Discard younger / older primary did not find a decision \ n " <nl> " Using discard - least - changes instead \ n "); <nl> case ASB_DISCARD_ZERO_CHG : <nl> if ( ch_peer == 0 && ch_self == 0 ) {
mmm drivers / infiniband / hw / amso1100 / c2_qp . c <nl> ppp drivers / infiniband / hw / amso1100 / c2_qp . c <nl> int c2_qp_modify ( struct c2_dev * c2dev , struct c2_qp * qp , <nl>  <nl> if ( attr_mask & IB_QP_STATE ) { <nl> /* Ensure the state is valid */ <nl> - if ( attr -> qp_state < 0 || attr -> qp_state > IB_QPS_ERR ) <nl> - return - EINVAL ; <nl> + if ( attr -> qp_state < 0 || attr -> qp_state > IB_QPS_ERR ) { <nl> + err = - EINVAL ; <nl> + goto bail0 ; <nl> + } <nl>  <nl> wr . next_qp_state = cpu_to_be32 ( to_c2_state ( attr -> qp_state )); <nl>  <nl> int c2_qp_modify ( struct c2_dev * c2dev , struct c2_qp * qp , <nl> if ( attr -> cur_qp_state != IB_QPS_RTR && <nl> attr -> cur_qp_state != IB_QPS_RTS && <nl> attr -> cur_qp_state != IB_QPS_SQD && <nl> - attr -> cur_qp_state != IB_QPS_SQE ) <nl> - return - EINVAL ; <nl> - else <nl> + attr -> cur_qp_state != IB_QPS_SQE ) { <nl> + err = - EINVAL ; <nl> + goto bail0 ; <nl> + } else <nl> wr . next_qp_state = <nl> cpu_to_be32 ( to_c2_state ( attr -> cur_qp_state )); <nl> 
mmm net / ceph / osdmap . c <nl> ppp net / ceph / osdmap . c <nl> static int __decode_pool_names ( void ** p , void * end , struct ceph_osdmap * map ) <nl> ceph_decode_32_safe ( p , end , pool , bad ); <nl> ceph_decode_32_safe ( p , end , len , bad ); <nl> dout (" pool % d len % d \ n ", pool , len ); <nl> + ceph_decode_need ( p , end , len , bad ); <nl> pi = __lookup_pg_pool (& map -> pg_pools , pool ); <nl> if ( pi ) { <nl> + char * name = kstrndup (* p , len , GFP_NOFS ); <nl> + <nl> + if (! name ) <nl> + return - ENOMEM ; <nl> kfree ( pi -> name ); <nl> - pi -> name = kmalloc ( len + 1 , GFP_NOFS ); <nl> - if ( pi -> name ) { <nl> - memcpy ( pi -> name , * p , len ); <nl> - pi -> name [ len ] = '\ 0 '; <nl> - dout (" name is % s \ n ", pi -> name ); <nl> - } <nl> + pi -> name = name ; <nl> + dout (" name is % s \ n ", pi -> name ); <nl> } <nl> * p += len ; <nl> }
mmm drivers / tty / hvc / hvcs . c <nl> ppp drivers / tty / hvc / hvcs . c <nl> static int __devinit hvcs_initialize ( void ) <nl> num_ttys_to_alloc = hvcs_parm_num_devs ; <nl>  <nl> hvcs_tty_driver = alloc_tty_driver ( num_ttys_to_alloc ); <nl> - if (! hvcs_tty_driver ) <nl> + if (! hvcs_tty_driver ) { <nl> + mutex_unlock (& hvcs_init_mutex ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if ( hvcs_alloc_index_list ( num_ttys_to_alloc )) { <nl> rc = - ENOMEM ;
mmm drivers / hid / hid - logitech - dj . c <nl> ppp drivers / hid / hid - logitech - dj . c <nl> static void logi_dj_recv_add_djhid_device ( struct dj_receiver_dev * djrcv_dev , <nl> return ; <nl> } <nl>  <nl> - if (( dj_report -> device_index < DJ_DEVICE_INDEX_MIN ) || <nl> - ( dj_report -> device_index > DJ_DEVICE_INDEX_MAX )) { <nl> - dev_err (& djrcv_hdev -> dev , "% s : invalid device index :% d \ n ", <nl> - __func__ , dj_report -> device_index ); <nl> - return ; <nl> - } <nl> - <nl> if ( djrcv_dev -> paired_dj_devices [ dj_report -> device_index ]) { <nl> /* The device is already known . No need to reallocate it . */ <nl> dbg_hid ("% s : device is already known \ n ", __func__ ); <nl> static int logi_dj_raw_event ( struct hid_device * hdev , <nl> * device ( via hid_input_report () ) and return 1 so hid - core does not do <nl> * anything else with it . <nl> */ <nl> + if (( dj_report -> device_index < DJ_DEVICE_INDEX_MIN ) || <nl> + ( dj_report -> device_index > DJ_DEVICE_INDEX_MAX )) { <nl> + dev_err (& hdev -> dev , "% s : invalid device index :% d \ n ", <nl> + __func__ , dj_report -> device_index ); <nl> + return false ; <nl> + } <nl>  <nl> spin_lock_irqsave (& djrcv_dev -> lock , flags ); <nl> if ( dj_report -> report_id == REPORT_ID_DJ_SHORT ) {
mmm drivers / md / raid5 - cache . c <nl> ppp drivers / md / raid5 - cache . c <nl> static bool r5l_has_free_space ( struct r5l_log * log , sector_t size ) <nl> return log -> device_size > used_size + size ; <nl> } <nl>  <nl> - static void r5l_free_io_unit ( struct r5l_log * log , struct r5l_io_unit * io ) <nl> -{ <nl> - __free_page ( io -> meta_page ); <nl> - kmem_cache_free ( log -> io_kc , io ); <nl> -} <nl> - <nl> static void __r5l_set_io_unit_state ( struct r5l_io_unit * io , <nl> enum r5l_io_unit_state state ) <nl> { <nl> static void r5l_log_endio ( struct bio * bio ) <nl> md_error ( log -> rdev -> mddev , log -> rdev ); <nl>  <nl> bio_put ( bio ); <nl> + __free_page ( io -> meta_page ); <nl>  <nl> spin_lock_irqsave (& log -> io_list_lock , flags ); <nl> __r5l_set_io_unit_state ( io , IO_UNIT_IO_END ); <nl> static bool r5l_complete_finished_ios ( struct r5l_log * log ) <nl> log -> next_cp_seq = io -> seq ; <nl>  <nl> list_del (& io -> log_sibling ); <nl> - r5l_free_io_unit ( log , io ); <nl> + kmem_cache_free ( log -> io_kc , io ); <nl>  <nl> found = true ; <nl> }
mmm kernel / irq / numa_migrate . c <nl> ppp kernel / irq / numa_migrate . c <nl> static struct irq_desc * __real_move_irq_desc ( struct irq_desc * old_desc , <nl>  <nl> struct irq_desc * move_irq_desc ( struct irq_desc * desc , int node ) <nl> { <nl> - /* those all static , do move them */ <nl> - if ( desc -> irq < NR_IRQS_LEGACY ) <nl> + /* those static or target node is - 1 , do not move them */ <nl> + if ( desc -> irq < NR_IRQS_LEGACY || node == - 1 ) <nl> return desc ; <nl>  <nl> if ( desc -> node != node )
mmm sound / pci / hda / patch_realtek . c <nl> ppp sound / pci / hda / patch_realtek . c <nl> static const struct snd_pci_quirk alc662_fixup_tbl [] = { <nl> SND_PCI_QUIRK ( 0x1025 , 0x038b , " Acer Aspire 8943G ", ALC662_FIXUP_ASPIRE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05d8 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x1028 , 0x05db , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> + SND_PCI_QUIRK ( 0x1028 , 0x0626 , " Dell ", ALC668_FIXUP_DELL_MIC_NO_PRESENCE ), <nl> SND_PCI_QUIRK ( 0x103c , 0x1632 , " HP RP5800 ", ALC662_FIXUP_HP_RP5800 ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1477 , " ASUS N56VZ ", ALC662_FIXUP_BASS_CHMAP ), <nl> SND_PCI_QUIRK ( 0x1043 , 0x1bf3 , " ASUS N76VZ ", ALC662_FIXUP_BASS_CHMAP ),
mmm drivers / iio / health / max30102 . c <nl> ppp drivers / iio / health / max30102 . c <nl> # define MAX30102_DRV_NAME " max30102 " <nl> # define MAX30102_PART_NUMBER 0x15 <nl>  <nl> + enum max3012_led_idx { <nl> + MAX30102_LED_RED , <nl> + MAX30102_LED_IR , <nl> +}; <nl> + <nl> # define MAX30102_REG_INT_STATUS 0x00 <nl> # define MAX30102_REG_INT_STATUS_PWR_RDY BIT ( 0 ) <nl> # define MAX30102_REG_INT_STATUS_PROX_INT BIT ( 4 ) <nl> static const struct regmap_config max30102_regmap_config = { <nl> . val_bits = 8 , <nl> }; <nl>  <nl> - static const unsigned long max30102_scan_masks [] = { 0x3 , 0 }; <nl> + static const unsigned long max30102_scan_masks [] = { <nl> + BIT ( MAX30102_LED_RED ) | BIT ( MAX30102_LED_IR ), <nl> + 0 <nl> +}; <nl>  <nl> # define MAX30102_INTENSITY_CHANNEL ( _si , _mod ) { \ <nl> . type = IIO_INTENSITY , \ <nl> static const unsigned long max30102_scan_masks [] = { 0x3 , 0 }; <nl> } <nl>  <nl> static const struct iio_chan_spec max30102_channels [] = { <nl> - MAX30102_INTENSITY_CHANNEL ( 0 , IIO_MOD_LIGHT_RED ), <nl> - MAX30102_INTENSITY_CHANNEL ( 1 , IIO_MOD_LIGHT_IR ), <nl> + MAX30102_INTENSITY_CHANNEL ( MAX30102_LED_RED , IIO_MOD_LIGHT_RED ), <nl> + MAX30102_INTENSITY_CHANNEL ( MAX30102_LED_IR , IIO_MOD_LIGHT_IR ), <nl> { <nl> . type = IIO_TEMP , <nl> . info_mask_separate =
mmm net / mac80211 / util . c <nl> ppp net / mac80211 / util . c <nl> static void __ieee80211_wake_queue ( struct ieee80211_hw * hw , int queue , <nl> if ( WARN_ON ( queue >= hw -> queues )) <nl> return ; <nl>  <nl> + if (! test_bit ( reason , & local -> queue_stop_reasons [ queue ])) <nl> + return ; <nl> + <nl> __clear_bit ( reason , & local -> queue_stop_reasons [ queue ]); <nl>  <nl> if ( local -> queue_stop_reasons [ queue ] != 0 ) <nl> static void __ieee80211_stop_queue ( struct ieee80211_hw * hw , int queue , <nl> if ( WARN_ON ( queue >= hw -> queues )) <nl> return ; <nl>  <nl> + if ( test_bit ( reason , & local -> queue_stop_reasons [ queue ])) <nl> + return ; <nl> + <nl> __set_bit ( reason , & local -> queue_stop_reasons [ queue ]); <nl>  <nl> rcu_read_lock ();
mmm sound / soc / codecs / wm8753 . c <nl> ppp sound / soc / codecs / wm8753 . c <nl> static int __devexit wm8753_spi_remove ( struct spi_device * spi ) <nl>  <nl> snd_soc_unregister_codec (& spi -> dev ); <nl> regmap_exit ( wm8753 -> regmap ); <nl> - kfree ( wm8753 ); <nl> return 0 ; <nl> } <nl> 
mmm drivers / infiniband / hw / nes / nes_cm . c <nl> ppp drivers / infiniband / hw / nes / nes_cm . c <nl> static int mini_cm_del_listen ( struct nes_cm_core * cm_core , <nl> static inline int mini_cm_accelerated ( struct nes_cm_core * cm_core , <nl> struct nes_cm_node * cm_node ) <nl> { <nl> - cm_node -> accelerated = 1 ; <nl> + cm_node -> accelerated = true ; <nl>  <nl> if ( cm_node -> accept_pend ) { <nl> BUG_ON (! cm_node -> listener );mmm drivers / infiniband / hw / nes / nes_cm . h <nl> ppp drivers / infiniband / hw / nes / nes_cm . h <nl> static int mini_cm_del_listen ( struct nes_cm_core * cm_core , <nl> static inline int mini_cm_accelerated ( struct nes_cm_core * cm_core , <nl> struct nes_cm_node * cm_node ) <nl> { <nl> - cm_node -> accelerated = 1 ; <nl> + cm_node -> accelerated = true ; <nl>  <nl> if ( cm_node -> accept_pend ) { <nl> BUG_ON (! cm_node -> listener ); <nl> struct nes_cm_node { <nl> u16 mpa_frame_size ; <nl> struct iw_cm_id * cm_id ; <nl> struct list_head list ; <nl> - int accelerated ; <nl> + bool accelerated ; <nl> struct nes_cm_listener * listener ; <nl> enum nes_cm_conn_type conn_type ; <nl> struct nes_vnic * nesvnic ;
mmm arch / arm64 / include / asm / arch_timer . h <nl> ppp arch / arm64 / include / asm / arch_timer . h <nl> DECLARE_PER_CPU ( const struct arch_timer_erratum_workaround *, <nl> u64 _val ; \ <nl> if ( needs_unstable_timer_counter_workaround ()) { \ <nl> const struct arch_timer_erratum_workaround * wa ; \ <nl> - preempt_disable (); \ <nl> + preempt_disable_notrace (); \ <nl> wa = __this_cpu_read ( timer_unstable_counter_workaround ); \ <nl> if ( wa && wa -> read_ ## reg ) \ <nl> _val = wa -> read_ ## reg (); \ <nl> else \ <nl> _val = read_sysreg ( reg ); \ <nl> - preempt_enable (); \ <nl> + preempt_enable_notrace (); \ <nl> } else { \ <nl> _val = read_sysreg ( reg ); \ <nl> } \
mmm drivers / net / wireless / ath / ath10k / mac . c <nl> ppp drivers / net / wireless / ath / ath10k / mac . c <nl> static int ath10k_abort_scan ( struct ath10k * ar ) <nl> ret = ath10k_wmi_stop_scan ( ar , & arg ); <nl> if ( ret ) { <nl> ath10k_warn (" could not submit wmi stop scan (% d )\ n ", ret ); <nl> + spin_lock_bh (& ar -> data_lock ); <nl> + ar -> scan . in_progress = false ; <nl> + ath10k_offchan_tx_purge ( ar ); <nl> + spin_unlock_bh (& ar -> data_lock ); <nl> return - EIO ; <nl> } <nl> 
mmm drivers / net / irda / smsc - ircc2 . c <nl> ppp drivers / net / irda / smsc - ircc2 . c <nl> static void smsc_ircc_sir_wait_hw_transmitter_finish ( struct smsc_ircc_cb * self ) <nl> while ( count -- > 0 && !( inb ( iobase + UART_LSR ) & UART_LSR_TEMT )) <nl> udelay ( 1 ); <nl>  <nl> - if ( count == 0 ) <nl> + if ( count < 0 ) <nl> IRDA_DEBUG ( 0 , "% s (): stuck transmitter \ n ", __func__ ); <nl> } <nl> 
mmm net / ipv4 / netfilter / ipt_MASQUERADE . c <nl> ppp net / ipv4 / netfilter / ipt_MASQUERADE . c <nl> masquerade_target ( struct sk_buff ** pskb , <nl> IP_NF_ASSERT ( ct && ( ctinfo == IP_CT_NEW || ctinfo == IP_CT_RELATED <nl> || ctinfo == IP_CT_RELATED + IP_CT_IS_REPLY )); <nl>  <nl> + /* Source address is 0 . 0 . 0 . 0 - locally generated packet that is <nl> + * probably not supposed to be masqueraded . <nl> + */ <nl> + if ( ct -> tuplehash [ IP_CT_DIR_ORIGINAL ]. tuple . src . ip == 0 ) <nl> + return NF_ACCEPT ; <nl> + <nl> mr = targinfo ; <nl> rt = ( struct rtable *)(* pskb )-> dst ; <nl> newsrc = inet_select_addr ( out , rt -> rt_gateway , RT_SCOPE_UNIVERSE );
mmm fs / udf / super . c <nl> ppp fs / udf / super . c <nl> static int udf_load_logicalvol ( struct super_block * sb , sector_t block , <nl> struct genericPartitionMap * gpm ; <nl> uint16_t ident ; <nl> struct buffer_head * bh ; <nl> + unsigned int table_len ; <nl> int ret = 0 ; <nl>  <nl> bh = udf_read_tagged ( sb , block , block , & ident ); <nl> static int udf_load_logicalvol ( struct super_block * sb , sector_t block , <nl> return 1 ; <nl> BUG_ON ( ident != TAG_IDENT_LVD ); <nl> lvd = ( struct logicalVolDesc *) bh -> b_data ; <nl> + table_len = le32_to_cpu ( lvd -> mapTableLength ); <nl> + if ( sizeof (* lvd ) + table_len > sb -> s_blocksize ) { <nl> + udf_err ( sb , " error loading logical volume descriptor : " <nl> + " Partition table too long (% u > % lu )\ n ", table_len , <nl> + sb -> s_blocksize - sizeof (* lvd )); <nl> + goto out_bh ; <nl> + } <nl>  <nl> ret = udf_sb_alloc_partition_maps ( sb , le32_to_cpu ( lvd -> numPartitionMaps )); <nl> if ( ret ) <nl> goto out_bh ; <nl>  <nl> for ( i = 0 , offset = 0 ; <nl> - i < sbi -> s_partitions && offset < le32_to_cpu ( lvd -> mapTableLength ); <nl> + i < sbi -> s_partitions && offset < table_len ; <nl> i ++, offset += gpm -> partitionMapLength ) { <nl> struct udf_part_map * map = & sbi -> s_partmaps [ i ]; <nl> gpm = ( struct genericPartitionMap *)
mmm drivers / spi / omap2_mcspi . c <nl> ppp drivers / spi / omap2_mcspi . c <nl> omap2_mcspi_txrx_pio ( struct spi_device * spi , struct spi_transfer * xfer ) <nl> rx_reg = base + OMAP2_MCSPI_RX0 ; <nl> chstat_reg = base + OMAP2_MCSPI_CHSTAT0 ; <nl>  <nl> + if ( c < ( word_len >> 3 )) <nl> + return 0 ; <nl> + <nl> if ( word_len <= 8 ) { <nl> u8 * rx ; <nl> const u8 * tx ; <nl> omap2_mcspi_txrx_pio ( struct spi_device * spi , struct spi_transfer * xfer ) <nl> dev_vdbg (& spi -> dev , " read -% d % 02x \ n ", <nl> word_len , *( rx - 1 )); <nl> } <nl> - } while ( c ); <nl> + } while ( c > ( word_len >> 3 )); <nl> } else if ( word_len <= 16 ) { <nl> u16 * rx ; <nl> const u16 * tx ; <nl> omap2_mcspi_txrx_pio ( struct spi_device * spi , struct spi_transfer * xfer ) <nl> dev_vdbg (& spi -> dev , " read -% d % 04x \ n ", <nl> word_len , *( rx - 1 )); <nl> } <nl> - } while ( c ); <nl> + } while ( c > ( word_len >> 3 )); <nl> } else if ( word_len <= 32 ) { <nl> u32 * rx ; <nl> const u32 * tx ; <nl> omap2_mcspi_txrx_pio ( struct spi_device * spi , struct spi_transfer * xfer ) <nl> dev_vdbg (& spi -> dev , " read -% d % 08x \ n ", <nl> word_len , *( rx - 1 )); <nl> } <nl> - } while ( c ); <nl> + } while ( c > ( word_len >> 3 )); <nl> } <nl>  <nl> /* for TX_ONLY mode , be sure all words have shifted out */
mmm drivers / net / ethernet / intel / igb / igb_main . c <nl> ppp drivers / net / ethernet / intel / igb / igb_main . c <nl> void igb_update_stats ( struct igb_adapter * adapter , <nl> bytes = 0 ; <nl> packets = 0 ; <nl> for ( i = 0 ; i < adapter -> num_rx_queues ; i ++) { <nl> - u32 rqdpc_tmp = rd32 ( E1000_RQDPC ( i )) & 0x0FFF ; <nl> + u32 rqdpc = rd32 ( E1000_RQDPC ( i )); <nl> struct igb_ring * ring = adapter -> rx_ring [ i ]; <nl>  <nl> - ring -> rx_stats . drops += rqdpc_tmp ; <nl> - net_stats -> rx_fifo_errors += rqdpc_tmp ; <nl> + if ( rqdpc ) { <nl> + ring -> rx_stats . drops += rqdpc ; <nl> + net_stats -> rx_fifo_errors += rqdpc ; <nl> + } <nl>  <nl> do { <nl> start = u64_stats_fetch_begin_bh (& ring -> rx_syncp );
mmm sound / soc / fsl / imx - ssi . c <nl> ppp sound / soc / fsl / imx - ssi . c <nl> static int imx_ssi_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> ssi -> irq = platform_get_irq ( pdev , 0 ); <nl> + if ( ssi -> irq < 0 ) { <nl> + dev_err (& pdev -> dev , " Failed to get IRQ : % d \ n ", ssi -> irq ); <nl> + return ssi -> irq ; <nl> + } <nl>  <nl> ssi -> clk = devm_clk_get (& pdev -> dev , NULL ); <nl> if ( IS_ERR ( ssi -> clk )) {
mmm drivers / usb / serial / ftdi_sio . h <nl> ppp drivers / usb / serial / ftdi_sio . h <nl> # define ADI_VID 0x0456 <nl> # define ADI_GNICE_PID 0xF000 <nl>  <nl> +/* <nl> + * JETI SPECTROMETER SPECBOS 1201 <nl> + * http :// www . jeti . com / products / sys / scb / scb1201 . php <nl> + */ <nl> +# define JETI_VID 0x0c6c <nl> +# define JETI_SPC1201_PID 0x04b2 <nl> + <nl> /* <nl> * BmRequestType : 1100 0000b <nl> * bRequest : FTDI_E2_READmmm drivers / usb / serial / ftdi_sio . c <nl> ppp drivers / usb / serial / ftdi_sio . c <nl> # define ADI_VID 0x0456 <nl> # define ADI_GNICE_PID 0xF000 <nl>  <nl> +/* <nl> + * JETI SPECTROMETER SPECBOS 1201 <nl> + * http :// www . jeti . com / products / sys / scb / scb1201 . php <nl> + */ <nl> +# define JETI_VID 0x0c6c <nl> +# define JETI_SPC1201_PID 0x04b2 <nl> + <nl> /* <nl> * BmRequestType : 1100 0000b <nl> * bRequest : FTDI_E2_READ <nl> static struct usb_device_id id_table_combined [] = { <nl> { USB_DEVICE ( DE_VID , WHT_PID ) }, <nl> { USB_DEVICE ( ADI_VID , ADI_GNICE_PID ), <nl> . driver_info = ( kernel_ulong_t )& ftdi_jtag_quirk }, <nl> + { USB_DEVICE ( JETI_VID , JETI_SPC1201_PID ) }, <nl> { }, /* Optional parameter entry */ <nl> { } /* Terminating entry */ <nl> };
mmm mm / vmscan . c <nl> ppp mm / vmscan . c <nl> void unregister_shrinker ( struct shrinker * shrinker ) <nl> down_write (& shrinker_rwsem ); <nl> list_del (& shrinker -> list ); <nl> up_write (& shrinker_rwsem ); <nl> + kfree ( shrinker -> nr_deferred ); <nl> } <nl> EXPORT_SYMBOL ( unregister_shrinker ); <nl> 
mmm drivers / iommu / dmar . c <nl> ppp drivers / iommu / dmar . c <nl> int __init dmar_parse_dev_scope ( void * start , void * end , int * cnt , <nl> if ( scope -> entry_type == ACPI_DMAR_SCOPE_TYPE_ENDPOINT || <nl> scope -> entry_type == ACPI_DMAR_SCOPE_TYPE_BRIDGE ) <nl> (* cnt )++; <nl> - else if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC ) { <nl> + else if ( scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_IOAPIC && <nl> + scope -> entry_type != ACPI_DMAR_SCOPE_TYPE_HPET ) { <nl> pr_warn (" Unsupported device scope \ n "); <nl> } <nl> start += scope -> length ;
mmm net / sctp / input . c <nl> ppp net / sctp / input . c <nl> int sctp_rcv ( struct sk_buff * skb ) <nl> */ <nl> sctp_bh_lock_sock ( sk ); <nl>  <nl> + if ( sk != rcvr -> sk ) { <nl> + /* Our cached sk is different from the rcvr -> sk . This is <nl> + * because migrate ()/ accept () may have moved the association <nl> + * to a new socket and released all the sockets . So now we <nl> + * are holding a lock on the old socket while the user may <nl> + * be doing something with the new socket . Switch our veiw <nl> + * of the current sk . <nl> + */ <nl> + sctp_bh_unlock_sock ( sk ); <nl> + sk = rcvr -> sk ; <nl> + sctp_bh_lock_sock ( sk ); <nl> + } <nl> + <nl> if ( sock_owned_by_user ( sk )) { <nl> SCTP_INC_STATS_BH ( SCTP_MIB_IN_PKT_BACKLOG ); <nl> sctp_add_backlog ( sk , skb );
mmm drivers / gpu / drm / radeon / atombios_crtc . c <nl> ppp drivers / gpu / drm / radeon / atombios_crtc . c <nl> static u32 atombios_adjust_pll ( struct drm_crtc * crtc , <nl> if ( radeon_crtc -> ss . refdiv ) { <nl> radeon_crtc -> pll_flags |= RADEON_PLL_USE_REF_DIV ; <nl> radeon_crtc -> pll_reference_div = radeon_crtc -> ss . refdiv ; <nl> - if ( rdev -> family >= CHIP_RV770 ) <nl> + if ( ASIC_IS_AVIVO ( rdev ) && <nl> + rdev -> family != CHIP_RS780 && <nl> + rdev -> family != CHIP_RS880 ) <nl> radeon_crtc -> pll_flags |= RADEON_PLL_USE_FRAC_FB_DIV ; <nl> } <nl> }
mmm drivers / block / loop . c <nl> ppp drivers / block / loop . c <nl> static int lo_open ( struct block_device * bdev , fmode_t mode ) <nl> return err ; <nl> } <nl>  <nl> - static void lo_release ( struct gendisk * disk , fmode_t mode ) <nl> + static void __lo_release ( struct loop_device * lo ) <nl> { <nl> - struct loop_device * lo = disk -> private_data ; <nl> int err ; <nl>  <nl> if ( atomic_dec_return (& lo -> lo_refcnt )) <nl> static void lo_release ( struct gendisk * disk , fmode_t mode ) <nl> mutex_unlock (& lo -> lo_ctl_mutex ); <nl> } <nl>  <nl> + static void lo_release ( struct gendisk * disk , fmode_t mode ) <nl> +{ <nl> + mutex_lock (& loop_index_mutex ); <nl> + __lo_release ( disk -> private_data ); <nl> + mutex_unlock (& loop_index_mutex ); <nl> +} <nl> + <nl> static const struct block_device_operations lo_fops = { <nl> . owner = THIS_MODULE , <nl> . open = lo_open ,
mmm net / ipv6 / ip6_fib . c <nl> ppp net / ipv6 / ip6_fib . c <nl> int fib6_add ( struct fib6_node * root , struct rt6_info * rt , struct nl_info * info ) <nl> fn = fib6_add_1 ( root , & rt -> rt6i_dst . addr , rt -> rt6i_dst . plen , <nl> offsetof ( struct rt6_info , rt6i_dst ), allow_create , <nl> replace_required ); <nl> - <nl> if ( IS_ERR ( fn )) { <nl> err = PTR_ERR ( fn ); <nl> + fn = NULL ; <nl> goto out ; <nl> } <nl> 
mmm drivers / scsi / ufs / ufs - qcom . c <nl> ppp drivers / scsi / ufs / ufs - qcom . c <nl> static int ufs_qcom_pwr_change_notify ( struct ufs_hba * hba , <nl> return ret ; <nl> } <nl>  <nl> + static u32 ufs_qcom_get_ufs_hci_version ( struct ufs_hba * hba ) <nl> +{ <nl> + struct ufs_qcom_host * host = hba -> priv ; <nl> + <nl> + if ( host -> hw_ver . major == 0x1 ) <nl> + return UFSHCI_VERSION_11 ; <nl> + else <nl> + return UFSHCI_VERSION_20 ; <nl> +} <nl> + <nl> /** <nl> * ufs_qcom_advertise_quirks - advertise the known QCOM UFS controller quirks <nl> * @ hba : host controller instance <nl> static void ufs_qcom_advertise_quirks ( struct ufs_hba * hba ) <nl>  <nl> if ( host -> hw_ver . major >= 0x2 ) { <nl> hba -> quirks |= UFSHCD_QUIRK_BROKEN_LCC ; <nl> + hba -> quirks |= UFSHCD_QUIRK_BROKEN_UFS_HCI_VERSION ; <nl>  <nl> if (! ufs_qcom_cap_qunipro ( host )) <nl> /* Legacy UniPro mode still need following quirks */ <nl> static const struct ufs_hba_variant_ops ufs_hba_qcom_vops = { <nl> . name = " qcom ", <nl> . init = ufs_qcom_init , <nl> . exit = ufs_qcom_exit , <nl> + . get_ufs_hci_version = ufs_qcom_get_ufs_hci_version , <nl> . clk_scale_notify = ufs_qcom_clk_scale_notify , <nl> . setup_clocks = ufs_qcom_setup_clocks , <nl> . hce_enable_notify = ufs_qcom_hce_enable_notify ,
mmm drivers / gpu / drm / mediatek / mtk_drm_drv . c <nl> ppp drivers / gpu / drm / mediatek / mtk_drm_drv . c <nl> static void mtk_drm_unbind ( struct device * dev ) <nl> { <nl> struct mtk_drm_private * private = dev_get_drvdata ( dev ); <nl>  <nl> - drm_put_dev ( private -> drm ); <nl> + drm_dev_unregister ( private -> drm ); <nl> + drm_dev_unref ( private -> drm ); <nl> private -> drm = NULL ; <nl> } <nl> 
mmm drivers / scsi / libata - eh . c <nl> ppp drivers / scsi / libata - eh . c <nl> static int ata_eh_recover ( struct ata_port * ap , ata_prereset_fn_t prereset , <nl> down_xfermask = 0 ; <nl> rc = 0 ; <nl>  <nl> + /* if UNLOADING , finish immediately */ <nl> + if ( ap -> flags & ATA_FLAG_UNLOADING ) <nl> + goto out ; <nl> + <nl> /* skip EH if possible . */ <nl> if ( ata_eh_skip_recovery ( ap )) <nl> ehc -> i . action = 0 ;
mmm drivers / cdrom / cdrom . c <nl> ppp drivers / cdrom / cdrom . c <nl> static int dvd_read_manufact ( struct cdrom_device_info * cdi , dvd_struct * s , <nl> goto out ; <nl>  <nl> s -> manufact . len = buf [ 0 ] << 8 | buf [ 1 ]; <nl> - if ( s -> manufact . len < 0 || s -> manufact . len > 2048 ) { <nl> + if ( s -> manufact . len < 0 ) { <nl> cdinfo ( CD_WARNING , " Received invalid manufacture info length " <nl> " (% d )\ n ", s -> manufact . len ); <nl> ret = - EIO ; <nl> } else { <nl> + if ( s -> manufact . len > 2048 ) { <nl> + cdinfo ( CD_WARNING , " Received invalid manufacture info " <nl> + " length (% d ): truncating to 2048 \ n ", <nl> + s -> manufact . len ); <nl> + s -> manufact . len = 2048 ; <nl> + } <nl> memcpy ( s -> manufact . value , & buf [ 4 ], s -> manufact . len ); <nl> } <nl> 
mmm drivers / net / wireless / wl12xx / wl1271_main . c <nl> ppp drivers / net / wireless / wl12xx / wl1271_main . c <nl> static int wl1271_op_hw_scan ( struct ieee80211_hw * hw , <nl> static int wl1271_op_set_rts_threshold ( struct ieee80211_hw * hw , u32 value ) <nl> { <nl> struct wl1271 * wl = hw -> priv ; <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> mutex_lock (& wl -> mutex ); <nl>  <nl> + if ( unlikely ( wl -> state == WL1271_STATE_OFF )) <nl> + goto out ; <nl> + <nl> ret = wl1271_ps_elp_wakeup ( wl , false ); <nl> if ( ret < 0 ) <nl> goto out ;
mmm fs / inode . c <nl> ppp fs / inode . c <nl> int inode_init_always ( struct super_block * sb , struct inode * inode ) <nl> mapping -> a_ops = & empty_aops ; <nl> mapping -> host = inode ; <nl> mapping -> flags = 0 ; <nl> + mapping -> wb_err = 0 ; <nl> atomic_set (& mapping -> i_mmap_writable , 0 ); <nl> mapping_set_gfp_mask ( mapping , GFP_HIGHUSER_MOVABLE ); <nl> mapping -> private_data = NULL ;
mmm net / mac80211 / rx . c <nl> ppp net / mac80211 / rx . c <nl> ieee80211_deliver_skb ( struct ieee80211_rx_data * rx ) <nl> } <nl>  <nl> if ( xmit_skb ) { <nl> - /* send to wireless media */ <nl> + /* <nl> + * Send to wireless media and increase priority by 256 to <nl> + * keep the received priority instead of reclassifying <nl> + * the frame ( see cfg80211_classify8021d ). <nl> + */ <nl> + xmit_skb -> priority += 256 ; <nl> xmit_skb -> protocol = htons ( ETH_P_802_3 ); <nl> skb_reset_network_header ( xmit_skb ); <nl> skb_reset_mac_header ( xmit_skb );
mmm drivers / scsi / cxlflash / main . c <nl> ppp drivers / scsi / cxlflash / main . c <nl> static int start_afu ( struct cxlflash_cfg * cfg ) <nl>  <nl> init_pcr ( cfg ); <nl>  <nl> + /* After an AFU reset , RRQ entries are stale , clear them */ <nl> + memset (& afu -> rrq_entry , 0 , sizeof ( afu -> rrq_entry )); <nl> + <nl> /* Initialize RRQ pointers */ <nl> afu -> hrrq_start = & afu -> rrq_entry [ 0 ]; <nl> afu -> hrrq_end = & afu -> rrq_entry [ NUM_RRQ_ENTRY - 1 ];
mmm net / ipv6 / seg6_iptunnel . c <nl> ppp net / ipv6 / seg6_iptunnel . c <nl> static int seg6_input ( struct sk_buff * skb ) <nl> skb_dst_set ( skb , dst ); <nl> } <nl>  <nl> + err = skb_cow_head ( skb , LL_RESERVED_SPACE ( dst -> dev )); <nl> + if ( unlikely ( err )) <nl> + return err ; <nl> + <nl> return dst_input ( skb ); <nl> } <nl>  <nl> static int seg6_output ( struct net * net , struct sock * sk , struct sk_buff * skb ) <nl> skb_dst_drop ( skb ); <nl> skb_dst_set ( skb , dst ); <nl>  <nl> + err = skb_cow_head ( skb , LL_RESERVED_SPACE ( dst -> dev )); <nl> + if ( unlikely ( err )) <nl> + goto drop ; <nl> + <nl> return dst_output ( net , sk , skb ); <nl> drop : <nl> kfree_skb ( skb );
mmm block / blk - core . c <nl> ppp block / blk - core . c <nl> static int __end_that_request_first ( struct request * req , int error , <nl> } else { <nl> int idx = bio -> bi_idx + next_idx ; <nl>  <nl> - if ( unlikely ( bio -> bi_idx >= bio -> bi_vcnt )) { <nl> + if ( unlikely ( idx >= bio -> bi_vcnt )) { <nl> blk_dump_rq_flags ( req , " __end_that "); <nl> printk ( KERN_ERR "% s : bio idx % d >= vcnt % d \ n ", <nl> - __func__ , bio -> bi_idx , bio -> bi_vcnt ); <nl> + __func__ , idx , bio -> bi_vcnt ); <nl> break ; <nl> } <nl> 
mmm net / nfc / digital_core . c <nl> ppp net / nfc / digital_core . c <nl> static void digital_wq_cmd ( struct work_struct * work ) <nl> return ; <nl> } <nl>  <nl> + cmd -> pending = 1 ; <nl> + <nl> mutex_unlock (& ddev -> cmd_lock ); <nl>  <nl> if ( cmd -> req )
mmm net / core / dev . c <nl> ppp net / core / dev . c <nl> static struct sk_buff * validate_xmit_skb ( struct sk_buff * skb , struct net_device <nl>  <nl> segs = skb_gso_segment ( skb , features ); <nl> if ( IS_ERR ( segs )) { <nl> - segs = NULL ; <nl> + goto out_kfree_skb ; <nl> } else if ( segs ) { <nl> consume_skb ( skb ); <nl> skb = segs ;
mmm drivers / w1 / w1 . c <nl> ppp drivers / w1 / w1 . c <nl> static ssize_t w1_master_attribute_store_search ( struct device * dev , <nl> mutex_lock (& md -> mutex ); <nl> md -> search_count = tmp ; <nl> mutex_unlock (& md -> mutex ); <nl> - wake_up_process ( md -> thread ); <nl> + /* Only wake if it is going to be searching . */ <nl> + if ( tmp ) <nl> + wake_up_process ( md -> thread ); <nl>  <nl> return count ; <nl> }
mmm drivers / net / ethernet / broadcom / bnx2x / bnx2x_sriov . c <nl> ppp drivers / net / ethernet / broadcom / bnx2x / bnx2x_sriov . c <nl> void bnx2x_disable_sriov ( struct bnx2x * bp ) <nl> static int bnx2x_vf_ndo_sanity ( struct bnx2x * bp , int vfidx , <nl> struct bnx2x_virtf * vf ) <nl> { <nl> + if ( bp -> state != BNX2X_STATE_OPEN ) { <nl> + BNX2X_ERR (" vf ndo called though PF is down \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if (! IS_SRIOV ( bp )) { <nl> BNX2X_ERR (" vf ndo called though sriov is disabled \ n "); <nl> return - EINVAL ;
mmm kernel / time / timekeeping . c <nl> ppp kernel / time / timekeeping . c <nl> struct timekeeper { <nl> u32 mult ; <nl> }; <nl>  <nl> - struct timekeeper timekeeper ; <nl> + static struct timekeeper timekeeper ; <nl>  <nl> /** <nl> * timekeeper_setup_internals - Set up internals to use clocksource clock . <nl> static struct timespec total_sleep_time ; <nl> /* <nl> * The raw monotonic time for the CLOCK_MONOTONIC_RAW posix clock . <nl> */ <nl> - struct timespec raw_time ; <nl> + static struct timespec raw_time ; <nl>  <nl> /* flag for if timekeeping is suspended */ <nl> int __read_mostly timekeeping_suspended ;
mmm drivers / net / wireless / ath / ath9k / main . c <nl> ppp drivers / net / wireless / ath / ath9k / main . c <nl> static void ath9k_stop ( struct ieee80211_hw * hw ) <nl> ath9k_ps_restore ( sc ); <nl>  <nl> sc -> ps_idle = true ; <nl> + ath9k_set_wiphy_idle ( aphy , true ); <nl> ath_radio_disable ( sc , hw ); <nl>  <nl> sc -> sc_flags |= SC_OP_INVALID ;mmm drivers / net / wireless / ath / ath9k / pci . c <nl> ppp drivers / net / wireless / ath / ath9k / pci . c <nl> static void ath9k_stop ( struct ieee80211_hw * hw ) <nl> ath9k_ps_restore ( sc ); <nl>  <nl> sc -> ps_idle = true ; <nl> + ath9k_set_wiphy_idle ( aphy , true ); <nl> ath_radio_disable ( sc , hw ); <nl>  <nl> sc -> sc_flags |= SC_OP_INVALID ; <nl> static int ath_pci_resume ( struct device * device ) <nl> ath9k_hw_set_gpio ( sc -> sc_ah , sc -> sc_ah -> led_pin , 1 ); <nl>  <nl> sc -> ps_idle = true ; <nl> + ath9k_set_wiphy_idle ( aphy , true ); <nl> ath_radio_disable ( sc , hw ); <nl>  <nl> return 0 ;
mmm sound / soc / blackfin / bf5xx - i2s . c <nl> ppp sound / soc / blackfin / bf5xx - i2s . c <nl> static int bf5xx_i2s_hw_params ( struct snd_pcm_substream * substream , <nl> bf5xx_i2s -> tcr2 |= 7 ; <nl> bf5xx_i2s -> rcr2 |= 7 ; <nl> sport_handle -> wdsize = 1 ; <nl> + break ; <nl> case SNDRV_PCM_FORMAT_S16_LE : <nl> bf5xx_i2s -> tcr2 |= 15 ; <nl> bf5xx_i2s -> rcr2 |= 15 ;
mmm drivers / iio / industrialio - buffer . c <nl> ppp drivers / iio / industrialio - buffer . c <nl> int iio_sw_buffer_preenable ( struct iio_dev * indio_dev ) <nl> buffer -> scan_mask ); <nl> else <nl> indio_dev -> active_scan_mask = buffer -> scan_mask ; <nl> + <nl> + if ( indio_dev -> active_scan_mask == NULL ) <nl> + return - EINVAL ; <nl> + <nl> iio_update_demux ( indio_dev ); <nl>  <nl> if ( indio_dev -> info -> update_scan_mode )
mmm drivers / memory / omap - gpmc . c <nl> ppp drivers / memory / omap - gpmc . c <nl> static void gpmc_cs_show_timings ( int cs , const char * desc ) <nl> pr_info (" gpmc cs % i access configuration :\ n ", cs ); <nl> GPMC_GET_RAW_BOOL ( GPMC_CS_CONFIG1 , 4 , 4 , " time - para - granularity "); <nl> GPMC_GET_RAW ( GPMC_CS_CONFIG1 , 8 , 9 , " mux - add - data "); <nl> - GPMC_GET_RAW_MAX ( GPMC_CS_CONFIG1 , 12 , 13 , <nl> + GPMC_GET_RAW_SHIFT_MAX ( GPMC_CS_CONFIG1 , 12 , 13 , 1 , <nl> GPMC_CONFIG1_DEVICESIZE_MAX , " device - width "); <nl> GPMC_GET_RAW ( GPMC_CS_CONFIG1 , 16 , 17 , " wait - pin "); <nl> GPMC_GET_RAW_BOOL ( GPMC_CS_CONFIG1 , 21 , 21 , " wait - on - write ");
mmm drivers / usb / serial / pl2303 . c <nl> ppp drivers / usb / serial / pl2303 . c <nl> static int pl2303_tiocmset ( struct tty_struct * tty , <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl>  <nl> mutex_lock (& serial -> disc_mutex ); <nl> - if (! serial -> disconnected ) <nl> + if (! serial -> disconnected ) { <nl> ret = pl2303_set_control_lines ( port , control ); <nl> - else <nl> + if ( ret ) <nl> + ret = usb_translate_errors ( ret ); <nl> + } else { <nl> ret = - ENODEV ; <nl> + } <nl> mutex_unlock (& serial -> disc_mutex ); <nl>  <nl> return ret ;
mmm sound / pci / hda / hda_codec . c <nl> ppp sound / pci / hda / hda_codec . c <nl> void snd_hda_codec_setup_stream ( struct hda_codec * codec , hda_nid_t nid , <nl> " NID = 0x % x , stream = 0x % x , channel =% d , format = 0x % x \ n ", <nl> nid , stream_tag , channel_id , format ); <nl> p = get_hda_cvt_setup ( codec , nid ); <nl> - if (! p ) <nl> + if (! p || p -> active ) <nl> return ; <nl>  <nl> if ( codec -> pcm_format_first ) <nl> void __snd_hda_codec_cleanup_stream ( struct hda_codec * codec , hda_nid_t nid , <nl>  <nl> snd_printdd (" hda_codec_cleanup_stream : NID = 0x % x \ n ", nid ); <nl> p = get_hda_cvt_setup ( codec , nid ); <nl> - if ( p ) { <nl> + if ( p && p -> active ) { <nl> /* here we just clear the active flag when do_now isn ' t set ; <nl> * actual clean - ups will be done later in <nl> * purify_inactive_streams () called from snd_hda_codec_prpapre ()
mmm drivers / md / raid1 . c <nl> ppp drivers / md / raid1 . c <nl> static int raid1_remove_disk ( struct mddev * mddev , struct md_rdev * rdev ) <nl> int number = rdev -> raid_disk ; <nl> struct mirror_info * p = conf -> mirrors + number ; <nl>  <nl> + if ( rdev != p -> rdev ) <nl> + p = conf -> mirrors + conf -> raid_disks + number ; <nl> + <nl> print_conf ( conf ); <nl> if ( rdev == p -> rdev ) { <nl> if ( test_bit ( In_sync , & rdev -> flags ) || <nl> static int raid1_remove_disk ( struct mddev * mddev , struct md_rdev * rdev ) <nl> err = - EBUSY ; <nl> p -> rdev = rdev ; <nl> goto abort ; <nl> + } else { <nl> + clear_bit ( Replacement , & rdev -> flags ); <nl> + clear_bit ( WantReplacement , & rdev -> flags ); <nl> } <nl> err = md_integrity_register ( mddev ); <nl> }
mmm sound / drivers / pcsp / pcsp_input . c <nl> ppp sound / drivers / pcsp / pcsp_input . c <nl> static void pcspkr_do_sound ( unsigned int count ) <nl> spin_lock_irqsave (& i8253_lock , flags ); <nl>  <nl> if ( count ) { <nl> - /* enable counter 2 */ <nl> - outb_p ( inb_p ( 0x61 ) | 3 , 0x61 ); <nl> /* set command for counter 2 , 2 byte write */ <nl> outb_p ( 0xB6 , 0x43 ); <nl> /* select desired HZ */ <nl> outb_p ( count & 0xff , 0x42 ); <nl> outb (( count >> 8 ) & 0xff , 0x42 ); <nl> + /* enable counter 2 */ <nl> + outb_p ( inb_p ( 0x61 ) | 3 , 0x61 ); <nl> } else { <nl> /* disable counter 2 */ <nl> outb ( inb_p ( 0x61 ) & 0xFC , 0x61 );
mmm drivers / net / wireless / ath9k / main . c <nl> ppp drivers / net / wireless / ath9k / main . c <nl> int ath_descdma_setup ( struct ath_softc * sc , struct ath_descdma * dd , <nl> DPRINTF ( sc , ATH_DBG_CONFIG , "% s DMA : % u buffers % u desc / buf \ n ", <nl> name , nbuf , ndesc ); <nl>  <nl> + INIT_LIST_HEAD ( head ); <nl> /* ath_desc must be a multiple of DWORDs */ <nl> if (( sizeof ( struct ath_desc ) % 4 ) != 0 ) { <nl> DPRINTF ( sc , ATH_DBG_FATAL , " ath_desc not DWORD aligned \ n "); <nl> int ath_descdma_setup ( struct ath_softc * sc , struct ath_descdma * dd , <nl> } <nl> dd -> dd_bufptr = bf ; <nl>  <nl> - INIT_LIST_HEAD ( head ); <nl> for ( i = 0 ; i < nbuf ; i ++, bf ++, ds += ndesc ) { <nl> bf -> bf_desc = ds ; <nl> bf -> bf_daddr = DS2PHYS ( dd , ds );
mmm net / ipv4 / ping . c <nl> ppp net / ipv4 / ping . c <nl> int ping_init_sock ( struct sock * sk ) <nl> { <nl> struct net * net = sock_net ( sk ); <nl> kgid_t group = current_egid (); <nl> - struct group_info * group_info = get_current_groups (); <nl> - int i , j , count = group_info -> ngroups ; <nl> + struct group_info * group_info ; <nl> + int i , j , count ; <nl> kgid_t low , high ; <nl> + int ret = 0 ; <nl>  <nl> inet_get_ping_group_range_net ( net , & low , & high ); <nl> if ( gid_lte ( low , group ) && gid_lte ( group , high )) <nl> return 0 ; <nl>  <nl> + group_info = get_current_groups (); <nl> + count = group_info -> ngroups ; <nl> for ( i = 0 ; i < group_info -> nblocks ; i ++) { <nl> int cp_count = min_t ( int , NGROUPS_PER_BLOCK , count ); <nl> for ( j = 0 ; j < cp_count ; j ++) { <nl> kgid_t gid = group_info -> blocks [ i ][ j ]; <nl> if ( gid_lte ( low , gid ) && gid_lte ( gid , high )) <nl> - return 0 ; <nl> + goto out_release_group ; <nl> } <nl>  <nl> count -= cp_count ; <nl> } <nl>  <nl> - return - EACCES ; <nl> + ret = - EACCES ; <nl> + <nl> + out_release_group : <nl> + put_group_info ( group_info ); <nl> + return ret ; <nl> } <nl> EXPORT_SYMBOL_GPL ( ping_init_sock ); <nl> 
mmm drivers / video / via / via_i2c . h <nl> ppp drivers / video / via / via_i2c . h <nl>  <nl> struct via_i2c_stuff { <nl> u16 i2c_port ; /* GPIO or I2C port */ <nl> + u16 is_active ; /* Being used as I2C ? */ <nl> struct i2c_adapter adapter ; <nl> struct i2c_algo_bit_data algo ; <nl> };mmm drivers / video / via / via_i2c . c <nl> ppp drivers / video / via / via_i2c . c <nl>  <nl> struct via_i2c_stuff { <nl> u16 i2c_port ; /* GPIO or I2C port */ <nl> + u16 is_active ; /* Being used as I2C ? */ <nl> struct i2c_adapter adapter ; <nl> struct i2c_algo_bit_data algo ; <nl> }; <nl> int viafb_i2c_readbyte ( u8 adap , u8 slave_addr , u8 index , u8 * pdata ) <nl> u8 mm1 [] = { 0x00 }; <nl> struct i2c_msg msgs [ 2 ]; <nl>  <nl> + if (! via_i2c_par [ adap ]. is_active ) <nl> + return - ENODEV ; <nl> * pdata = 0 ; <nl> msgs [ 0 ]. flags = 0 ; <nl> msgs [ 1 ]. flags = I2C_M_RD ; <nl> int viafb_i2c_writebyte ( u8 adap , u8 slave_addr , u8 index , u8 data ) <nl> u8 msg [ 2 ] = { index , data }; <nl> struct i2c_msg msgs ; <nl>  <nl> + if (! via_i2c_par [ adap ]. is_active ) <nl> + return - ENODEV ; <nl> msgs . flags = 0 ; <nl> msgs . addr = slave_addr / 2 ; <nl> msgs . len = 2 ; <nl> int viafb_i2c_readbytes ( u8 adap , u8 slave_addr , u8 index , u8 * buff , int buff_len <nl> u8 mm1 [] = { 0x00 }; <nl> struct i2c_msg msgs [ 2 ]; <nl>  <nl> + if (! via_i2c_par [ adap ]. is_active ) <nl> + return - ENODEV ; <nl> msgs [ 0 ]. flags = 0 ; <nl> msgs [ 1 ]. flags = I2C_M_RD ; <nl> msgs [ 0 ]. addr = msgs [ 1 ]. addr = slave_addr / 2 ; <nl> static int viafb_i2c_probe ( struct platform_device * platdev ) <nl> struct via_port_cfg * adap_cfg = configs ++; <nl> struct via_i2c_stuff * i2c_stuff = & via_i2c_par [ i ]; <nl>  <nl> + i2c_stuff -> is_active = 0 ; <nl> if ( adap_cfg -> type == 0 || adap_cfg -> mode != VIA_MODE_I2C ) <nl> continue ; <nl> - <nl> ret = create_i2c_bus (& i2c_stuff -> adapter , <nl> & i2c_stuff -> algo , adap_cfg , <nl> NULL ); /* FIXME : PCIDEV */ <nl> if ( ret < 0 ) { <nl> printk ( KERN_ERR " viafb : cannot create i2c bus % u :% d \ n ", <nl> i , ret ); <nl> - /* FIXME : properly release previous busses */ <nl> - return ret ; <nl> + continue ; /* Still try to make the rest */ <nl> } <nl> + i2c_stuff -> is_active = 1 ; <nl> } <nl>  <nl> return 0 ; <nl> static int viafb_i2c_remove ( struct platform_device * platdev ) <nl> * Only remove those entries in the array that we ' ve <nl> * actually used ( and thus initialized algo_data ) <nl> */ <nl> - if ( i2c_stuff -> adapter . algo_data == & i2c_stuff -> algo ) <nl> + if ( i2c_stuff -> is_active ) <nl> i2c_del_adapter (& i2c_stuff -> adapter ); <nl> } <nl> return 0 ;
mmm drivers / staging / greybus / audio_codec . c <nl> ppp drivers / staging / greybus / audio_codec . c <nl> static int gbcodec_trigger ( struct snd_pcm_substream * substream , int cmd , <nl> dev_err ( dai -> dev , "% d : Error during % s stream \ n ", ret , <nl> start ? " Start " : " Stop "); <nl>  <nl> + /* in case device removed , return 0 for stop trigger */ <nl> + if ( stop && ( ret == - ENODEV )) <nl> + ret = 0 ; <nl> + <nl> func_exit : <nl> mutex_unlock (& gb -> lock ); <nl> return ret ;
mmm drivers / gpu / drm / amd / amdgpu / amdgpu_device . c <nl> ppp drivers / gpu / drm / amd / amdgpu / amdgpu_device . c <nl> int amdgpu_device_ip_suspend ( struct amdgpu_device * adev ) <nl> if ( amdgpu_sriov_vf ( adev )) <nl> amdgpu_virt_request_full_gpu ( adev , false ); <nl>  <nl> + /* ungate SMC block powergating */ <nl> + if ( adev -> powerplay . pp_feature & PP_GFXOFF_MASK ) <nl> + amdgpu_device_ip_set_powergating_state ( adev , <nl> + AMD_IP_BLOCK_TYPE_SMC , <nl> + AMD_CG_STATE_UNGATE ); <nl> + <nl> /* ungate SMC block first */ <nl> r = amdgpu_device_ip_set_clockgating_state ( adev , AMD_IP_BLOCK_TYPE_SMC , <nl> AMD_CG_STATE_UNGATE );
mmm fs / btrfs / super . c <nl> ppp fs / btrfs / super . c <nl> static struct dentry * get_default_root ( struct super_block * sb , <nl> */ <nl> dir_id = btrfs_super_root_dir (& root -> fs_info -> super_copy ); <nl> di = btrfs_lookup_dir_item ( NULL , root , path , dir_id , " default ", 7 , 0 ); <nl> - if ( IS_ERR ( di )) <nl> + if ( IS_ERR ( di )) { <nl> + btrfs_free_path ( path ); <nl> return ERR_CAST ( di ); <nl> + } <nl> if (! di ) { <nl> /* <nl> * Ok the default dir item isn ' t there . This is weird sincemmm fs / btrfs / inode . c <nl> ppp fs / btrfs / inode . c <nl> static struct dentry * get_default_root ( struct super_block * sb , <nl> */ <nl> dir_id = btrfs_super_root_dir (& root -> fs_info -> super_copy ); <nl> di = btrfs_lookup_dir_item ( NULL , root , path , dir_id , " default ", 7 , 0 ); <nl> - if ( IS_ERR ( di )) <nl> + if ( IS_ERR ( di )) { <nl> + btrfs_free_path ( path ); <nl> return ERR_CAST ( di ); <nl> + } <nl> if (! di ) { <nl> /* <nl> * Ok the default dir item isn ' t there . This is weird since <nl> static int btrfs_symlink ( struct inode * dir , struct dentry * dentry , <nl> datasize ); <nl> if ( err ) { <nl> drop_inode = 1 ; <nl> + btrfs_free_path ( path ); <nl> goto out_unlock ; <nl> } <nl> leaf = path -> nodes [ 0 ];
mmm kernel / sched_fair . c <nl> ppp kernel / sched_fair . c <nl> static void task_fork_fair ( struct task_struct * p ) <nl>  <nl> update_rq_clock ( rq ); <nl>  <nl> - if ( unlikely ( task_cpu ( p ) != this_cpu )) <nl> + if ( unlikely ( task_cpu ( p ) != this_cpu )) { <nl> + rcu_read_lock (); <nl> __set_task_cpu ( p , this_cpu ); <nl> + rcu_read_unlock (); <nl> + } <nl>  <nl> update_curr ( cfs_rq ); <nl> 
mmm kernel / auditsc . c <nl> ppp kernel / auditsc . c <nl> static int audit_log_single_execve_arg ( struct audit_context * context , <nl> * for strings that are too long , we should not have created <nl> * any . <nl> */ <nl> - if ( unlikely (( len = - 1 ) || len > MAX_ARG_STRLEN - 1 )) { <nl> + if ( unlikely (( len == - 1 ) || len > MAX_ARG_STRLEN - 1 )) { <nl> WARN_ON ( 1 ); <nl> send_sig ( SIGKILL , current , 0 ); <nl> + return - 1 ; <nl> } <nl>  <nl> /* walk the whole argument looking for non - ascii chars */ <nl> static int audit_log_single_execve_arg ( struct audit_context * context , <nl> if ( ret ) { <nl> WARN_ON ( 1 ); <nl> send_sig ( SIGKILL , current , 0 ); <nl> + return - 1 ; <nl> } <nl> buf [ to_send ] = '\ 0 '; <nl> has_cntl = audit_string_contains_control ( buf , to_send ); <nl> static int audit_log_single_execve_arg ( struct audit_context * context , <nl> if ( ret ) { <nl> WARN_ON ( 1 ); <nl> send_sig ( SIGKILL , current , 0 ); <nl> + return - 1 ; <nl> } <nl> buf [ to_send ] = '\ 0 '; <nl> 
mmm include / linux / rcupdate . h <nl> ppp include / linux / rcupdate . h <nl> extern struct debug_obj_descr rcuhead_debug_descr ; <nl>  <nl> static inline void debug_rcu_head_queue ( struct rcu_head * head ) <nl> { <nl> + WARN_ON_ONCE (( unsigned long ) head & 0x3 ); <nl> debug_object_activate ( head , & rcuhead_debug_descr ); <nl> debug_object_active_state ( head , & rcuhead_debug_descr , <nl> STATE_RCU_HEAD_READY ,
mmm drivers / acpi / acpica / exstorob . c <nl> ppp drivers / acpi / acpica / exstorob . c <nl> acpi_ex_store_buffer_to_buffer ( union acpi_operand_object * source_desc , <nl>  <nl> ACPI_FUNCTION_TRACE_PTR ( ex_store_buffer_to_buffer , source_desc ); <nl>  <nl> + /* If Source and Target are the same , just return */ <nl> + <nl> + if ( source_desc == target_desc ) { <nl> + return_ACPI_STATUS ( AE_OK ); <nl> + } <nl> + <nl> /* We know that source_desc is a buffer by now */ <nl>  <nl> buffer = ACPI_CAST_PTR ( u8 , source_desc -> buffer . pointer ); <nl> acpi_ex_store_string_to_string ( union acpi_operand_object * source_desc , <nl>  <nl> ACPI_FUNCTION_TRACE_PTR ( ex_store_string_to_string , source_desc ); <nl>  <nl> + /* If Source and Target are the same , just return */ <nl> + <nl> + if ( source_desc == target_desc ) { <nl> + return_ACPI_STATUS ( AE_OK ); <nl> + } <nl> + <nl> /* We know that source_desc is a string by now */ <nl>  <nl> buffer = ACPI_CAST_PTR ( u8 , source_desc -> string . pointer );
mmm tools / perf / util / callchain . c <nl> ppp tools / perf / util / callchain . c <nl> __append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> void append_chain ( struct callchain_node * root , struct ip_callchain * chain , <nl> struct symbol ** syms ) <nl> { <nl> + if (! chain -> nr ) <nl> + return ; <nl> __append_chain_children ( root , chain , syms , 0 ); <nl> }
mmm tools / perf / util / event . c <nl> ppp tools / perf / util / event . c <nl> int perf_event__synthesize_thread_map ( struct perf_tool * tool , <nl> if ( comm_event == NULL ) <nl> goto out ; <nl>  <nl> - mmap_event = malloc ( sizeof ( mmap_event -> mmap ) + machine -> id_hdr_size ); <nl> + mmap_event = malloc ( sizeof ( mmap_event -> mmap2 ) + machine -> id_hdr_size ); <nl> if ( mmap_event == NULL ) <nl> goto out_free_comm ; <nl>  <nl> int perf_event__synthesize_threads ( struct perf_tool * tool , <nl> if ( comm_event == NULL ) <nl> goto out ; <nl>  <nl> - mmap_event = malloc ( sizeof ( mmap_event -> mmap ) + machine -> id_hdr_size ); <nl> + mmap_event = malloc ( sizeof ( mmap_event -> mmap2 ) + machine -> id_hdr_size ); <nl> if ( mmap_event == NULL ) <nl> goto out_free_comm ; <nl> 
mmm drivers / net / wireless / mediatek / mt76 / dma . c <nl> ppp drivers / net / wireless / mediatek / mt76 / dma . c <nl> mt76_add_fragment ( struct mt76_dev * dev , struct mt76_queue * q , void * data , <nl> struct page * page = virt_to_head_page ( data ); <nl> int offset = data - page_address ( page ); <nl> struct sk_buff * skb = q -> rx_head ; <nl> + struct skb_shared_info * shinfo = skb_shinfo ( skb ); <nl>  <nl> - offset += q -> buf_offset ; <nl> - skb_add_rx_frag ( skb , skb_shinfo ( skb )-> nr_frags , page , offset , len , <nl> - q -> buf_size ); <nl> + if ( shinfo -> nr_frags < ARRAY_SIZE ( shinfo -> frags )) { <nl> + offset += q -> buf_offset ; <nl> + skb_add_rx_frag ( skb , shinfo -> nr_frags , page , offset , len , <nl> + q -> buf_size ); <nl> + } <nl>  <nl> if ( more ) <nl> return ;
mmm arch / arm / mach - omap2 / opp . c <nl> ppp arch / arm / mach - omap2 / opp . c <nl> int __init omap_init_opp_table ( struct omap_opp_def * opp_def , <nl> omap_table_init = 1 ; <nl>  <nl> /* Lets now register with OPP library */ <nl> - for ( i = 0 ; i < opp_def_size ; i ++) { <nl> + for ( i = 0 ; i < opp_def_size ; i ++, opp_def ++) { <nl> struct omap_hwmod * oh ; <nl> struct device * dev ; <nl>  <nl> int __init omap_init_opp_table ( struct omap_opp_def * opp_def , <nl> __func__ , opp_def -> freq , <nl> opp_def -> hwmod_name , i , r ); <nl> } <nl> - opp_def ++; <nl> } <nl>  <nl> return 0 ;
mmm arch / sparc / kernel / devices . c <nl> ppp arch / sparc / kernel / devices . c <nl> static int __cpu_find_by ( int (* compare )( int , int , void *), void * compare_arg , <nl> int err = check_cpu_node ( dp -> node , & cur_inst , <nl> compare , compare_arg , <nl> prom_node , mid ); <nl> - if (! err ) <nl> + if (! err ) { <nl> + of_node_put ( dp ); <nl> return 0 ; <nl> + } <nl> } <nl>  <nl> return - ENODEV ;
mmm fs / btrfs / ctree . c <nl> ppp fs / btrfs / ctree . c <nl> __tree_mod_log_free_eb ( struct btrfs_fs_info * fs_info , struct extent_buffer * eb ) <nl> u32 nritems ; <nl> int ret ; <nl>  <nl> + if ( btrfs_header_level ( eb ) == 0 ) <nl> + return ; <nl> + <nl> nritems = btrfs_header_nritems ( eb ); <nl> for ( i = nritems - 1 ; i >= 0 ; i --) { <nl> ret = tree_mod_log_insert_key_locked ( fs_info , eb , i ,
mmm fs / btrfs / xattr . c <nl> ppp fs / btrfs / xattr . c <nl> ssize_t btrfs_listxattr ( struct dentry * dentry , char * buffer , size_t size ) <nl>  <nl> if (! buffer || ( name_len + 1 ) > size_left ) { <nl> ret = - ERANGE ; <nl> - break ; <nl> + goto err ; <nl> } <nl>  <nl> name_ptr = ( unsigned long )( di + 1 );
mmm net / tls / tls_sw . c <nl> ppp net / tls / tls_sw . c <nl> # include < net / strparser . h > <nl> # include < net / tls . h > <nl>  <nl> +# define MAX_IV_SIZE TLS_CIPHER_AES_GCM_128_IV_SIZE <nl> + <nl> static int tls_do_decryption ( struct sock * sk , <nl> struct scatterlist * sgin , <nl> struct scatterlist * sgout , <nl> static int decrypt_skb ( struct sock * sk , struct sk_buff * skb , <nl> { <nl> struct tls_context * tls_ctx = tls_get_ctx ( sk ); <nl> struct tls_sw_context * ctx = tls_sw_ctx ( tls_ctx ); <nl> - char iv [ TLS_CIPHER_AES_GCM_128_SALT_SIZE + tls_ctx -> rx . iv_size ]; <nl> + char iv [ TLS_CIPHER_AES_GCM_128_SALT_SIZE + MAX_IV_SIZE ]; <nl> struct scatterlist sgin_arr [ MAX_SKB_FRAGS + 2 ]; <nl> struct scatterlist * sgin = & sgin_arr [ 0 ]; <nl> struct strp_msg * rxm = strp_msg ( skb ); <nl> int tls_set_sw_offload ( struct sock * sk , struct tls_context * ctx , int tx ) <nl> goto free_priv ; <nl> } <nl>  <nl> + /* Sanity - check the IV size for stack allocations . */ <nl> + if ( iv_size > MAX_IV_SIZE ) { <nl> + rc = - EINVAL ; <nl> + goto free_priv ; <nl> + } <nl> + <nl> cctx -> prepend_size = TLS_HEADER_SIZE + nonce_size ; <nl> cctx -> tag_size = tag_size ; <nl> cctx -> overhead_size = cctx -> prepend_size + cctx -> tag_size ;
mmm drivers / staging / comedi / drivers / usbduxfast . c <nl> ppp drivers / staging / comedi / drivers / usbduxfast . c <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl> { <nl> int i = 0 ; <nl> unsigned char * fp = ( char *) firmwarePtr ; <nl> - unsigned char * firmwareBinary = NULL ; <nl> + unsigned char * firmwareBinary ; <nl> int res = 0 ; <nl> int maxAddr = 0 ; <nl>  <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl> j ++; <nl> if ( j >= sizeof ( buf )) { <nl> printk (" comedi_ : usbduxfast : bogus firmware file !\ n "); <nl> + kfree ( firmwareBinary ); <nl> return - 1 ; <nl> } <nl> } <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl>  <nl> if ( buf [ 0 ] != ':') { <nl> printk (" comedi_ : usbduxfast : upload : not an ihex record : % s ", buf ); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl>  <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl>  <nl> if ( maxAddr >= FIRMWARE_MAX_LEN ) { <nl> printk (" comedi_ : usbduxfast : firmware upload goes beyond FX2 RAM boundaries ."); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl> // printk (" comedi_ : usbduxfast : off =% x , len =% x :", off , len ); <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl>  <nl> if ( type != 0 ) { <nl> printk (" comedi_ : usbduxfast : unsupported record type : % u \ n ", type ); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl> mmm drivers / staging / comedi / drivers / usbdux . c <nl> ppp drivers / staging / comedi / drivers / usbdux . c <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl> { <nl> int i = 0 ; <nl> unsigned char * fp = ( char *) firmwarePtr ; <nl> - unsigned char * firmwareBinary = NULL ; <nl> + unsigned char * firmwareBinary ; <nl> int res = 0 ; <nl> int maxAddr = 0 ; <nl>  <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl> j ++; <nl> if ( j >= sizeof ( buf )) { <nl> printk (" comedi_ : usbduxfast : bogus firmware file !\ n "); <nl> + kfree ( firmwareBinary ); <nl> return - 1 ; <nl> } <nl> } <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl>  <nl> if ( buf [ 0 ] != ':') { <nl> printk (" comedi_ : usbduxfast : upload : not an ihex record : % s ", buf ); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl>  <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl>  <nl> if ( maxAddr >= FIRMWARE_MAX_LEN ) { <nl> printk (" comedi_ : usbduxfast : firmware upload goes beyond FX2 RAM boundaries ."); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl> // printk (" comedi_ : usbduxfast : off =% x , len =% x :", off , len ); <nl> static int read_firmware ( usbduxfastsub_t * usbduxfastsub , void * firmwarePtr , <nl>  <nl> if ( type != 0 ) { <nl> printk (" comedi_ : usbduxfast : unsupported record type : % u \ n ", type ); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl>  <nl> static int read_firmware ( struct usbduxsub * usbduxsub , void * firmwarePtr , <nl> struct device * dev = & usbduxsub -> interface -> dev ; <nl> int i = 0 ; <nl> unsigned char * fp = ( char *) firmwarePtr ; <nl> - unsigned char * firmwareBinary = NULL ; <nl> + unsigned char * firmwareBinary ; <nl> int res = 0 ; <nl> int maxAddr = 0 ; <nl>  <nl> static int read_firmware ( struct usbduxsub * usbduxsub , void * firmwarePtr , <nl> j ++; <nl> if ( j >= sizeof ( buf )) { <nl> dev_err ( dev , " comedi_ : bogus firmware file !\ n "); <nl> + kfree ( firmwareBinary ); <nl> return - 1 ; <nl> } <nl> } <nl> static int read_firmware ( struct usbduxsub * usbduxsub , void * firmwarePtr , <nl> if ( buf [ 0 ] != ':') { <nl> dev_err ( dev , " comedi_ : upload : not an ihex record : % s ", <nl> buf ); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl>  <nl> static int read_firmware ( struct usbduxsub * usbduxsub , void * firmwarePtr , <nl> if ( maxAddr >= FIRMWARE_MAX_LEN ) { <nl> dev_err ( dev , " comedi_ : firmware upload goes " <nl> " beyond FX2 RAM boundaries .\ n "); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl> /* dev_dbg ( dev , " comedi_ : off =% x , len =% x :\ n ", off , len ); */ <nl> static int read_firmware ( struct usbduxsub * usbduxsub , void * firmwarePtr , <nl> if ( type != 0 ) { <nl> dev_err ( dev , " comedi_ : unsupported record type : % u \ n ", <nl> type ); <nl> + kfree ( firmwareBinary ); <nl> return - EFAULT ; <nl> } <nl> 
mmm drivers / s390 / cio / device . c <nl> ppp drivers / s390 / cio / device . c <nl> static void __ccw_device_pm_restore ( struct ccw_device * cdev ) <nl> * available again . Kick re - detection . <nl> */ <nl> cdev -> private -> flags . resuming = 1 ; <nl> + cdev -> private -> path_new_mask = LPM_ANYPATH ; <nl> css_schedule_eval ( sch -> schid ); <nl> spin_unlock_irq ( sch -> lock ); <nl> css_complete_work ();
mmm drivers / mmc / host / atmel - mci . c <nl> ppp drivers / mmc / host / atmel - mci . c <nl> static int atmci_regs_show ( struct seq_file * s , void * v ) <nl> atmci_show_status_reg ( s , " SR ", buf [ MCI_SR / 4 ]); <nl> atmci_show_status_reg ( s , " IMR ", buf [ MCI_IMR / 4 ]); <nl>  <nl> + kfree ( buf ); <nl> + <nl> return 0 ; <nl> } <nl> 
mmm drivers / media / i2c / soc_camera / ov5642 . c <nl> ppp drivers / media / i2c / soc_camera / ov5642 . c <nl> static int ov5642_set_fmt ( struct v4l2_subdev * sd , <nl> mf -> field = V4L2_FIELD_NONE ; <nl>  <nl> if ( format -> which == V4L2_SUBDEV_FORMAT_ACTIVE ) <nl> - priv -> fmt = ov5642_find_datafmt ( mf -> code ); <nl> + priv -> fmt = fmt ; <nl> else <nl> cfg -> try_fmt = * mf ; <nl> return 0 ;
mmm drivers / tty / synclinkmp . c <nl> ppp drivers / tty / synclinkmp . c <nl> static int hdlcdev_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> HDLC_FLAG_TXC_TXCPIN | HDLC_FLAG_TXC_DPLL | <nl> HDLC_FLAG_TXC_BRG | HDLC_FLAG_TXC_RXCPIN ); <nl>  <nl> + memset (& new_line , 0 , sizeof ( new_line )); <nl> switch ( flags ){ <nl> case ( HDLC_FLAG_RXC_RXCPIN | HDLC_FLAG_TXC_TXCPIN ): new_line . clock_type = CLOCK_EXT ; break ; <nl> case ( HDLC_FLAG_RXC_BRG | HDLC_FLAG_TXC_BRG ): new_line . clock_type = CLOCK_INT ; break ;mmm drivers / tty / synclink . c <nl> ppp drivers / tty / synclink . c <nl> static int hdlcdev_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> HDLC_FLAG_TXC_TXCPIN | HDLC_FLAG_TXC_DPLL | <nl> HDLC_FLAG_TXC_BRG | HDLC_FLAG_TXC_RXCPIN ); <nl>  <nl> + memset (& new_line , 0 , sizeof ( new_line )); <nl> switch ( flags ){ <nl> case ( HDLC_FLAG_RXC_RXCPIN | HDLC_FLAG_TXC_TXCPIN ): new_line . clock_type = CLOCK_EXT ; break ; <nl> case ( HDLC_FLAG_RXC_BRG | HDLC_FLAG_TXC_BRG ): new_line . clock_type = CLOCK_INT ; break ; <nl> static int hdlcdev_ioctl ( struct net_device * dev , struct ifreq * ifr , int cmd ) <nl> HDLC_FLAG_TXC_TXCPIN | HDLC_FLAG_TXC_DPLL | <nl> HDLC_FLAG_TXC_BRG | HDLC_FLAG_TXC_RXCPIN ); <nl>  <nl> + memset (& new_line , 0 , sizeof ( new_line )); <nl> switch ( flags ){ <nl> case ( HDLC_FLAG_RXC_RXCPIN | HDLC_FLAG_TXC_TXCPIN ): new_line . clock_type = CLOCK_EXT ; break ; <nl> case ( HDLC_FLAG_RXC_BRG | HDLC_FLAG_TXC_BRG ): new_line . clock_type = CLOCK_INT ; break ;
mmm net / bluetooth / l2cap_core . c <nl> ppp net / bluetooth / l2cap_core . c <nl> static int l2cap_parse_conf_req ( struct l2cap_chan * chan , void * data , size_t data <nl> l2cap_add_conf_opt (& ptr , L2CAP_CONF_RFC , <nl> sizeof ( rfc ), ( unsigned long ) & rfc , endptr - ptr ); <nl>  <nl> - if ( test_bit ( FLAG_EFS_ENABLE , & chan -> flags )) { <nl> + if ( remote_efs && <nl> + test_bit ( FLAG_EFS_ENABLE , & chan -> flags )) { <nl> chan -> remote_id = efs . id ; <nl> chan -> remote_stype = efs . stype ; <nl> chan -> remote_msdu = le16_to_cpu ( efs . msdu );
mmm net / batman - adv / soft - interface . c <nl> ppp net / batman - adv / soft - interface . c <nl> static int interface_tx ( struct sk_buff * skb , struct net_device * soft_iface ) <nl> struct hard_iface * primary_if = NULL ; <nl> struct bcast_packet * bcast_packet ; <nl> struct vlan_ethhdr * vhdr ; <nl> + static const uint8_t stp_addr [ ETH_ALEN ] = { 0x01 , 0x80 , 0xC2 , 0x00 , 0x00 , <nl> + 0x00 }; <nl> unsigned int header_len = 0 ; <nl> int data_len = skb -> len , ret ; <nl> short vid = - 1 ; <nl> static int interface_tx ( struct sk_buff * skb , struct net_device * soft_iface ) <nl> /* Register the client MAC in the transtable */ <nl> tt_local_add ( soft_iface , ethhdr -> h_source , skb -> skb_iif ); <nl>  <nl> + /* don ' t accept stp packets . STP does not help in meshes . <nl> + * better use the bridge loop avoidance ... <nl> + */ <nl> + if ( compare_eth ( ethhdr -> h_dest , stp_addr )) <nl> + goto dropped ; <nl> + <nl> if ( is_multicast_ether_addr ( ethhdr -> h_dest )) { <nl> do_bcast = true ; <nl> 
mmm drivers / staging / ozwpan / ozhcd . c <nl> ppp drivers / staging / ozwpan / ozhcd . c <nl> void oz_hcd_pd_reset ( void * hpd , void * hport ) <nl> /* <nl> * Context : softirq <nl> */ <nl> - void oz_hcd_get_desc_cnf ( void * hport , u8 req_id , int status , const u8 * desc , <nl> - int length , int offset , int total_size ) <nl> + void oz_hcd_get_desc_cnf ( void * hport , u8 req_id , u8 status , const u8 * desc , <nl> + u8 length , u16 offset , u16 total_size ) <nl> { <nl> struct oz_port * port = hport ; <nl> struct urb * urb ; <nl> void oz_hcd_get_desc_cnf ( void * hport , u8 req_id , int status , const u8 * desc , <nl> if (! urb ) <nl> return ; <nl> if ( status == 0 ) { <nl> - int copy_len ; <nl> - int required_size = urb -> transfer_buffer_length ; <nl> + unsigned int copy_len ; <nl> + unsigned int required_size = urb -> transfer_buffer_length ; <nl>  <nl> if ( required_size > total_size ) <nl> required_size = total_size ;mmm drivers / staging / ozwpan / ozusbif . h <nl> ppp drivers / staging / ozwpan / ozusbif . h <nl> void oz_hcd_pd_reset ( void * hpd , void * hport ) <nl> /* <nl> * Context : softirq <nl> */ <nl> - void oz_hcd_get_desc_cnf ( void * hport , u8 req_id , int status , const u8 * desc , <nl> - int length , int offset , int total_size ) <nl> + void oz_hcd_get_desc_cnf ( void * hport , u8 req_id , u8 status , const u8 * desc , <nl> + u8 length , u16 offset , u16 total_size ) <nl> { <nl> struct oz_port * port = hport ; <nl> struct urb * urb ; <nl> void oz_hcd_get_desc_cnf ( void * hport , u8 req_id , int status , const u8 * desc , <nl> if (! urb ) <nl> return ; <nl> if ( status == 0 ) { <nl> - int copy_len ; <nl> - int required_size = urb -> transfer_buffer_length ; <nl> + unsigned int copy_len ; <nl> + unsigned int required_size = urb -> transfer_buffer_length ; <nl>  <nl> if ( required_size > total_size ) <nl> required_size = total_size ; <nl> void oz_usb_request_heartbeat ( void * hpd ); <nl>  <nl> /* Confirmation functions . <nl> */ <nl> - void oz_hcd_get_desc_cnf ( void * hport , u8 req_id , int status , <nl> - const u8 * desc , int length , int offset , int total_size ); <nl> + void oz_hcd_get_desc_cnf ( void * hport , u8 req_id , u8 status , <nl> + const u8 * desc , u8 length , u16 offset , u16 total_size ); <nl> void oz_hcd_control_cnf ( void * hport , u8 req_id , u8 rcode , <nl> const u8 * data , int data_len ); <nl> 
mmm drivers / media / pci / solo6x10 / solo6x10 - eeprom . c <nl> ppp drivers / media / pci / solo6x10 / solo6x10 - eeprom . c <nl> unsigned int solo_eeprom_ewen ( struct solo_dev * solo_dev , int w_en ) <nl> __be16 solo_eeprom_read ( struct solo_dev * solo_dev , int loc ) <nl> { <nl> int read_cmd = loc | ( EE_READ_CMD << ADDR_LEN ); <nl> - unsigned short retval = 0 ; <nl> + u16 retval = 0 ; <nl> int i ; <nl>  <nl> solo_eeprom_cmd ( solo_dev , read_cmd );
mmm arch / x86 / kvm / mmu / paging_tmpl . h <nl> ppp arch / x86 / kvm / mmu / paging_tmpl . h <nl> struct guest_walker { <nl> gpa_t pte_gpa [ PT_MAX_FULL_LEVELS ]; <nl> pt_element_t __user * ptep_user [ PT_MAX_FULL_LEVELS ]; <nl> bool pte_writable [ PT_MAX_FULL_LEVELS ]; <nl> - unsigned pt_access ; <nl> - unsigned pte_access ; <nl> + unsigned int pt_access [ PT_MAX_FULL_LEVELS ]; <nl> + unsigned int pte_access ; <nl> gfn_t gfn ; <nl> struct x86_exception fault ; <nl> }; <nl> static int FNAME ( walk_addr_generic )( struct guest_walker * walker , <nl> } <nl>  <nl> walker -> ptes [ walker -> level - 1 ] = pte ; <nl> + <nl> + /* Convert to ACC_ * _MASK flags for struct guest_walker . */ <nl> + walker -> pt_access [ walker -> level - 1 ] = FNAME ( gpte_access )( pt_access ^ walk_nx_mask ); <nl> } while (! is_last_gpte ( mmu , walker -> level , pte )); <nl>  <nl> pte_pkey = FNAME ( gpte_pkeys )( vcpu , pte ); <nl> accessed_dirty = have_ad ? pte_access & PT_GUEST_ACCESSED_MASK : 0 ; <nl>  <nl> /* Convert to ACC_ * _MASK flags for struct guest_walker . */ <nl> - walker -> pt_access = FNAME ( gpte_access )( pt_access ^ walk_nx_mask ); <nl> walker -> pte_access = FNAME ( gpte_access )( pte_access ^ walk_nx_mask ); <nl> errcode = permission_fault ( vcpu , mmu , walker -> pte_access , pte_pkey , access ); <nl> if ( unlikely ( errcode )) <nl> static int FNAME ( walk_addr_generic )( struct guest_walker * walker , <nl> } <nl>  <nl> pgprintk ("% s : pte % llx pte_access % x pt_access % x \ n ", <nl> - __func__ , ( u64 ) pte , walker -> pte_access , walker -> pt_access ); <nl> + __func__ , ( u64 ) pte , walker -> pte_access , <nl> + walker -> pt_access [ walker -> level - 1 ]); <nl> return 1 ; <nl>  <nl> error : <nl> static int FNAME ( fetch )( struct kvm_vcpu * vcpu , gpa_t addr , <nl> bool huge_page_disallowed = exec && nx_huge_page_workaround_enabled ; <nl> struct kvm_mmu_page * sp = NULL ; <nl> struct kvm_shadow_walk_iterator it ; <nl> - unsigned direct_access , access = gw -> pt_access ; <nl> + unsigned int direct_access , access ; <nl> int top_level , level , req_level , ret ; <nl> gfn_t base_gfn = gw -> gfn ; <nl>  <nl> static int FNAME ( fetch )( struct kvm_vcpu * vcpu , gpa_t addr , <nl> sp = NULL ; <nl> if (! is_shadow_present_pte (* it . sptep )) { <nl> table_gfn = gw -> table_gfn [ it . level - 2 ]; <nl> + access = gw -> pt_access [ it . level - 2 ]; <nl> sp = kvm_mmu_get_page ( vcpu , table_gfn , addr , it . level - 1 , <nl> false , access ); <nl> }
mmm arch / arm / mach - omap2 / hsmmc . h <nl> ppp arch / arm / mach - omap2 / hsmmc . h <nl> struct omap2_hsmmc_info { <nl> bool nonremovable ; /* Nonremovable e . g . eMMC */ <nl> bool power_saving ; /* Try to sleep or power off when possible */ <nl> bool no_off ; /* power_saving and power is not to go off */ <nl> + bool no_off_init ; /* no power off when not in MMC sleep state */ <nl> bool vcc_aux_disable_is_sleep ; /* Regulator off remapped to sleep */ <nl> int gpio_cd ; /* or - EINVAL */ <nl> int gpio_wp ; /* or - EINVAL */mmm drivers / mmc / host / omap_hsmmc . c <nl> ppp drivers / mmc / host / omap_hsmmc . c <nl> struct omap2_hsmmc_info { <nl> bool nonremovable ; /* Nonremovable e . g . eMMC */ <nl> bool power_saving ; /* Try to sleep or power off when possible */ <nl> bool no_off ; /* power_saving and power is not to go off */ <nl> + bool no_off_init ; /* no power off when not in MMC sleep state */ <nl> bool vcc_aux_disable_is_sleep ; /* Regulator off remapped to sleep */ <nl> int gpio_cd ; /* or - EINVAL */ <nl> int gpio_wp ; /* or - EINVAL */ <nl> static int omap_hsmmc_reg_get ( struct omap_hsmmc_host * host ) <nl> reg = regulator_get ( host -> dev , " vmmc_aux "); <nl> host -> vcc_aux = IS_ERR ( reg ) ? NULL : reg ; <nl>  <nl> + /* For eMMC do not power off when not in sleep state */ <nl> + if ( mmc_slot ( host ). no_regulator_off_init ) <nl> + return 0 ; <nl> /* <nl> * UGLY HACK : workaround regulator framework bugs . <nl> * When the bootloader leaves a supply active , it ' smmm arch / arm / plat - omap / include / plat / mmc . h <nl> ppp arch / arm / plat - omap / include / plat / mmc . h <nl> struct omap2_hsmmc_info { <nl> bool nonremovable ; /* Nonremovable e . g . eMMC */ <nl> bool power_saving ; /* Try to sleep or power off when possible */ <nl> bool no_off ; /* power_saving and power is not to go off */ <nl> + bool no_off_init ; /* no power off when not in MMC sleep state */ <nl> bool vcc_aux_disable_is_sleep ; /* Regulator off remapped to sleep */ <nl> int gpio_cd ; /* or - EINVAL */ <nl> int gpio_wp ; /* or - EINVAL */ <nl> static int omap_hsmmc_reg_get ( struct omap_hsmmc_host * host ) <nl> reg = regulator_get ( host -> dev , " vmmc_aux "); <nl> host -> vcc_aux = IS_ERR ( reg ) ? NULL : reg ; <nl>  <nl> + /* For eMMC do not power off when not in sleep state */ <nl> + if ( mmc_slot ( host ). no_regulator_off_init ) <nl> + return 0 ; <nl> /* <nl> * UGLY HACK : workaround regulator framework bugs . <nl> * When the bootloader leaves a supply active , it ' s <nl> struct omap_mmc_platform_data { <nl> /* If using power_saving and the MMC power is not to go off */ <nl> unsigned no_off : 1 ; <nl>  <nl> + /* eMMC does not handle power off when not in sleep state */ <nl> + unsigned no_regulator_off_init : 1 ; <nl> + <nl> /* Regulator off remapped to sleep */ <nl> unsigned vcc_aux_disable_is_sleep : 1 ; <nl> mmm arch / arm / mach - omap2 / board - 4430sdp . c <nl> ppp arch / arm / mach - omap2 / board - 4430sdp . c <nl> struct omap2_hsmmc_info { <nl> bool nonremovable ; /* Nonremovable e . g . eMMC */ <nl> bool power_saving ; /* Try to sleep or power off when possible */ <nl> bool no_off ; /* power_saving and power is not to go off */ <nl> + bool no_off_init ; /* no power off when not in MMC sleep state */ <nl> bool vcc_aux_disable_is_sleep ; /* Regulator off remapped to sleep */ <nl> int gpio_cd ; /* or - EINVAL */ <nl> int gpio_wp ; /* or - EINVAL */ <nl> static int omap_hsmmc_reg_get ( struct omap_hsmmc_host * host ) <nl> reg = regulator_get ( host -> dev , " vmmc_aux "); <nl> host -> vcc_aux = IS_ERR ( reg ) ? NULL : reg ; <nl>  <nl> + /* For eMMC do not power off when not in sleep state */ <nl> + if ( mmc_slot ( host ). no_regulator_off_init ) <nl> + return 0 ; <nl> /* <nl> * UGLY HACK : workaround regulator framework bugs . <nl> * When the bootloader leaves a supply active , it ' s <nl> struct omap_mmc_platform_data { <nl> /* If using power_saving and the MMC power is not to go off */ <nl> unsigned no_off : 1 ; <nl>  <nl> + /* eMMC does not handle power off when not in sleep state */ <nl> + unsigned no_regulator_off_init : 1 ; <nl> + <nl> /* Regulator off remapped to sleep */ <nl> unsigned vcc_aux_disable_is_sleep : 1 ; <nl>  <nl> static struct omap2_hsmmc_info mmc [] = { <nl> . gpio_wp = - EINVAL , <nl> . nonremovable = true , <nl> . ocr_mask = MMC_VDD_29_30 , <nl> + . no_off_init = true , <nl> }, <nl> { <nl> . mmc = 1 ,mmm arch / arm / mach - omap2 / hsmmc . c <nl> ppp arch / arm / mach - omap2 / hsmmc . c <nl> struct omap2_hsmmc_info { <nl> bool nonremovable ; /* Nonremovable e . g . eMMC */ <nl> bool power_saving ; /* Try to sleep or power off when possible */ <nl> bool no_off ; /* power_saving and power is not to go off */ <nl> + bool no_off_init ; /* no power off when not in MMC sleep state */ <nl> bool vcc_aux_disable_is_sleep ; /* Regulator off remapped to sleep */ <nl> int gpio_cd ; /* or - EINVAL */ <nl> int gpio_wp ; /* or - EINVAL */ <nl> static int omap_hsmmc_reg_get ( struct omap_hsmmc_host * host ) <nl> reg = regulator_get ( host -> dev , " vmmc_aux "); <nl> host -> vcc_aux = IS_ERR ( reg ) ? NULL : reg ; <nl>  <nl> + /* For eMMC do not power off when not in sleep state */ <nl> + if ( mmc_slot ( host ). no_regulator_off_init ) <nl> + return 0 ; <nl> /* <nl> * UGLY HACK : workaround regulator framework bugs . <nl> * When the bootloader leaves a supply active , it ' s <nl> struct omap_mmc_platform_data { <nl> /* If using power_saving and the MMC power is not to go off */ <nl> unsigned no_off : 1 ; <nl>  <nl> + /* eMMC does not handle power off when not in sleep state */ <nl> + unsigned no_regulator_off_init : 1 ; <nl> + <nl> /* Regulator off remapped to sleep */ <nl> unsigned vcc_aux_disable_is_sleep : 1 ; <nl>  <nl> static struct omap2_hsmmc_info mmc [] = { <nl> . gpio_wp = - EINVAL , <nl> . nonremovable = true , <nl> . ocr_mask = MMC_VDD_29_30 , <nl> + . no_off_init = true , <nl> }, <nl> { <nl> . mmc = 1 , <nl> static int __init omap_hsmmc_pdata_init ( struct omap2_hsmmc_info * c , <nl> if ( c -> no_off ) <nl> mmc -> slots [ 0 ]. no_off = 1 ; <nl>  <nl> + if ( c -> no_off_init ) <nl> + mmc -> slots [ 0 ]. no_regulator_off_init = c -> no_off_init ; <nl> + <nl> if ( c -> vcc_aux_disable_is_sleep ) <nl> mmc -> slots [ 0 ]. vcc_aux_disable_is_sleep = 1 ; <nl> 
mmm sound / soc / s3c24xx / s3c64xx - i2s . c <nl> ppp sound / soc / s3c24xx / s3c64xx - i2s . c <nl> static int s3c64xx_i2s_set_sysclk ( struct snd_soc_dai * cpu_dai , <nl> struct clk * s3c64xx_i2s_get_clock ( struct snd_soc_dai * dai ) <nl> { <nl> struct s3c_i2sv2_info * i2s = to_info ( dai ); <nl> + u32 iismod = readl ( i2s -> regs + S3C2412_IISMOD ); <nl>  <nl> - return i2s -> iis_cclk ; <nl> + if ( iismod & S3C64XX_IISMOD_IMS_SYSMUX ) <nl> + return i2s -> iis_cclk ; <nl> + else <nl> + return i2s -> iis_pclk ; <nl> } <nl> EXPORT_SYMBOL_GPL ( s3c64xx_i2s_get_clock ); <nl> 
mmm drivers / tty / pty . c <nl> ppp drivers / tty / pty . c <nl> static void pty_close ( struct tty_struct * tty , struct file * filp ) <nl> mutex_unlock (& devpts_mutex ); <nl> } <nl> # endif <nl> + tty_unlock ( tty ); <nl> tty_vhangup ( tty -> link ); <nl> + tty_lock ( tty ); <nl> } <nl> } <nl> 
mmm drivers / net / mv643xx_eth . c <nl> ppp drivers / net / mv643xx_eth . c <nl> static int mv643xx_eth_receive_queue ( struct net_device * dev ) <nl> struct pkt_info pkt_info ; <nl>  <nl> # ifdef MV643XX_NAPI <nl> - while ( eth_port_receive ( mp , & pkt_info ) == ETH_OK && budget > 0 ) { <nl> + while ( budget -- > 0 && eth_port_receive ( mp , & pkt_info ) == ETH_OK ) { <nl> # else <nl> while ( eth_port_receive ( mp , & pkt_info ) == ETH_OK ) { <nl> # endif <nl> mp -> rx_ring_skbs --; <nl> received_packets ++; <nl> -# ifdef MV643XX_NAPI <nl> - budget --; <nl> -# endif <nl> + <nl> /* Update statistics . Note byte count includes 4 byte CRC count */ <nl> stats -> rx_packets ++; <nl> stats -> rx_bytes += pkt_info . byte_cnt ;
mmm arch / x86 / kvm / hyperv . c <nl> ppp arch / x86 / kvm / hyperv . c <nl> static int synic_set_msr ( struct kvm_vcpu_hv_synic * synic , <nl> struct kvm_vcpu * vcpu = hv_synic_to_vcpu ( synic ); <nl> int ret ; <nl>  <nl> - if (! synic -> active && ! host ) <nl> + if (! synic -> active && (! host || data )) <nl> return 1 ; <nl>  <nl> trace_kvm_hv_synic_set_msr ( vcpu -> vcpu_id , msr , data , host ); <nl> static int synic_set_msr ( struct kvm_vcpu_hv_synic * synic , <nl> case HV_X64_MSR_EOM : { <nl> int i ; <nl>  <nl> + if (! synic -> active ) <nl> + break ; <nl> + <nl> for ( i = 0 ; i < ARRAY_SIZE ( synic -> sint ); i ++) <nl> kvm_hv_notify_acked_sint ( vcpu , i ); <nl> break ; <nl> static int stimer_set_config ( struct kvm_vcpu_hv_stimer * stimer , u64 config , <nl> struct kvm_vcpu_hv * hv_vcpu = to_hv_vcpu ( vcpu ); <nl> struct kvm_vcpu_hv_synic * synic = to_hv_synic ( vcpu ); <nl>  <nl> - if (! synic -> active && ! host ) <nl> + if (! synic -> active && (! host || config )) <nl> return 1 ; <nl>  <nl> if ( unlikely (! host && hv_vcpu -> enforce_cpuid && new_config . direct_mode && <nl> static int stimer_set_count ( struct kvm_vcpu_hv_stimer * stimer , u64 count , <nl> struct kvm_vcpu * vcpu = hv_stimer_to_vcpu ( stimer ); <nl> struct kvm_vcpu_hv_synic * synic = to_hv_synic ( vcpu ); <nl>  <nl> - if (! synic -> active && ! host ) <nl> + if (! synic -> active && (! host || count )) <nl> return 1 ; <nl>  <nl> trace_kvm_hv_stimer_set_count ( hv_stimer_to_vcpu ( stimer )-> vcpu_id ,
mmm drivers / acpi / acpi_video . c <nl> ppp drivers / acpi / acpi_video . c <nl> static struct dmi_system_id video_dmi_table [] = { <nl> DMI_MATCH ( DMI_PRODUCT_NAME , " PORTEGE R830 "), <nl> }, <nl> }, <nl> + { <nl> + /* https :// bugzilla . kernel . org / show_bug . cgi ? id = 21012 */ <nl> + . callback = video_disable_backlight_sysfs_if , <nl> + . ident = " Toshiba Satellite R830 ", <nl> + . matches = { <nl> + DMI_MATCH ( DMI_SYS_VENDOR , " TOSHIBA "), <nl> + DMI_MATCH ( DMI_PRODUCT_NAME , " SATELLITE R830 "), <nl> + }, <nl> + }, <nl> /* <nl> * Some machine ' s _DOD IDs don ' t have bit 31 ( Device ID Scheme ) set <nl> * but the IDs actually follow the Device ID Scheme .
mmm drivers / bluetooth / btbcm . c <nl> ppp drivers / bluetooth / btbcm . c <nl> EXPORT_SYMBOL_GPL ( btbcm_setup_patchram ); <nl> int btbcm_setup_apple ( struct hci_dev * hdev ) <nl> { <nl> struct sk_buff * skb ; <nl> + int err ; <nl> + <nl> + /* Reset */ <nl> + err = btbcm_reset ( hdev ); <nl> + if ( err ) <nl> + return err ; <nl>  <nl> /* Read Verbose Config Version Info */ <nl> skb = btbcm_read_verbose_config ( hdev ); <nl> if (! IS_ERR ( skb )) { <nl> - BT_INFO ("% s : BCM : chip id % u build % 4 . 4u ", hdev -> name , skb -> data [ 1 ], <nl> - get_unaligned_le16 ( skb -> data + 5 )); <nl> + BT_INFO ("% s : BCM : chip id % u build % 4 . 4u ", hdev -> name , <nl> + skb -> data [ 1 ], get_unaligned_le16 ( skb -> data + 5 )); <nl> kfree_skb ( skb ); <nl> } <nl> 
mmm net / batman - adv / bat_iv_ogm . c <nl> ppp net / batman - adv / bat_iv_ogm . c <nl> batadv_iv_ogm_orig_get ( struct batadv_priv * bat_priv , const uint8_t * addr ) <nl> free_bcast_own : <nl> kfree ( orig_node -> bat_iv . bcast_own ); <nl> free_orig_node : <nl> + /* free twice , as batadv_orig_node_new sets refcount to 2 */ <nl> + batadv_orig_node_free_ref ( orig_node ); <nl> batadv_orig_node_free_ref ( orig_node ); <nl>  <nl> return NULL ;
mmm drivers / serial / 8250_early . c <nl> ppp drivers / serial / 8250_early . c <nl> static int __init parse_options ( struct early_uart_device * device , char * options ) <nl>  <nl> if (( options = strchr ( options , ','))) { <nl> options ++; <nl> - device -> baud = simple_strtoul ( options , 0 , 0 ); <nl> + device -> baud = simple_strtoul ( options , NULL , 0 ); <nl> length = min ( strcspn ( options , " "), sizeof ( device -> options )); <nl> strncpy ( device -> options , options , length ); <nl> } else {
mmm drivers / input / misc / ixp4xx - beeper . c <nl> ppp drivers / input / misc / ixp4xx - beeper . c <nl> static int ixp4xx_spkr_event ( struct input_dev * dev , unsigned int type , unsigned <nl>  <nl> static irqreturn_t ixp4xx_spkr_interrupt ( int irq , void * dev_id ) <nl> { <nl> + unsigned int pin = ( unsigned int ) dev_id ; <nl> + <nl> /* clear interrupt */ <nl> * IXP4XX_OSST = IXP4XX_OSST_TIMER_2_PEND ; <nl>  <nl> /* flip the beeper output */ <nl> - * IXP4XX_GPIO_GPOUTR ^= ( 1 << ( unsigned int ) dev_id ); <nl> + gpio_set_value ( pin , ! gpio_get_value ( pin )); <nl>  <nl> return IRQ_HANDLED ; <nl> } <nl> static int ixp4xx_spkr_probe ( struct platform_device * dev ) <nl> input_dev -> sndbit [ 0 ] = BIT_MASK ( SND_BELL ) | BIT_MASK ( SND_TONE ); <nl> input_dev -> event = ixp4xx_spkr_event ; <nl>  <nl> + err = gpio_request ( dev -> id , " ixp4 - beeper "); <nl> + if ( err ) <nl> + goto err_free_device ; <nl> + <nl> err = request_irq ( IRQ_IXP4XX_TIMER2 , & ixp4xx_spkr_interrupt , <nl> IRQF_NO_SUSPEND , " ixp4xx - beeper ", <nl> ( void *) dev -> id ); <nl> if ( err ) <nl> - goto err_free_device ; <nl> + goto err_free_gpio ; <nl>  <nl> err = input_register_device ( input_dev ); <nl> if ( err ) <nl> static int ixp4xx_spkr_probe ( struct platform_device * dev ) <nl>  <nl> err_free_irq : <nl> free_irq ( IRQ_IXP4XX_TIMER2 , ( void *) dev -> id ); <nl> + err_free_gpio : <nl> + gpio_free ( dev -> id ); <nl> err_free_device : <nl> input_free_device ( input_dev ); <nl>  <nl> static int ixp4xx_spkr_remove ( struct platform_device * dev ) <nl> ixp4xx_spkr_control ( pin , 0 ); <nl>  <nl> free_irq ( IRQ_IXP4XX_TIMER2 , ( void *) dev -> id ); <nl> + gpio_free ( dev -> id ); <nl>  <nl> return 0 ; <nl> }
mmm net / netfilter / nf_conntrack_proto_dccp . c <nl> ppp net / netfilter / nf_conntrack_proto_dccp . c <nl> static bool dccp_new ( struct nf_conn * ct , const struct sk_buff * skb , <nl> const char * msg ; <nl> u_int8_t state ; <nl>  <nl> - dh = skb_header_pointer ( skb , dataoff , sizeof ( _dh ), & dh ); <nl> + dh = skb_header_pointer ( skb , dataoff , sizeof ( _dh ), & _dh ); <nl> BUG_ON ( dh == NULL ); <nl>  <nl> state = dccp_state_table [ CT_DCCP_ROLE_CLIENT ][ dh -> dccph_type ][ CT_DCCP_NONE ]; <nl> static int dccp_packet ( struct nf_conn * ct , const struct sk_buff * skb , <nl> u_int8_t type , old_state , new_state ; <nl> enum ct_dccp_roles role ; <nl>  <nl> - dh = skb_header_pointer ( skb , dataoff , sizeof ( _dh ), & dh ); <nl> + dh = skb_header_pointer ( skb , dataoff , sizeof ( _dh ), & _dh ); <nl> BUG_ON ( dh == NULL ); <nl> type = dh -> dccph_type ; <nl>  <nl> static int dccp_error ( struct net * net , struct nf_conn * tmpl , <nl> unsigned int cscov ; <nl> const char * msg ; <nl>  <nl> - dh = skb_header_pointer ( skb , dataoff , sizeof ( _dh ), & dh ); <nl> + dh = skb_header_pointer ( skb , dataoff , sizeof ( _dh ), & _dh ); <nl> if ( dh == NULL ) { <nl> msg = " nf_ct_dccp : short packet "; <nl> goto out_invalid ;
mmm arch / arm / mach - iop13xx / pci . c <nl> ppp arch / arm / mach - iop13xx / pci . c <nl> int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> which_atu = 0 ; <nl> } <nl>  <nl> - if (! which_atu ) <nl> + if (! which_atu ) { <nl> + kfree ( res ); <nl> return 0 ; <nl> + } <nl>  <nl> switch ( which_atu ) { <nl> case IOP13XX_INIT_ATU_ATUX : <nl> int iop13xx_pci_setup ( int nr , struct pci_sys_data * sys ) <nl> sys -> map_irq = iop13xx_pcie_map_irq ; <nl> break ; <nl> default : <nl> + kfree ( res ); <nl> return 0 ; <nl> } <nl> 
mmm arch / i386 / kernel / pci - dma . c <nl> ppp arch / i386 / kernel / pci - dma . c <nl> int dma_declare_coherent_memory ( struct device * dev , dma_addr_t bus_addr , <nl> { <nl> void __iomem * mem_base = NULL ; <nl> int pages = size >> PAGE_SHIFT ; <nl> - int bitmap_size = ( pages + 31 )/ 32 ; <nl> + int bitmap_size = BITS_TO_LONGS ( pages ) * sizeof ( long ); <nl>  <nl> if (( flags & ( DMA_MEMORY_MAP | DMA_MEMORY_IO )) == 0 ) <nl> goto out ;mmm arch / cris / arch - v32 / drivers / pci / dma . c <nl> ppp arch / cris / arch - v32 / drivers / pci / dma . c <nl> int dma_declare_coherent_memory ( struct device * dev , dma_addr_t bus_addr , <nl> { <nl> void __iomem * mem_base = NULL ; <nl> int pages = size >> PAGE_SHIFT ; <nl> - int bitmap_size = ( pages + 31 )/ 32 ; <nl> + int bitmap_size = BITS_TO_LONGS ( pages ) * sizeof ( long ); <nl>  <nl> if (( flags & ( DMA_MEMORY_MAP | DMA_MEMORY_IO )) == 0 ) <nl> goto out ; <nl> int dma_declare_coherent_memory ( struct device * dev , dma_addr_t bus_addr , <nl> { <nl> void __iomem * mem_base ; <nl> int pages = size >> PAGE_SHIFT ; <nl> - int bitmap_size = ( pages + 31 )/ 32 ; <nl> + int bitmap_size = BITS_TO_LONGS ( pages ) * sizeof ( long ); <nl>  <nl> if (( flags & ( DMA_MEMORY_MAP | DMA_MEMORY_IO )) == 0 ) <nl> goto out ;
mmm fs / jbd2 / transaction . c <nl> ppp fs / jbd2 / transaction . c <nl> int jbd2_journal_start_reserved ( handle_t * handle , unsigned int type , <nl> */ <nl> ret = start_this_handle ( journal , handle , GFP_NOFS ); <nl> if ( ret < 0 ) { <nl> + handle -> h_journal = journal ; <nl> jbd2_journal_free_reserved ( handle ); <nl> return ret ; <nl> }
mmm drivers / media / video / gspca / gspca . c <nl> ppp drivers / media / video / gspca / gspca . c <nl> static int alloc_and_submit_int_urb ( struct gspca_dev * gspca_dev , <nl> void * buffer = NULL ; <nl> int ret = - EINVAL ; <nl>  <nl> - buffer_len = ep -> wMaxPacketSize ; <nl> + buffer_len = le16_to_cpu ( ep -> wMaxPacketSize ); <nl> interval = ep -> bInterval ; <nl> PDEBUG ( D_PROBE , " found int in endpoint : 0x % x , " <nl> " buffer_len =% u , interval =% u ", <nl> static int alloc_and_submit_int_urb ( struct gspca_dev * gspca_dev , <nl> goto error ; <nl> } <nl>  <nl> - buffer = usb_buffer_alloc ( dev , ep -> wMaxPacketSize , <nl> + buffer = usb_buffer_alloc ( dev , buffer_len , <nl> GFP_KERNEL , & urb -> transfer_dma ); <nl> if (! buffer ) { <nl> ret = - ENOMEM ;
mmm drivers / base / property . c <nl> ppp drivers / base / property . c <nl> int fwnode_graph_parse_endpoint ( const struct fwnode_handle * fwnode , <nl> return fwnode_call_int_op ( fwnode , graph_parse_endpoint , endpoint ); <nl> } <nl> EXPORT_SYMBOL ( fwnode_graph_parse_endpoint ); <nl> + <nl> + void * device_get_match_data ( struct device * dev ) <nl> +{ <nl> + return fwnode_call_ptr_op ( dev_fwnode ( dev ), device_get_match_data , <nl> + dev ); <nl> +} <nl> + EXPORT_SYMBOL_GPL ( device_get_match_data );mmm include / linux / property . h <nl> ppp include / linux / property . h <nl> int fwnode_graph_parse_endpoint ( const struct fwnode_handle * fwnode , <nl> return fwnode_call_int_op ( fwnode , graph_parse_endpoint , endpoint ); <nl> } <nl> EXPORT_SYMBOL ( fwnode_graph_parse_endpoint ); <nl> + <nl> + void * device_get_match_data ( struct device * dev ) <nl> +{ <nl> + return fwnode_call_ptr_op ( dev_fwnode ( dev ), device_get_match_data , <nl> + dev ); <nl> +} <nl> + EXPORT_SYMBOL_GPL ( device_get_match_data ); <nl> bool device_dma_supported ( struct device * dev ); <nl>  <nl> enum dev_dma_attr device_get_dma_attr ( struct device * dev ); <nl>  <nl> + void * device_get_match_data ( struct device * dev ); <nl> + <nl> int device_get_phy_mode ( struct device * dev ); <nl>  <nl> void * device_get_mac_address ( struct device * dev , char * addr , int alen );mmm include / linux / fwnode . h <nl> ppp include / linux / fwnode . h <nl> int fwnode_graph_parse_endpoint ( const struct fwnode_handle * fwnode , <nl> return fwnode_call_int_op ( fwnode , graph_parse_endpoint , endpoint ); <nl> } <nl> EXPORT_SYMBOL ( fwnode_graph_parse_endpoint ); <nl> + <nl> + void * device_get_match_data ( struct device * dev ) <nl> +{ <nl> + return fwnode_call_ptr_op ( dev_fwnode ( dev ), device_get_match_data , <nl> + dev ); <nl> +} <nl> + EXPORT_SYMBOL_GPL ( device_get_match_data ); <nl> bool device_dma_supported ( struct device * dev ); <nl>  <nl> enum dev_dma_attr device_get_dma_attr ( struct device * dev ); <nl>  <nl> + void * device_get_match_data ( struct device * dev ); <nl> + <nl> int device_get_phy_mode ( struct device * dev ); <nl>  <nl> void * device_get_mac_address ( struct device * dev , char * addr , int alen ); <nl> # include < linux / types . h > <nl>  <nl> struct fwnode_operations ; <nl> + struct device ; <nl>  <nl> struct fwnode_handle { <nl> struct fwnode_handle * secondary ; <nl> struct fwnode_reference_args { <nl> * struct fwnode_operations - Operations for fwnode interface <nl> * @ get : Get a reference to an fwnode . <nl> * @ put : Put a reference to an fwnode . <nl> + * @ device_get_match_data : Return the device driver match data . <nl> * @ property_present : Return true if a property is present . <nl> * @ property_read_integer_array : Read an array of integer properties . Return <nl> * zero on success , a negative error code <nl> struct fwnode_operations { <nl> struct fwnode_handle *(* get )( struct fwnode_handle * fwnode ); <nl> void (* put )( struct fwnode_handle * fwnode ); <nl> bool (* device_is_available )( const struct fwnode_handle * fwnode ); <nl> + void *(* device_get_match_data )( const struct fwnode_handle * fwnode , <nl> + const struct device * dev ); <nl> bool (* property_present )( const struct fwnode_handle * fwnode , <nl> const char * propname ); <nl> int (* property_read_int_array )( const struct fwnode_handle * fwnode ,
mmm arch / x86 / mm / discontig_32 . c <nl> ppp arch / x86 / mm / discontig_32 . c <nl> EXPORT_SYMBOL_GPL ( memory_add_physaddr_to_nid ); <nl> void __init acpi_numa_slit_init ( struct acpi_table_slit * slit ) <nl> { <nl> } <nl> + <nl> + void __init <nl> + acpi_numa_processor_affinity_init ( struct acpi_srat_cpu_affinity * pa ) <nl> +{ <nl> +} <nl> # endif
mmm kernel / trace / trace_events_hist . c <nl> ppp kernel / trace / trace_events_hist . c <nl> static int create_tracing_map_fields ( struct hist_trigger_data * hist_data ) <nl> struct tracing_map * map = hist_data -> map ; <nl> struct ftrace_event_field * field ; <nl> struct hist_field * hist_field ; <nl> - int i , idx ; <nl> + int i , idx = 0 ; <nl>  <nl> for_each_hist_field ( i , hist_data ) { <nl> hist_field = hist_data -> fields [ i ];
mmm fs / f2fs / xattr . c <nl> ppp fs / f2fs / xattr . c <nl> static int __f2fs_setxattr ( struct inode * inode , int index , <nl> goto exit ; <nl> } <nl>  <nl> - if ( f2fs_xattr_value_same ( here , value , size )) <nl> + if ( value && f2fs_xattr_value_same ( here , value , size )) <nl> goto exit ; <nl> } else if (( flags & XATTR_REPLACE )) { <nl> error = - ENODATA ;
mmm drivers / base / regmap / regcache . c <nl> ppp drivers / base / regmap / regcache . c <nl> static int regcache_hw_init ( struct regmap * map ) <nl>  <nl> /* calculate the size of reg_defaults */ <nl> for ( count = 0 , i = 0 ; i < map -> num_reg_defaults_raw ; i ++) <nl> - if (! regmap_volatile ( map , i * map -> reg_stride )) <nl> + if ( regmap_readable ( map , i * map -> reg_stride ) && <nl> + ! regmap_volatile ( map , i * map -> reg_stride )) <nl> count ++; <nl>  <nl> - /* all registers are volatile , so just bypass */ <nl> + /* all registers are unreadable or volatile , so just bypass */ <nl> if (! count ) { <nl> map -> cache_bypass = true ; <nl> return 0 ;
mmm arch / x86 / kvm / lapic . c <nl> ppp arch / x86 / kvm / lapic . c <nl> static void start_apic_timer ( struct kvm_lapic * apic ) <nl> { <nl> ktime_t now = apic -> lapic_timer . timer . base -> get_time (); <nl>  <nl> - apic -> lapic_timer . period = apic_get_reg ( apic , APIC_TMICT ) * <nl> + apic -> lapic_timer . period = ( u64 ) apic_get_reg ( apic , APIC_TMICT ) * <nl> APIC_BUS_CYCLE_NS * apic -> divide_count ; <nl> atomic_set (& apic -> lapic_timer . pending , 0 ); <nl> 
mmm tools / perf / util / symbol . c <nl> ppp tools / perf / util / symbol . c <nl> static int dso__load_sym ( struct dso * self , struct map * map , const char * name , <nl>  <nl> section_name = elf_sec__name (& shdr , secstrs ); <nl>  <nl> + /* On ARM , symbols for thumb functions have 1 added to <nl> + * the symbol address as a flag - remove it */ <nl> + if (( ehdr . e_machine == EM_ARM ) && <nl> + ( map -> type == MAP__FUNCTION ) && <nl> + ( sym . st_value & 1 )) <nl> + -- sym . st_value ; <nl> + <nl> if ( self -> kernel != DSO_TYPE_USER || kmodule ) { <nl> char dso_name [ PATH_MAX ]; <nl> 
mmm drivers / rapidio / devices / tsi721 . c <nl> ppp drivers / rapidio / devices / tsi721 . c <nl> static int tsi721_rio_map_inb_mem ( struct rio_mport * mport , dma_addr_t lstart , <nl> } else if ( ibw_start < ( ib_win -> rstart + ib_win -> size ) && <nl> ( ibw_start + ibw_size ) > ib_win -> rstart ) { <nl> /* Return error if address translation involved */ <nl> - if ( direct && ib_win -> xlat ) { <nl> + if (! direct || ib_win -> xlat ) { <nl> ret = - EFAULT ; <nl> break ; <nl> }
mmm drivers / usb / phy / phy - msm - usb . c <nl> ppp drivers / usb / phy / phy - msm - usb . c <nl> static int msm_otg_read_dt ( struct platform_device * pdev , struct msm_otg * motg ) <nl> motg -> pdata = pdata ; <nl>  <nl> id = of_match_device ( msm_otg_dt_match , & pdev -> dev ); <nl> - pdata -> phy_type = ( int ) id -> data ; <nl> + pdata -> phy_type = ( enum msm_usb_phy_type ) id -> data ; <nl>  <nl> motg -> link_rst = devm_reset_control_get (& pdev -> dev , " link "); <nl> if ( IS_ERR ( motg -> link_rst ))
mmm drivers / net / wireless / ath / ath10k / htt_rx . c <nl> ppp drivers / net / wireless / ath / ath10k / htt_rx . c <nl> static int ath10k_htt_rx_amsdu_pop ( struct ath10k_htt * htt , <nl> while ( msdu_chained --) { <nl> struct sk_buff * next = ath10k_htt_rx_netbuf_pop ( htt ); <nl>  <nl> + if (! next ) { <nl> + ath10k_warn ( ar , " failed to pop chained msdu \ n "); <nl> + ath10k_htt_rx_free_msdu_chain (* head_msdu ); <nl> + * head_msdu = NULL ; <nl> + msdu = NULL ; <nl> + htt -> rx_confused = true ; <nl> + break ; <nl> + } <nl> + <nl> skb_trim ( next , 0 ); <nl> skb_put ( next , min ( msdu_len , HTT_RX_BUF_SIZE )); <nl> msdu_len -= next -> len ;
mmm arch / powerpc / platforms / powernv / pci - ioda . c <nl> ppp arch / powerpc / platforms / powernv / pci - ioda . c <nl> static void pnv_ioda_release_pe ( struct pnv_ioda_pe * pe ) <nl> struct pnv_phb * phb = pe -> phb ; <nl> struct pnv_ioda_pe * slave , * tmp ; <nl>  <nl> - /* Release slave PEs in compound PE */ <nl> - if ( pe -> flags & PNV_IODA_PE_MASTER ) { <nl> - list_for_each_entry_safe ( slave , tmp , & pe -> slaves , list ) <nl> - pnv_ioda_release_pe ( slave ); <nl> - } <nl> - <nl> list_del (& pe -> list ); <nl> switch ( phb -> type ) { <nl> case PNV_PHB_IODA1 : <nl> static void pnv_ioda_release_pe ( struct pnv_ioda_pe * pe ) <nl>  <nl> pnv_ioda_release_pe_seg ( pe ); <nl> pnv_ioda_deconfigure_pe ( pe -> phb , pe ); <nl> + <nl> + /* Release slave PEs in the compound PE */ <nl> + if ( pe -> flags & PNV_IODA_PE_MASTER ) { <nl> + list_for_each_entry_safe ( slave , tmp , & pe -> slaves , list ) { <nl> + list_del (& slave -> list ); <nl> + pnv_ioda_free_pe ( slave ); <nl> + } <nl> + } <nl> + <nl> pnv_ioda_free_pe ( pe ); <nl> } <nl> 
mmm drivers / nfc / pn533 / pn533 . c <nl> ppp drivers / nfc / pn533 / pn533 . c <nl> static int pn533_data_exchange_complete ( struct pn533 * dev , void * _arg , <nl> */ <nl> void pn533_recv_frame ( struct pn533 * dev , struct sk_buff * skb , int status ) <nl> { <nl> + if (! dev -> cmd ) <nl> + goto sched_wq ; <nl> + <nl> dev -> cmd -> status = status ; <nl>  <nl> + if ( status != 0 ) { <nl> + dev_dbg ( dev -> dev , "% s : Error received : % d \ n ", __func__ , status ); <nl> + goto sched_wq ; <nl> + } <nl> + <nl> if ( skb == NULL ) { <nl> pr_err (" NULL Frame -> link is dead \ n "); <nl> goto sched_wq ;
mmm fs / xfs / xfs_linux . h <nl> ppp fs / xfs / xfs_linux . h <nl> static inline uint64_t howmany_64 ( uint64_t x , uint32_t y ) <nl> # endif /* DEBUG */ <nl>  <nl> # ifdef CONFIG_XFS_RT <nl> -# define XFS_IS_REALTIME_INODE ( ip ) (( ip )-> i_d . di_flags & XFS_DIFLAG_REALTIME ) <nl> + <nl> +/* <nl> + * make sure we ignore the inode flag if the filesystem doesn ' t have a <nl> + * configured realtime device . <nl> + */ <nl> +# define XFS_IS_REALTIME_INODE ( ip ) \ <nl> + ((( ip )-> i_d . di_flags & XFS_DIFLAG_REALTIME ) && \ <nl> + ( ip )-> i_mount -> m_rtdev_targp ) <nl> # else <nl> # define XFS_IS_REALTIME_INODE ( ip ) ( 0 ) <nl> # endif
mmm drivers / isdn / gigaset / usb - gigaset . c <nl> ppp drivers / isdn / gigaset / usb - gigaset . c <nl> MODULE_PARM_DESC ( cidmode , " Call - ID mode "); <nl> # define GIGASET_MODULENAME " usb_gigaset " <nl> # define GIGASET_DEVNAME " ttyGU " <nl>  <nl> -# define IF_WRITEBUF 2000 /* arbitrary limit */ <nl> +/* length limit according to Siemens 3070usb - protokoll . doc ch . 2 . 1 */ <nl> +# define IF_WRITEBUF 264 <nl>  <nl> /* Values for the Gigaset M105 Data */ <nl> # define USB_M105_VENDOR_ID 0x0681
mmm drivers / pci / host / pcie - xilinx . c <nl> ppp drivers / pci / host / pcie - xilinx . c <nl> static void xilinx_msi_teardown_irq ( struct msi_controller * chip , <nl> unsigned int irq ) <nl> { <nl> xilinx_pcie_destroy_msi ( irq ); <nl> + irq_dispose_mapping ( irq ); <nl> } <nl>  <nl> /**
mmm crypto / algif_aead . c <nl> ppp crypto / algif_aead . c <nl> static void aead_release ( void * private ) <nl> struct aead_tfm * tfm = private ; <nl>  <nl> crypto_free_aead ( tfm -> aead ); <nl> + crypto_put_default_null_skcipher2 (); <nl> kfree ( tfm ); <nl> } <nl>  <nl> static void aead_sock_destruct ( struct sock * sk ) <nl> unsigned int ivlen = crypto_aead_ivsize ( tfm ); <nl>  <nl> af_alg_pull_tsgl ( sk , ctx -> used , NULL , 0 ); <nl> - crypto_put_default_null_skcipher2 (); <nl> sock_kzfree_s ( sk , ctx -> iv , ivlen ); <nl> sock_kfree_s ( sk , ctx , ctx -> len ); <nl> af_alg_release_parent ( sk );
mmm drivers / net / dsa / mv88e6xxx / chip . c <nl> ppp drivers / net / dsa / mv88e6xxx / chip . c <nl> static void mv88e6xxx_remove ( struct mdio_device * mdiodev ) <nl> if ( chip -> irq > 0 ) { <nl> if ( chip -> info -> g2_irqs > 0 ) <nl> mv88e6xxx_g2_irq_free ( chip ); <nl> + mutex_lock (& chip -> reg_lock ); <nl> mv88e6xxx_g1_irq_free ( chip ); <nl> + mutex_unlock (& chip -> reg_lock ); <nl> } <nl> } <nl> 
mmm drivers / media / video / tuner - xc2028 . c <nl> ppp drivers / media / video / tuner - xc2028 . c <nl> static int load_firmware ( struct dvb_frontend * fe , unsigned int type , <nl> (* p ) & 0x7f ); <nl> return - EINVAL ; <nl> } <nl> + break ; <nl> default : <nl> tuner_info (" Invalid RESET code % d \ n ", <nl> size & 0x7f );
mmm arch / mips / kernel / process . c <nl> ppp arch / mips / kernel / process . c <nl> static inline int is_sp_move_ins ( union mips_instruction * ip ) <nl> */ <nl> if ( mm_insn_16bit ( ip -> halfword [ 1 ])) { <nl> return ( ip -> mm16_r3_format . opcode == mm_pool16d_op && <nl> - ip -> mm16_r3_format . simmediate && mm_addiusp_func ) || <nl> + ip -> mm16_r3_format . simmediate & mm_addiusp_func ) || <nl> ( ip -> mm16_r5_format . opcode == mm_pool16d_op && <nl> ip -> mm16_r5_format . rt == 29 ); <nl> }
mmm drivers / ata / ata_piix . c <nl> ppp drivers / ata / ata_piix . c <nl> static const struct ich_laptop ich_laptop [] = { <nl> { 0x27DF , 0x0005 , 0x0280 }, /* ICH7 on Acer 5602WLMi */ <nl> { 0x27DF , 0x1025 , 0x0110 }, /* ICH7 on Acer 3682WLMi */ <nl> { 0x27DF , 0x1043 , 0x1267 }, /* ICH7 on Asus W5F */ <nl> + { 0x24CA , 0x1025 , 0x0061 }, /* ICH4 on ACER Aspire 2023WLMi */ <nl> /* end marker */ <nl> { 0 , } <nl> };
mmm drivers / scsi / st . c <nl> ppp drivers / scsi / st . c <nl> static const char * verstr = " 20080224 "; <nl> # include < linux / cdev . h > <nl> # include < linux / delay . h > <nl> # include < linux / mutex . h > <nl> +# include < linux / smp_lock . h > <nl>  <nl> # include < asm / uaccess . h > <nl> # include < asm / dma . h > <nl> static int check_tape ( struct scsi_tape * STp , struct file * filp ) <nl> } <nl>  <nl>  <nl> - <nl> /* Open the device . Needs to be called with BKL only because of incrementing the SCSI host <nl> + <nl> /* Open the device . Needs to take the BKL only because of incrementing the SCSI host <nl> module count . */ <nl> static int st_open ( struct inode * inode , struct file * filp ) <nl> { <nl> static int st_open ( struct inode * inode , struct file * filp ) <nl> int dev = TAPE_NR ( inode ); <nl> char * name ; <nl>  <nl> + lock_kernel (); <nl> /* <nl> * We really want to do nonseekable_open ( inode , filp ); here , but some <nl> * versions of tar incorrectly call lseek on tapes and bail out if that <nl> static int st_open ( struct inode * inode , struct file * filp ) <nl> */ <nl> filp -> f_mode &= ~( FMODE_PREAD | FMODE_PWRITE ); <nl>  <nl> - if (!( STp = scsi_tape_get ( dev ))) <nl> + if (!( STp = scsi_tape_get ( dev ))) { <nl> + unlock_kernel (); <nl> return - ENXIO ; <nl> + } <nl>  <nl> write_lock (& st_dev_arr_lock ); <nl> filp -> private_data = STp ; <nl> static int st_open ( struct inode * inode , struct file * filp ) <nl> if ( STp -> in_use ) { <nl> write_unlock (& st_dev_arr_lock ); <nl> scsi_tape_put ( STp ); <nl> + unlock_kernel (); <nl> DEB ( printk ( ST_DEB_MSG "% s : Device already in use .\ n ", name ); ) <nl> return (- EBUSY ); <nl> } <nl> static int st_open ( struct inode * inode , struct file * filp ) <nl> retval = (- EIO ); <nl> goto err_out ; <nl> } <nl> + unlock_kernel (); <nl> return 0 ; <nl>  <nl> err_out : <nl> normalize_buffer ( STp -> buffer ); <nl> STp -> in_use = 0 ; <nl> scsi_tape_put ( STp ); <nl> + unlock_kernel (); <nl> return retval ; <nl>  <nl> }
mmm drivers / gpu / drm / nouveau / nvkm / engine / device / base . c <nl> ppp drivers / gpu / drm / nouveau / nvkm / engine / device / base . c <nl> static const struct nvkm_device_chip <nl> nv134_chipset = { <nl> . name = " GP104 ", <nl> . mc = gp100_mc_new , <nl> + . pci = gp100_pci_new , <nl> . top = gk104_top_new , <nl> }; <nl> 
mmm drivers / usb / usbip / usbip_common . c <nl> ppp drivers / usb / usbip / usbip_common . c <nl> int usbip_recv_xbuff ( struct usbip_device * ud , struct urb * urb ) <nl> if (!( size > 0 )) <nl> return 0 ; <nl>  <nl> + if ( size > urb -> transfer_buffer_length ) { <nl> + /* should not happen , probably malicious packet */ <nl> + if ( ud -> side == USBIP_STUB ) { <nl> + usbip_event_add ( ud , SDEV_EVENT_ERROR_TCP ); <nl> + return 0 ; <nl> + } else { <nl> + usbip_event_add ( ud , VDEV_EVENT_ERROR_TCP ); <nl> + return - EPIPE ; <nl> + } <nl> + } <nl> + <nl> ret = usbip_recv ( ud -> tcp_socket , urb -> transfer_buffer , size ); <nl> if ( ret != size ) { <nl> dev_err (& urb -> dev -> dev , " recv xbuf , % d \ n ", ret );
mmm drivers / usb / gadget / s3c - hsotg . c <nl> ppp drivers / usb / gadget / s3c - hsotg . c <nl> static void s3c_hsotg_core_init ( struct s3c_hsotg * hsotg ) <nl> /* Clear any pending interrupts */ <nl> writel ( 0xffffffff , hsotg -> regs + S3C_GINTSTS ); <nl>  <nl> - writel ( S3C_GINTSTS_DisconnInt | S3C_GINTSTS_SessReqInt | <nl> + writel ( S3C_GINTSTS_ErlySusp | S3C_GINTSTS_SessReqInt | <nl> S3C_GINTSTS_GOUTNakEff | S3C_GINTSTS_GINNakEff | <nl> S3C_GINTSTS_ConIDStsChng | S3C_GINTSTS_USBRst | <nl> S3C_GINTSTS_EnumDone | S3C_GINTSTS_OTGInt | <nl> - S3C_GINTSTS_USBSusp | S3C_GINTSTS_WkUpInt | <nl> - S3C_GINTSTS_ErlySusp , <nl> + S3C_GINTSTS_USBSusp | S3C_GINTSTS_WkUpInt , <nl> hsotg -> regs + S3C_GINTMSK ); <nl>  <nl> if ( using_dma ( hsotg )) <nl> static irqreturn_t s3c_hsotg_irq ( int irq , void * pw ) <nl> writel ( otgint , hsotg -> regs + S3C_GOTGINT ); <nl> } <nl>  <nl> - if ( gintsts & S3C_GINTSTS_DisconnInt ) { <nl> - dev_dbg ( hsotg -> dev , "% s : DisconnInt \ n ", __func__ ); <nl> - writel ( S3C_GINTSTS_DisconnInt , hsotg -> regs + S3C_GINTSTS ); <nl> - <nl> - s3c_hsotg_disconnect_irq ( hsotg ); <nl> - } <nl> - <nl> if ( gintsts & S3C_GINTSTS_SessReqInt ) { <nl> dev_dbg ( hsotg -> dev , "% s : SessReqInt \ n ", __func__ ); <nl> writel ( S3C_GINTSTS_SessReqInt , hsotg -> regs + S3C_GINTSTS );
mmm drivers / acpi / resource . c <nl> ppp drivers / acpi / resource . c <nl> bool acpi_dev_resource_memory ( struct acpi_resource * ares , struct resource * res ) <nl> switch ( ares -> type ) { <nl> case ACPI_RESOURCE_TYPE_MEMORY24 : <nl> memory24 = & ares -> data . memory24 ; <nl> + if (! memory24 -> address_length ) <nl> + return false ; <nl> acpi_dev_get_memresource ( res , memory24 -> minimum , <nl> memory24 -> address_length , <nl> memory24 -> write_protect ); <nl> break ; <nl> case ACPI_RESOURCE_TYPE_MEMORY32 : <nl> memory32 = & ares -> data . memory32 ; <nl> + if (! memory32 -> address_length ) <nl> + return false ; <nl> acpi_dev_get_memresource ( res , memory32 -> minimum , <nl> memory32 -> address_length , <nl> memory32 -> write_protect ); <nl> break ; <nl> case ACPI_RESOURCE_TYPE_FIXED_MEMORY32 : <nl> fixed_memory32 = & ares -> data . fixed_memory32 ; <nl> + if (! fixed_memory32 -> address_length ) <nl> + return false ; <nl> acpi_dev_get_memresource ( res , fixed_memory32 -> address , <nl> fixed_memory32 -> address_length , <nl> fixed_memory32 -> write_protect ); <nl> bool acpi_dev_resource_io ( struct acpi_resource * ares , struct resource * res ) <nl> switch ( ares -> type ) { <nl> case ACPI_RESOURCE_TYPE_IO : <nl> io = & ares -> data . io ; <nl> + if (! io -> address_length ) <nl> + return false ; <nl> acpi_dev_get_ioresource ( res , io -> minimum , <nl> io -> address_length , <nl> io -> io_decode ); <nl> break ; <nl> case ACPI_RESOURCE_TYPE_FIXED_IO : <nl> fixed_io = & ares -> data . fixed_io ; <nl> + if (! fixed_io -> address_length ) <nl> + return false ; <nl> acpi_dev_get_ioresource ( res , fixed_io -> address , <nl> fixed_io -> address_length , <nl> ACPI_DECODE_10 );
mmm sound / core / compress_offload . c <nl> ppp sound / core / compress_offload . c <nl> static int snd_compr_allocate_buffer ( struct snd_compr_stream * stream , <nl> unsigned int buffer_size ; <nl> void * buffer ; <nl>  <nl> + if ( params -> buffer . fragment_size == 0 || <nl> + params -> buffer . fragments > SIZE_MAX / params -> buffer . fragment_size ) <nl> + return - EINVAL ; <nl> + <nl> buffer_size = params -> buffer . fragment_size * params -> buffer . fragments ; <nl> if ( stream -> ops -> copy ) { <nl> buffer = NULL ;
mmm drivers / regulator / core . c <nl> ppp drivers / regulator / core . c <nl> static ssize_t regulator_name_show ( struct device * dev , <nl> struct regulator_dev * rdev = dev_get_drvdata ( dev ); <nl> const char * name ; <nl>  <nl> - if ( rdev -> constraints -> name ) <nl> + if ( rdev -> constraints && rdev -> constraints -> name ) <nl> name = rdev -> constraints -> name ; <nl> else if ( rdev -> desc -> name ) <nl> name = rdev -> desc -> name ;
mmm fs / fuse / dev . c <nl> ppp fs / fuse / dev . c <nl> static void process_init_reply ( struct fuse_conn * fc , struct fuse_req * req ) <nl> int i ; <nl> struct fuse_init_out * arg = & req -> misc . init_out ; <nl>  <nl> - if ( arg -> major != FUSE_KERNEL_VERSION ) <nl> + if ( req -> out . h . error || arg -> major != FUSE_KERNEL_VERSION ) <nl> fc -> conn_error = 1 ; <nl> else { <nl> fc -> minor = arg -> minor ;
mmm net / ipv4 / tcp_input . c <nl> ppp net / ipv4 / tcp_input . c <nl> static void tcp_mark_head_lost ( struct sock * sk , int packets ) <nl> cnt += tcp_skb_pcount ( skb ); <nl>  <nl> if ( cnt > packets ) { <nl> - if ( tcp_is_sack ( tp ) || ( oldcnt >= packets )) <nl> + if (( tcp_is_sack ( tp ) && ! tcp_is_fack ( tp )) || <nl> + ( oldcnt >= packets )) <nl> break ; <nl>  <nl> mss = skb_shinfo ( skb )-> gso_size ;
mmm drivers / pci / quirks . c <nl> ppp drivers / pci / quirks . c <nl> static int pci_quirk_amd_sb_acs ( struct pci_dev * dev , u16 acs_flags ) <nl> # endif <nl> } <nl>  <nl> + static int pci_quirk_cavium_acs ( struct pci_dev * dev , u16 acs_flags ) <nl> +{ <nl> + /* <nl> + * Cavium devices matching this quirk do not perform peer - to - peer <nl> + * with other functions , allowing masking out these bits as if they <nl> + * were unimplemented in the ACS capability . <nl> + */ <nl> + acs_flags &= ~( PCI_ACS_SV | PCI_ACS_TB | PCI_ACS_RR | <nl> + PCI_ACS_CR | PCI_ACS_UF | PCI_ACS_DT ); <nl> + <nl> + return acs_flags ? 0 : 1 ; <nl> +} <nl> + <nl> /* <nl> * Many Intel PCH root ports do provide ACS - like features to disable peer <nl> * transactions and validate bus numbers in requests , but do not provide an <nl> static const struct pci_dev_acs_enabled { <nl> { PCI_VENDOR_ID_INTEL , PCI_ANY_ID , pci_quirk_intel_pch_acs }, <nl> { 0x19a2 , 0x710 , pci_quirk_mf_endpoint_acs }, /* Emulex BE3 - R */ <nl> { 0x10df , 0x720 , pci_quirk_mf_endpoint_acs }, /* Emulex Skyhawk - R */ <nl> + /* Cavium ThunderX */ <nl> + { PCI_VENDOR_ID_CAVIUM , PCI_ANY_ID , pci_quirk_cavium_acs }, <nl> { 0 } <nl> }; <nl> 
mmm net / ipv4 / fib_semantics . c <nl> ppp net / ipv4 / fib_semantics . c <nl> void free_fib_info ( struct fib_info * fi ) <nl> # endif <nl> call_rcu (& fi -> rcu , free_fib_info_rcu ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( free_fib_info ); <nl>  <nl> void fib_release_info ( struct fib_info * fi ) <nl> {
mmm drivers / gpu / drm / i915 / intel_lvds . c <nl> ppp drivers / gpu / drm / i915 / intel_lvds . c <nl> static void intel_lvds_mode_set ( struct drm_encoder * encoder , <nl> /** <nl> * Detect the LVDS connection . <nl> * <nl> - * This always returns CONNECTOR_STATUS_CONNECTED . This connector should only have <nl> - * been set up if the LVDS was actually connected anyway . <nl> + * Since LVDS doesn ' t have hotlug , we use the lid as a proxy . Open means <nl> + * connected and closed means disconnected . We also send hotplug events as <nl> + * needed , using lid status notification from the input layer . <nl> */ <nl> static enum drm_connector_status intel_lvds_detect ( struct drm_connector * connector ) <nl> { <nl> - return connector_status_connected ; <nl> + enum drm_connector_status status = connector_status_connected ; <nl> + <nl> + if (! acpi_lid_open ()) <nl> + status = connector_status_disconnected ; <nl> + <nl> + return status ; <nl> } <nl>  <nl> /**
mmm drivers / infiniband / hw / mlx4 / qp . c <nl> ppp drivers / infiniband / hw / mlx4 / qp . c <nl> static int create_qp_common ( struct mlx4_ib_dev * dev , struct ib_pd * pd , <nl> int qpn ; <nl> int err ; <nl> struct ib_qp_cap backup_cap ; <nl> - struct mlx4_ib_sqp * sqp ; <nl> + struct mlx4_ib_sqp * sqp = NULL ; <nl> struct mlx4_ib_qp * qp ; <nl> enum mlx4_ib_qp_type qp_type = ( enum mlx4_ib_qp_type ) init_attr -> qp_type ; <nl> struct mlx4_ib_cq * mcq ; <nl> static int create_qp_common ( struct mlx4_ib_dev * dev , struct ib_pd * pd , <nl> mlx4_db_free ( dev -> dev , & qp -> db ); <nl>  <nl> err : <nl> - if (!* caller_qp ) <nl> + if ( sqp ) <nl> + kfree ( sqp ); <nl> + else if (!* caller_qp ) <nl> kfree ( qp ); <nl> return err ; <nl> }
mmm include / linux / lockdep . h <nl> ppp include / linux / lockdep . h <nl> struct lock_chain { <nl> }; <nl>  <nl> # define MAX_LOCKDEP_KEYS_BITS 11 <nl> -# define MAX_LOCKDEP_KEYS ( 1UL << MAX_LOCKDEP_KEYS_BITS ) <nl> +/* <nl> + * Subtract one because we offset hlock -> class_idx by 1 in order <nl> + * to make 0 mean no class . This avoids overflowing the class_idx <nl> + * bitfield and hitting the BUG in hlock_class (). <nl> + */ <nl> +# define MAX_LOCKDEP_KEYS (( 1UL << MAX_LOCKDEP_KEYS_BITS ) - 1 ) <nl>  <nl> struct held_lock { <nl> /*
mmm drivers / mfd / 88pm800 . c <nl> ppp drivers / mfd / 88pm800 . c <nl> static int device_rtc_init ( struct pm80x_chip * chip , <nl> { <nl> int ret ; <nl>  <nl> - rtc_devs [ 0 ]. platform_data = pdata -> rtc ; <nl> - rtc_devs [ 0 ]. pdata_size = <nl> - pdata -> rtc ? sizeof ( struct pm80x_rtc_pdata ) : 0 ; <nl> + if ( pdata ) { <nl> + rtc_devs [ 0 ]. platform_data = pdata -> rtc ; <nl> + rtc_devs [ 0 ]. pdata_size = <nl> + pdata -> rtc ? sizeof ( struct pm80x_rtc_pdata ) : 0 ; <nl> + } <nl> ret = mfd_add_devices ( chip -> dev , 0 , & rtc_devs [ 0 ], <nl> ARRAY_SIZE ( rtc_devs ), NULL , 0 , NULL ); <nl> if ( ret ) { <nl> static int pm800_probe ( struct i2c_client * client , <nl> goto err_device_init ; <nl> } <nl>  <nl> - if ( pdata -> plat_config ) <nl> + if ( pdata && pdata -> plat_config ) <nl> pdata -> plat_config ( chip , pdata ); <nl>  <nl> return 0 ;
mmm security / smack / smack_lsm . c <nl> ppp security / smack / smack_lsm . c <nl> static int smack_cred_prepare ( struct cred * new , const struct cred * old , <nl> if ( new_tsp == NULL ) <nl> return - ENOMEM ; <nl>  <nl> + new -> security = new_tsp ; <nl> + <nl> rc = smk_copy_rules (& new_tsp -> smk_rules , & old_tsp -> smk_rules , gfp ); <nl> if ( rc != 0 ) <nl> return rc ; <nl> static int smack_cred_prepare ( struct cred * new , const struct cred * old , <nl> if ( rc != 0 ) <nl> return rc ; <nl>  <nl> - new -> security = new_tsp ; <nl> return 0 ; <nl> } <nl> 
mmm drivers / target / loopback / tcm_loop . c <nl> ppp drivers / target / loopback / tcm_loop . c <nl> static void tcm_loop_submission_work ( struct work_struct * work ) <nl> return ; <nl>  <nl> out_done : <nl> + kmem_cache_free ( tcm_loop_cmd_cache , tl_cmd ); <nl> sc -> scsi_done ( sc ); <nl> return ; <nl> }
mmm arch / s390 / kernel / signal . c <nl> ppp arch / s390 / kernel / signal . c <nl> static int setup_rt_frame ( int sig , struct k_sigaction * ka , siginfo_t * info , <nl> } else { <nl> regs -> gprs [ 14 ] = ( unsigned long ) <nl> frame -> retcode | PSW_ADDR_AMODE ; <nl> - err |= __put_user ( S390_SYSCALL_OPCODE | __NR_rt_sigreturn , <nl> - ( u16 __user *)( frame -> retcode )); <nl> + if ( __put_user ( S390_SYSCALL_OPCODE | __NR_rt_sigreturn , <nl> + ( u16 __user *)( frame -> retcode ))) <nl> + goto give_sigsegv ; <nl> } <nl>  <nl> /* Set up backchain . */
mmm drivers / ide / ide - io . c <nl> ppp drivers / ide / ide - io . c <nl> void ide_do_drive_cmd ( ide_drive_t * drive , struct request * rq ) <nl>  <nl> spin_lock_irqsave ( q -> queue_lock , flags ); <nl> __elv_add_request ( q , rq , ELEVATOR_INSERT_FRONT , 0 ); <nl> - blk_start_queueing ( q ); <nl> spin_unlock_irqrestore ( q -> queue_lock , flags ); <nl> } <nl> EXPORT_SYMBOL ( ide_do_drive_cmd );
mmm fs / jbd2 / transaction . c <nl> ppp fs / jbd2 / transaction . c <nl> int jbd2__journal_restart ( handle_t * handle , int nblocks , gfp_t gfp_mask ) <nl>  <nl> rwsem_release (& journal -> j_trans_commit_map , 1 , _THIS_IP_ ); <nl> handle -> h_buffer_credits = nblocks ; <nl> + /* <nl> + * Restore the original nofs context because the journal restart <nl> + * is basically the same thing as journal stop and start . <nl> + * start_this_handle will start a new nofs context . <nl> + */ <nl> + memalloc_nofs_restore ( handle -> saved_alloc_context ); <nl> ret = start_this_handle ( journal , handle , gfp_mask ); <nl> return ret ; <nl> }
mmm drivers / scsi / aacraid / commctrl . c <nl> ppp drivers / scsi / aacraid / commctrl . c <nl> static int aac_send_raw_srb ( struct aac_dev * dev , void __user * arg ) <nl> goto cleanup ; <nl> } <nl>  <nl> - if ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr ))) { <nl> + if (( fibsize < ( sizeof ( struct user_aac_srb ) - sizeof ( struct user_sgentry ))) || <nl> + ( fibsize > ( dev -> max_fib_size - sizeof ( struct aac_fibhdr )))) { <nl> rcode = - EINVAL ; <nl> goto cleanup ; <nl> }
mmm drivers / parisc / lba_pci . c <nl> ppp drivers / parisc / lba_pci . c <nl> lba_legacy_resources ( struct parisc_device * pa_dev , struct lba_device * lba_dev ) <nl> r -> name = " LBA PCI Busses "; <nl> r -> start = lba_num & 0xff ; <nl> r -> end = ( lba_num >> 8 ) & 0xff ; <nl> + r -> flags = IORESOURCE_BUS ; <nl>  <nl> /* Set up local PCI Bus resources - we don ' t need them for <nl> ** Legacy boxes but it ' s nice to see in / proc / iomem .
mmm drivers / pinctrl / freescale / pinctrl - imx . c <nl> ppp drivers / pinctrl / freescale / pinctrl - imx . c <nl> static void imx_pinconf_group_dbg_show ( struct pinctrl_dev * pctldev , <nl> const char * name ; <nl> int i , ret ; <nl>  <nl> - if ( group > pctldev -> num_groups ) <nl> + if ( group >= pctldev -> num_groups ) <nl> return ; <nl>  <nl> seq_puts ( s , "\ n ");
mmm drivers / gpu / drm / i915 / i915_debugfs . c <nl> ppp drivers / gpu / drm / i915 / i915_debugfs . c <nl> static int i915_displayport_test_data_show ( struct seq_file * m , void * data ) <nl> if ( connector -> status == connector_status_connected && <nl> connector -> encoder != NULL ) { <nl> intel_dp = enc_to_intel_dp ( connector -> encoder ); <nl> - seq_printf ( m , "% lx ", intel_dp -> compliance . test_data . edid ); <nl> + if ( intel_dp -> compliance . test_type == <nl> + DP_TEST_LINK_EDID_READ ) <nl> + seq_printf ( m , "% lx ", <nl> + intel_dp -> compliance . test_data . edid ); <nl> } else <nl> seq_puts ( m , " 0 "); <nl> }mmm drivers / gpu / drm / i915 / intel_dp . c <nl> ppp drivers / gpu / drm / i915 / intel_dp . c <nl> static int i915_displayport_test_data_show ( struct seq_file * m , void * data ) <nl> if ( connector -> status == connector_status_connected && <nl> connector -> encoder != NULL ) { <nl> intel_dp = enc_to_intel_dp ( connector -> encoder ); <nl> - seq_printf ( m , "% lx ", intel_dp -> compliance . test_data . edid ); <nl> + if ( intel_dp -> compliance . test_type == <nl> + DP_TEST_LINK_EDID_READ ) <nl> + seq_printf ( m , "% lx ", <nl> + intel_dp -> compliance . test_data . edid ); <nl> } else <nl> seq_puts ( m , " 0 "); <nl> } <nl> static uint8_t intel_dp_autotest_video_pattern ( struct intel_dp * intel_dp ) <nl>  <nl> static uint8_t intel_dp_autotest_edid ( struct intel_dp * intel_dp ) <nl> { <nl> - uint8_t test_result = DP_TEST_NAK ; <nl> + uint8_t test_result = DP_TEST_ACK ; <nl> struct intel_connector * intel_connector = intel_dp -> attached_connector ; <nl> struct drm_connector * connector = & intel_connector -> base ; <nl>  <nl> static uint8_t intel_dp_autotest_edid ( struct intel_dp * intel_dp ) <nl> DRM_DEBUG_KMS (" Failed to write EDID checksum \ n "); <nl>  <nl> test_result = DP_TEST_ACK | DP_TEST_EDID_CHECKSUM_WRITE ; <nl> - intel_dp -> compliance . test_data . edid = INTEL_DP_RESOLUTION_STANDARD ; <nl> + intel_dp -> compliance . test_data . edid = INTEL_DP_RESOLUTION_PREFERRED ; <nl> } <nl>  <nl> /* Set test active flag here so userspace doesn ' t interrupt things */
mmm drivers / net / wireless / iwlwifi / iwl - trans . h <nl> ppp drivers / net / wireless / iwlwifi / iwl - trans . h <nl> struct iwl_trans { <nl> static inline void iwl_trans_configure ( struct iwl_trans * trans , <nl> const struct iwl_trans_config * trans_cfg ) <nl> { <nl> - /* <nl> - * only set the op_mode for the moment . Later on , this function will do <nl> - * more <nl> - */ <nl> trans -> op_mode = trans_cfg -> op_mode ; <nl>  <nl> trans -> ops -> configure ( trans , trans_cfg ); <nl> static inline void iwl_trans_stop_hw ( struct iwl_trans * trans , <nl>  <nl> trans -> ops -> stop_hw ( trans , op_mode_leaving ); <nl>  <nl> + if ( op_mode_leaving ) <nl> + trans -> op_mode = NULL ; <nl> + <nl> trans -> state = IWL_TRANS_NO_FW ; <nl> } <nl> 
mmm fs / btrfs / volumes . c <nl> ppp fs / btrfs / volumes . c <nl> int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> cur_devices -> num_devices --; <nl> cur_devices -> total_devices --; <nl> + /* Update total_devices of the parent fs_devices if it ' s seed */ <nl> + if ( cur_devices != fs_devices ) <nl> + fs_devices -> total_devices --; <nl>  <nl> if ( test_bit ( BTRFS_DEV_STATE_MISSING , & device -> dev_state )) <nl> cur_devices -> missing_devices --;
mmm tools / perf / util / symbol . h <nl> ppp tools / perf / util / symbol . h <nl> struct symbol_conf { <nl> show_hist_headers , <nl> branch_callstack , <nl> has_filter , <nl> - show_ref_callgraph ; <nl> + show_ref_callgraph , <nl> + hide_unresolved ; <nl> const char * vmlinux_name , <nl> * kallsyms_name , <nl> * source_prefix ,mmm tools / perf / util / machine . c <nl> ppp tools / perf / util / machine . c <nl> struct symbol_conf { <nl> show_hist_headers , <nl> branch_callstack , <nl> has_filter , <nl> - show_ref_callgraph ; <nl> + show_ref_callgraph , <nl> + hide_unresolved ; <nl> const char * vmlinux_name , <nl> * kallsyms_name , <nl> * source_prefix , <nl> static int add_callchain_ip ( struct thread * thread , <nl> } <nl> } <nl>  <nl> + if ( symbol_conf . hide_unresolved && al . sym == NULL ) <nl> + return 0 ; <nl> return callchain_cursor_append (& callchain_cursor , al . addr , al . map , al . sym ); <nl> } <nl>  <nl> static int thread__resolve_callchain_sample ( struct thread * thread , <nl> static int unwind_entry ( struct unwind_entry * entry , void * arg ) <nl> { <nl> struct callchain_cursor * cursor = arg ; <nl> + <nl> + if ( symbol_conf . hide_unresolved && entry -> sym == NULL ) <nl> + return 0 ; <nl> return callchain_cursor_append ( cursor , entry -> ip , <nl> entry -> map , entry -> sym ); <nl> }mmm tools / perf / builtin - report . c <nl> ppp tools / perf / builtin - report . c <nl> struct symbol_conf { <nl> show_hist_headers , <nl> branch_callstack , <nl> has_filter , <nl> - show_ref_callgraph ; <nl> + show_ref_callgraph , <nl> + hide_unresolved ; <nl> const char * vmlinux_name , <nl> * kallsyms_name , <nl> * source_prefix , <nl> static int add_callchain_ip ( struct thread * thread , <nl> } <nl> } <nl>  <nl> + if ( symbol_conf . hide_unresolved && al . sym == NULL ) <nl> + return 0 ; <nl> return callchain_cursor_append (& callchain_cursor , al . addr , al . map , al . sym ); <nl> } <nl>  <nl> static int thread__resolve_callchain_sample ( struct thread * thread , <nl> static int unwind_entry ( struct unwind_entry * entry , void * arg ) <nl> { <nl> struct callchain_cursor * cursor = arg ; <nl> + <nl> + if ( symbol_conf . hide_unresolved && entry -> sym == NULL ) <nl> + return 0 ; <nl> return callchain_cursor_append ( cursor , entry -> ip , <nl> entry -> map , entry -> sym ); <nl> } <nl> struct report { <nl> struct perf_tool tool ; <nl> struct perf_session * session ; <nl> bool use_tui , use_gtk , use_stdio ; <nl> - bool hide_unresolved ; <nl> bool dont_use_callchains ; <nl> bool show_full_info ; <nl> bool show_threads ; <nl> static int process_sample_event ( struct perf_tool * tool , <nl> struct hist_entry_iter iter = { <nl> . evsel = evsel , <nl> . sample = sample , <nl> - . hide_unresolved = rep -> hide_unresolved , <nl> + . hide_unresolved = symbol_conf . hide_unresolved , <nl> . add_entry_cb = hist_iter__report_callback , <nl> }; <nl> int ret = 0 ; <nl> static int process_sample_event ( struct perf_tool * tool , <nl> return - 1 ; <nl> } <nl>  <nl> - if ( rep -> hide_unresolved && al . sym == NULL ) <nl> + if ( symbol_conf . hide_unresolved && al . sym == NULL ) <nl> goto out_put ; <nl>  <nl> if ( rep -> cpu_list && ! test_bit ( sample -> cpu , rep -> cpu_bitmap )) <nl> int cmd_report ( int argc , const char ** argv , const char * prefix __maybe_unused ) <nl> OPT_STRING_NOEMPTY (' t ', " field - separator ", & symbol_conf . field_sep , " separator ", <nl> " separator for columns , no spaces will be added between " <nl> " columns '.' is reserved ."), <nl> - OPT_BOOLEAN (' U ', " hide - unresolved ", & report . hide_unresolved , <nl> + OPT_BOOLEAN (' U ', " hide - unresolved ", & symbol_conf . hide_unresolved , <nl> " Only display entries resolved to a symbol "), <nl> OPT_STRING ( 0 , " symfs ", & symbol_conf . symfs , " directory ", <nl> " Look for files with symbols relative to this directory "),
mmm drivers / net / wireless / intel / iwlwifi / fw / dbg . c <nl> ppp drivers / net / wireless / intel / iwlwifi / fw / dbg . c <nl> static struct scatterlist * alloc_sgtable ( int size ) <nl> if ( new_page ) <nl> __free_page ( new_page ); <nl> } <nl> + kfree ( table ); <nl> return NULL ; <nl> } <nl> alloc_size = min_t ( int , size , PAGE_SIZE );
mmm kernel / fork . c <nl> ppp kernel / fork . c <nl> static __latent_entropy struct task_struct * copy_process ( <nl> /* ok , now we should be set up .. */ <nl> p -> pid = pid_nr ( pid ); <nl> if ( clone_flags & CLONE_THREAD ) { <nl> - p -> exit_signal = - 1 ; <nl> p -> group_leader = current -> group_leader ; <nl> p -> tgid = current -> tgid ; <nl> } else { <nl> - if ( clone_flags & CLONE_PARENT ) <nl> - p -> exit_signal = current -> group_leader -> exit_signal ; <nl> - else <nl> - p -> exit_signal = args -> exit_signal ; <nl> p -> group_leader = p ; <nl> p -> tgid = p -> pid ; <nl> } <nl> static __latent_entropy struct task_struct * copy_process ( <nl> if ( clone_flags & ( CLONE_PARENT | CLONE_THREAD )) { <nl> p -> real_parent = current -> real_parent ; <nl> p -> parent_exec_id = current -> parent_exec_id ; <nl> + if ( clone_flags & CLONE_THREAD ) <nl> + p -> exit_signal = - 1 ; <nl> + else <nl> + p -> exit_signal = current -> group_leader -> exit_signal ; <nl> } else { <nl> p -> real_parent = current ; <nl> p -> parent_exec_id = current -> self_exec_id ; <nl> + p -> exit_signal = args -> exit_signal ; <nl> } <nl>  <nl> klp_copy_process ( p );
mmm net / unix / af_unix . c <nl> ppp net / unix / af_unix . c <nl> static struct pernet_operations unix_net_ops = { <nl> static int __init af_unix_init ( void ) <nl> { <nl> int rc = - 1 ; <nl> - struct sk_buff * dummy_skb ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct unix_skb_parms ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct unix_skb_parms ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> rc = proto_register (& unix_proto , 1 ); <nl> if ( rc != 0 ) {
mmm drivers / staging / ozwpan / ozproto . c <nl> ppp drivers / staging / ozwpan / ozproto . c <nl> static void oz_add_farewell ( struct oz_pd * pd , u8 ep_num , u8 index , <nl> return ; <nl> f -> ep_num = ep_num ; <nl> f -> index = index ; <nl> + f -> len = len ; <nl> memcpy ( f -> report , report , len ); <nl> oz_dbg ( ON , " RX : Adding farewell report \ n "); <nl> spin_lock (& g_polling_lock );
mmm drivers / net / wireless / ath / ath6kl / txrx . c <nl> ppp drivers / net / wireless / ath / ath6kl / txrx . c <nl> void ath6kl_rx ( struct htc_target * target , struct htc_packet * packet ) <nl> /* aggregation code will handle the skb */ <nl> return ; <nl> } <nl> - } <nl> + } else if (! is_broadcast_ether_addr ( datap -> h_dest )) <nl> + vif -> net_stats . multicast ++; <nl>  <nl> ath6kl_deliver_frames_to_nw_stack ( vif -> ndev , skb ); <nl> }
mmm arch / powerpc / kvm / 44x_tlb . c <nl> ppp arch / powerpc / kvm / 44x_tlb . c <nl> int kvmppc_mmu_dtlb_index ( struct kvm_vcpu * vcpu , gva_t eaddr ) <nl> return kvmppc_44x_tlb_index ( vcpu , eaddr , vcpu -> arch . pid , as ); <nl> } <nl>  <nl> + void kvmppc_mmu_itlb_miss ( struct kvm_vcpu * vcpu ) <nl> +{ <nl> +} <nl> + <nl> + void kvmppc_mmu_dtlb_miss ( struct kvm_vcpu * vcpu ) <nl> +{ <nl> +} <nl> + <nl> static void kvmppc_44x_shadow_release ( struct kvmppc_vcpu_44x * vcpu_44x , <nl> unsigned int stlb_index ) <nl> {mmm arch / powerpc / kvm / booke . c <nl> ppp arch / powerpc / kvm / booke . c <nl> int kvmppc_mmu_dtlb_index ( struct kvm_vcpu * vcpu , gva_t eaddr ) <nl> return kvmppc_44x_tlb_index ( vcpu , eaddr , vcpu -> arch . pid , as ); <nl> } <nl>  <nl> + void kvmppc_mmu_itlb_miss ( struct kvm_vcpu * vcpu ) <nl> +{ <nl> +} <nl> + <nl> + void kvmppc_mmu_dtlb_miss ( struct kvm_vcpu * vcpu ) <nl> +{ <nl> +} <nl> + <nl> static void kvmppc_44x_shadow_release ( struct kvmppc_vcpu_44x * vcpu_44x , <nl> unsigned int stlb_index ) <nl> { <nl> int kvmppc_handle_exit ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> kvmppc_booke_queue_irqprio ( vcpu , BOOKE_IRQPRIO_DTLB_MISS ); <nl> vcpu -> arch . dear = vcpu -> arch . fault_dear ; <nl> vcpu -> arch . esr = vcpu -> arch . fault_esr ; <nl> + kvmppc_mmu_dtlb_miss ( vcpu ); <nl> kvmppc_account_exit ( vcpu , DTLB_REAL_MISS_EXITS ); <nl> r = RESUME_GUEST ; <nl> break ; <nl> int kvmppc_handle_exit ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> if ( gtlb_index < 0 ) { <nl> /* The guest didn ' t have a mapping for it . */ <nl> kvmppc_booke_queue_irqprio ( vcpu , BOOKE_IRQPRIO_ITLB_MISS ); <nl> + kvmppc_mmu_itlb_miss ( vcpu ); <nl> kvmppc_account_exit ( vcpu , ITLB_REAL_MISS_EXITS ); <nl> break ; <nl> }mmm arch / powerpc / include / asm / kvm_ppc . h <nl> ppp arch / powerpc / include / asm / kvm_ppc . h <nl> int kvmppc_mmu_dtlb_index ( struct kvm_vcpu * vcpu , gva_t eaddr ) <nl> return kvmppc_44x_tlb_index ( vcpu , eaddr , vcpu -> arch . pid , as ); <nl> } <nl>  <nl> + void kvmppc_mmu_itlb_miss ( struct kvm_vcpu * vcpu ) <nl> +{ <nl> +} <nl> + <nl> + void kvmppc_mmu_dtlb_miss ( struct kvm_vcpu * vcpu ) <nl> +{ <nl> +} <nl> + <nl> static void kvmppc_44x_shadow_release ( struct kvmppc_vcpu_44x * vcpu_44x , <nl> unsigned int stlb_index ) <nl> { <nl> int kvmppc_handle_exit ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> kvmppc_booke_queue_irqprio ( vcpu , BOOKE_IRQPRIO_DTLB_MISS ); <nl> vcpu -> arch . dear = vcpu -> arch . fault_dear ; <nl> vcpu -> arch . esr = vcpu -> arch . fault_esr ; <nl> + kvmppc_mmu_dtlb_miss ( vcpu ); <nl> kvmppc_account_exit ( vcpu , DTLB_REAL_MISS_EXITS ); <nl> r = RESUME_GUEST ; <nl> break ; <nl> int kvmppc_handle_exit ( struct kvm_run * run , struct kvm_vcpu * vcpu , <nl> if ( gtlb_index < 0 ) { <nl> /* The guest didn ' t have a mapping for it . */ <nl> kvmppc_booke_queue_irqprio ( vcpu , BOOKE_IRQPRIO_ITLB_MISS ); <nl> + kvmppc_mmu_itlb_miss ( vcpu ); <nl> kvmppc_account_exit ( vcpu , ITLB_REAL_MISS_EXITS ); <nl> break ; <nl> } <nl> extern int kvmppc_mmu_dtlb_index ( struct kvm_vcpu * vcpu , gva_t eaddr ); <nl> extern int kvmppc_mmu_itlb_index ( struct kvm_vcpu * vcpu , gva_t eaddr ); <nl> extern gpa_t kvmppc_mmu_xlate ( struct kvm_vcpu * vcpu , unsigned int gtlb_index , <nl> gva_t eaddr ); <nl> + extern void kvmppc_mmu_dtlb_miss ( struct kvm_vcpu * vcpu ); <nl> + extern void kvmppc_mmu_itlb_miss ( struct kvm_vcpu * vcpu ); <nl>  <nl> extern struct kvm_vcpu * kvmppc_core_vcpu_create ( struct kvm * kvm , <nl> unsigned int id );
mmm drivers / usb / dwc3 / dwc3 - qcom . c <nl> ppp drivers / usb / dwc3 / dwc3 - qcom . c <nl> static int dwc3_qcom_probe ( struct platform_device * pdev ) <nl>  <nl> if ( qcom -> acpi_pdata -> is_urs ) { <nl> qcom -> urs_usb = dwc3_qcom_create_urs_usb_platdev ( dev ); <nl> - if (! qcom -> urs_usb ) { <nl> + if ( IS_ERR_OR_NULL ( qcom -> urs_usb )) { <nl> dev_err ( dev , " failed to create URS USB platdev \ n "); <nl> - return - ENODEV ; <nl> + if (! qcom -> urs_usb ) <nl> + return - ENODEV ; <nl> + else <nl> + return PTR_ERR ( qcom -> urs_usb ); <nl> } <nl> } <nl> }
mmm net / sched / ematch . c <nl> ppp net / sched / ematch . c <nl> int tcf_em_tree_validate ( struct tcf_proto * tp , struct rtattr * rta , <nl> struct tcf_ematch_tree_hdr * tree_hdr ; <nl> struct tcf_ematch * em ; <nl>  <nl> + if (! rta ) { <nl> + memset ( tree , 0 , sizeof (* tree )); <nl> + return 0 ; <nl> + } <nl> + <nl> if ( rtattr_parse_nested ( tb , TCA_EMATCH_TREE_MAX , rta ) < 0 ) <nl> goto errout ; <nl> 
mmm drivers / usb / otg / nop - usb - xceiv . c <nl> ppp drivers / usb / otg / nop - usb - xceiv . c <nl> static int nop_usb_xceiv_probe ( struct platform_device * pdev ) <nl> enum usb_phy_type type = USB_PHY_TYPE_USB2 ; <nl> int err ; <nl> u32 clk_rate = 0 ; <nl> + bool needs_vcc = false ; <nl> + bool needs_reset = false ; <nl>  <nl> nop = devm_kzalloc (& pdev -> dev , sizeof (* nop ), GFP_KERNEL ); <nl> if (! nop ) <nl> static int nop_usb_xceiv_probe ( struct platform_device * pdev ) <nl> if ( of_property_read_u32 ( node , " clock - frequency ", & clk_rate )) <nl> clk_rate = 0 ; <nl>  <nl> + needs_vcc = of_property_read_bool ( node , " vcc - supply "); <nl> + needs_reset = of_property_read_bool ( node , " reset - supply "); <nl> + <nl> } else if ( pdata ) { <nl> type = pdata -> type ; <nl> clk_rate = pdata -> clk_rate ; <nl> + needs_vcc = pdata -> needs_vcc ; <nl> + needs_reset = pdata -> needs_reset ; <nl> } <nl>  <nl> nop -> clk = devm_clk_get (& pdev -> dev , " main_clk "); <nl> static int nop_usb_xceiv_probe ( struct platform_device * pdev ) <nl> if ( IS_ERR ( nop -> vcc )) { <nl> dev_dbg (& pdev -> dev , " Error getting vcc regulator : % ld \ n ", <nl> PTR_ERR ( nop -> vcc )); <nl> + if ( needs_vcc ) <nl> + return - EPROBE_DEFER ; <nl> } <nl>  <nl> nop -> reset = devm_regulator_get (& pdev -> dev , " reset "); <nl> if ( IS_ERR ( nop -> reset )) { <nl> dev_dbg (& pdev -> dev , " Error getting reset regulator : % ld \ n ", <nl> PTR_ERR ( nop -> reset )); <nl> + if ( needs_reset ) <nl> + return - EPROBE_DEFER ; <nl> } <nl>  <nl> nop -> dev = & pdev -> dev ;
mmm fs / nfsd / nfs4proc . c <nl> ppp fs / nfsd / nfs4proc . c <nl> nfsd4_layout_verify ( struct svc_export * exp , unsigned int layout_type ) <nl> return NULL ; <nl> } <nl>  <nl> - if (!( exp -> ex_layout_types & ( 1 << layout_type ))) { <nl> + if ( layout_type >= LAYOUT_TYPE_MAX || <nl> + !( exp -> ex_layout_types & ( 1 << layout_type ))) { <nl> dprintk ("% s : layout type % d not supported \ n ", <nl> __func__ , layout_type ); <nl> return NULL ;
mmm net / mctp / device . c <nl> ppp net / mctp / device . c <nl> void mctp_dev_hold ( struct mctp_dev * mdev ) <nl> void mctp_dev_put ( struct mctp_dev * mdev ) <nl> { <nl> if ( mdev && refcount_dec_and_test (& mdev -> refs )) { <nl> + kfree ( mdev -> addrs ); <nl> dev_put ( mdev -> dev ); <nl> kfree_rcu ( mdev , rcu ); <nl> } <nl> static void mctp_unregister ( struct net_device * dev ) <nl>  <nl> mctp_route_remove_dev ( mdev ); <nl> mctp_neigh_remove_dev ( mdev ); <nl> - kfree ( mdev -> addrs ); <nl>  <nl> mctp_dev_put ( mdev ); <nl> }
mmm drivers / pci / pci - sysfs . c <nl> ppp drivers / pci / pci - sysfs . c <nl> static int pci_create_attr ( struct pci_dev * pdev , int num , int write_combine ) <nl> res_attr -> size = pci_resource_len ( pdev , num ); <nl> res_attr -> private = & pdev -> resource [ num ]; <nl> retval = sysfs_create_bin_file (& pdev -> dev . kobj , res_attr ); <nl> + if ( retval ) <nl> + kfree ( res_attr ); <nl> } else <nl> retval = - ENOMEM ; <nl> 
mmm net / dccp / proto . c <nl> ppp net / dccp / proto . c <nl> int dccp_disconnect ( struct sock * sk , int flags ) <nl> sk -> sk_err = ECONNRESET ; <nl>  <nl> dccp_clear_xmit_timers ( sk ); <nl> + <nl> __skb_queue_purge (& sk -> sk_receive_queue ); <nl> + __skb_queue_purge (& sk -> sk_write_queue ); <nl> if ( sk -> sk_send_head != NULL ) { <nl> __kfree_skb ( sk -> sk_send_head ); <nl> sk -> sk_send_head = NULL ;
mmm fs / jffs2 / write . c <nl> ppp fs / jffs2 / write . c <nl> int jffs2_do_unlink ( struct jffs2_sb_info * c , struct jffs2_inode_info * dir_f , <nl> struct jffs2_full_dirent ** prev = & dir_f -> dents ; <nl> uint32_t nhash = full_name_hash ( name , namelen ); <nl>  <nl> + /* We don ' t actually want to reserve any space , but we do <nl> + want to be holding the alloc_sem when we write to flash */ <nl> + down (& c -> alloc_sem ); <nl> down (& dir_f -> sem ); <nl>  <nl> while ((* prev ) && (* prev )-> nhash <= nhash ) {
mmm fs / cifs / smb2pdu . c <nl> ppp fs / cifs / smb2pdu . c <nl> SMB2_negotiate ( const unsigned int xid , struct cifs_ses * ses ) <nl> } else if ( rsp -> DialectRevision == cpu_to_le16 ( SMB21_PROT_ID )) { <nl> /* ops set to 3 . 0 by default for default so update */ <nl> ses -> server -> ops = & smb21_operations ; <nl> - } else if ( rsp -> DialectRevision == cpu_to_le16 ( SMB311_PROT_ID )) <nl> + ses -> server -> vals = & smb21_values ; <nl> + } else if ( rsp -> DialectRevision == cpu_to_le16 ( SMB311_PROT_ID )) { <nl> ses -> server -> ops = & smb311_operations ; <nl> + ses -> server -> vals = & smb311_values ; <nl> + } <nl> } else if ( le16_to_cpu ( rsp -> DialectRevision ) != <nl> ses -> server -> vals -> protocol_id ) { <nl> /* if requested single dialect ensure returned dialect matched */
mmm kernel / cgroup . c <nl> ppp kernel / cgroup . c <nl> static long cgroup_create ( struct cgroup * parent , struct dentry * dentry , <nl> struct cgroup * cgrp ; <nl> struct cgroup_name * name ; <nl> struct cgroupfs_root * root = parent -> root ; <nl> - int ssid , err = 0 ; <nl> + int ssid , err ; <nl> struct cgroup_subsys * ss ; <nl> struct super_block * sb = root -> sb ; <nl>  <nl> static long cgroup_create ( struct cgroup * parent , struct dentry * dentry , <nl> return - ENOMEM ; <nl>  <nl> name = cgroup_alloc_name ( dentry ); <nl> - if (! name ) <nl> + if (! name ) { <nl> + err = - ENOMEM ; <nl> goto err_free_cgrp ; <nl> + } <nl> rcu_assign_pointer ( cgrp -> name , name ); <nl>  <nl> /* <nl> static long cgroup_create ( struct cgroup * parent , struct dentry * dentry , <nl> * a half - baked cgroup . <nl> */ <nl> cgrp -> id = idr_alloc (& root -> cgroup_idr , NULL , 1 , 0 , GFP_KERNEL ); <nl> - if ( cgrp -> id < 0 ) <nl> + if ( cgrp -> id < 0 ) { <nl> + err = - ENOMEM ; <nl> goto err_free_name ; <nl> + } <nl>  <nl> /* <nl> * Only live parents can have children . Note that the liveliness
mmm net / batman - adv / icmp_socket . c <nl> ppp net / batman - adv / icmp_socket . c <nl> static ssize_t bat_socket_read ( struct file * file , char __user * buf , <nl>  <nl> spin_unlock_bh (& socket_client -> lock ); <nl>  <nl> - error = copy_to_user ( buf , & socket_packet -> icmp_packet , <nl> - socket_packet -> icmp_len ); <nl> + packet_len = min ( count , socket_packet -> icmp_len ); <nl> + error = copy_to_user ( buf , & socket_packet -> icmp_packet , packet_len ); <nl>  <nl> - packet_len = socket_packet -> icmp_len ; <nl> kfree ( socket_packet ); <nl>  <nl> if ( error )
mmm drivers / usb / core / devio . c <nl> ppp drivers / usb / core / devio . c <nl> static int usbdev_open ( struct inode * inode , struct file * file ) <nl> struct dev_state * ps ; <nl> int ret ; <nl>  <nl> + lock_kernel (); <nl> /* Protect against simultaneous removal or release */ <nl> mutex_lock (& usbfs_mutex ); <nl>  <nl> static int usbdev_open ( struct inode * inode , struct file * file ) <nl> if ( ret ) <nl> kfree ( ps ); <nl> mutex_unlock (& usbfs_mutex ); <nl> + unlock_kernel (); <nl> return ret ; <nl> } <nl> 
mmm drivers / scsi / pmcraid . c <nl> ppp drivers / scsi / pmcraid . c <nl> static long pmcraid_ioctl_passthrough ( <nl> pmcraid_err (" couldn ' t build passthrough ioadls \ n "); <nl> goto out_free_buffer ; <nl> } <nl> + } else if ( request_size < 0 ) { <nl> + rc = - EINVAL ; <nl> + goto out_free_buffer ; <nl> } <nl>  <nl> /* If data is being written into the device , copy the data from user
mmm drivers / net / ethernet / emulex / benet / be_cmds . c <nl> ppp drivers / net / ethernet / emulex / benet / be_cmds . c <nl> int be_cmd_if_create ( struct be_adapter * adapter , u32 cap_flags , u32 en_flags , <nl> if (! status ) { <nl> struct be_cmd_resp_if_create * resp = embedded_payload ( wrb ); <nl> * if_handle = le32_to_cpu ( resp -> interface_id ); <nl> + <nl> + /* Hack to retrieve VF ' s pmac - id on BE3 */ <nl> + if ( BE3_chip ( adapter ) && ! be_physfn ( adapter )) <nl> + adapter -> pmac_id [ 0 ] = le32_to_cpu ( resp -> pmac_id ); <nl> } <nl>  <nl> err :
mmm drivers / net / wireless / mwifiex / ie . c <nl> ppp drivers / net / wireless / mwifiex / ie . c <nl> int mwifiex_del_mgmt_ies ( struct mwifiex_private * priv ) <nl> ar_ie , & priv -> assocresp_idx ); <nl>  <nl> done : <nl> + kfree ( gen_ie ); <nl> kfree ( beacon_ie ); <nl> kfree ( pr_ie ); <nl> kfree ( ar_ie );
mmm drivers / i2c / busses / i2c - highlander . c <nl> ppp drivers / i2c / busses / i2c - highlander . c <nl> static int highlander_i2c_smbus_xfer ( struct i2c_adapter * adap , u16 addr , <nl> union i2c_smbus_data * data ) <nl> { <nl> struct highlander_i2c_dev * dev = i2c_get_adapdata ( adap ); <nl> - int read = read_write & I2C_SMBUS_READ ; <nl> u16 tmp ; <nl>  <nl> init_completion (& dev -> cmd_complete ); <nl> static int highlander_i2c_smbus_xfer ( struct i2c_adapter * adap , u16 addr , <nl> highlander_i2c_done ( dev ); <nl>  <nl> /* Set slave address */ <nl> - iowrite16 (( addr << 1 ) | read , dev -> base + SMSMADR ); <nl> + iowrite16 (( addr << 1 ) | read_write , dev -> base + SMSMADR ); <nl>  <nl> highlander_i2c_command ( dev , command , dev -> buf_len ); <nl>  <nl> - if ( read ) <nl> + if ( read_write == I2C_SMBUS_READ ) <nl> return highlander_i2c_read ( dev ); <nl> else <nl> return highlander_i2c_write ( dev );
mmm include / net / inet_connection_sock . h <nl> ppp include / net / inet_connection_sock . h <nl> struct inet_connection_sock { <nl>  <nl> u32 probe_timestamp ; <nl> } icsk_mtup ; <nl> - u32 icsk_ca_priv [ 16 ]; <nl> u32 icsk_user_timeout ; <nl> -# define ICSK_CA_PRIV_SIZE ( 16 * sizeof ( u32 )) <nl> + <nl> + u64 icsk_ca_priv [ 64 / sizeof ( u64 )]; <nl> +# define ICSK_CA_PRIV_SIZE ( 8 * sizeof ( u64 )) <nl> }; <nl>  <nl> # define ICSK_TIME_RETRANS 1 /* Retransmit timer */
mmm drivers / net / ethernet / marvell / mv643xx_eth . c <nl> ppp drivers / net / ethernet / marvell / mv643xx_eth . c <nl> static int mv643xx_eth_shared_of_add_port ( struct platform_device * pdev , <nl> if (! ppdev ) <nl> return - ENOMEM ; <nl> ppdev -> dev . coherent_dma_mask = DMA_BIT_MASK ( 32 ); <nl> + ppdev -> dev . of_node = pnp ; <nl>  <nl> ret = platform_device_add_resources ( ppdev , & res , 1 ); <nl> if ( ret )
mmm drivers / staging / wlags49_h2 / wl_priv . c <nl> ppp drivers / staging / wlags49_h2 / wl_priv . c <nl> int wvlan_uil_put_info ( struct uilreq * urq , struct wl_private * lp ) <nl> ltv_t * pLtv ; <nl> bool_t ltvAllocated = FALSE ; <nl> ENCSTRCT sEncryption ; <nl> + size_t len ; <nl>  <nl> # ifdef USE_WDS <nl> hcf_16 hcfPort = HCF_PORT_0 ; <nl> int wvlan_uil_put_info ( struct uilreq * urq , struct wl_private * lp ) <nl> break ; <nl> case CFG_CNF_OWN_NAME : <nl> memset ( lp -> StationName , 0 , sizeof ( lp -> StationName )); <nl> - memcpy (( void *) lp -> StationName , ( void *)& pLtv -> u . u8 [ 2 ], ( size_t ) pLtv -> u . u16 [ 0 ]); <nl> + len = min_t ( size_t , pLtv -> u . u16 [ 0 ], sizeof ( lp -> StationName )); <nl> + strlcpy ( lp -> StationName , & pLtv -> u . u8 [ 2 ], len ); <nl> pLtv -> u . u16 [ 0 ] = CNV_INT_TO_LITTLE ( pLtv -> u . u16 [ 0 ]); <nl> break ; <nl> case CFG_CNF_LOAD_BALANCING : <nl> int wvlan_set_station_nickname ( struct net_device * dev , <nl> { <nl> struct wl_private * lp = wl_priv ( dev ); <nl> unsigned long flags ; <nl> + size_t len ; <nl> int ret = 0 ; <nl> /*------------------------------------------------------------------------*/ <nl>  <nl> int wvlan_set_station_nickname ( struct net_device * dev , <nl> wl_lock ( lp , & flags ); <nl>  <nl> memset ( lp -> StationName , 0 , sizeof ( lp -> StationName )); <nl> - <nl> - memcpy ( lp -> StationName , extra , wrqu -> data . length ); <nl> + len = min_t ( size_t , wrqu -> data . length , sizeof ( lp -> StationName )); <nl> + strlcpy ( lp -> StationName , extra , len ); <nl>  <nl> /* Commit the adapter parameters */ <nl> wl_apply ( lp );
mmm arch / arm64 / kernel / traps . c <nl> ppp arch / arm64 / kernel / traps . c <nl> static void dump_backtrace ( struct pt_regs * regs , struct task_struct * tsk ) <nl> unsigned long irq_stack_ptr ; <nl> int skip ; <nl>  <nl> + pr_debug ("% s ( regs = % p tsk = % p )\ n ", __func__ , regs , tsk ); <nl> + <nl> + if (! tsk ) <nl> + tsk = current ; <nl> + <nl> /* <nl> * Switching between stacks is valid when tracing current and in <nl> * non - preemptible context . <nl> static void dump_backtrace ( struct pt_regs * regs , struct task_struct * tsk ) <nl> else <nl> irq_stack_ptr = 0 ; <nl>  <nl> - pr_debug ("% s ( regs = % p tsk = % p )\ n ", __func__ , regs , tsk ); <nl> - <nl> - if (! tsk ) <nl> - tsk = current ; <nl> - <nl> if ( tsk == current ) { <nl> frame . fp = ( unsigned long ) __builtin_frame_address ( 0 ); <nl> frame . sp = current_stack_pointer ;mmm arch / arm64 / kernel / stacktrace . c <nl> ppp arch / arm64 / kernel / stacktrace . c <nl> static void dump_backtrace ( struct pt_regs * regs , struct task_struct * tsk ) <nl> unsigned long irq_stack_ptr ; <nl> int skip ; <nl>  <nl> + pr_debug ("% s ( regs = % p tsk = % p )\ n ", __func__ , regs , tsk ); <nl> + <nl> + if (! tsk ) <nl> + tsk = current ; <nl> + <nl> /* <nl> * Switching between stacks is valid when tracing current and in <nl> * non - preemptible context . <nl> static void dump_backtrace ( struct pt_regs * regs , struct task_struct * tsk ) <nl> else <nl> irq_stack_ptr = 0 ; <nl>  <nl> - pr_debug ("% s ( regs = % p tsk = % p )\ n ", __func__ , regs , tsk ); <nl> - <nl> - if (! tsk ) <nl> - tsk = current ; <nl> - <nl> if ( tsk == current ) { <nl> frame . fp = ( unsigned long ) __builtin_frame_address ( 0 ); <nl> frame . sp = current_stack_pointer ; <nl> int notrace unwind_frame ( struct task_struct * tsk , struct stackframe * frame ) <nl> unsigned long fp = frame -> fp ; <nl> unsigned long irq_stack_ptr ; <nl>  <nl> + if (! tsk ) <nl> + tsk = current ; <nl> + <nl> /* <nl> * Switching between stacks is valid when tracing current and in <nl> * non - preemptible context . <nl> int notrace unwind_frame ( struct task_struct * tsk , struct stackframe * frame ) <nl> frame -> pc = READ_ONCE_NOCHECK (*( unsigned long *)( fp + 8 )); <nl>  <nl> # ifdef CONFIG_FUNCTION_GRAPH_TRACER <nl> - if ( tsk && tsk -> ret_stack && <nl> + if ( tsk -> ret_stack && <nl> ( frame -> pc == ( unsigned long ) return_to_handler )) { <nl> /* <nl> * This is a case where function graph tracer has
mmm kernel / time / tick - sched . c <nl> ppp kernel / time / tick - sched . c <nl> static void tick_nohz_handler ( struct clock_event_device * dev ) <nl> tick_sched_do_timer ( now ); <nl> tick_sched_handle ( ts , regs ); <nl>  <nl> + /* No need to reprogram if we are running tickless */ <nl> + if ( unlikely ( ts -> tick_stopped )) <nl> + return ; <nl> + <nl> while ( tick_nohz_reprogram ( ts , now )) { <nl> now = ktime_get (); <nl> tick_do_update_jiffies64 ( now );
mmm drivers / gpu / drm / i915 / i915_irq . c <nl> ppp drivers / gpu / drm / i915 / i915_irq . c <nl> static inline void intel_hpd_irq_handler ( struct drm_device * dev , <nl> const u32 * hpd ) <nl> { <nl> drm_i915_private_t * dev_priv = dev -> dev_private ; <nl> - unsigned long irqflags ; <nl> int i ; <nl> bool storm_detected = false ; <nl>  <nl> if (! hotplug_trigger ) <nl> return ; <nl>  <nl> - spin_lock_irqsave (& dev_priv -> irq_lock , irqflags ); <nl> - <nl> + spin_lock (& dev_priv -> irq_lock ); <nl> for ( i = 1 ; i < HPD_NUM_PINS ; i ++) { <nl>  <nl> if (!( hpd [ i ] & hotplug_trigger ) || <nl> static inline void intel_hpd_irq_handler ( struct drm_device * dev , <nl> } <nl> } <nl>  <nl> - spin_unlock_irqrestore (& dev_priv -> irq_lock , irqflags ); <nl> - <nl> if ( storm_detected ) <nl> dev_priv -> display . hpd_irq_setup ( dev ); <nl> + spin_unlock (& dev_priv -> irq_lock ); <nl>  <nl> queue_work ( dev_priv -> wq , <nl> & dev_priv -> hotplug_work ); <nl> static void i915_hpd_irq_setup ( struct drm_device * dev ) <nl> struct intel_encoder * intel_encoder ; <nl> u32 hotplug_en ; <nl>  <nl> + assert_spin_locked (& dev_priv -> irq_lock ); <nl> + <nl> if ( I915_HAS_HOTPLUG ( dev )) { <nl> hotplug_en = I915_READ ( PORT_HOTPLUG_EN ); <nl> hotplug_en &= ~ HOTPLUG_INT_EN_MASK ; <nl> void intel_hpd_init ( struct drm_device * dev ) <nl> struct drm_i915_private * dev_priv = dev -> dev_private ; <nl> struct drm_mode_config * mode_config = & dev -> mode_config ; <nl> struct drm_connector * connector ; <nl> + unsigned long irqflags ; <nl> int i ; <nl>  <nl> for ( i = 1 ; i < HPD_NUM_PINS ; i ++) { <nl> void intel_hpd_init ( struct drm_device * dev ) <nl> if (! connector -> polled && I915_HAS_HOTPLUG ( dev ) && intel_connector -> encoder -> hpd_pin > HPD_NONE ) <nl> connector -> polled = DRM_CONNECTOR_POLL_HPD ; <nl> } <nl> + <nl> + /* Interrupt setup is already guaranteed to be single - threaded , this is <nl> + * just to make the assert_spin_locked checks happy . */ <nl> + spin_lock_irqsave (& dev_priv -> irq_lock , irqflags ); <nl> if ( dev_priv -> display . hpd_irq_setup ) <nl> dev_priv -> display . hpd_irq_setup ( dev ); <nl> + spin_unlock_irqrestore (& dev_priv -> irq_lock , irqflags ); <nl> }
mmm drivers / staging / vt6655 / device_main . c <nl> ppp drivers / staging / vt6655 / device_main . c <nl> static int device_rx_srv ( struct vnt_private * pDevice , unsigned int uIdx ) <nl> pRD = pRD -> next ) { <nl> if ( works ++ > 15 ) <nl> break ; <nl> + <nl> + if (! pRD -> pRDInfo -> skb ) <nl> + break ; <nl> + <nl> if ( vnt_receive_frame ( pDevice , pRD )) { <nl> if (! device_alloc_rx_buf ( pDevice , pRD )) { <nl> dev_err (& pDevice -> pcid -> dev ,
mmm net / sched / sch_api . c <nl> ppp net / sched / sch_api . c <nl> int tc_classify ( struct sk_buff * skb , struct tcf_proto * tp , <nl> tp = otp ; <nl>  <nl> if ( verd ++ >= MAX_REC_LOOP ) { <nl> - printk (" rule prio % u protocol % 02x reclassify loop , " <nl> - " packet dropped \ n ", <nl> - tp -> prio & 0xffff , ntohs ( tp -> protocol )); <nl> + if ( net_ratelimit ()) <nl> + printk ( KERN_NOTICE <nl> + "% s : packet reclassify loop " <nl> + " rule prio % u protocol % 02x \ n ", <nl> + tp -> q -> ops -> id , <nl> + tp -> prio & 0xffff , ntohs ( tp -> protocol )); <nl> return TC_ACT_SHOT ; <nl> } <nl> skb -> tc_verd = SET_TC_VERD ( skb -> tc_verd , verd );
mmm drivers / infiniband / hw / cxgb4 / mem . c <nl> ppp drivers / infiniband / hw / cxgb4 / mem . c <nl> static int _c4iw_write_mem_inline ( struct c4iw_rdev * rdev , u32 addr , u32 len , <nl> if ( i == ( num_wqe - 1 )) { <nl> req -> wr . wr_hi = cpu_to_be32 ( FW_WR_OP_V ( FW_ULPTX_WR ) | <nl> FW_WR_COMPL_F ); <nl> - req -> wr . wr_lo = ( __force __be64 )& wr_wait ; <nl> + req -> wr . wr_lo = ( __force __be64 )( unsigned long )& wr_wait ; <nl> } else <nl> req -> wr . wr_hi = cpu_to_be32 ( FW_WR_OP_V ( FW_ULPTX_WR )); <nl> req -> wr . wr_mid = cpu_to_be32 (
mmm include / linux / mmzone . h <nl> ppp include / linux / mmzone . h <nl> typedef struct pglist_data { <nl> # include < linux / memory_hotplug . h > <nl>  <nl> extern struct mutex zonelists_mutex ; <nl> - void get_zone_counts ( unsigned long * active , unsigned long * inactive , <nl> - unsigned long * free ); <nl> void build_all_zonelists ( void * data ); <nl> void wakeup_kswapd ( struct zone * zone , int order ); <nl> int zone_watermark_ok ( struct zone * z , int order , unsigned long mark ,
mmm crypto / asymmetric_keys / x509_cert_parser . c <nl> ppp crypto / asymmetric_keys / x509_cert_parser . c <nl> int x509_note_signature ( void * context , size_t hdrlen , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( strcmp ( ctx -> cert -> sig -> pkey_algo , " rsa ") == 0 ) { <nl> + /* Discard the BIT STRING metadata */ <nl> + if ( vlen < 1 || *( const u8 *) value != 0 ) <nl> + return - EBADMSG ; <nl> + <nl> + value ++; <nl> + vlen --; <nl> + } <nl> + <nl> ctx -> cert -> raw_sig = value ; <nl> ctx -> cert -> raw_sig_size = vlen ; <nl> return 0 ;
mmm drivers / mfd / tc6393xb . c <nl> ppp drivers / mfd / tc6393xb . c <nl> static int tc6393xb_resume ( struct platform_device * dev ) <nl> int ret ; <nl> int i ; <nl>  <nl> - clk_prepare_enable ( tc6393xb -> clk ); <nl> + ret = clk_prepare_enable ( tc6393xb -> clk ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = tcpd -> resume ( dev ); <nl> if ( ret )
mmm net / ipv4 / tcp . c <nl> ppp net / ipv4 / tcp . c <nl> __setup (" thash_entries =", set_thash_entries ); <nl>  <nl> static void __init tcp_init_mem ( void ) <nl> { <nl> - unsigned long limit = nr_free_buffer_pages () / 8 ; <nl> + unsigned long limit = nr_free_buffer_pages () / 16 ; <nl> + <nl> limit = max ( limit , 128UL ); <nl> - sysctl_tcp_mem [ 0 ] = limit / 4 * 3 ; <nl> - sysctl_tcp_mem [ 1 ] = limit ; <nl> - sysctl_tcp_mem [ 2 ] = sysctl_tcp_mem [ 0 ] * 2 ; <nl> + sysctl_tcp_mem [ 0 ] = limit / 4 * 3 ; /* 4 . 68 % */ <nl> + sysctl_tcp_mem [ 1 ] = limit ; /* 6 . 25 % */ <nl> + sysctl_tcp_mem [ 2 ] = sysctl_tcp_mem [ 0 ] * 2 ; /* 9 . 37 % */ <nl> } <nl>  <nl> void __init tcp_init ( void )
mmm drivers / gpu / drm / bridge / synopsys / dw - hdmi . c <nl> ppp drivers / gpu / drm / bridge / synopsys / dw - hdmi . c <nl> int dw_hdmi_probe ( struct platform_device * pdev , <nl> const struct dw_hdmi_plat_data * plat_data ) <nl> { <nl> struct dw_hdmi * hdmi ; <nl> - int ret ; <nl>  <nl> hdmi = __dw_hdmi_probe ( pdev , plat_data ); <nl> if ( IS_ERR ( hdmi )) <nl> return PTR_ERR ( hdmi ); <nl>  <nl> - ret = drm_bridge_add (& hdmi -> bridge ); <nl> - if ( ret < 0 ) { <nl> - __dw_hdmi_remove ( hdmi ); <nl> - return ret ; <nl> - } <nl> + drm_bridge_add (& hdmi -> bridge ); <nl>  <nl> return 0 ; <nl> }
mmm drivers / md / md . c <nl> ppp drivers / md / md . c <nl> static int get_bitmap_file ( struct mddev * mddev , void __user * arg ) <nl> char * ptr ; <nl> int err ; <nl>  <nl> - file = kmalloc ( sizeof (* file ), GFP_NOIO ); <nl> + file = kzalloc ( sizeof (* file ), GFP_NOIO ); <nl> if (! file ) <nl> return - ENOMEM ; <nl> 
mmm net / sctp / associola . c <nl> ppp net / sctp / associola . c <nl> struct sctp_chunk * sctp_assoc_lookup_asconf_ack ( <nl> * ack chunk whose serial number matches that of the request . <nl> */ <nl> list_for_each_entry ( ack , & asoc -> asconf_ack_list , transmitted_list ) { <nl> + if ( sctp_chunk_pending ( ack )) <nl> + continue ; <nl> if ( ack -> subh . addip_hdr -> serial == serial ) { <nl> sctp_chunk_hold ( ack ); <nl> return ack ;mmm include / net / sctp / sctp . h <nl> ppp include / net / sctp / sctp . h <nl> struct sctp_chunk * sctp_assoc_lookup_asconf_ack ( <nl> * ack chunk whose serial number matches that of the request . <nl> */ <nl> list_for_each_entry ( ack , & asoc -> asconf_ack_list , transmitted_list ) { <nl> + if ( sctp_chunk_pending ( ack )) <nl> + continue ; <nl> if ( ack -> subh . addip_hdr -> serial == serial ) { <nl> sctp_chunk_hold ( ack ); <nl> return ack ; <nl> static inline void sctp_assoc_pending_pmtu ( struct sock * sk , struct sctp_associat <nl> asoc -> pmtu_pending = 0 ; <nl> } <nl>  <nl> + static inline bool sctp_chunk_pending ( const struct sctp_chunk * chunk ) <nl> +{ <nl> + return ! list_empty (& chunk -> list ); <nl> +} <nl> + <nl> /* Walk through a list of TLV parameters . Don ' t trust the <nl> * individual parameter lengths and instead depend on <nl> * the chunk length to indicate when to stop . Make sure
mmm arch / parisc / kernel / perf . c <nl> ppp arch / parisc / kernel / perf . c <nl> # include < linux / init . h > <nl> # include < linux / proc_fs . h > <nl> # include < linux / miscdevice . h > <nl> +# include < linux / smp_lock . h > <nl> # include < linux / spinlock . h > <nl>  <nl> # include < asm / uaccess . h > <nl> printk (" Preparing to start counters \ n "); <nl> */ <nl> static int perf_open ( struct inode * inode , struct file * file ) <nl> { <nl> + lock_kernel (); <nl> spin_lock (& perf_lock ); <nl> if ( perf_enabled ) { <nl> spin_unlock (& perf_lock ); <nl> + unlock_kernel (); <nl> return - EBUSY ; <nl> } <nl> perf_enabled = 1 ; <nl> spin_unlock (& perf_lock ); <nl> + unlock_kernel (); <nl>  <nl> return 0 ; <nl> }
mmm kernel / cgroup . c <nl> ppp kernel / cgroup . c <nl> static long cgroup_create ( struct cgroup * parent , struct dentry * dentry , <nl>  <nl> mutex_lock (& cgroup_mutex ); <nl>  <nl> - cgrp -> flags = 0 ; <nl> INIT_LIST_HEAD (& cgrp -> sibling ); <nl> INIT_LIST_HEAD (& cgrp -> children ); <nl> INIT_LIST_HEAD (& cgrp -> css_sets ); <nl> static long cgroup_create ( struct cgroup * parent , struct dentry * dentry , <nl> cgrp -> root = parent -> root ; <nl> cgrp -> top_cgroup = parent -> top_cgroup ; <nl>  <nl> + if ( notify_on_release ( parent )) <nl> + set_bit ( CGRP_NOTIFY_ON_RELEASE , & cgrp -> flags ); <nl> + <nl> for_each_subsys ( root , ss ) { <nl> struct cgroup_subsys_state * css = ss -> create ( ss , cgrp ); <nl> if ( IS_ERR ( css )) {
mmm kernel / audit . c <nl> ppp kernel / audit . c <nl> static void audit_log_feature_change ( int which , u32 old_feature , u32 new_feature <nl> { <nl> struct audit_buffer * ab ; <nl>  <nl> + if ( audit_enabled == AUDIT_OFF ) <nl> + return ; <nl> + <nl> ab = audit_log_start ( NULL , GFP_KERNEL , AUDIT_FEATURE_CHANGE ); <nl> audit_log_format ( ab , " feature =% s old =% d new =% d old_lock =% d new_lock =% d res =% d ", <nl> audit_feature_names [ which ], !! old_feature , !! new_feature ,
mmm drivers / block / aoe / aoenet . c <nl> ppp drivers / block / aoe / aoenet . c <nl> aoenet_rcv ( struct sk_buff * skb , struct net_device * ifp , struct packet_type * pt , <nl> aoecmd_cfg_rsp ( skb ); <nl> break ; <nl> default : <nl> + if ( h -> cmd >= AOECMD_VEND_MIN ) <nl> + break ; /* don ' t complain about vendor commands */ <nl> printk ( KERN_INFO " aoe : unknown cmd % d \ n ", h -> cmd ); <nl> } <nl> exit :mmm drivers / block / aoe / aoe . h <nl> ppp drivers / block / aoe / aoe . h <nl> aoenet_rcv ( struct sk_buff * skb , struct net_device * ifp , struct packet_type * pt , <nl> aoecmd_cfg_rsp ( skb ); <nl> break ; <nl> default : <nl> + if ( h -> cmd >= AOECMD_VEND_MIN ) <nl> + break ; /* don ' t complain about vendor commands */ <nl> printk ( KERN_INFO " aoe : unknown cmd % d \ n ", h -> cmd ); <nl> } <nl> exit : <nl> enum { <nl> AOECMD_ATA , <nl> AOECMD_CFG , <nl> + AOECMD_VEND_MIN = 0xf0 , <nl>  <nl> AOEFL_RSP = ( 1 << 3 ), <nl> AOEFL_ERR = ( 1 << 2 ),
mmm kernel / irq / irqdomain . c <nl> ppp kernel / irq / irqdomain . c <nl> static inline void debugfs_remove_domain_dir ( struct irq_domain * d ) { } <nl> # endif <nl>  <nl> const struct fwnode_operations irqchip_fwnode_ops ; <nl> + EXPORT_SYMBOL_GPL ( irqchip_fwnode_ops ); <nl>  <nl> /** <nl> * irq_domain_alloc_fwnode - Allocate a fwnode_handle suitable for
mmm net / bluetooth / hci_event . c <nl> ppp net / bluetooth / hci_event . c <nl> static inline void hci_pin_code_request_evt ( struct hci_dev * hdev , struct sk_buff <nl> hci_dev_lock ( hdev ); <nl>  <nl> conn = hci_conn_hash_lookup_ba ( hdev , ACL_LINK , & ev -> bdaddr ); <nl> - if ( conn && conn -> state == BT_CONNECTED ) { <nl> + if (! conn ) <nl> + goto unlock ; <nl> + <nl> + if ( conn -> state == BT_CONNECTED ) { <nl> hci_conn_hold ( conn ); <nl> conn -> disc_timeout = HCI_PAIRING_TIMEOUT ; <nl> hci_conn_put ( conn ); <nl> static inline void hci_pin_code_request_evt ( struct hci_dev * hdev , struct sk_buff <nl> mgmt_pin_code_request ( hdev -> id , & ev -> bdaddr , secure ); <nl> } <nl>  <nl> + unlock : <nl> hci_dev_unlock ( hdev ); <nl> } <nl> 
mmm drivers / net / wireless / marvell / mwifiex / scan . c <nl> ppp drivers / net / wireless / marvell / mwifiex / scan . c <nl> mwifiex_cmd_append_vsie_tlv ( struct mwifiex_private * priv , <nl> vs_param_set -> header . len = <nl> cpu_to_le16 (((( u16 ) priv -> vs_ie [ id ]. ie [ 1 ]) <nl> & 0x00FF ) + 2 ); <nl> + if ( le16_to_cpu ( vs_param_set -> header . len ) > <nl> + MWIFIEX_MAX_VSIE_LEN ) { <nl> + mwifiex_dbg ( priv -> adapter , ERROR , <nl> + " Invalid param length !\ n "); <nl> + break ; <nl> + } <nl> + <nl> memcpy ( vs_param_set -> ie , priv -> vs_ie [ id ]. ie , <nl> le16_to_cpu ( vs_param_set -> header . len )); <nl> * buffer += le16_to_cpu ( vs_param_set -> header . len ) +
mmm net / bridge / netfilter / ebtables . c <nl> ppp net / bridge / netfilter / ebtables . c <nl> static int ebt_size_mwt ( struct compat_ebt_entry_mwt * match32 , <nl> if ( match_kern ) <nl> match_kern -> match_size = ret ; <nl>  <nl> - WARN_ON ( type == EBT_COMPAT_TARGET && size_left ); <nl> + if ( WARN_ON ( type == EBT_COMPAT_TARGET && size_left )) <nl> + return - EINVAL ; <nl> + <nl> match32 = ( struct compat_ebt_entry_mwt *) buf ; <nl> } <nl>  <nl> static int size_entry_mwt ( struct ebt_entry * entry , const unsigned char * base , <nl> * <nl> * offsets are relative to beginning of struct ebt_entry ( i . e ., 0 ). <nl> */ <nl> + for ( i = 0 ; i < 4 ; ++ i ) { <nl> + if ( offsets [ i ] >= * total ) <nl> + return - EINVAL ; <nl> + if ( i == 0 ) <nl> + continue ; <nl> + if ( offsets [ i - 1 ] > offsets [ i ]) <nl> + return - EINVAL ; <nl> + } <nl> + <nl> for ( i = 0 , j = 1 ; j < 4 ; j ++, i ++) { <nl> struct compat_ebt_entry_mwt * match32 ; <nl> unsigned int size ;
mmm arch / arm / mach - integrator / pci_v3 . c <nl> ppp arch / arm / mach - integrator / pci_v3 . c <nl> # include < linux / spinlock . h > <nl> # include < linux / init . h > <nl> # include < linux / io . h > <nl> -# include < video / vga . h > <nl>  <nl> # include < mach / hardware . h > <nl> # include < mach / platform . h > <nl> void __init pci_v3_preinit ( void ) <nl>  <nl> pcibios_min_io = 0x6000 ; <nl> pcibios_min_mem = 0x00100000 ; <nl> - vga_base = PCI_MEMORY_VADDR ; <nl>  <nl> /* <nl> * Hook in our fault handler for PCI errorsmmm arch / arm / mach - integrator / integrator_ap . c <nl> ppp arch / arm / mach - integrator / integrator_ap . c <nl> # include < linux / spinlock . h > <nl> # include < linux / init . h > <nl> # include < linux / io . h > <nl> -# include < video / vga . h > <nl>  <nl> # include < mach / hardware . h > <nl> # include < mach / platform . h > <nl> void __init pci_v3_preinit ( void ) <nl>  <nl> pcibios_min_io = 0x6000 ; <nl> pcibios_min_mem = 0x00100000 ; <nl> - vga_base = PCI_MEMORY_VADDR ; <nl>  <nl> /* <nl> * Hook in our fault handler for PCI errors <nl> # include < linux / interrupt . h > <nl> # include < linux / io . h > <nl> # include < linux / mtd / physmap . h > <nl> +# include < video / vga . h > <nl>  <nl> # include < mach / hardware . h > <nl> # include < mach / platform . h > <nl> static struct map_desc ap_io_desc [] __initdata = { <nl> static void __init ap_map_io ( void ) <nl> { <nl> iotable_init ( ap_io_desc , ARRAY_SIZE ( ap_io_desc )); <nl> + vga_base = PCI_MEMORY_VADDR ; <nl> } <nl>  <nl> # define INTEGRATOR_SC_VALID_INT 0x003fffff
mmm drivers / usb / misc / iowarrior . c <nl> ppp drivers / usb / misc / iowarrior . c <nl> static int iowarrior_probe ( struct usb_interface * interface , <nl> iface_desc = interface -> cur_altsetting ; <nl> dev -> product_id = le16_to_cpu ( udev -> descriptor . idProduct ); <nl>  <nl> - if ( iface_desc -> desc . bNumEndpoints < 1 ) { <nl> - dev_err (& interface -> dev , " Invalid number of endpoints \ n "); <nl> - retval = - EINVAL ; <nl> - goto error ; <nl> - } <nl> - <nl> /* set up the endpoint information */ <nl> for ( i = 0 ; i < iface_desc -> desc . bNumEndpoints ; ++ i ) { <nl> endpoint = & iface_desc -> endpoint [ i ]. desc ; <nl> static int iowarrior_probe ( struct usb_interface * interface , <nl> /* this one will match for the IOWarrior56 only */ <nl> dev -> int_out_endpoint = endpoint ; <nl> } <nl> + <nl> + if (! dev -> int_in_endpoint ) { <nl> + dev_err (& interface -> dev , " no interrupt - in endpoint found \ n "); <nl> + retval = - ENODEV ; <nl> + goto error ; <nl> + } <nl> + <nl> /* we have to check the report_size often , so remember it in the endianness suitable for our machine */ <nl> dev -> report_size = usb_endpoint_maxp ( dev -> int_in_endpoint ); <nl> if (( dev -> interface -> cur_altsetting -> desc . bInterfaceNumber == 0 ) &&
mmm drivers / media / video / cx88 / cx88 - dvb . c <nl> ppp drivers / media / video / cx88 / cx88 - dvb . c <nl> static const u8 samsung_smt_7020_inittab [] = { <nl> static int samsung_smt_7020_tuner_set_params ( struct dvb_frontend * fe , <nl> struct dvb_frontend_parameters * params ) <nl> { <nl> + struct dtv_frontend_properties * c = & fe -> dtv_property_cache ; <nl> struct cx8802_dev * dev = fe -> dvb -> priv ; <nl> u8 buf [ 4 ]; <nl> u32 div ; <nl> static int samsung_smt_7020_tuner_set_params ( struct dvb_frontend * fe , <nl> . buf = buf , <nl> . len = sizeof ( buf ) }; <nl>  <nl> - div = params -> frequency / 125 ; <nl> + div = c -> frequency / 125 ; <nl>  <nl> buf [ 0 ] = ( div >> 8 ) & 0x7f ; <nl> buf [ 1 ] = div & 0xff ; <nl> buf [ 2 ] = 0x84 ; /* 0xC4 */ <nl> buf [ 3 ] = 0x00 ; <nl>  <nl> - if ( params -> frequency < 1500000 ) <nl> + if ( c -> frequency < 1500000 ) <nl> buf [ 3 ] |= 0x10 ; <nl>  <nl> if ( fe -> ops . i2c_gate_ctrl )
mmm arch / x86 / kvm / vmx . c <nl> ppp arch / x86 / kvm / vmx . c <nl> static inline void nested_release_vmcs12 ( struct vcpu_vmx * vmx ) <nl> */ <nl> static void free_nested ( struct vcpu_vmx * vmx ) <nl> { <nl> - if (! vmx -> nested . vmxon ) <nl> + if (! vmx -> nested . vmxon && ! vmx -> nested . smm . vmxon ) <nl> return ; <nl>  <nl> vmx -> nested . vmxon = false ; <nl> + vmx -> nested . smm . vmxon = false ; <nl> free_vpid ( vmx -> nested . vpid02 ); <nl> vmx -> nested . posted_intr_nv = - 1 ; <nl> vmx -> nested . current_vmptr = - 1ull ;
mmm drivers / phy / phy - core . c <nl> ppp drivers / phy / phy - core . c <nl> static struct phy * _of_phy_get ( struct device_node * np , int index ) <nl> if ( ret ) <nl> return ERR_PTR (- ENODEV ); <nl>  <nl> + /* This phy type handled by the usb - phy subsystem for now */ <nl> + if ( of_device_is_compatible ( args . np , " usb - nop - xceiv ")) <nl> + return ERR_PTR (- ENODEV ); <nl> + <nl> mutex_lock (& phy_provider_mutex ); <nl> phy_provider = of_phy_provider_lookup ( args . np ); <nl> if ( IS_ERR ( phy_provider ) || ! try_module_get ( phy_provider -> owner )) {
mmm drivers / net / wireless / realtek / rtlwifi / pci . c <nl> ppp drivers / net / wireless / realtek / rtlwifi / pci . c <nl> int rtl_pci_reset_trx_ring ( struct ieee80211_hw * hw ) <nl> dev_kfree_skb_irq ( skb ); <nl> ring -> idx = ( ring -> idx + 1 ) % ring -> entries ; <nl> } <nl> + <nl> + if ( rtlpriv -> use_new_trx_flow ) { <nl> + rtlpci -> tx_ring [ i ]. cur_tx_rp = 0 ; <nl> + rtlpci -> tx_ring [ i ]. cur_tx_wp = 0 ; <nl> + } <nl> + <nl> ring -> idx = 0 ; <nl> + ring -> entries = rtlpci -> txringcount [ i ]; <nl> } <nl> } <nl> spin_unlock_irqrestore (& rtlpriv -> locks . irq_th_lock , flags );
mmm drivers / mtd / nand / pxa3xx_nand . c <nl> ppp drivers / mtd / nand / pxa3xx_nand . c <nl> static int pxa3xx_nand_probe ( struct platform_device * pdev ) <nl> info -> variant = pxa3xx_nand_get_variant ( pdev ); <nl> probe_success = 0 ; <nl> for ( cs = 0 ; cs < pdata -> num_cs ; cs ++) { <nl> + struct mtd_info * mtd = info -> host [ cs ]-> mtd ; <nl> info -> cs = cs ; <nl> - ret = pxa3xx_nand_scan ( info -> host [ cs ]-> mtd ); <nl> + ret = pxa3xx_nand_scan ( mtd ); <nl> if ( ret ) { <nl> dev_warn (& pdev -> dev , " failed to scan nand at cs % d \ n ", <nl> cs ); <nl> static int pxa3xx_nand_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> ppdata . of_node = pdev -> dev . of_node ; <nl> - ret = mtd_device_parse_register ( info -> host [ cs ]-> mtd , NULL , <nl> + ret = mtd_device_parse_register ( mtd , NULL , <nl> & ppdata , pdata -> parts [ cs ], <nl> pdata -> nr_parts [ cs ]); <nl> if (! ret )
mmm net / tipc / link . c <nl> ppp net / tipc / link . c <nl> void tipc_link_reset ( struct tipc_link * l ) <nl> int tipc_link_xmit ( struct tipc_link * l , struct sk_buff_head * list , <nl> struct sk_buff_head * xmitq ) <nl> { <nl> - struct tipc_msg * hdr = buf_msg ( skb_peek ( list )); <nl> struct sk_buff_head * backlogq = & l -> backlogq ; <nl> struct sk_buff_head * transmq = & l -> transmq ; <nl> struct sk_buff * skb , * _skb ; <nl> int tipc_link_xmit ( struct tipc_link * l , struct sk_buff_head * list , <nl> u16 ack = l -> rcv_nxt - 1 ; <nl> u16 seqno = l -> snd_nxt ; <nl> int pkt_cnt = skb_queue_len ( list ); <nl> - int imp = msg_importance ( hdr ); <nl> unsigned int mss = tipc_link_mss ( l ); <nl> unsigned int cwin = l -> window ; <nl> unsigned int mtu = l -> mtu ; <nl> + struct tipc_msg * hdr ; <nl> bool new_bundle ; <nl> int rc = 0 ; <nl> + int imp ; <nl> + <nl> + if ( pkt_cnt <= 0 ) <nl> + return 0 ; <nl>  <nl> + hdr = buf_msg ( skb_peek ( list )); <nl> if ( unlikely ( msg_size ( hdr ) > mtu )) { <nl> pr_warn (" Too large msg , purging xmit list % d % d % d % d % d !\ n ", <nl> skb_queue_len ( list ), msg_user ( hdr ), <nl> int tipc_link_xmit ( struct tipc_link * l , struct sk_buff_head * list , <nl> return - EMSGSIZE ; <nl> } <nl>  <nl> + imp = msg_importance ( hdr ); <nl> /* Allow oversubscription of one data msg per source at congestion */ <nl> if ( unlikely ( l -> backlog [ imp ]. len >= l -> backlog [ imp ]. limit )) { <nl> if ( imp == TIPC_SYSTEM_IMPORTANCE ) {
mmm drivers / platform / x86 / fujitsu - laptop . c <nl> ppp drivers / platform / x86 / fujitsu - laptop . c <nl> static int acpi_fujitsu_laptop_leds_register ( struct acpi_device * device ) <nl> if ( call_fext_func ( device , <nl> FUNC_LEDS , 0x0 , 0x0 , 0x0 ) & LOGOLAMP_POWERON ) { <nl> led = devm_kzalloc (& device -> dev , sizeof (* led ), GFP_KERNEL ); <nl> + if (! led ) <nl> + return - ENOMEM ; <nl> + <nl> led -> name = " fujitsu :: logolamp "; <nl> led -> brightness_set_blocking = logolamp_set ; <nl> led -> brightness_get = logolamp_get ; <nl> static int acpi_fujitsu_laptop_leds_register ( struct acpi_device * device ) <nl> FUNC_LEDS , 0x0 , 0x0 , 0x0 ) & KEYBOARD_LAMPS ) && <nl> ( call_fext_func ( device , FUNC_BUTTONS , 0x0 , 0x0 , 0x0 ) == 0x0 )) { <nl> led = devm_kzalloc (& device -> dev , sizeof (* led ), GFP_KERNEL ); <nl> + if (! led ) <nl> + return - ENOMEM ; <nl> + <nl> led -> name = " fujitsu :: kblamps "; <nl> led -> brightness_set_blocking = kblamps_set ; <nl> led -> brightness_get = kblamps_get ; <nl> static int acpi_fujitsu_laptop_leds_register ( struct acpi_device * device ) <nl> */ <nl> if ( call_fext_func ( device , FUNC_BUTTONS , 0x0 , 0x0 , 0x0 ) & BIT ( 24 )) { <nl> led = devm_kzalloc (& device -> dev , sizeof (* led ), GFP_KERNEL ); <nl> + if (! led ) <nl> + return - ENOMEM ; <nl> + <nl> led -> name = " fujitsu :: radio_led "; <nl> led -> brightness_set_blocking = radio_led_set ; <nl> led -> brightness_get = radio_led_get ; <nl> static int acpi_fujitsu_laptop_leds_register ( struct acpi_device * device ) <nl> ( call_fext_func ( device , <nl> FUNC_LEDS , 0x2 , ECO_LED , 0x0 ) != UNSUPPORTED_CMD )) { <nl> led = devm_kzalloc (& device -> dev , sizeof (* led ), GFP_KERNEL ); <nl> + if (! led ) <nl> + return - ENOMEM ; <nl> + <nl> led -> name = " fujitsu :: eco_led "; <nl> led -> brightness_set_blocking = eco_led_set ; <nl> led -> brightness_get = eco_led_get ;
mmm drivers / gpu / drm / i915 / i915_debugfs . c <nl> ppp drivers / gpu / drm / i915 / i915_debugfs . c <nl> static int i915_context_status ( struct seq_file * m , void * unused ) <nl> } <nl>  <nl> list_for_each_entry ( ctx , & dev_priv -> context_list , link ) { <nl> + if ( ctx -> obj == NULL ) <nl> + continue ; <nl> + <nl> seq_puts ( m , " HW context "); <nl> describe_ctx ( m , ctx ); <nl> for_each_ring ( ring , dev_priv , i )
mmm net / bluetooth / l2cap_sock . c <nl> ppp net / bluetooth / l2cap_sock . c <nl> static int l2cap_sock_getname ( struct socket * sock , struct sockaddr * addr , <nl>  <nl> BT_DBG (" sock % p , sk % p ", sock , sk ); <nl>  <nl> + if ( peer && sk -> sk_state != BT_CONNECTED ) <nl> + return - ENOTCONN ; <nl> + <nl> memset ( la , 0 , sizeof ( struct sockaddr_l2 )); <nl> addr -> sa_family = AF_BLUETOOTH ; <nl> * len = sizeof ( struct sockaddr_l2 );
mmm kernel / bpf / verifier . c <nl> ppp kernel / bpf / verifier . c <nl> static int adjust_scalar_min_max_vals ( struct bpf_verifier_env * env , <nl> u64 umin_val , umax_val ; <nl> u64 insn_bitness = ( BPF_CLASS ( insn -> code ) == BPF_ALU64 ) ? 64 : 32 ; <nl>  <nl> + if ( insn_bitness == 32 ) { <nl> + /* Relevant for 32 - bit RSH : Information can propagate towards <nl> + * LSB , so it isn ' t sufficient to only truncate the output to <nl> + * 32 bits . <nl> + */ <nl> + coerce_reg_to_size ( dst_reg , 4 ); <nl> + coerce_reg_to_size (& src_reg , 4 ); <nl> + } <nl> + <nl> smin_val = src_reg . smin_value ; <nl> smax_val = src_reg . smax_value ; <nl> umin_val = src_reg . umin_value ; <nl> static int adjust_scalar_min_max_vals ( struct bpf_verifier_env * env , <nl> if ( BPF_CLASS ( insn -> code ) != BPF_ALU64 ) { <nl> /* 32 - bit ALU ops are ( 32 , 32 )-> 32 */ <nl> coerce_reg_to_size ( dst_reg , 4 ); <nl> - coerce_reg_to_size (& src_reg , 4 ); <nl> } <nl>  <nl> __reg_deduce_bounds ( dst_reg );
mmm drivers / media / pci / saa7164 / saa7164 - cmd . c <nl> ppp drivers / media / pci / saa7164 / saa7164 - cmd . c <nl> int saa7164_cmd_send ( struct saa7164_dev * dev , u8 id , enum tmComResCmd command , <nl> dprintk ( DBGLVL_CMD , <nl> "% s () UNKNOWN OR INVALID CONTROL \ n ", <nl> __func__ ); <nl> + ret = SAA_ERR_NOT_SUPPORTED ; <nl> + break ; <nl> default : <nl> dprintk ( DBGLVL_CMD , "% s () UNKNOWN \ n ", __func__ ); <nl> ret = SAA_ERR_NOT_SUPPORTED ;
mmm drivers / bus / arm - ccn . c <nl> ppp drivers / bus / arm - ccn . c <nl> static void arm_ccn_pmu_xp_dt_config ( struct perf_event * event , int enable ) <nl> struct arm_ccn_component * xp ; <nl> u32 val , dt_cfg ; <nl>  <nl> + /* Nothing to do for cycle counter */ <nl> + if ( hw -> idx == CCN_IDX_PMU_CYCLE_COUNTER ) <nl> + return ; <nl> + <nl> if ( CCN_CONFIG_TYPE ( event -> attr . config ) == CCN_TYPE_XP ) <nl> xp = & ccn -> xp [ CCN_CONFIG_XP ( event -> attr . config )]; <nl> else
mmm drivers / firewire / core - cdev . c <nl> ppp drivers / firewire / core - cdev . c <nl> static void outbound_phy_packet_callback ( struct fw_packet * packet , <nl> { <nl> struct outbound_phy_packet_event * e = <nl> container_of ( packet , struct outbound_phy_packet_event , p ); <nl> + struct client * e_client ; <nl>  <nl> switch ( status ) { <nl> /* expected : */ <nl> static void outbound_phy_packet_callback ( struct fw_packet * packet , <nl> } <nl> e -> phy_packet . data [ 0 ] = packet -> timestamp ; <nl>  <nl> + e_client = e -> client ; <nl> queue_event ( e -> client , & e -> event , & e -> phy_packet , <nl> sizeof ( e -> phy_packet ) + e -> phy_packet . length , NULL , 0 ); <nl> - client_put ( e -> client ); <nl> + client_put ( e_client ); <nl> } <nl>  <nl> static int ioctl_send_phy_packet ( struct client * client , union ioctl_arg * arg )
mmm drivers / clk / shmobile / r8a7795 - cpg - mssr . c <nl> ppp drivers / clk / shmobile / r8a7795 - cpg - mssr . c <nl> static const struct mssr_mod_clk r8a7795_mod_clks [] __initconst = { <nl> DEF_MOD (" scif2 ", 310 , R8A7795_CLK_S3D4 ), <nl> DEF_MOD (" pcie1 ", 318 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" pcie0 ", 319 , R8A7795_CLK_S3D1 ), <nl> + DEF_MOD (" usb3 - if1 ", 327 , R8A7795_CLK_S3D1 ), <nl> + DEF_MOD (" usb3 - if0 ", 328 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" intc - ap ", 408 , R8A7795_CLK_S3D1 ), <nl> DEF_MOD (" audmac0 ", 502 , R8A7795_CLK_S3D4 ), <nl> DEF_MOD (" audmac1 ", 501 , R8A7795_CLK_S3D4 ),
mmm drivers / char / agp / sis - agp . c <nl> ppp drivers / char / agp / sis - agp . c <nl> static int sis_fetch_size ( void ) <nl> values = A_SIZE_8 ( agp_bridge -> driver -> aperture_sizes ); <nl> for ( i = 0 ; i < agp_bridge -> driver -> num_aperture_sizes ; i ++) { <nl> if (( temp_size == values [ i ]. size_value ) || <nl> - (( temp_size & ~( 0x03 )) == <nl> - ( values [ i ]. size_value & ~( 0x03 )))) { <nl> + (( temp_size & ~( 0x07 )) == <nl> + ( values [ i ]. size_value & ~( 0x07 )))) { <nl> agp_bridge -> previous_size = <nl> agp_bridge -> current_size = ( void *) ( values + i ); <nl> 
mmm drivers / crypto / inside - secure / safexcel . c <nl> ppp drivers / crypto / inside - secure / safexcel . c <nl> static int safexcel_probe ( struct platform_device * pdev ) <nl> snprintf ( irq_name , 6 , " ring % d ", i ); <nl> irq = safexcel_request_ring_irq ( pdev , irq_name , safexcel_irq_ring , <nl> ring_irq ); <nl> - <nl> - if ( irq < 0 ) <nl> + if ( irq < 0 ) { <nl> + ret = irq ; <nl> goto err_clk ; <nl> + } <nl>  <nl> priv -> ring [ i ]. work_data . priv = priv ; <nl> priv -> ring [ i ]. work_data . ring = i ;
mmm net / ipv6 / ndisc . c <nl> ppp net / ipv6 / ndisc . c <nl> static struct sk_buff * ndisc_build_skb ( struct net_device * dev , <nl> len += ndisc_opt_addr_space ( dev ); <nl>  <nl> skb = sock_alloc_send_skb ( sk , <nl> - ( MAX_HEADER + sizeof ( struct ipv6hdr ) + <nl> + ( sizeof ( struct ipv6hdr ) + <nl> len + hlen + tlen ), <nl> 1 , & err ); <nl> if (! skb ) { <nl> void ndisc_send_redirect ( struct sk_buff * skb , const struct in6_addr * target ) <nl> hlen = LL_RESERVED_SPACE ( dev ); <nl> tlen = dev -> needed_tailroom ; <nl> buff = sock_alloc_send_skb ( sk , <nl> - ( MAX_HEADER + sizeof ( struct ipv6hdr ) + <nl> + ( sizeof ( struct ipv6hdr ) + <nl> len + hlen + tlen ), <nl> 1 , & err ); <nl> if ( buff == NULL ) {
mmm kernel / sched / fair . c <nl> ppp kernel / sched / fair . c <nl> void free_fair_sched_group ( struct task_group * tg ) <nl>  <nl> int alloc_fair_sched_group ( struct task_group * tg , struct task_group * parent ) <nl> { <nl> - struct cfs_rq * cfs_rq ; <nl> struct sched_entity * se ; <nl> + struct cfs_rq * cfs_rq ; <nl> + struct rq * rq ; <nl> int i ; <nl>  <nl> tg -> cfs_rq = kzalloc ( sizeof ( cfs_rq ) * nr_cpu_ids , GFP_KERNEL ); <nl> int alloc_fair_sched_group ( struct task_group * tg , struct task_group * parent ) <nl> init_cfs_bandwidth ( tg_cfs_bandwidth ( tg )); <nl>  <nl> for_each_possible_cpu ( i ) { <nl> + rq = cpu_rq ( i ); <nl> + <nl> cfs_rq = kzalloc_node ( sizeof ( struct cfs_rq ), <nl> GFP_KERNEL , cpu_to_node ( i )); <nl> if (! cfs_rq ) <nl> int alloc_fair_sched_group ( struct task_group * tg , struct task_group * parent ) <nl> init_cfs_rq ( cfs_rq ); <nl> init_tg_cfs_entry ( tg , cfs_rq , se , i , parent -> se [ i ]); <nl> init_entity_runnable_average ( se ); <nl> + <nl> + raw_spin_lock_irq (& rq -> lock ); <nl> post_init_entity_util_avg ( se ); <nl> + raw_spin_unlock_irq (& rq -> lock ); <nl> } <nl>  <nl> return 1 ;mmm kernel / sched / core . c <nl> ppp kernel / sched / core . c <nl> void free_fair_sched_group ( struct task_group * tg ) <nl>  <nl> int alloc_fair_sched_group ( struct task_group * tg , struct task_group * parent ) <nl> { <nl> - struct cfs_rq * cfs_rq ; <nl> struct sched_entity * se ; <nl> + struct cfs_rq * cfs_rq ; <nl> + struct rq * rq ; <nl> int i ; <nl>  <nl> tg -> cfs_rq = kzalloc ( sizeof ( cfs_rq ) * nr_cpu_ids , GFP_KERNEL ); <nl> int alloc_fair_sched_group ( struct task_group * tg , struct task_group * parent ) <nl> init_cfs_bandwidth ( tg_cfs_bandwidth ( tg )); <nl>  <nl> for_each_possible_cpu ( i ) { <nl> + rq = cpu_rq ( i ); <nl> + <nl> cfs_rq = kzalloc_node ( sizeof ( struct cfs_rq ), <nl> GFP_KERNEL , cpu_to_node ( i )); <nl> if (! cfs_rq ) <nl> int alloc_fair_sched_group ( struct task_group * tg , struct task_group * parent ) <nl> init_cfs_rq ( cfs_rq ); <nl> init_tg_cfs_entry ( tg , cfs_rq , se , i , parent -> se [ i ]); <nl> init_entity_runnable_average ( se ); <nl> + <nl> + raw_spin_lock_irq (& rq -> lock ); <nl> post_init_entity_util_avg ( se ); <nl> + raw_spin_unlock_irq (& rq -> lock ); <nl> } <nl>  <nl> return 1 ; <nl> void wake_up_new_task ( struct task_struct * p ) <nl> */ <nl> set_task_cpu ( p , select_task_rq ( p , task_cpu ( p ), SD_BALANCE_FORK , 0 )); <nl> # endif <nl> - /* Post initialize new task ' s util average when its cfs_rq is set */ <nl> + rq = __task_rq_lock ( p , & rf ); <nl> post_init_entity_util_avg (& p -> se ); <nl>  <nl> - rq = __task_rq_lock ( p , & rf ); <nl> activate_task ( rq , p , 0 ); <nl> p -> on_rq = TASK_ON_RQ_QUEUED ; <nl> trace_sched_wakeup_new ( p );
mmm drivers / platform / mellanox / mlxreg - hotplug . c <nl> ppp drivers / platform / mellanox / mlxreg - hotplug . c <nl> mlxreg_hotplug_health_work_helper ( struct mlxreg_hotplug_priv_data * priv , <nl> { <nl> struct mlxreg_core_data * data = item -> data ; <nl> u32 regval ; <nl> - int i , ret ; <nl> + int i , ret = 0 ; <nl>  <nl> for ( i = 0 ; i < item -> count ; i ++, data ++) { <nl> /* Mask event . */
mmm drivers / staging / android / ion / ion_system_heap . c <nl> ppp drivers / staging / android / ion / ion_system_heap . c <nl> int ion_system_heap_map_user ( struct ion_heap * heap , struct ion_buffer * buffer , <nl> remap_pfn_range ( vma , addr , page_to_pfn ( sg_page ( sg )), <nl> sg_dma_len ( sg ), vma -> vm_page_prot ); <nl> addr += sg_dma_len ( sg ); <nl> + if ( addr >= vma -> vm_end ) <nl> + return 0 ; <nl> } <nl> return 0 ; <nl> }
mmm drivers / thermal / thermal_hwmon . c <nl> ppp drivers / thermal / thermal_hwmon . c <nl> int thermal_add_hwmon_sysfs ( struct thermal_zone_device * tz ) <nl>  <nl> INIT_LIST_HEAD (& hwmon -> tz_list ); <nl> strlcpy ( hwmon -> type , tz -> type , THERMAL_NAME_LENGTH ); <nl> - hwmon -> device = hwmon_device_register ( NULL ); <nl> + hwmon -> device = hwmon_device_register (& tz -> device ); <nl> if ( IS_ERR ( hwmon -> device )) { <nl> result = PTR_ERR ( hwmon -> device ); <nl> goto free_mem ;
mmm net / l2tp / l2tp_ip6 . c <nl> ppp net / l2tp / l2tp_ip6 . c <nl> static int l2tp_ip6_recvmsg ( struct kiocb * iocb , struct sock * sk , <nl> lsa -> l2tp_addr = ipv6_hdr ( skb )-> saddr ; <nl> lsa -> l2tp_flowinfo = 0 ; <nl> lsa -> l2tp_scope_id = 0 ; <nl> + lsa -> l2tp_conn_id = 0 ; <nl> if ( ipv6_addr_type (& lsa -> l2tp_addr ) & IPV6_ADDR_LINKLOCAL ) <nl> lsa -> l2tp_scope_id = IP6CB ( skb )-> iif ; <nl> }
mmm net / llc / af_llc . c <nl> ppp net / llc / af_llc . c <nl> static void llc_cmsg_rcv ( struct msghdr * msg , struct sk_buff * skb ) <nl> if ( llc -> cmsg_flags & LLC_CMSG_PKTINFO ) { <nl> struct llc_pktinfo info ; <nl>  <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . lpi_ifindex = llc_sk ( skb -> sk )-> dev -> ifindex ; <nl> llc_pdu_decode_dsap ( skb , & info . lpi_sap ); <nl> llc_pdu_decode_da ( skb , info . lpi_mac );
mmm net / core / flow_dissector . c <nl> ppp net / core / flow_dissector . c <nl> bool skb_flow_dissect ( const struct sk_buff * skb , struct flow_keys * flow ) <nl> if ( poff >= 0 ) { <nl> __be32 * ports , _ports ; <nl>  <nl> - nhoff += poff ; <nl> - ports = skb_header_pointer ( skb , nhoff , sizeof ( _ports ), & _ports ); <nl> + ports = skb_header_pointer ( skb , nhoff + poff , <nl> + sizeof ( _ports ), & _ports ); <nl> if ( ports ) <nl> flow -> ports = * ports ; <nl> }
mmm fs / f2fs / data . c <nl> ppp fs / f2fs / data . c <nl> static int __get_data_block ( struct inode * inode , sector_t iblock , <nl> if (! err ) { <nl> map_bh ( bh , inode -> i_sb , map . m_pblk ); <nl> bh -> b_state = ( bh -> b_state & ~ F2FS_MAP_FLAGS ) | map . m_flags ; <nl> - bh -> b_size = map . m_len << inode -> i_blkbits ; <nl> + bh -> b_size = ( u64 ) map . m_len << inode -> i_blkbits ; <nl> } <nl> return err ; <nl> }
mmm drivers / staging / android / sw_sync . c <nl> ppp drivers / staging / android / sw_sync . c <nl> static int sw_sync_open ( struct inode * inode , struct file * file ) <nl> get_task_comm ( task_comm , current ); <nl>  <nl> obj = sw_sync_timeline_create ( task_comm ); <nl> - if ( obj == NULL ) <nl> + if (! obj ) <nl> return - ENOMEM ; <nl>  <nl> file -> private_data = obj ; <nl> static long sw_sync_ioctl_create_fence ( struct sw_sync_timeline * obj , <nl> } <nl>  <nl> pt = sw_sync_pt_create ( obj , data . value ); <nl> - if ( pt == NULL ) { <nl> + if (! pt ) { <nl> err = - ENOMEM ; <nl> goto err ; <nl> } <nl>  <nl> data . name [ sizeof ( data . name ) - 1 ] = '\ 0 '; <nl> fence = sync_fence_create ( data . name , pt ); <nl> - if ( fence == NULL ) { <nl> + if (! fence ) { <nl> sync_pt_free ( pt ); <nl> err = - ENOMEM ; <nl> goto err ;
mmm drivers / mmc / host / mxcmmc . c <nl> ppp drivers / mmc / host / mxcmmc . c <nl> static int mxcmci_probe ( struct platform_device * pdev ) <nl> goto out_release_mem ; <nl> } <nl>  <nl> - mmc_of_parse ( mmc ); <nl> + ret = mmc_of_parse ( mmc ); <nl> + if ( ret ) <nl> + goto out_free ; <nl> mmc -> ops = & mxcmci_ops ; <nl>  <nl> /* For devicetree parsing , the bus width is read from devicetree */
mmm arch / arm / kvm / mmu . c <nl> ppp arch / arm / kvm / mmu . c <nl> static int user_mem_abort ( struct kvm_vcpu * vcpu , phys_addr_t fault_ipa , <nl> struct kvm_mmu_memory_cache * memcache = & vcpu -> arch . mmu_page_cache ; <nl> struct vm_area_struct * vma ; <nl> pfn_t pfn ; <nl> + pgprot_t mem_type = PAGE_S2 ; <nl>  <nl> write_fault = kvm_is_write_fault ( kvm_vcpu_get_hsr ( vcpu )); <nl> if ( fault_status == FSC_PERM && ! write_fault ) { <nl> static int user_mem_abort ( struct kvm_vcpu * vcpu , phys_addr_t fault_ipa , <nl> if ( is_error_pfn ( pfn )) <nl> return - EFAULT ; <nl>  <nl> + if ( kvm_is_mmio_pfn ( pfn )) <nl> + mem_type = PAGE_S2_DEVICE ; <nl> + <nl> spin_lock (& kvm -> mmu_lock ); <nl> if ( mmu_notifier_retry ( kvm , mmu_seq )) <nl> goto out_unlock ; <nl> static int user_mem_abort ( struct kvm_vcpu * vcpu , phys_addr_t fault_ipa , <nl> hugetlb = transparent_hugepage_adjust (& pfn , & fault_ipa ); <nl>  <nl> if ( hugetlb ) { <nl> - pmd_t new_pmd = pfn_pmd ( pfn , PAGE_S2 ); <nl> + pmd_t new_pmd = pfn_pmd ( pfn , mem_type ); <nl> new_pmd = pmd_mkhuge ( new_pmd ); <nl> if ( writable ) { <nl> kvm_set_s2pmd_writable (& new_pmd ); <nl> static int user_mem_abort ( struct kvm_vcpu * vcpu , phys_addr_t fault_ipa , <nl> coherent_cache_guest_page ( vcpu , hva & PMD_MASK , PMD_SIZE ); <nl> ret = stage2_set_pmd_huge ( kvm , memcache , fault_ipa , & new_pmd ); <nl> } else { <nl> - pte_t new_pte = pfn_pte ( pfn , PAGE_S2 ); <nl> + pte_t new_pte = pfn_pte ( pfn , mem_type ); <nl> if ( writable ) { <nl> kvm_set_s2pte_writable (& new_pte ); <nl> kvm_set_pfn_dirty ( pfn ); <nl> } <nl> coherent_cache_guest_page ( vcpu , hva , PAGE_SIZE ); <nl> - ret = stage2_set_pte ( kvm , memcache , fault_ipa , & new_pte , false ); <nl> + ret = stage2_set_pte ( kvm , memcache , fault_ipa , & new_pte , <nl> + mem_type == PAGE_S2_DEVICE ); <nl> } <nl>  <nl> 
mmm net / netfilter / nf_tables_api . c <nl> ppp net / netfilter / nf_tables_api . c <nl> static struct nft_stats __percpu * nft_stats_alloc ( const struct nlattr * attr ) <nl> static void nft_chain_stats_replace ( struct nft_base_chain * chain , <nl> struct nft_stats __percpu * newstats ) <nl> { <nl> + if ( newstats == NULL ) <nl> + return ; <nl> + <nl> if ( chain -> stats ) { <nl> struct nft_stats __percpu * oldstats = <nl> nft_dereference ( chain -> stats );
mmm net / ipv6 / ipv6_sockglue . c <nl> ppp net / ipv6 / ipv6_sockglue . c <nl> static int do_ipv6_setsockopt ( struct sock * sk , int level , int optname , <nl> break ; <nl>  <nl> case IPV6_TRANSPARENT : <nl> + if (! capable ( CAP_NET_ADMIN )) { <nl> + retv = - EPERM ; <nl> + break ; <nl> + } <nl> if ( optlen < sizeof ( int )) <nl> goto e_inval ; <nl> /* we don ' t have a separate transparent bit for IPV6 we use the one in the IPv4 socket */
mmm arch / s390 / kernel / early . c <nl> ppp arch / s390 / kernel / early . c <nl> static void __init reset_tod_clock ( void ) <nl> disabled_wait ( 0 ); <nl>  <nl> sched_clock_base_cc = TOD_UNIX_EPOCH ; <nl> + S390_lowcore . last_update_clock = sched_clock_base_cc ; <nl> } <nl>  <nl> # ifdef CONFIG_SHARED_KERNEL <nl> static noinline __init void create_kernel_nss ( void ) <nl> return ; <nl> } <nl>  <nl> + /* re - initialize cputime accounting . */ <nl> + sched_clock_base_cc = get_clock (); <nl> + S390_lowcore . last_update_clock = sched_clock_base_cc ; <nl> + S390_lowcore . last_update_timer = 0x7fffffffffffffffULL ; <nl> + S390_lowcore . user_timer = 0 ; <nl> + S390_lowcore . system_timer = 0 ; <nl> + asm volatile (" SPT 0 (% 0 )" : : " a " (& S390_lowcore . last_update_timer )); <nl> + <nl> /* re - setup boot command line with new ipl vm parms */ <nl> ipl_update_parameters (); <nl> setup_boot_command_line ();
mmm sound / soc / intel / sst - haswell - ipc . c <nl> ppp sound / soc / intel / sst - haswell - ipc . c <nl> int sst_hsw_dsp_runtime_resume ( struct sst_hsw * hsw ) <nl> ret = wait_event_timeout ( hsw -> boot_wait , hsw -> boot_complete , <nl> msecs_to_jiffies ( IPC_BOOT_MSECS )); <nl> if ( ret == 0 ) { <nl> - dev_err ( hsw -> dev , " error : audio DSP boot timeout \ n "); <nl> + dev_err ( hsw -> dev , " error : audio DSP boot timeout IPCD 0x % x IPCX 0x % x \ n ", <nl> + sst_dsp_shim_read_unlocked ( hsw -> dsp , SST_IPCD ), <nl> + sst_dsp_shim_read_unlocked ( hsw -> dsp , SST_IPCX )); <nl> return - EIO ; <nl> } <nl>  <nl> int sst_hsw_dsp_init ( struct device * dev , struct sst_pdata * pdata ) <nl> msecs_to_jiffies ( IPC_BOOT_MSECS )); <nl> if ( ret == 0 ) { <nl> ret = - EIO ; <nl> - dev_err ( hsw -> dev , " error : ADSP boot timeout \ n "); <nl> + dev_err ( hsw -> dev , " error : audio DSP boot timeout IPCD 0x % x IPCX 0x % x \ n ", <nl> + sst_dsp_shim_read_unlocked ( hsw -> dsp , SST_IPCD ), <nl> + sst_dsp_shim_read_unlocked ( hsw -> dsp , SST_IPCX )); <nl> goto boot_err ; <nl> } <nl> 
mmm drivers / net / wireless / ath / ath10k / usb . c <nl> ppp drivers / net / wireless / ath / ath10k / usb . c <nl> static int ath10k_usb_hif_tx_sg ( struct ath10k * ar , u8 pipe_id , <nl> ath10k_dbg ( ar , ATH10K_DBG_USB_BULK , <nl> " usb bulk transmit failed : % d \ n ", ret ); <nl> usb_unanchor_urb ( urb ); <nl> + usb_free_urb ( urb ); <nl> ret = - EINVAL ; <nl> goto err_free_urb_to_pipe ; <nl> }
mmm kernel / audit . c <nl> ppp kernel / audit . c <nl> void audit_log_n_hex ( struct audit_buffer * ab , const unsigned char * buf , <nl> int i , avail , new_len ; <nl> unsigned char * ptr ; <nl> struct sk_buff * skb ; <nl> - static const unsigned char * hex = " 0123456789ABCDEF "; <nl>  <nl> if (! ab ) <nl> return ; <nl> void audit_log_n_hex ( struct audit_buffer * ab , const unsigned char * buf , <nl> } <nl>  <nl> ptr = skb_tail_pointer ( skb ); <nl> - for ( i = 0 ; i < len ; i ++) { <nl> - * ptr ++ = hex [( buf [ i ] & 0xF0 )>> 4 ]; /* Upper nibble */ <nl> - * ptr ++ = hex [ buf [ i ] & 0x0F ]; /* Lower nibble */ <nl> - } <nl> + for ( i = 0 ; i < len ; i ++) <nl> + ptr = hex_byte_pack_upper ( ptr , buf [ i ]); <nl> * ptr = 0 ; <nl> skb_put ( skb , len << 1 ); /* new string is twice the old string */ <nl> }
mmm drivers / mmc / host / omap . c <nl> ppp drivers / mmc / host / omap . c <nl> static inline void set_cmd_timeout ( struct mmc_omap_host * host , struct mmc_reques <nl>  <nl> static inline void set_data_timeout ( struct mmc_omap_host * host , struct mmc_request * req ) <nl> { <nl> - int timeout ; <nl> + unsigned int timeout , cycle_ns ; <nl> u16 reg ; <nl>  <nl> - /* Convert ns to clock cycles by assuming 20MHz frequency <nl> - * 1 cycle at 20MHz = 500 ns <nl> - */ <nl> - timeout = req -> data -> timeout_clks + req -> data -> timeout_ns / 500 ; <nl> + cycle_ns = 1000000000 / host -> current_slot -> fclk_freq ; <nl> + timeout = req -> data -> timeout_ns / cycle_ns ; <nl> + timeout += req -> data -> timeout_clks ; <nl>  <nl> /* Check if we need to use timeout multiplier register */ <nl> reg = OMAP_MMC_READ ( host , SDIO );
mmm lib / vsprintf . c <nl> ppp lib / vsprintf . c <nl> EXPORT_SYMBOL ( snprintf ); <nl> * @...: Arguments for the format string <nl> * <nl> * The return value is the number of characters written into @ buf not including <nl> - * the trailing '\ 0 '. If @ size is <= 0 the function returns 0 . <nl> + * the trailing '\ 0 '. If @ size is == 0 the function returns 0 . <nl> */ <nl>  <nl> int scnprintf ( char * buf , size_t size , const char * fmt , ...) <nl> int scnprintf ( char * buf , size_t size , const char * fmt , ...) <nl> i = vsnprintf ( buf , size , fmt , args ); <nl> va_end ( args ); <nl>  <nl> - return ( i >= size ) ? ( size - 1 ) : i ; <nl> + if ( likely ( i < size )) <nl> + return i ; <nl> + if ( size != 0 ) <nl> + return size - 1 ; <nl> + return 0 ; <nl> } <nl> EXPORT_SYMBOL ( scnprintf ); <nl> 
mmm drivers / scsi / libsas / sas_expander . c <nl> ppp drivers / scsi / libsas / sas_expander . c <nl> static void smp_task_timedout ( struct timer_list * t ) <nl> unsigned long flags ; <nl>  <nl> spin_lock_irqsave (& task -> task_state_lock , flags ); <nl> - if (!( task -> task_state_flags & SAS_TASK_STATE_DONE )) <nl> + if (!( task -> task_state_flags & SAS_TASK_STATE_DONE )) { <nl> task -> task_state_flags |= SAS_TASK_STATE_ABORTED ; <nl> + complete (& task -> slow_task -> completion ); <nl> + } <nl> spin_unlock_irqrestore (& task -> task_state_lock , flags ); <nl> - <nl> - complete (& task -> slow_task -> completion ); <nl> } <nl>  <nl> static void smp_task_done ( struct sas_task * task ) <nl> { <nl> - if (! del_timer (& task -> slow_task -> timer )) <nl> - return ; <nl> + del_timer (& task -> slow_task -> timer ); <nl> complete (& task -> slow_task -> completion ); <nl> } <nl> 
mmm sound / soc / soc - core . c <nl> ppp sound / soc / soc - core . c <nl> static void snd_soc_instantiate_card ( struct snd_soc_card * card ) <nl> snd_soc_dapm_add_routes (& card -> dapm , card -> dapm_routes , <nl> card -> num_dapm_routes ); <nl>  <nl> + snd_soc_dapm_new_widgets (& card -> dapm ); <nl> + <nl> for ( i = 0 ; i < card -> num_links ; i ++) { <nl> dai_link = & card -> dai_link [ i ]; <nl> 
mmm drivers / iio / chemical / ccs811 . c <nl> ppp drivers / iio / chemical / ccs811 . c <nl> static int ccs811_start_sensor_application ( struct i2c_client * client ) <nl> if ( ret < 0 ) <nl> return ret ; <nl>  <nl> + if (( ret & CCS811_STATUS_FW_MODE_APPLICATION )) <nl> + return 0 ; <nl> + <nl> if (( ret & CCS811_STATUS_APP_VALID_MASK ) != <nl> CCS811_STATUS_APP_VALID_LOADED ) <nl> return - EIO ;
mmm sound / pci / hda / hda_codec . c <nl> ppp sound / pci / hda / hda_codec . c <nl> int snd_hda_create_spdif_out_ctls ( struct hda_codec * codec , hda_nid_t nid ) <nl> } <nl> for ( dig_mix = dig_mixes ; dig_mix -> name ; dig_mix ++) { <nl> kctl = snd_ctl_new1 ( dig_mix , codec ); <nl> + if (! kctl ) <nl> + return - ENOMEM ; <nl> kctl -> id . index = idx ; <nl> kctl -> private_value = nid ; <nl> err = snd_hda_ctl_add ( codec , kctl ); <nl> snd_hda_attach_pcm ( struct hda_codec * codec , struct hda_pcm * pcm ) <nl> struct hda_pcm_stream * info ; <nl> int stream , err ; <nl>  <nl> - if (! pcm -> name ) <nl> + if ( snd_BUG_ON (! pcm -> name )) <nl> return - EINVAL ; <nl> for ( stream = 0 ; stream < 2 ; stream ++) { <nl> info = & pcm -> stream [ stream ];
mmm drivers / md / dm . c <nl> ppp drivers / md / dm . c <nl> static void local_exit ( void ) <nl> DMINFO (" cleaned up "); <nl> } <nl>  <nl> - int (* _inits [])( void ) __initdata = { <nl> + static int (* _inits [])( void ) __initdata = { <nl> local_init , <nl> dm_target_init , <nl> dm_linear_init , <nl> int (* _inits [])( void ) __initdata = { <nl> dm_interface_init , <nl> }; <nl>  <nl> - void (* _exits [])( void ) = { <nl> + static void (* _exits [])( void ) = { <nl> local_exit , <nl> dm_target_exit , <nl> dm_linear_exit ,
mmm mm / migrate . c <nl> ppp mm / migrate . c <nl> static int do_pages_stat ( struct mm_struct * mm , unsigned long nr_pages , <nl> int err ; <nl>  <nl> for ( i = 0 ; i < nr_pages ; i += chunk_nr ) { <nl> - if ( chunk_nr + i > nr_pages ) <nl> + if ( chunk_nr > nr_pages - i ) <nl> chunk_nr = nr_pages - i ; <nl>  <nl> err = copy_from_user ( chunk_pages , & pages [ i ],
mmm drivers / net / can / slcan . c <nl> ppp drivers / net / can / slcan . c <nl> static void slc_bump ( struct slcan * sl ) <nl> u32 tmpid ; <nl> char * cmd = sl -> rbuff ; <nl>  <nl> - cf . can_id = 0 ; <nl> + memset (& cf , 0 , sizeof ( cf )); <nl>  <nl> switch (* cmd ) { <nl> case ' r ': <nl> static void slc_bump ( struct slcan * sl ) <nl> else <nl> return ; <nl>  <nl> - *( u64 *) (& cf . data ) = 0 ; /* clear payload */ <nl> - <nl> /* RTR frames may have a dlc > 0 but they never have any data bytes */ <nl> if (!( cf . can_id & CAN_RTR_FLAG )) { <nl> for ( i = 0 ; i < cf . can_dlc ; i ++) {
mmm drivers / net / ethernet / qlogic / qlcnic / qlcnic_83xx_hw . c <nl> ppp drivers / net / ethernet / qlogic / qlcnic / qlcnic_83xx_hw . c <nl> static void qlcnic_83xx_mailbox_worker ( struct work_struct * work ) <nl> __func__ , cmd -> cmd_op , cmd -> type , ahw -> pci_func , <nl> ahw -> op_mode ); <nl> clear_bit ( QLC_83XX_MBX_READY , & mbx -> status ); <nl> + qlcnic_dump_mbx ( adapter , cmd ); <nl> qlcnic_83xx_idc_request_reset ( adapter , <nl> QLCNIC_FORCE_FW_DUMP_KEY ); <nl> cmd -> rsp_opcode = QLCNIC_RCODE_TIMEOUT ;
mmm drivers / scsi / storvsc_drv . c <nl> ppp drivers / scsi / storvsc_drv . c <nl> static int storvsc_device_configure ( struct scsi_device * sdevice ) <nl>  <nl> /* <nl> * If the host is WIN8 or WIN8 R2 , claim conformance to SPC - 3 <nl> - * if the device is a MSFT virtual device . <nl> + * if the device is a MSFT virtual device . If the host is <nl> + * WIN10 or newer , allow write_same . <nl> */ <nl> if (! strncmp ( sdevice -> vendor , " Msft ", 4 )) { <nl> switch ( vmstor_proto_version ) { <nl> static int storvsc_device_configure ( struct scsi_device * sdevice ) <nl> sdevice -> scsi_level = SCSI_SPC_3 ; <nl> break ; <nl> } <nl> + <nl> + if ( vmstor_proto_version >= VMSTOR_PROTO_VERSION_WIN10 ) <nl> + sdevice -> no_write_same = 0 ; <nl> } <nl>  <nl> return 0 ;
mmm sound / pci / als4000 . c <nl> ppp sound / pci / als4000 . c <nl> static void snd_als4000_configure ( struct snd_sb * chip ) <nl> /* SPECS_PAGE : 39 */ <nl> for ( i = ALS4K_GCR91_DMA0_ADDR ; i <= ALS4K_GCR96_DMA3_MODE_COUNT ; ++ i ) <nl> snd_als4k_gcr_write ( chip , i , 0 ); <nl> - <nl> + /* enable burst mode to prevent dropouts during high PCI bus usage */ <nl> snd_als4k_gcr_write ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL , <nl> - snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL )); <nl> + snd_als4k_gcr_read ( chip , ALS4K_GCR99_DMA_EMULATION_CTRL ) | 0x04 ); <nl> spin_unlock_irq (& chip -> reg_lock ); <nl> } <nl> 
mmm arch / x86 / kvm / lapic . c <nl> ppp arch / x86 / kvm / lapic . c <nl> static u32 apic_get_tmcct ( struct kvm_lapic * apic ) <nl> ASSERT ( apic != NULL ); <nl>  <nl> /* if initial count is 0 , current count should also be 0 */ <nl> - if ( kvm_apic_get_reg ( apic , APIC_TMICT ) == 0 ) <nl> + if ( kvm_apic_get_reg ( apic , APIC_TMICT ) == 0 || <nl> + apic -> lapic_timer . period == 0 ) <nl> return 0 ; <nl>  <nl> remaining = hrtimer_get_remaining (& apic -> lapic_timer . timer );
mmm drivers / dma / ste_dma40 . c <nl> ppp drivers / dma / ste_dma40 . c <nl> struct d40_base { <nl> int irq ; <nl> int num_phy_chans ; <nl> int num_log_chans ; <nl> + struct device_dma_parameters dma_parms ; <nl> struct dma_device dma_both ; <nl> struct dma_device dma_slave ; <nl> struct dma_device dma_memcpy ; <nl> static int __init d40_probe ( struct platform_device * pdev ) <nl> if ( err ) <nl> goto failure ; <nl>  <nl> + base -> dev -> dma_parms = & base -> dma_parms ; <nl> + err = dma_set_max_seg_size ( base -> dev , STEDMA40_MAX_SEG_SIZE ); <nl> + if ( err ) { <nl> + d40_err (& pdev -> dev , " Failed to set dma max seg size \ n "); <nl> + goto failure ; <nl> + } <nl> + <nl> d40_hw_init ( base ); <nl>  <nl> dev_info ( base -> dev , " initialized \ n ");
mmm drivers / net / ethernet / emulex / benet / be_cmds . c <nl> ppp drivers / net / ethernet / emulex / benet / be_cmds . c <nl> static int be_mac_to_link_speed ( int mac_speed ) <nl> return 1000 ; <nl> case PHY_LINK_SPEED_10GBPS : <nl> return 10000 ; <nl> + case PHY_LINK_SPEED_20GBPS : <nl> + return 20000 ; <nl> + case PHY_LINK_SPEED_25GBPS : <nl> + return 25000 ; <nl> + case PHY_LINK_SPEED_40GBPS : <nl> + return 40000 ; <nl> } <nl> return 0 ; <nl> }mmm drivers / net / ethernet / emulex / benet / be_cmds . h <nl> ppp drivers / net / ethernet / emulex / benet / be_cmds . h <nl> static int be_mac_to_link_speed ( int mac_speed ) <nl> return 1000 ; <nl> case PHY_LINK_SPEED_10GBPS : <nl> return 10000 ; <nl> + case PHY_LINK_SPEED_20GBPS : <nl> + return 20000 ; <nl> + case PHY_LINK_SPEED_25GBPS : <nl> + return 25000 ; <nl> + case PHY_LINK_SPEED_40GBPS : <nl> + return 40000 ; <nl> } <nl> return 0 ; <nl> } <nl> enum { <nl> PHY_LINK_SPEED_10MBPS = 0x1 , <nl> PHY_LINK_SPEED_100MBPS = 0x2 , <nl> PHY_LINK_SPEED_1GBPS = 0x3 , <nl> - PHY_LINK_SPEED_10GBPS = 0x4 <nl> + PHY_LINK_SPEED_10GBPS = 0x4 , <nl> + PHY_LINK_SPEED_20GBPS = 0x5 , <nl> + PHY_LINK_SPEED_25GBPS = 0x6 , <nl> + PHY_LINK_SPEED_40GBPS = 0x7 <nl> }; <nl>  <nl> struct be_cmd_resp_link_status {
mmm sound / usb / quirks . c <nl> ppp sound / usb / quirks . c <nl> static int snd_usb_fasttrackpro_boot_quirk ( struct usb_device * dev ) <nl> * rules <nl> */ <nl> err = usb_driver_set_configuration ( dev , 2 ); <nl> - if ( err < 0 ) { <nl> + if ( err < 0 ) <nl> snd_printdd (" error usb_driver_set_configuration : % d \ n ", <nl> err ); <nl> - return - ENODEV ; <nl> - } <nl> + /* Always return an error , so that we stop creating a device <nl> + that will just be destroyed and recreated with a new <nl> + configuration */ <nl> + return - ENODEV ; <nl> } else <nl> snd_printk ( KERN_INFO " usb - audio : Fast Track Pro config OK \ n "); <nl> 
mmm net / core / sock . c <nl> ppp net / core / sock . c <nl> int sock_setsockopt ( struct socket * sock , int level , int optname , <nl> val = min_t ( u32 , val , sysctl_wmem_max ); <nl> set_sndbuf : <nl> sk -> sk_userlocks |= SOCK_SNDBUF_LOCK ; <nl> - sk -> sk_sndbuf = max_t ( u32 , val * 2 , SOCK_MIN_SNDBUF ); <nl> + sk -> sk_sndbuf = max_t ( int , val * 2 , SOCK_MIN_SNDBUF ); <nl> /* Wake up sending tasks if we upped the value . */ <nl> sk -> sk_write_space ( sk ); <nl> break ; <nl> int sock_setsockopt ( struct socket * sock , int level , int optname , <nl> * returning the value we actually used in getsockopt <nl> * is the most desirable behavior . <nl> */ <nl> - sk -> sk_rcvbuf = max_t ( u32 , val * 2 , SOCK_MIN_RCVBUF ); <nl> + sk -> sk_rcvbuf = max_t ( int , val * 2 , SOCK_MIN_RCVBUF ); <nl> break ; <nl>  <nl> case SO_RCVBUFFORCE :
mmm drivers / md / dm . c <nl> ppp drivers / md / dm . c <nl> struct mapped_device * dm_get_from_kobject ( struct kobject * kobj ) <nl>  <nl> md = container_of ( kobj , struct mapped_device , kobj_holder . kobj ); <nl>  <nl> - if ( test_bit ( DMF_FREEING , & md -> flags ) || <nl> - dm_deleting_md ( md )) <nl> - return NULL ; <nl> - <nl> + spin_lock (& _minor_lock ); <nl> + if ( test_bit ( DMF_FREEING , & md -> flags ) || dm_deleting_md ( md )) { <nl> + md = NULL ; <nl> + goto out ; <nl> + } <nl> dm_get ( md ); <nl> + out : <nl> + spin_unlock (& _minor_lock ); <nl> + <nl> return md ; <nl> } <nl> 
mmm drivers / dma / sun6i - dma . c <nl> ppp drivers / dma / sun6i - dma . c <nl> static enum dma_status sun6i_dma_tx_status ( struct dma_chan * chan , <nl> size_t bytes = 0 ; <nl>  <nl> ret = dma_cookie_status ( chan , cookie , state ); <nl> - if ( ret == DMA_COMPLETE ) <nl> + if ( ret == DMA_COMPLETE || ! state ) <nl> return ret ; <nl>  <nl> spin_lock_irqsave (& vchan -> vc . lock , flags );
mmm fs / xfs / libxfs / xfs_iext_tree . c <nl> ppp fs / xfs / libxfs / xfs_iext_tree . c <nl> xfs_iext_remove_node ( <nl> node -> ptrs [ nr_entries ] = NULL ; <nl>  <nl> if ( pos == 0 && nr_entries > 0 ) { <nl> - xfs_iext_update_node ( ifp , offset , node -> keys [ 0 ], level , <nl> - node ); <nl> + xfs_iext_update_node ( ifp , offset , node -> keys [ 0 ], level , node ); <nl> offset = node -> keys [ 0 ]; <nl> } <nl> 
mmm drivers / iio / imu / inv_mpu6050 / inv_mpu_core . c <nl> ppp drivers / iio / imu / inv_mpu6050 / inv_mpu_core . c <nl> static int inv_mpu_probe ( struct i2c_client * client , <nl> { <nl> struct inv_mpu6050_state * st ; <nl> struct iio_dev * indio_dev ; <nl> + struct inv_mpu6050_platform_data * pdata ; <nl> int result ; <nl>  <nl> if (! i2c_check_functionality ( client -> adapter , <nl> static int inv_mpu_probe ( struct i2c_client * client , <nl>  <nl> st = iio_priv ( indio_dev ); <nl> st -> client = client ; <nl> - st -> plat_data = *( struct inv_mpu6050_platform_data <nl> - *) dev_get_platdata (& client -> dev ); <nl> + pdata = ( struct inv_mpu6050_platform_data <nl> + *) dev_get_platdata (& client -> dev ); <nl> + if ( pdata ) <nl> + st -> plat_data = * pdata ; <nl> /* power is turned on inside check chip type */ <nl> result = inv_check_and_setup_chip ( st , id ); <nl> if ( result )
mmm drivers / dma / imx - sdma . c <nl> ppp drivers / dma / imx - sdma . c <nl> struct sdma_firmware_header { <nl>  <nl> struct sdma_engine { <nl> struct device * dev ; <nl> + struct device_dma_parameters dma_parms ; <nl> struct sdma_channel channel [ MAX_DMA_CHANNELS ]; <nl> struct sdma_channel_control * channel_control ; <nl> void __iomem * regs ; <nl> static int __init sdma_probe ( struct platform_device * pdev ) <nl> sdma -> dma_device . device_prep_dma_cyclic = sdma_prep_dma_cyclic ; <nl> sdma -> dma_device . device_control = sdma_control ; <nl> sdma -> dma_device . device_issue_pending = sdma_issue_pending ; <nl> + sdma -> dma_device . dev -> dma_parms = & sdma -> dma_parms ; <nl> + dma_set_max_seg_size ( sdma -> dma_device . dev , 65535 ); <nl>  <nl> ret = dma_async_device_register (& sdma -> dma_device ); <nl> if ( ret ) {
mmm drivers / dma / tegra20 - apb - dma . c <nl> ppp drivers / dma / tegra20 - apb - dma . c <nl> static struct tegra_dma_desc * tegra_dma_desc_get ( <nl> if ( async_tx_test_ack (& dma_desc -> txd )) { <nl> list_del (& dma_desc -> node ); <nl> spin_unlock_irqrestore (& tdc -> lock , flags ); <nl> + dma_desc -> txd . flags = 0 ; <nl> return dma_desc ; <nl> } <nl> } <nl> struct dma_async_tx_descriptor * tegra_dma_prep_dma_cyclic ( <nl> TEGRA_APBDMA_AHBSEQ_WRAP_SHIFT ; <nl> ahb_seq |= TEGRA_APBDMA_AHBSEQ_BUS_WIDTH_32 ; <nl>  <nl> - csr |= TEGRA_APBDMA_CSR_FLOW | TEGRA_APBDMA_CSR_IE_EOC ; <nl> + csr |= TEGRA_APBDMA_CSR_FLOW ; <nl> + if ( flags & DMA_PREP_INTERRUPT ) <nl> + csr |= TEGRA_APBDMA_CSR_IE_EOC ; <nl> csr |= tdc -> dma_sconfig . slave_id << TEGRA_APBDMA_CSR_REQ_SEL_SHIFT ; <nl>  <nl> apb_seq |= TEGRA_APBDMA_APBSEQ_WRAP_WORD_1 ; <nl> struct dma_async_tx_descriptor * tegra_dma_prep_dma_cyclic ( <nl> mem += len ; <nl> } <nl> sg_req -> last_sg = true ; <nl> - dma_desc -> txd . flags = 0 ; <nl> + if ( flags & DMA_CTRL_ACK ) <nl> + dma_desc -> txd . flags = DMA_CTRL_ACK ; <nl>  <nl> /* <nl> * Make sure that mode should not be conflicting with currently
mmm fs / btrfs / transaction . c <nl> ppp fs / btrfs / transaction . c <nl> struct btrfs_trans_handle * btrfs_start_ioctl_transaction ( struct btrfs_root * root <nl> } <nl>  <nl> /* wait for a transaction commit to be fully complete */ <nl> - static noinline int wait_for_commit ( struct btrfs_root * root , <nl> + static noinline void wait_for_commit ( struct btrfs_root * root , <nl> struct btrfs_transaction * commit ) <nl> { <nl> wait_event ( commit -> commit_wait , commit -> commit_done ); <nl> - return 0 ; <nl> } <nl>  <nl> int btrfs_wait_for_commit ( struct btrfs_root * root , u64 transid ) <nl> int btrfs_commit_transaction ( struct btrfs_trans_handle * trans , <nl> atomic_inc (& cur_trans -> use_count ); <nl> btrfs_end_transaction ( trans , root ); <nl>  <nl> - ret = wait_for_commit ( root , cur_trans ); <nl> - BUG_ON ( ret ); <nl> + wait_for_commit ( root , cur_trans ); <nl>  <nl> put_transaction ( cur_trans ); <nl> 
mmm include / linux / f2fs_fs . h <nl> ppp include / linux / f2fs_fs . h <nl> struct f2fs_nat_block { <nl> # define SIT_VBLOCK_MAP_SIZE 64 <nl> # define SIT_ENTRY_PER_BLOCK ( PAGE_SIZE / sizeof ( struct f2fs_sit_entry )) <nl>  <nl> +/* <nl> + * F2FS uses 4 bytes to represent block address . As a result , supported size of <nl> + * disk is 16 TB and it equals to 16 * 1024 * 1024 / 2 segments . <nl> + */ <nl> +# define F2FS_MAX_SEGMENT (( 16 * 1024 * 1024 ) / 2 ) <nl> + <nl> /* <nl> * Note that f2fs_sit_entry -> vblocks has the following bit - field information . <nl> * [ 15 : 10 ] : allocation type such as CURSEG_XXXX_TYPEmmm fs / f2fs / super . c <nl> ppp fs / f2fs / super . c <nl> struct f2fs_nat_block { <nl> # define SIT_VBLOCK_MAP_SIZE 64 <nl> # define SIT_ENTRY_PER_BLOCK ( PAGE_SIZE / sizeof ( struct f2fs_sit_entry )) <nl>  <nl> +/* <nl> + * F2FS uses 4 bytes to represent block address . As a result , supported size of <nl> + * disk is 16 TB and it equals to 16 * 1024 * 1024 / 2 segments . <nl> + */ <nl> +# define F2FS_MAX_SEGMENT (( 16 * 1024 * 1024 ) / 2 ) <nl> + <nl> /* <nl> * Note that f2fs_sit_entry -> vblocks has the following bit - field information . <nl> * [ 15 : 10 ] : allocation type such as CURSEG_XXXX_TYPE <nl> static int sanity_check_raw_super ( struct f2fs_sb_info * sbi , <nl> return 1 ; <nl> } <nl>  <nl> + if ( le32_to_cpu ( raw_super -> segment_count ) > F2FS_MAX_SEGMENT ) { <nl> + f2fs_msg ( sb , KERN_INFO , <nl> + " Invalid segment count (% u )", <nl> + le32_to_cpu ( raw_super -> segment_count )); <nl> + return 1 ; <nl> + } <nl> + <nl> /* check CP / SIT / NAT / SSA / MAIN_AREA area boundary */ <nl> if ( sanity_check_area_boundary ( sbi , bh )) <nl> return 1 ;
mmm kernel / signal . c <nl> ppp kernel / signal . c <nl> do_send_specific ( pid_t tgid , pid_t pid , int sig , struct siginfo * info ) <nl>  <nl> static int do_tkill ( pid_t tgid , pid_t pid , int sig ) <nl> { <nl> - struct siginfo info ; <nl> + struct siginfo info = {}; <nl>  <nl> info . si_signo = sig ; <nl> info . si_errno = 0 ;
mmm drivers / staging / typec / fusb302 / fusb302 . c <nl> ppp drivers / staging / typec / fusb302 / fusb302 . c <nl> static int fusb302_pd_send_message ( struct fusb302_chip * chip , <nl> } <nl> /* packsym tells the FUSB302 chip that the next X bytes are payload */ <nl> buf [ pos ++] = FUSB302_TKN_PACKSYM | ( len & 0x1F ); <nl> - buf [ pos ++] = msg -> header & 0xFF ; <nl> - buf [ pos ++] = ( msg -> header >> 8 ) & 0xFF ; <nl> + memcpy (& buf [ pos ], & msg -> header , sizeof ( msg -> header )); <nl> + pos += sizeof ( msg -> header ); <nl>  <nl> len -= 2 ; <nl> memcpy (& buf [ pos ], msg -> payload , len );
mmm drivers / gpu / drm / drm_drv . c <nl> ppp drivers / gpu / drm / drm_drv . c <nl> long drm_ioctl ( struct file * filp , <nl> retcode = - EFAULT ; <nl> goto err_i1 ; <nl> } <nl> - } <nl> + } else <nl> + memset ( kdata , 0 , _IOC_SIZE ( cmd )); <nl> + <nl> if ( ioctl -> flags & DRM_UNLOCKED ) <nl> retcode = func ( dev , kdata , file_priv ); <nl> else {
mmm drivers / media / platform / vimc / vimc - core . c <nl> ppp drivers / media / platform / vimc / vimc - core . c <nl> static int vimc_probe ( struct platform_device * pdev ) <nl> if ( ret ) { <nl> media_device_cleanup (& vimc -> mdev ); <nl> vimc_rm_subdevs ( vimc ); <nl> - kfree ( vimc ); <nl> return ret ; <nl> } <nl> 
mmm drivers / media / platform / davinci / dm355_ccdc . c <nl> ppp drivers / media / platform / davinci / dm355_ccdc . c <nl> static int ccdc_config_vdfc ( struct ccdc_vertical_dft * dfc ) <nl> */ <nl> static void ccdc_config_csc ( struct ccdc_csc * csc ) <nl> { <nl> - u32 val1 , val2 ; <nl> + u32 val1 = 0 , val2 ; <nl> int i ; <nl>  <nl> if (! csc -> enable )
mmm drivers / pci / pcie / aer / aerdrv_core . c <nl> ppp drivers / pci / pcie / aer / aerdrv_core . c <nl> static void aer_recover_work_func ( struct work_struct * work ) <nl> continue ; <nl> } <nl> cper_print_aer ( pdev , entry . severity , entry . regs ); <nl> - do_recovery ( pdev , entry . severity ); <nl> + if ( entry . severity != AER_CORRECTABLE ) <nl> + do_recovery ( pdev , entry . severity ); <nl> pci_dev_put ( pdev ); <nl> } <nl> }
mmm fs / nfs / pnfs . c <nl> ppp fs / nfs / pnfs . c <nl> void pnfs_error_mark_layout_for_return ( struct inode * inode , <nl> struct pnfs_layout_segment * lseg ) <nl> { <nl> struct pnfs_layout_hdr * lo = NFS_I ( inode )-> layout ; <nl> - int iomode = pnfs_iomode_to_fail_bit ( lseg -> pls_range . iomode ); <nl> struct pnfs_layout_range range = { <nl> . iomode = lseg -> pls_range . iomode , <nl> . offset = 0 , <nl> void pnfs_error_mark_layout_for_return ( struct inode * inode , <nl> LIST_HEAD ( free_me ); <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> - /* set failure bit so that pnfs path will be retried later */ <nl> - pnfs_layout_set_fail_bit ( lo , iomode ); <nl> if ( lo -> plh_return_iomode == 0 ) <nl> lo -> plh_return_iomode = range . iomode ; <nl> else if ( lo -> plh_return_iomode != range . iomode )mmm fs / nfs / filelayout / filelayout . c <nl> ppp fs / nfs / filelayout / filelayout . c <nl> void pnfs_error_mark_layout_for_return ( struct inode * inode , <nl> struct pnfs_layout_segment * lseg ) <nl> { <nl> struct pnfs_layout_hdr * lo = NFS_I ( inode )-> layout ; <nl> - int iomode = pnfs_iomode_to_fail_bit ( lseg -> pls_range . iomode ); <nl> struct pnfs_layout_range range = { <nl> . iomode = lseg -> pls_range . iomode , <nl> . offset = 0 , <nl> void pnfs_error_mark_layout_for_return ( struct inode * inode , <nl> LIST_HEAD ( free_me ); <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> - /* set failure bit so that pnfs path will be retried later */ <nl> - pnfs_layout_set_fail_bit ( lo , iomode ); <nl> if ( lo -> plh_return_iomode == 0 ) <nl> lo -> plh_return_iomode = range . iomode ; <nl> else if ( lo -> plh_return_iomode != range . iomode ) <nl> static int filelayout_async_handle_error ( struct rpc_task * task , <nl> task -> tk_status ); <nl> nfs4_mark_deviceid_unavailable ( devid ); <nl> pnfs_error_mark_layout_for_return ( inode , lseg ); <nl> + pnfs_set_lo_fail ( lseg ); <nl> rpc_wake_up (& tbl -> slot_tbl_waitq ); <nl> /* fall through */ <nl> default :
mmm security / selinux / hooks . c <nl> ppp security / selinux / hooks . c <nl> void selinux_complete_init ( void ) <nl>  <nl> /* Set up any superblocks initialized prior to the policy load . */ <nl> printk ( KERN_INFO " SELinux : Setting up existing superblocks .\ n "); <nl> + spin_lock (& sb_lock ); <nl> spin_lock (& sb_security_lock ); <nl> next_sb : <nl> if (! list_empty (& superblock_security_head )) { <nl> void selinux_complete_init ( void ) <nl> struct superblock_security_struct , <nl> list ); <nl> struct super_block * sb = sbsec -> sb ; <nl> - spin_lock (& sb_lock ); <nl> sb -> s_count ++; <nl> - spin_unlock (& sb_lock ); <nl> spin_unlock (& sb_security_lock ); <nl> + spin_unlock (& sb_lock ); <nl> down_read (& sb -> s_umount ); <nl> if ( sb -> s_root ) <nl> superblock_doinit ( sb , NULL ); <nl> drop_super ( sb ); <nl> + spin_lock (& sb_lock ); <nl> spin_lock (& sb_security_lock ); <nl> list_del_init (& sbsec -> list ); <nl> goto next_sb ; <nl> } <nl> spin_unlock (& sb_security_lock ); <nl> + spin_unlock (& sb_lock ); <nl> } <nl>  <nl> /* SELinux requires early initialization in order to label
mmm drivers / char / tpm / tpm - chip . c <nl> ppp drivers / char / tpm / tpm - chip . c <nl> struct tpm_chip * tpmm_chip_alloc ( struct device * dev , <nl>  <nl> device_initialize (& chip -> dev ); <nl>  <nl> - chip -> cdev . owner = chip -> pdev -> driver -> owner ; <nl> cdev_init (& chip -> cdev , & tpm_fops ); <nl> + chip -> cdev . owner = chip -> pdev -> driver -> owner ; <nl> + chip -> cdev . kobj . parent = & chip -> dev . kobj ; <nl>  <nl> return chip ; <nl> }
mmm sound / core / timer . c <nl> ppp sound / core / timer . c <nl> static int snd_timer_user_tselect ( struct file * file , <nl> if ( err < 0 ) <nl> goto __err ; <nl>  <nl> + tu -> qhead = tu -> qtail = tu -> qused = 0 ; <nl> kfree ( tu -> queue ); <nl> tu -> queue = NULL ; <nl> kfree ( tu -> tqueue );
mmm drivers / gpu / drm / nouveau / nvkm / engine / device / base . c <nl> ppp drivers / gpu / drm / nouveau / nvkm / engine / device / base . c <nl> nv134_chipset = { <nl> . top = gk104_top_new , <nl> . disp = gp104_disp_new , <nl> . dma = gf119_dma_new , <nl> + . fifo = gp100_fifo_new , <nl> }; <nl>  <nl> static int
mmm include / net / nfc / nci_core . h <nl> ppp include / net / nfc / nci_core . h <nl> struct nci_ops { <nl> int (* send )( struct nci_dev * ndev , struct sk_buff * skb ); <nl> int (* setup )( struct nci_dev * ndev ); <nl> __u32 (* get_rfprotocol )( struct nci_dev * ndev , __u8 rf_protocol ); <nl> + int (* discover_se )( struct nci_dev * ndev ); <nl> }; <nl>  <nl> # define NCI_MAX_SUPPORTED_RF_INTERFACES 4mmm net / nfc / nci / core . c <nl> ppp net / nfc / nci / core . c <nl> struct nci_ops { <nl> int (* send )( struct nci_dev * ndev , struct sk_buff * skb ); <nl> int (* setup )( struct nci_dev * ndev ); <nl> __u32 (* get_rfprotocol )( struct nci_dev * ndev , __u8 rf_protocol ); <nl> + int (* discover_se )( struct nci_dev * ndev ); <nl> }; <nl>  <nl> # define NCI_MAX_SUPPORTED_RF_INTERFACES 4 <nl> static int nci_disable_se ( struct nfc_dev * nfc_dev , u32 se_idx ) <nl>  <nl> static int nci_discover_se ( struct nfc_dev * nfc_dev ) <nl> { <nl> + struct nci_dev * ndev = nfc_get_drvdata ( nfc_dev ); <nl> + <nl> + if ( ndev -> ops -> discover_se ) <nl> + return ndev -> ops -> discover_se ( ndev ); <nl> + <nl> return 0 ; <nl> } <nl> 
mmm virt / kvm / irqchip . c <nl> ppp virt / kvm / irqchip . c <nl> int kvm_set_irq_routing ( struct kvm * kvm , <nl> goto out ; <nl>  <nl> r = - EINVAL ; <nl> - if ( ue -> flags ) <nl> + if ( ue -> flags ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> r = setup_routing_entry ( new , e , ue ); <nl> - if ( r ) <nl> + if ( r ) { <nl> + kfree ( e ); <nl> goto out ; <nl> + } <nl> ++ ue ; <nl> } <nl> 
mmm kernel / relay . c <nl> ppp kernel / relay . c <nl> struct rchan * relay_open ( const char * base_filename , <nl>  <nl> kref_put (& chan -> kref , relay_destroy_channel ); <nl> mutex_unlock (& relay_channels_mutex ); <nl> + kfree ( chan ); <nl> return NULL ; <nl> } <nl> EXPORT_SYMBOL_GPL ( relay_open );
mmm net / bridge / br_ioctl . c <nl> ppp net / bridge / br_ioctl . c <nl> static int get_fdb_entries ( struct net_bridge * br , void __user * userbuf , <nl> { <nl> int num ; <nl> void * buf ; <nl> - size_t size = maxnum * sizeof ( struct __fdb_entry ); <nl> + size_t size ; <nl>  <nl> - if ( size > PAGE_SIZE ) { <nl> - size = PAGE_SIZE ; <nl> + /* Clamp size to PAGE_SIZE , test maxnum to avoid overflow */ <nl> + if ( maxnum > PAGE_SIZE / sizeof ( struct __fdb_entry )) <nl> maxnum = PAGE_SIZE / sizeof ( struct __fdb_entry ); <nl> - } <nl> + <nl> + size = maxnum * sizeof ( struct __fdb_entry ); <nl>  <nl> buf = kmalloc ( size , GFP_USER ); <nl> if (! buf )
mmm drivers / gpu / drm / i915 / intel_display . c <nl> ppp drivers / gpu / drm / i915 / intel_display . c <nl> static int intel_atomic_check ( struct drm_device * dev , <nl> struct intel_crtc_state * pipe_config = <nl> to_intel_crtc_state ( crtc_state ); <nl>  <nl> + memset (& to_intel_crtc ( crtc )-> atomic , 0 , <nl> + sizeof ( struct intel_crtc_atomic_commit )); <nl> + <nl> /* Catch I915_MODE_FLAG_INHERITED */ <nl> if ( crtc_state -> mode . private_flags != crtc -> state -> mode . private_flags ) <nl> crtc_state -> mode_changed = true ;
mmm net / key / af_key . c <nl> ppp net / key / af_key . c <nl> static int pfkey_register ( struct sock * sk , struct sk_buff * skb , const struct sad <nl> pfk -> registered |= ( 1 << hdr -> sadb_msg_satype ); <nl> } <nl>  <nl> + mutex_lock (& pfkey_mutex ); <nl> xfrm_probe_algs (); <nl>  <nl> supp_skb = compose_sadb_supported ( hdr , GFP_KERNEL | __GFP_ZERO ); <nl> + mutex_unlock (& pfkey_mutex ); <nl> + <nl> if (! supp_skb ) { <nl> if ( hdr -> sadb_msg_satype != SADB_SATYPE_UNSPEC ) <nl> pfk -> registered &= ~( 1 << hdr -> sadb_msg_satype );
mmm net / ipv6 / af_inet6 . c <nl> ppp net / ipv6 / af_inet6 . c <nl> static struct pernet_operations inet6_net_ops = { <nl>  <nl> static int __init inet6_init ( void ) <nl> { <nl> - struct sk_buff * dummy_skb ; <nl> struct list_head * r ; <nl> int err = 0 ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct inet6_skb_parm ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct inet6_skb_parm ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> /* Register the socket - side information for inet6_create . */ <nl> for ( r = & inetsw6 [ 0 ]; r < & inetsw6 [ SOCK_MAX ]; ++ r )
mmm drivers / scsi / esas2r / esas2r_ioctl . c <nl> ppp drivers / scsi / esas2r / esas2r_ioctl . c <nl> int esas2r_ioctl_handler ( void * hostdata , int cmd , void __user * arg ) <nl>  <nl> rq = esas2r_alloc_request ( a ); <nl> if ( rq == NULL ) { <nl> - up (& a -> nvram_semaphore ); <nl> - ioctl -> data . prw . code = 0 ; <nl> - break ; <nl> + kfree ( ioctl ); <nl> + esas2r_log ( ESAS2R_LOG_WARN , <nl> + " could not allocate an internal request "); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> code = esas2r_write_params ( a , rq ,
mmm net / mac80211 / cfg . c <nl> ppp net / mac80211 / cfg . c <nl> static int ieee80211_set_cqm_rssi_config ( struct wiphy * wiphy , <nl>  <nl> bss_conf -> cqm_rssi_thold = rssi_thold ; <nl> bss_conf -> cqm_rssi_hyst = rssi_hyst ; <nl> + sdata -> u . mgd . last_cqm_event_signal = 0 ; <nl>  <nl> /* tell the driver upon association , unless already associated */ <nl> if ( sdata -> u . mgd . associated &&
mmm drivers / gpu / drm / radeon / radeon_uvd . c <nl> ppp drivers / gpu / drm / radeon / radeon_uvd . c <nl> static int radeon_uvd_cs_reloc ( struct radeon_cs_parser * p , <nl> return - EINVAL ; <nl> } <nl>  <nl> - if (( start >> 28 ) != ( end >> 28 )) { <nl> + if (( start >> 28 ) != (( end - 1 ) >> 28 )) { <nl> DRM_ERROR (" reloc % LX -% LX crossing 256MB boundary !\ n ", <nl> start , end ); <nl> return - EINVAL ;
mmm drivers / tty / serial / serial_core . c <nl> ppp drivers / tty / serial / serial_core . c <nl> static void uart_close ( struct tty_struct * tty , struct file * filp ) <nl>  <nl> pr_debug (" uart_close (% d ) called \ n ", uport -> line ); <nl>  <nl> - mutex_lock (& port -> mutex ); <nl> spin_lock_irqsave (& port -> lock , flags ); <nl>  <nl> if ( tty_hung_up_p ( filp )) { <nl> static void uart_close ( struct tty_struct * tty , struct file * filp ) <nl> uart_wait_until_sent ( tty , uport -> timeout ); <nl> } <nl>  <nl> + mutex_lock (& port -> mutex ); <nl> uart_shutdown ( tty , state ); <nl> uart_flush_buffer ( tty ); <nl> 
mmm net / ipv4 / tcp . c <nl> ppp net / ipv4 / tcp . c <nl> int tcp_read_sock ( struct sock * sk , read_descriptor_t * desc , <nl> sk_eat_skb ( sk , skb , 0 ); <nl> if (! desc -> count ) <nl> break ; <nl> + tp -> copied_seq = seq ; <nl> } <nl> tp -> copied_seq = seq ; <nl> 
mmm include / linux / mm . h <nl> ppp include / linux / mm . h <nl> unsigned long vmalloc_to_pfn ( const void * addr ); <nl> * On nommu , vmalloc / vfree wrap through kmalloc / kfree directly , so there <nl> * is no special casing required . <nl> */ <nl> - static inline int is_vmalloc_addr ( const void * x ) <nl> + static inline bool is_vmalloc_addr ( const void * x ) <nl> { <nl> # ifdef CONFIG_MMU <nl> unsigned long addr = ( unsigned long ) x ; <nl>  <nl> return addr >= VMALLOC_START && addr < VMALLOC_END ; <nl> # else <nl> - return 0 ; <nl> + return false ; <nl> # endif <nl> } <nl> # ifdef CONFIG_MMU
mmm drivers / mmc / host / au1xmmc . c <nl> ppp drivers / mmc / host / au1xmmc . c <nl> static struct platform_driver au1xmmc_driver = { <nl> . resume = au1xmmc_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl> mmm drivers / mmc / host / davinci_mmc . c <nl> ppp drivers / mmc / host / davinci_mmc . c <nl> static struct platform_driver au1xmmc_driver = { <nl> . resume = au1xmmc_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl>  <nl> static const struct dev_pm_ops davinci_mmcsd_pm = { <nl> static struct platform_driver davinci_mmcsd_driver = { <nl> . driver = { <nl> . name = " davinci_mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = davinci_mmcsd_pm_ops , <nl> . of_match_table = davinci_mmc_dt_ids , <nl> },mmm drivers / mmc / host / wmt - sdmmc . c <nl> ppp drivers / mmc / host / wmt - sdmmc . c <nl> static struct platform_driver au1xmmc_driver = { <nl> . resume = au1xmmc_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl>  <nl> static const struct dev_pm_ops davinci_mmcsd_pm = { <nl> static struct platform_driver davinci_mmcsd_driver = { <nl> . driver = { <nl> . name = " davinci_mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = davinci_mmcsd_pm_ops , <nl> . of_match_table = davinci_mmc_dt_ids , <nl> }, <nl> static struct platform_driver wmt_mci_driver = { <nl> . remove = wmt_mci_remove , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> . pm = wmt_mci_pm_ops , <nl> . of_match_table = wmt_mci_dt_ids , <nl> },mmm drivers / mmc / host / sdhci - acpi . c <nl> ppp drivers / mmc / host / sdhci - acpi . c <nl> static struct platform_driver au1xmmc_driver = { <nl> . resume = au1xmmc_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl>  <nl> static const struct dev_pm_ops davinci_mmcsd_pm = { <nl> static struct platform_driver davinci_mmcsd_driver = { <nl> . driver = { <nl> . name = " davinci_mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = davinci_mmcsd_pm_ops , <nl> . of_match_table = davinci_mmc_dt_ids , <nl> }, <nl> static struct platform_driver wmt_mci_driver = { <nl> . remove = wmt_mci_remove , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> . pm = wmt_mci_pm_ops , <nl> . of_match_table = wmt_mci_dt_ids , <nl> }, <nl> static const struct dev_pm_ops sdhci_acpi_pm_ops = { <nl> static struct platform_driver sdhci_acpi_driver = { <nl> . driver = { <nl> . name = " sdhci - acpi ", <nl> - . owner = THIS_MODULE , <nl> . acpi_match_table = sdhci_acpi_ids , <nl> . pm = & sdhci_acpi_pm_ops , <nl> },mmm drivers / mmc / host / tmio_mmc . c <nl> ppp drivers / mmc / host / tmio_mmc . c <nl> static struct platform_driver au1xmmc_driver = { <nl> . resume = au1xmmc_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl>  <nl> static const struct dev_pm_ops davinci_mmcsd_pm = { <nl> static struct platform_driver davinci_mmcsd_driver = { <nl> . driver = { <nl> . name = " davinci_mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = davinci_mmcsd_pm_ops , <nl> . of_match_table = davinci_mmc_dt_ids , <nl> }, <nl> static struct platform_driver wmt_mci_driver = { <nl> . remove = wmt_mci_remove , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> . pm = wmt_mci_pm_ops , <nl> . of_match_table = wmt_mci_dt_ids , <nl> }, <nl> static const struct dev_pm_ops sdhci_acpi_pm_ops = { <nl> static struct platform_driver sdhci_acpi_driver = { <nl> . driver = { <nl> . name = " sdhci - acpi ", <nl> - . owner = THIS_MODULE , <nl> . acpi_match_table = sdhci_acpi_ids , <nl> . pm = & sdhci_acpi_pm_ops , <nl> }, <nl> static const struct dev_pm_ops tmio_mmc_dev_pm_ops = { <nl> static struct platform_driver tmio_mmc_driver = { <nl> . driver = { <nl> . name = " tmio - mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = & tmio_mmc_dev_pm_ops , <nl> }, <nl> . probe = tmio_mmc_probe ,mmm drivers / mmc / host / wbsd . c <nl> ppp drivers / mmc / host / wbsd . c <nl> static struct platform_driver au1xmmc_driver = { <nl> . resume = au1xmmc_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl>  <nl> static const struct dev_pm_ops davinci_mmcsd_pm = { <nl> static struct platform_driver davinci_mmcsd_driver = { <nl> . driver = { <nl> . name = " davinci_mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = davinci_mmcsd_pm_ops , <nl> . of_match_table = davinci_mmc_dt_ids , <nl> }, <nl> static struct platform_driver wmt_mci_driver = { <nl> . remove = wmt_mci_remove , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> . pm = wmt_mci_pm_ops , <nl> . of_match_table = wmt_mci_dt_ids , <nl> }, <nl> static const struct dev_pm_ops sdhci_acpi_pm_ops = { <nl> static struct platform_driver sdhci_acpi_driver = { <nl> . driver = { <nl> . name = " sdhci - acpi ", <nl> - . owner = THIS_MODULE , <nl> . acpi_match_table = sdhci_acpi_ids , <nl> . pm = & sdhci_acpi_pm_ops , <nl> }, <nl> static const struct dev_pm_ops tmio_mmc_dev_pm_ops = { <nl> static struct platform_driver tmio_mmc_driver = { <nl> . driver = { <nl> . name = " tmio - mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = & tmio_mmc_dev_pm_ops , <nl> }, <nl> . probe = tmio_mmc_probe , <nl> static struct platform_driver wbsd_driver = { <nl> . resume = wbsd_platform_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl> mmm drivers / mmc / host / usdhi6rol0 . c <nl> ppp drivers / mmc / host / usdhi6rol0 . c <nl> static struct platform_driver au1xmmc_driver = { <nl> . resume = au1xmmc_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl>  <nl> static const struct dev_pm_ops davinci_mmcsd_pm = { <nl> static struct platform_driver davinci_mmcsd_driver = { <nl> . driver = { <nl> . name = " davinci_mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = davinci_mmcsd_pm_ops , <nl> . of_match_table = davinci_mmc_dt_ids , <nl> }, <nl> static struct platform_driver wmt_mci_driver = { <nl> . remove = wmt_mci_remove , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> . pm = wmt_mci_pm_ops , <nl> . of_match_table = wmt_mci_dt_ids , <nl> }, <nl> static const struct dev_pm_ops sdhci_acpi_pm_ops = { <nl> static struct platform_driver sdhci_acpi_driver = { <nl> . driver = { <nl> . name = " sdhci - acpi ", <nl> - . owner = THIS_MODULE , <nl> . acpi_match_table = sdhci_acpi_ids , <nl> . pm = & sdhci_acpi_pm_ops , <nl> }, <nl> static const struct dev_pm_ops tmio_mmc_dev_pm_ops = { <nl> static struct platform_driver tmio_mmc_driver = { <nl> . driver = { <nl> . name = " tmio - mmc ", <nl> - . owner = THIS_MODULE , <nl> . pm = & tmio_mmc_dev_pm_ops , <nl> }, <nl> . probe = tmio_mmc_probe , <nl> static struct platform_driver wbsd_driver = { <nl> . resume = wbsd_platform_resume , <nl> . driver = { <nl> . name = DRIVER_NAME , <nl> - . owner = THIS_MODULE , <nl> }, <nl> }; <nl>  <nl> static struct platform_driver usdhi6_driver = { <nl> . remove = usdhi6_remove , <nl> . driver = { <nl> . name = " usdhi6rol0 ", <nl> - . owner = THIS_MODULE , <nl> . of_match_table = usdhi6_of_match , <nl> }, <nl> };
mmm net / ipv4 / ip_tunnel . c <nl> ppp net / ipv4 / ip_tunnel . c <nl> const struct ip_tunnel_encap_ops __rcu * <nl> int ip_tunnel_encap_add_ops ( const struct ip_tunnel_encap_ops * ops , <nl> unsigned int num ) <nl> { <nl> + if ( num >= MAX_IPTUN_ENCAP_OPS ) <nl> + return - ERANGE ; <nl> + <nl> return ! cmpxchg (( const struct ip_tunnel_encap_ops **) <nl> & iptun_encaps [ num ], <nl> NULL , ops ) ? 0 : - 1 ; <nl> int ip_tunnel_encap_del_ops ( const struct ip_tunnel_encap_ops * ops , <nl> { <nl> int ret ; <nl>  <nl> + if ( num >= MAX_IPTUN_ENCAP_OPS ) <nl> + return - ERANGE ; <nl> + <nl> ret = ( cmpxchg (( const struct ip_tunnel_encap_ops **) <nl> & iptun_encaps [ num ], <nl> ops , NULL ) == ops ) ? 0 : - 1 ;
mmm arch / powerpc / perf / core - book3s . c <nl> ppp arch / powerpc / perf / core - book3s . c <nl> static void power_pmu_bhrb_read ( struct cpu_hw_events * cpuhw ) <nl> /* invalid entry */ <nl> continue ; <nl>  <nl> + /* <nl> + * BHRB rolling buffer could very much contain the kernel <nl> + * addresses at this point . Check the privileges before <nl> + * exporting it to userspace ( avoid exposure of regions <nl> + * where we could have speculative execution ) <nl> + */ <nl> + if ( perf_paranoid_kernel () && ! capable ( CAP_SYS_ADMIN ) && <nl> + is_kernel_addr ( addr )) <nl> + continue ; <nl> + <nl> /* Branches are read most recent first ( ie . mfbhrb 0 is <nl> * the most recent branch ). <nl> * There are two types of valid entries :
mmm drivers / cpufreq / cpufreq_stats . c <nl> ppp drivers / cpufreq / cpufreq_stats . c <nl> cpufreq_stat_notifier_trans ( struct notifier_block * nb , unsigned long val , <nl> return 0 ; <nl> } <nl>  <nl> - static int __cpuinit cpufreq_stat_cpu_callback ( struct notifier_block * nfb , <nl> + static int cpufreq_stat_cpu_callback ( struct notifier_block * nfb , <nl> unsigned long action , void * hcpu ) <nl> { <nl> unsigned int cpu = ( unsigned long ) hcpu ;
mmm include / net / tcp . h <nl> ppp include / net / tcp . h <nl> static inline void tcp_check_send_head ( struct sock * sk , struct sk_buff * skb_unli <nl> { <nl> if ( sk -> sk_send_head == skb_unlinked ) <nl> sk -> sk_send_head = NULL ; <nl> + if ( tcp_sk ( sk )-> highest_sack == skb_unlinked ) <nl> + tcp_sk ( sk )-> highest_sack = NULL ; <nl> } <nl>  <nl> static inline void tcp_init_send_head ( struct sock * sk )
mmm drivers / net / ucc_geth . c <nl> ppp drivers / net / ucc_geth . c <nl> static int init_phy ( struct net_device * dev ) <nl> if ( priv -> phy_interface == PHY_INTERFACE_MODE_SGMII ) <nl> uec_configure_serdes ( dev ); <nl>  <nl> - phydev -> supported &= ( ADVERTISED_10baseT_Half | <nl> - ADVERTISED_10baseT_Full | <nl> - ADVERTISED_100baseT_Half | <nl> - ADVERTISED_100baseT_Full ); <nl> + phydev -> supported &= ( SUPPORTED_MII | <nl> + SUPPORTED_Autoneg | <nl> + ADVERTISED_10baseT_Half | <nl> + ADVERTISED_10baseT_Full | <nl> + ADVERTISED_100baseT_Half | <nl> + ADVERTISED_100baseT_Full ); <nl>  <nl> if ( priv -> max_speed == SPEED_1000 ) <nl> phydev -> supported |= ADVERTISED_1000baseT_Full ;
mmm drivers / scsi / libata - core . c <nl> ppp drivers / scsi / libata - core . c <nl> static void ata_pio_block ( struct ata_port * ap ) <nl>  <nl> ata_pio_sector ( qc ); <nl> } <nl> + <nl> + ata_altstatus ( ap ); /* flush */ <nl> } <nl>  <nl> static void ata_pio_error ( struct ata_port * ap ) <nl> static void atapi_packet_task ( void * _data ) <nl> spin_lock_irqsave (& ap -> host_set -> lock , flags ); <nl> ap -> flags &= ~ ATA_FLAG_NOINTR ; <nl> ata_data_xfer ( ap , qc -> cdb , qc -> dev -> cdb_len , 1 ); <nl> + ata_altstatus ( ap ); /* flush */ <nl> + <nl> if ( qc -> tf . protocol == ATA_PROT_ATAPI_DMA ) <nl> ap -> ops -> bmdma_start ( qc ); /* initiate bmdma */ <nl> spin_unlock_irqrestore (& ap -> host_set -> lock , flags ); <nl> } else { <nl> ata_data_xfer ( ap , qc -> cdb , qc -> dev -> cdb_len , 1 ); <nl> + ata_altstatus ( ap ); /* flush */ <nl>  <nl> /* PIO commands are handled by polling */ <nl> ap -> hsm_task_state = HSM_ST ;
mmm fs / afs / write . c <nl> ppp fs / afs / write . c <nl> int afs_prepare_write ( struct file * file , struct page * page , <nl> _leave (" = % d [ prep ]", ret ); <nl> return ret ; <nl> } <nl> - SetPageUptodate ( page ); <nl> } <nl>  <nl> try_again : <nl> int afs_commit_write ( struct file * file , struct page * page , <nl> spin_unlock (& vnode -> writeback_lock ); <nl> } <nl>  <nl> + SetPageUptodate ( page ); <nl> set_page_dirty ( page ); <nl> - <nl> if ( PageDirty ( page )) <nl> _debug (" dirtied "); <nl> 
mmm fs / xfs / libxfs / xfs_attr_leaf . c <nl> ppp fs / xfs / libxfs / xfs_attr_leaf . c <nl> xfs_attr_shortform_to_leaf ( <nl> ASSERT ( blkno == 0 ); <nl> error = xfs_attr3_leaf_create ( args , blkno , & bp ); <nl> if ( error ) { <nl> - error = xfs_da_shrink_inode ( args , 0 , bp ); <nl> - bp = NULL ; <nl> - if ( error ) <nl> + /* xfs_attr3_leaf_create may not have instantiated a block */ <nl> + if ( bp && ( xfs_da_shrink_inode ( args , 0 , bp ) != 0 )) <nl> goto out ; <nl> xfs_idata_realloc ( dp , size , XFS_ATTR_FORK ); /* try to put */ <nl> memcpy ( ifp -> if_u1 . if_data , tmpbuffer , size ); /* it back */
mmm mm / memcontrol . c <nl> ppp mm / memcontrol . c <nl> static int mem_cgroup_resize_max ( struct mem_cgroup * memcg , <nl> unsigned long max , bool memsw ) <nl> { <nl> bool enlarge = false ; <nl> + bool drained = false ; <nl> int ret ; <nl> bool limits_invariant ; <nl> struct page_counter * counter = memsw ? & memcg -> memsw : & memcg -> memory ; <nl> static int mem_cgroup_resize_max ( struct mem_cgroup * memcg , <nl> if (! ret ) <nl> break ; <nl>  <nl> + if (! drained ) { <nl> + drain_all_stock ( memcg ); <nl> + drained = true ; <nl> + continue ; <nl> + } <nl> + <nl> if (! try_to_free_mem_cgroup_pages ( memcg , 1 , <nl> GFP_KERNEL , ! memsw )) { <nl> ret = - EBUSY ;
mmm fs / xfs / xfs_attr_remote . c <nl> ppp fs / xfs / xfs_attr_remote . c <nl> xfs_attr3_rmt_verify ( <nl> if ( be32_to_cpu ( rmt -> rm_bytes ) > fsbsize - sizeof (* rmt )) <nl> return false ; <nl> if ( be32_to_cpu ( rmt -> rm_offset ) + <nl> - be32_to_cpu ( rmt -> rm_bytes ) >= XATTR_SIZE_MAX ) <nl> + be32_to_cpu ( rmt -> rm_bytes ) > XATTR_SIZE_MAX ) <nl> return false ; <nl> if ( rmt -> rm_owner == 0 ) <nl> return false ;
mmm drivers / acpi / fan . c <nl> ppp drivers / acpi / fan . c <nl> static int acpi_fan_probe ( struct platform_device * pdev ) <nl> struct thermal_cooling_device * cdev ; <nl> struct acpi_fan * fan ; <nl> struct acpi_device * device = ACPI_COMPANION (& pdev -> dev ); <nl> + char * name ; <nl>  <nl> fan = devm_kzalloc (& pdev -> dev , sizeof (* fan ), GFP_KERNEL ); <nl> if (! fan ) { <nl> static int acpi_fan_probe ( struct platform_device * pdev ) <nl> } <nl> } <nl>  <nl> - cdev = thermal_cooling_device_register (" Fan ", device , <nl> + if (! strncmp ( pdev -> name , " PNP0C0B ", strlen (" PNP0C0B "))) <nl> + name = " Fan "; <nl> + else <nl> + name = acpi_device_bid ( device ); <nl> + <nl> + cdev = thermal_cooling_device_register ( name , device , <nl> & fan_cooling_ops ); <nl> if ( IS_ERR ( cdev )) { <nl> result = PTR_ERR ( cdev );
mmm drivers / net / ethernet / intel / ixgbe / ixgbe_x550 . c <nl> ppp drivers / net / ethernet / intel / ixgbe / ixgbe_x550 . c <nl> static s32 ixgbe_reset_hw_X550em ( struct ixgbe_hw * hw ) <nl> hw -> phy . sfp_setup_needed = false ; <nl> } <nl>  <nl> + if ( status == IXGBE_ERR_SFP_NOT_SUPPORTED ) <nl> + return status ; <nl> + <nl> /* Reset PHY */ <nl> if (! hw -> phy . reset_disable && hw -> phy . ops . reset ) <nl> hw -> phy . ops . reset ( hw );
mmm sound / soc / codecs / tlv320aic31xx . c <nl> ppp sound / soc / codecs / tlv320aic31xx . c <nl> static int aic31xx_set_dai_sysclk ( struct snd_soc_dai * codec_dai , <nl> } <nl> aic31xx -> p_div = i ; <nl>  <nl> - for ( i = 0 ; aic31xx_divs [ i ]. mclk_p != freq / aic31xx -> p_div ; i ++) { <nl> - if ( i == ARRAY_SIZE ( aic31xx_divs )) { <nl> - dev_err ( aic31xx -> dev , "% s : Unsupported frequency % d \ n ", <nl> - __func__ , freq ); <nl> - return - EINVAL ; <nl> - } <nl> + for ( i = 0 ; i < ARRAY_SIZE ( aic31xx_divs ) && <nl> + aic31xx_divs [ i ]. mclk_p != freq / aic31xx -> p_div ; i ++) <nl> + ; <nl> + if ( i == ARRAY_SIZE ( aic31xx_divs )) { <nl> + dev_err ( aic31xx -> dev , "% s : Unsupported frequency % d \ n ", <nl> + __func__ , freq ); <nl> + return - EINVAL ; <nl> } <nl>  <nl> /* set clock on MCLK , BCLK , or GPIO1 as PLL input */
mmm drivers / base / core . c <nl> ppp drivers / base / core . c <nl> device_create_groups_vargs ( struct class * class , struct device * parent , <nl> goto error ; <nl> } <nl>  <nl> + device_initialize ( dev ); <nl> dev -> devt = devt ; <nl> dev -> class = class ; <nl> dev -> parent = parent ; <nl> device_create_groups_vargs ( struct class * class , struct device * parent , <nl> if ( retval ) <nl> goto error ; <nl>  <nl> - retval = device_register ( dev ); <nl> + retval = device_add ( dev ); <nl> if ( retval ) <nl> goto error ; <nl> 
mmm fs / hpfs / file . c <nl> ppp fs / hpfs / file . c <nl> static void hpfs_write_failed ( struct address_space * mapping , loff_t to ) <nl> { <nl> struct inode * inode = mapping -> host ; <nl>  <nl> + hpfs_lock ( inode -> i_sb ); <nl> + <nl> if ( to > inode -> i_size ) { <nl> truncate_pagecache ( inode , to , inode -> i_size ); <nl> hpfs_truncate ( inode ); <nl> } <nl> + <nl> + hpfs_unlock ( inode -> i_sb ); <nl> } <nl>  <nl> static int hpfs_write_begin ( struct file * file , struct address_space * mapping ,
mmm fs / xfs / xfs_super . c <nl> ppp fs / xfs / xfs_super . c <nl> xfs_fs_remount ( <nl>  <nl> /* ro -> rw */ <nl> if (( mp -> m_flags & XFS_MOUNT_RDONLY ) && !(* flags & MS_RDONLY )) { <nl> + if ( mp -> m_flags & XFS_MOUNT_NORECOVERY ) { <nl> + xfs_warn ( mp , <nl> + " ro -> rw transition prohibited on norecovery mount "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> mp -> m_flags &= ~ XFS_MOUNT_RDONLY ; <nl>  <nl> /*
mmm drivers / gpu / drm / amd / display / dc / dcn10 / dcn10_dpp . c <nl> ppp drivers / gpu / drm / amd / display / dc / dcn10 / dcn10_dpp . c <nl> static void dpp1_cm_set_regamma_pwl ( <nl> struct dpp * dpp_base , const struct pwl_params * params , enum opp_regamma mode ) <nl> { <nl> struct dcn10_dpp * dpp = TO_DCN10_DPP ( dpp_base ); <nl> - uint32_t re_mode ; <nl> + uint32_t re_mode = 0 ; <nl>  <nl> switch ( mode ) { <nl> case OPP_REGAMMA_BYPASS :
mmm drivers / iommu / intel - svm . c <nl> ppp drivers / iommu / intel - svm . c <nl> int intel_svm_bind_mm ( struct device * dev , int * pasid , int flags , struct svm_dev_ <nl> pasid_max - 1 , GFP_KERNEL ); <nl> if ( ret < 0 ) { <nl> kfree ( svm ); <nl> + kfree ( sdev ); <nl> goto out ; <nl> } <nl> svm -> pasid = ret ;
mmm drivers / rpmsg / rpmsg_char . c <nl> ppp drivers / rpmsg / rpmsg_char . c <nl> static ssize_t rpmsg_eptdev_write_iter ( struct kiocb * iocb , <nl> if (! kbuf ) <nl> return - ENOMEM ; <nl>  <nl> - if (! copy_from_iter_full ( kbuf , len , from )) <nl> - return - EFAULT ; <nl> + if (! copy_from_iter_full ( kbuf , len , from )) { <nl> + ret = - EFAULT ; <nl> + goto free_kbuf ; <nl> + } <nl>  <nl> if ( mutex_lock_interruptible (& eptdev -> ept_lock )) { <nl> ret = - ERESTARTSYS ;
mmm arch / x86 / kvm / x86 . c <nl> ppp arch / x86 / kvm / x86 . c <nl> static int vcpu_enter_guest ( struct kvm_vcpu * vcpu ) <nl> } <nl> if ( kvm_check_request ( KVM_REQ_TRIPLE_FAULT , vcpu )) { <nl> vcpu -> run -> exit_reason = KVM_EXIT_SHUTDOWN ; <nl> + vcpu -> mmio_needed = 0 ; <nl> r = 0 ; <nl> goto out ; <nl> }mmm arch / x86 / kvm / vmx . c <nl> ppp arch / x86 / kvm / vmx . c <nl> static int vcpu_enter_guest ( struct kvm_vcpu * vcpu ) <nl> } <nl> if ( kvm_check_request ( KVM_REQ_TRIPLE_FAULT , vcpu )) { <nl> vcpu -> run -> exit_reason = KVM_EXIT_SHUTDOWN ; <nl> + vcpu -> mmio_needed = 0 ; <nl> r = 0 ; <nl> goto out ; <nl> } <nl> static int handle_external_interrupt ( struct kvm_vcpu * vcpu ) <nl> static int handle_triple_fault ( struct kvm_vcpu * vcpu ) <nl> { <nl> vcpu -> run -> exit_reason = KVM_EXIT_SHUTDOWN ; <nl> + vcpu -> mmio_needed = 0 ; <nl> return 0 ; <nl> } <nl> 
mmm drivers / uwb / uwbd . c <nl> ppp drivers / uwb / uwbd . c <nl> static int uwbd ( void * param ) <nl> /** Start the UWB daemon */ <nl> void uwbd_start ( struct uwb_rc * rc ) <nl> { <nl> - rc -> uwbd . task = kthread_run ( uwbd , rc , " uwbd "); <nl> - if ( rc -> uwbd . task == NULL ) <nl> + struct task_struct * task = kthread_run ( uwbd , rc , " uwbd "); <nl> + if ( IS_ERR ( task )) { <nl> + rc -> uwbd . task = NULL ; <nl> printk ( KERN_ERR " UWB : Cannot start management daemon ; " <nl> " UWB won ' t work \ n "); <nl> - else <nl> + } else { <nl> + rc -> uwbd . task = task ; <nl> rc -> uwbd . pid = rc -> uwbd . task -> pid ; <nl> + } <nl> } <nl>  <nl> /* Stop the UWB daemon and free any unprocessed events */ <nl> void uwbd_stop ( struct uwb_rc * rc ) <nl> { <nl> - kthread_stop ( rc -> uwbd . task ); <nl> + if ( rc -> uwbd . task ) <nl> + kthread_stop ( rc -> uwbd . task ); <nl> uwbd_flush ( rc ); <nl> } <nl> 
mmm mm / page_isolation . c <nl> ppp mm / page_isolation . c <nl> static void unset_migratetype_isolate ( struct page * page , unsigned migratetype ) <nl>  <nl> zone = page_zone ( page ); <nl> spin_lock_irqsave (& zone -> lock , flags ); <nl> - if ( get_pageblock_migratetype ( page ) != MIGRATE_ISOLATE ) <nl> + if (! is_migrate_isolate_page ( page )) <nl> goto out ; <nl>  <nl> /* <nl> int undo_isolate_page_range ( unsigned long start_pfn , unsigned long end_pfn , <nl> pfn < end_pfn ; <nl> pfn += pageblock_nr_pages ) { <nl> page = __first_valid_page ( pfn , pageblock_nr_pages ); <nl> - if (! page || get_pageblock_migratetype ( page ) != MIGRATE_ISOLATE ) <nl> + if (! page || ! is_migrate_isolate_page ( page )) <nl> continue ; <nl> unset_migratetype_isolate ( page , migratetype ); <nl> } <nl> int test_pages_isolated ( unsigned long start_pfn , unsigned long end_pfn , <nl> */ <nl> for ( pfn = start_pfn ; pfn < end_pfn ; pfn += pageblock_nr_pages ) { <nl> page = __first_valid_page ( pfn , pageblock_nr_pages ); <nl> - if ( page && get_pageblock_migratetype ( page ) != MIGRATE_ISOLATE ) <nl> + if ( page && ! is_migrate_isolate_page ( page )) <nl> break ; <nl> } <nl> page = __first_valid_page ( start_pfn , end_pfn - start_pfn );
mmm drivers / xen / blkback / vbd . c <nl> ppp drivers / xen / blkback / vbd . c <nl> int vbd_create ( blkif_t * blkif , blkif_vdev_t handle , unsigned major , <nl>  <nl> vbd -> pdevice = MKDEV ( major , minor ); <nl>  <nl> - bdev = open_by_devnum ( vbd -> pdevice , <nl> - vbd -> readonly ? FMODE_READ : FMODE_WRITE ); <nl> + bdev = blkdev_get_by_dev ( vbd -> pdevice , vbd -> readonly ? <nl> + FMODE_READ : FMODE_WRITE , NULL ); <nl>  <nl> if ( IS_ERR ( bdev )) { <nl> DPRINTK (" vbd_creat : device % 08x could not be opened .\ n ",
mmm fs / ext4 / inline . c <nl> ppp fs / ext4 / inline . c <nl> int ext4_da_write_inline_data_begin ( struct address_space * mapping , <nl> handle_t * handle ; <nl> struct page * page ; <nl> struct ext4_iloc iloc ; <nl> + int retries ; <nl>  <nl> ret = ext4_get_inode_loc ( inode , & iloc ); <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + retry_journal : <nl> handle = ext4_journal_start ( inode , EXT4_HT_INODE , 1 ); <nl> if ( IS_ERR ( handle )) { <nl> ret = PTR_ERR ( handle ); <nl> int ext4_da_write_inline_data_begin ( struct address_space * mapping , <nl> inode , <nl> flags , <nl> fsdata ); <nl> + ext4_journal_stop ( handle ); <nl> + handle = NULL ; <nl> + if ( ret == - ENOSPC && <nl> + ext4_should_retry_alloc ( inode -> i_sb , & retries )) <nl> + goto retry_journal ; <nl> goto out ; <nl> } <nl> 
mmm arch / arm / mach - imx / mach - imx6sl . c <nl> ppp arch / arm / mach - imx / mach - imx6sl . c <nl> static void __init imx6sl_init_late ( void ) <nl> if ( IS_ENABLED ( CONFIG_ARM_IMX6Q_CPUFREQ )) <nl> platform_device_register_simple (" imx6q - cpufreq ", - 1 , NULL , 0 ); <nl>  <nl> - if ( cpu_is_imx6sl ()) <nl> + if ( IS_ENABLED ( CONFIG_SOC_IMX6SL ) && cpu_is_imx6sl ()) <nl> imx6sl_cpuidle_init (); <nl> - else <nl> + else if ( IS_ENABLED ( CONFIG_SOC_IMX6SLL )) <nl> imx6sx_cpuidle_init (); <nl> } <nl> 
mmm net / ipv4 / ip_tunnel_core . c <nl> ppp net / ipv4 / ip_tunnel_core . c <nl> int iptunnel_xmit ( struct sock * sk , struct rtable * rt , struct sk_buff * skb , <nl> __be32 src , __be32 dst , __u8 proto , <nl> __u8 tos , __u8 ttl , __be16 df , bool xnet ) <nl> { <nl> - int pkt_len = skb -> len ; <nl> + int pkt_len = skb -> len - skb_inner_network_offset ( skb ); <nl> struct iphdr * iph ; <nl> int err ; <nl> 
mmm drivers / video / omap2 / dss / display . c <nl> ppp drivers / video / omap2 / dss / display . c <nl> int omapdss_default_get_recommended_bpp ( struct omap_dss_device * dssdev ) <nl> case OMAP_DISPLAY_TYPE_VENC : <nl> case OMAP_DISPLAY_TYPE_SDI : <nl> case OMAP_DISPLAY_TYPE_HDMI : <nl> + case OMAP_DISPLAY_TYPE_DVI : <nl> return 24 ; <nl> default : <nl> BUG ();mmm include / video / omapdss . h <nl> ppp include / video / omapdss . h <nl> int omapdss_default_get_recommended_bpp ( struct omap_dss_device * dssdev ) <nl> case OMAP_DISPLAY_TYPE_VENC : <nl> case OMAP_DISPLAY_TYPE_SDI : <nl> case OMAP_DISPLAY_TYPE_HDMI : <nl> + case OMAP_DISPLAY_TYPE_DVI : <nl> return 24 ; <nl> default : <nl> BUG (); <nl> enum omap_display_type { <nl> OMAP_DISPLAY_TYPE_DSI = 1 << 3 , <nl> OMAP_DISPLAY_TYPE_VENC = 1 << 4 , <nl> OMAP_DISPLAY_TYPE_HDMI = 1 << 5 , <nl> + OMAP_DISPLAY_TYPE_DVI = 1 << 6 , <nl> }; <nl>  <nl> enum omap_plane {
mmm drivers / platform / x86 / hp - wmi . c <nl> ppp drivers / platform / x86 / hp - wmi . c <nl> static int __init hp_wmi_input_setup ( void ) <nl> int err ; <nl>  <nl> hp_wmi_input_dev = input_allocate_device (); <nl> + if (! hp_wmi_input_dev ) <nl> + return - ENOMEM ; <nl>  <nl> hp_wmi_input_dev -> name = " HP WMI hotkeys "; <nl> hp_wmi_input_dev -> phys = " wmi / input0 ";
mmm drivers / gpu / drm / sun4i / sun8i_vi_layer . c <nl> ppp drivers / gpu / drm / sun4i / sun8i_vi_layer . c <nl> static int sun8i_vi_layer_atomic_check ( struct drm_plane * plane , <nl> clip . x2 = crtc_state -> adjusted_mode . hdisplay ; <nl> clip . y2 = crtc_state -> adjusted_mode . vdisplay ; <nl>  <nl> + min_scale = DRM_PLANE_HELPER_NO_SCALING ; <nl> + max_scale = DRM_PLANE_HELPER_NO_SCALING ; <nl> + <nl> if ( layer -> mixer -> cfg -> scaler_mask & BIT ( layer -> channel )) { <nl> min_scale = SUN8I_VI_SCALER_SCALE_MIN ; <nl> max_scale = SUN8I_VI_SCALER_SCALE_MAX ;
mmm drivers / rtc / rtc - vt8500 . c <nl> ppp drivers / rtc / rtc - vt8500 . c <nl> static int vt8500_rtc_probe ( struct platform_device * pdev ) <nl> writel ( VT8500_RTC_CR_ENABLE , <nl> vt8500_rtc -> regbase + VT8500_RTC_CR ); <nl>  <nl> - vt8500_rtc -> rtc = rtc_device_register (" vt8500 - rtc ", & pdev -> dev , <nl> + vt8500_rtc -> rtc = devm_rtc_device_register (& pdev -> dev , " vt8500 - rtc ", <nl> & vt8500_rtc_ops , THIS_MODULE ); <nl> if ( IS_ERR ( vt8500_rtc -> rtc )) { <nl> ret = PTR_ERR ( vt8500_rtc -> rtc ); <nl> static int vt8500_rtc_probe ( struct platform_device * pdev ) <nl> if ( ret < 0 ) { <nl> dev_err (& pdev -> dev , " can ' t get irq % i , err % d \ n ", <nl> vt8500_rtc -> irq_alarm , ret ); <nl> - goto err_unreg ; <nl> + goto err_return ; <nl> } <nl>  <nl> return 0 ; <nl>  <nl> - err_unreg : <nl> - rtc_device_unregister ( vt8500_rtc -> rtc ); <nl> err_return : <nl> return ret ; <nl> } <nl> static int vt8500_rtc_remove ( struct platform_device * pdev ) <nl> { <nl> struct vt8500_rtc * vt8500_rtc = platform_get_drvdata ( pdev ); <nl>  <nl> - rtc_device_unregister ( vt8500_rtc -> rtc ); <nl> - <nl> /* Disable alarm matching */ <nl> writel ( 0 , vt8500_rtc -> regbase + VT8500_RTC_IS ); <nl> 
mmm net / ipv4 / route . c <nl> ppp net / ipv4 / route . c <nl> static int inet_rtm_getroute ( struct sk_buff * in_skb , struct nlmsghdr * nlh , <nl> if ( rtm -> rtm_flags & RTM_F_LOOKUP_TABLE ) <nl> table_id = rt -> rt_table_id ; <nl>  <nl> - if ( rtm -> rtm_flags & RTM_F_FIB_MATCH ) <nl> + if ( rtm -> rtm_flags & RTM_F_FIB_MATCH ) { <nl> + if (! res . fi ) { <nl> + err = fib_props [ res . type ]. error ; <nl> + if (! err ) <nl> + err = - EHOSTUNREACH ; <nl> + goto errout_free ; <nl> + } <nl> err = fib_dump_info ( skb , NETLINK_CB ( in_skb ). portid , <nl> nlh -> nlmsg_seq , RTM_NEWROUTE , table_id , <nl> rt -> rt_type , res . prefix , res . prefixlen , <nl> fl4 . flowi4_tos , res . fi , 0 ); <nl> - else <nl> + } else { <nl> err = rt_fill_info ( net , dst , src , table_id , & fl4 , skb , <nl> NETLINK_CB ( in_skb ). portid , nlh -> nlmsg_seq ); <nl> + } <nl> if ( err < 0 ) <nl> goto errout_free ; <nl> 
mmm drivers / s390 / net / qeth_l3_main . c <nl> ppp drivers / s390 / net / qeth_l3_main . c <nl> qeth_l3_add_mc_to_hash ( struct qeth_card * card , struct in_device * in4_dev ) <nl>  <nl> tmp -> u . a4 . addr = be32_to_cpu ( im4 -> multiaddr ); <nl> memcpy ( tmp -> mac , buf , sizeof ( tmp -> mac )); <nl> + tmp -> is_multicast = 1 ; <nl>  <nl> ipm = qeth_l3_ip_from_hash ( card , tmp ); <nl> if ( ipm ) {
mmm net / bluetooth / hci_event . c <nl> ppp net / bluetooth / hci_event . c <nl> static void hci_cc_le_set_scan_enable ( struct hci_dev * hdev , <nl>  <nl> schedule_delayed_work (& hdev -> adv_work , ADV_CLEAR_TIMEOUT ); <nl>  <nl> - if ( hdev -> discovery . type == DISCOV_TYPE_INTERLEAVED ) { <nl> + if ( hdev -> discovery . type == DISCOV_TYPE_INTERLEAVED && <nl> + hdev -> discovery . state == DISCOVERY_FINDING ) { <nl> mgmt_interleaved_discovery ( hdev ); <nl> } else { <nl> hci_dev_lock ( hdev );
mmm drivers / media / video / sh_mobile_ceu_camera . c <nl> ppp drivers / media / video / sh_mobile_ceu_camera . c <nl> static int sh_mobile_ceu_try_fmt ( struct soc_camera_device * icd , <nl>  <nl> /* FIXME : calculate using depth and bus width */ <nl>  <nl> - if ( f -> fmt . pix . height < 4 ) <nl> - f -> fmt . pix . height = 4 ; <nl> - if ( f -> fmt . pix . height > 1920 ) <nl> - f -> fmt . pix . height = 1920 ; <nl> - if ( f -> fmt . pix . width < 2 ) <nl> - f -> fmt . pix . width = 2 ; <nl> - if ( f -> fmt . pix . width > 2560 ) <nl> - f -> fmt . pix . width = 2560 ; <nl> - f -> fmt . pix . width &= ~ 0x01 ; <nl> - f -> fmt . pix . height &= ~ 0x03 ; <nl> + v4l_bound_align_image (& f -> fmt . pix . width , 2 , 2560 , 1 , <nl> + & f -> fmt . pix . height , 4 , 1920 , 2 , 0 ); <nl>  <nl> f -> fmt . pix . bytesperline = f -> fmt . pix . width * <nl> DIV_ROUND_UP ( xlate -> host_fmt -> depth , 8 );
mmm drivers / net / ethernet / intel / ixgbe / ixgbe_main . c <nl> ppp drivers / net / ethernet / intel / ixgbe / ixgbe_main . c <nl> static void * ixgbe_fwd_add ( struct net_device * pdev , struct net_device * vdev ) <nl> ( adapter -> num_rx_pools > IXGBE_MAX_MACVLANS )) <nl> return ERR_PTR (- EBUSY ); <nl>  <nl> - fwd_adapter = kcalloc ( 1 , sizeof ( struct ixgbe_fwd_adapter ), GFP_KERNEL ); <nl> + fwd_adapter = kzalloc ( sizeof (* fwd_adapter ), GFP_KERNEL ); <nl> if (! fwd_adapter ) <nl> return ERR_PTR (- ENOMEM ); <nl> 
mmm drivers / net / ethernet / emulex / benet / be . h <nl> ppp drivers / net / ethernet / emulex / benet / be . h <nl> struct be_tx_stats { <nl> u64 tx_compl ; <nl> ulong tx_jiffies ; <nl> u32 tx_stops ; <nl> + u32 tx_drv_drops ; /* pkts dropped by driver */ <nl> struct u64_stats_sync sync ; <nl> struct u64_stats_sync sync_compl ; <nl> };mmm drivers / net / ethernet / emulex / benet / be_ethtool . c <nl> ppp drivers / net / ethernet / emulex / benet / be_ethtool . c <nl> struct be_tx_stats { <nl> u64 tx_compl ; <nl> ulong tx_jiffies ; <nl> u32 tx_stops ; <nl> + u32 tx_drv_drops ; /* pkts dropped by driver */ <nl> struct u64_stats_sync sync ; <nl> struct u64_stats_sync sync_compl ; <nl> }; <nl> static const struct be_ethtool_stat et_tx_stats [] = { <nl> /* Number of times the TX queue was stopped due to lack <nl> * of spaces in the TXQ . <nl> */ <nl> - { DRVSTAT_TX_INFO ( tx_stops )} <nl> + { DRVSTAT_TX_INFO ( tx_stops )}, <nl> + /* Pkts dropped in the driver ' s transmit path */ <nl> + { DRVSTAT_TX_INFO ( tx_drv_drops )} <nl> }; <nl> # define ETHTOOL_TXSTATS_NUM ( ARRAY_SIZE ( et_tx_stats )) <nl> mmm drivers / net / ethernet / emulex / benet / be_main . c <nl> ppp drivers / net / ethernet / emulex / benet / be_main . c <nl> struct be_tx_stats { <nl> u64 tx_compl ; <nl> ulong tx_jiffies ; <nl> u32 tx_stops ; <nl> + u32 tx_drv_drops ; /* pkts dropped by driver */ <nl> struct u64_stats_sync sync ; <nl> struct u64_stats_sync sync_compl ; <nl> }; <nl> static const struct be_ethtool_stat et_tx_stats [] = { <nl> /* Number of times the TX queue was stopped due to lack <nl> * of spaces in the TXQ . <nl> */ <nl> - { DRVSTAT_TX_INFO ( tx_stops )} <nl> + { DRVSTAT_TX_INFO ( tx_stops )}, <nl> + /* Pkts dropped in the driver ' s transmit path */ <nl> + { DRVSTAT_TX_INFO ( tx_drv_drops )} <nl> }; <nl> # define ETHTOOL_TXSTATS_NUM ( ARRAY_SIZE ( et_tx_stats )) <nl>  <nl> static netdev_tx_t be_xmit ( struct sk_buff * skb , struct net_device * netdev ) <nl> u32 start = txq -> head ; <nl>  <nl> skb = be_xmit_workarounds ( adapter , skb , & skip_hw_vlan ); <nl> - if (! skb ) <nl> + if (! skb ) { <nl> + tx_stats ( txo )-> tx_drv_drops ++; <nl> return NETDEV_TX_OK ; <nl> + } <nl>  <nl> wrb_cnt = wrb_cnt_for_skb ( adapter , skb , & dummy_wrb ); <nl>  <nl> static netdev_tx_t be_xmit ( struct sk_buff * skb , struct net_device * netdev ) <nl> be_tx_stats_update ( txo , wrb_cnt , copied , gso_segs , stopped ); <nl> } else { <nl> txq -> head = start ; <nl> + tx_stats ( txo )-> tx_drv_drops ++; <nl> dev_kfree_skb_any ( skb ); <nl> } <nl> return NETDEV_TX_OK ;
mmm drivers / tty / serial / fsl_lpuart . c <nl> ppp drivers / tty / serial / fsl_lpuart . c <nl> static void lpuart_break_ctl ( struct uart_port * port , int break_state ) <nl> static void lpuart_setup_watermark ( struct lpuart_port * sport ) <nl> { <nl> unsigned char val , cr2 ; <nl> + unsigned char cr2_saved ; <nl>  <nl> cr2 = readb ( sport -> port . membase + UARTCR2 ); <nl> + cr2_saved = cr2 ; <nl> cr2 &= ~( UARTCR2_TIE | UARTCR2_TCIE | UARTCR2_TE | <nl> UARTCR2_RIE | UARTCR2_RE ); <nl> writeb ( cr2 , sport -> port . membase + UARTCR2 ); <nl> static void lpuart_setup_watermark ( struct lpuart_port * sport ) <nl>  <nl> writeb ( 2 , sport -> port . membase + UARTTWFIFO ); <nl> writeb ( 1 , sport -> port . membase + UARTRWFIFO ); <nl> + <nl> + /* Restore cr2 */ <nl> + writeb ( cr2_saved , sport -> port . membase + UARTCR2 ); <nl> } <nl>  <nl> static int lpuart_startup ( struct uart_port * port )
mmm net / socket . c <nl> ppp net / socket . c <nl> static int __sys_sendmsg ( struct socket * sock , struct msghdr __user * msg , <nl> * used_address -> name_len is initialized to UINT_MAX so that the first <nl> * destination address never matches . <nl> */ <nl> - if ( used_address && used_address -> name_len == msg_sys -> msg_namelen && <nl> - ! memcmp (& used_address -> name , msg -> msg_name , <nl> + if ( used_address && msg_sys -> msg_name && <nl> + used_address -> name_len == msg_sys -> msg_namelen && <nl> + ! memcmp (& used_address -> name , msg_sys -> msg_name , <nl> used_address -> name_len )) { <nl> err = sock_sendmsg_nosec ( sock , msg_sys , total_len ); <nl> goto out_freectl ; <nl> static int __sys_sendmsg ( struct socket * sock , struct msghdr __user * msg , <nl> */ <nl> if ( used_address && err >= 0 ) { <nl> used_address -> name_len = msg_sys -> msg_namelen ; <nl> - memcpy (& used_address -> name , msg -> msg_name , <nl> - used_address -> name_len ); <nl> + if ( msg_sys -> msg_name ) <nl> + memcpy (& used_address -> name , msg_sys -> msg_name , <nl> + used_address -> name_len ); <nl> } <nl>  <nl> out_freectl :
mmm drivers / media / rc / ir - lirc - codec . c <nl> ppp drivers / media / rc / ir - lirc - codec . c <nl> static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> val ); <nl>  <nl> case LIRC_SET_REC_CARRIER_RANGE : <nl> + if (! dev -> s_rx_carrier_range ) <nl> + return - ENOTTY ; <nl> + <nl> if ( val <= 0 ) <nl> return - EINVAL ; <nl>  <nl> static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> break ; <nl>  <nl> case LIRC_SET_REC_TIMEOUT_REPORTS : <nl> + if (! dev -> timeout ) <nl> + return - ENOTTY ; <nl> + <nl> lirc -> send_timeout_reports = !! val ; <nl> break ; <nl> 
mmm drivers / gpu / drm / sti / sti_hdmi . c <nl> ppp drivers / gpu / drm / sti / sti_hdmi . c <nl> static void sti_hdmi_disable ( struct drm_bridge * bridge ) <nl> clk_disable_unprepare ( hdmi -> clk_pix ); <nl>  <nl> hdmi -> enabled = false ; <nl> + <nl> + cec_notifier_set_phys_addr ( hdmi -> notifier , CEC_PHYS_ADDR_INVALID ); <nl> } <nl>  <nl> /** <nl> static int sti_hdmi_connector_get_modes ( struct drm_connector * connector ) <nl> DRM_DEBUG_KMS ("% s : % dx % d cm \ n ", <nl> ( hdmi -> hdmi_monitor ? " hdmi monitor " : " dvi monitor "), <nl> edid -> width_cm , edid -> height_cm ); <nl> + cec_notifier_set_phys_addr_from_edid ( hdmi -> notifier , edid ); <nl>  <nl> count = drm_add_edid_modes ( connector , edid ); <nl> drm_mode_connector_update_edid_property ( connector , edid ); <nl> sti_hdmi_connector_detect ( struct drm_connector * connector , bool force ) <nl> } <nl>  <nl> DRM_DEBUG_DRIVER (" hdmi cable disconnected \ n "); <nl> + cec_notifier_set_phys_addr ( hdmi -> notifier , CEC_PHYS_ADDR_INVALID ); <nl> return connector_status_disconnected ; <nl> } <nl>  <nl> static int sti_hdmi_probe ( struct platform_device * pdev ) <nl> goto release_adapter ; <nl> } <nl>  <nl> + hdmi -> notifier = cec_notifier_get (& pdev -> dev ); <nl> + if (! hdmi -> notifier ) <nl> + goto release_adapter ; <nl> + <nl> hdmi -> reset = devm_reset_control_get ( dev , " hdmi "); <nl> /* Take hdmi out of reset */ <nl> if (! IS_ERR ( hdmi -> reset )) <nl> static int sti_hdmi_remove ( struct platform_device * pdev ) <nl> { <nl> struct sti_hdmi * hdmi = dev_get_drvdata (& pdev -> dev ); <nl>  <nl> + cec_notifier_set_phys_addr ( hdmi -> notifier , CEC_PHYS_ADDR_INVALID ); <nl> + <nl> i2c_put_adapter ( hdmi -> ddc_adapt ); <nl> if ( hdmi -> audio_pdev ) <nl> platform_device_unregister ( hdmi -> audio_pdev ); <nl> component_del (& pdev -> dev , & sti_hdmi_ops ); <nl>  <nl> + cec_notifier_put ( hdmi -> notifier ); <nl> return 0 ; <nl> } <nl> mmm drivers / gpu / drm / sti / sti_hdmi . h <nl> ppp drivers / gpu / drm / sti / sti_hdmi . h <nl> static void sti_hdmi_disable ( struct drm_bridge * bridge ) <nl> clk_disable_unprepare ( hdmi -> clk_pix ); <nl>  <nl> hdmi -> enabled = false ; <nl> + <nl> + cec_notifier_set_phys_addr ( hdmi -> notifier , CEC_PHYS_ADDR_INVALID ); <nl> } <nl>  <nl> /** <nl> static int sti_hdmi_connector_get_modes ( struct drm_connector * connector ) <nl> DRM_DEBUG_KMS ("% s : % dx % d cm \ n ", <nl> ( hdmi -> hdmi_monitor ? " hdmi monitor " : " dvi monitor "), <nl> edid -> width_cm , edid -> height_cm ); <nl> + cec_notifier_set_phys_addr_from_edid ( hdmi -> notifier , edid ); <nl>  <nl> count = drm_add_edid_modes ( connector , edid ); <nl> drm_mode_connector_update_edid_property ( connector , edid ); <nl> sti_hdmi_connector_detect ( struct drm_connector * connector , bool force ) <nl> } <nl>  <nl> DRM_DEBUG_DRIVER (" hdmi cable disconnected \ n "); <nl> + cec_notifier_set_phys_addr ( hdmi -> notifier , CEC_PHYS_ADDR_INVALID ); <nl> return connector_status_disconnected ; <nl> } <nl>  <nl> static int sti_hdmi_probe ( struct platform_device * pdev ) <nl> goto release_adapter ; <nl> } <nl>  <nl> + hdmi -> notifier = cec_notifier_get (& pdev -> dev ); <nl> + if (! hdmi -> notifier ) <nl> + goto release_adapter ; <nl> + <nl> hdmi -> reset = devm_reset_control_get ( dev , " hdmi "); <nl> /* Take hdmi out of reset */ <nl> if (! IS_ERR ( hdmi -> reset )) <nl> static int sti_hdmi_remove ( struct platform_device * pdev ) <nl> { <nl> struct sti_hdmi * hdmi = dev_get_drvdata (& pdev -> dev ); <nl>  <nl> + cec_notifier_set_phys_addr ( hdmi -> notifier , CEC_PHYS_ADDR_INVALID ); <nl> + <nl> i2c_put_adapter ( hdmi -> ddc_adapt ); <nl> if ( hdmi -> audio_pdev ) <nl> platform_device_unregister ( hdmi -> audio_pdev ); <nl> component_del (& pdev -> dev , & sti_hdmi_ops ); <nl>  <nl> + cec_notifier_put ( hdmi -> notifier ); <nl> return 0 ; <nl> } <nl>  <nl> # include < linux / platform_device . h > <nl>  <nl> # include < drm / drmP . h > <nl> +# include < media / cec - notifier . h > <nl>  <nl> # define HDMI_STA 0x0010 <nl> # define HDMI_STA_DLL_LCK BIT ( 5 ) <nl> static const struct drm_prop_enum_list colorspace_mode_names [] = { <nl> * @ audio_pdev : ASoC hdmi - codec platform device <nl> * @ audio : hdmi audio parameters . <nl> * @ drm_connector : hdmi connector <nl> + * @ notifier : hotplug detect notifier <nl> */ <nl> struct sti_hdmi { <nl> struct device dev ; <nl> struct sti_hdmi { <nl> struct platform_device * audio_pdev ; <nl> struct hdmi_audio_params audio ; <nl> struct drm_connector * drm_connector ; <nl> + struct cec_notifier * notifier ; <nl> }; <nl>  <nl> u32 hdmi_read ( struct sti_hdmi * hdmi , int offset );
mmm drivers / hid / hid - magicmouse . c <nl> ppp drivers / hid / hid - magicmouse . c <nl> static void magicmouse_setup_input ( struct input_dev * input , struct hid_device * h <nl> __set_bit ( REL_HWHEEL , input -> relbit ); <nl> } <nl> } else { /* USB_DEVICE_ID_APPLE_MAGICTRACKPAD */ <nl> + /* input -> keybit is initialized with incorrect button info <nl> + * for Magic Trackpad . There really is only one physical <nl> + * button ( BTN_LEFT == BTN_MOUSE ). Make sure we don ' t <nl> + * advertise buttons that don ' t exist ... <nl> + */ <nl> + __clear_bit ( BTN_RIGHT , input -> keybit ); <nl> + __clear_bit ( BTN_MIDDLE , input -> keybit ); <nl> __set_bit ( BTN_MOUSE , input -> keybit ); <nl> __set_bit ( BTN_TOOL_FINGER , input -> keybit ); <nl> __set_bit ( BTN_TOOL_DOUBLETAP , input -> keybit );
mmm drivers / staging / media / imx / imx - media - capture . c <nl> ppp drivers / staging / media / imx / imx - media - capture . c <nl> static int vidioc_querycap ( struct file * file , void * fh , <nl> { <nl> struct capture_priv * priv = video_drvdata ( file ); <nl>  <nl> - strncpy ( cap -> driver , " imx - media - capture ", sizeof ( cap -> driver ) - 1 ); <nl> - strncpy ( cap -> card , " imx - media - capture ", sizeof ( cap -> card ) - 1 ); <nl> + strlcpy ( cap -> driver , " imx - media - capture ", sizeof ( cap -> driver )); <nl> + strlcpy ( cap -> card , " imx - media - capture ", sizeof ( cap -> card )); <nl> snprintf ( cap -> bus_info , sizeof ( cap -> bus_info ), <nl> " platform :% s ", priv -> src_sd -> name ); <nl> 
mmm drivers / gpu / drm / radeon / radeon_display . c <nl> ppp drivers / gpu / drm / radeon / radeon_display . c <nl> static void radeon_compute_pll_legacy ( struct radeon_pll * pll , <nl> max_fractional_feed_div = pll -> max_frac_feedback_div ; <nl> } <nl>  <nl> - for ( post_div = min_post_div ; post_div <= max_post_div ; ++ post_div ) { <nl> + for ( post_div = max_post_div ; post_div >= min_post_div ; -- post_div ) { <nl> uint32_t ref_div ; <nl>  <nl> if (( pll -> flags & RADEON_PLL_NO_ODD_POST_DIV ) && ( post_div & 1 ))
mmm fs / ext4 / super . c <nl> ppp fs / ext4 / super . c <nl> static int ext4_quota_enable ( struct super_block * sb , int type , int format_id , <nl> return PTR_ERR ( qf_inode ); <nl> } <nl>  <nl> + /* Don ' t account quota for quota files to avoid recursion */ <nl> + qf_inode -> i_flags |= S_NOQUOTA ; <nl> err = dquot_enable ( qf_inode , type , format_id , flags ); <nl> iput ( qf_inode ); <nl> 
mmm tools / hv / hv_kvp_daemon . c <nl> ppp tools / hv / hv_kvp_daemon . c <nl> int main ( void ) <nl> pfd . fd = fd ; <nl>  <nl> while ( 1 ) { <nl> + struct sockaddr * addr_p = ( struct sockaddr *) & addr ; <nl> + socklen_t addr_l = sizeof ( addr ); <nl> pfd . events = POLLIN ; <nl> pfd . revents = 0 ; <nl> poll (& pfd , 1 , - 1 ); <nl>  <nl> - len = recv ( fd , kvp_recv_buffer , sizeof ( kvp_recv_buffer ), 0 ); <nl> + len = recvfrom ( fd , kvp_recv_buffer , sizeof ( kvp_recv_buffer ), 0 , <nl> + addr_p , & addr_l ); <nl>  <nl> - if ( len < 0 ) { <nl> - syslog ( LOG_ERR , " recv failed ; error :% d ", len ); <nl> + if ( len < 0 || addr . nl_pid ) { <nl> + syslog ( LOG_ERR , " recvfrom failed ; pid :% u error :% d % s ", <nl> + addr . nl_pid , errno , strerror ( errno )); <nl> close ( fd ); <nl> return - 1 ; <nl> }
mmm net / phonet / pep . c <nl> ppp net / phonet / pep . c <nl> static struct sock * pep_sock_accept ( struct sock * sk , int flags , int * errp , <nl>  <nl> err = pep_accept_conn ( newsk , skb ); <nl> if ( err ) { <nl> + __sock_put ( sk ); <nl> sock_put ( newsk ); <nl> newsk = NULL ; <nl> goto drop ;
mmm drivers / gpu / drm / i915 / intel_sdvo . c <nl> ppp drivers / gpu / drm / i915 / intel_sdvo . c <nl> static bool intel_sdvo_detect_hdmi_audio ( struct drm_connector * connector ) <nl> edid = intel_sdvo_get_edid ( connector ); <nl> if ( edid != NULL && edid -> input & DRM_EDID_INPUT_DIGITAL ) <nl> has_audio = drm_detect_monitor_audio ( edid ); <nl> + kfree ( edid ); <nl>  <nl> return has_audio ; <nl> }
mmm drivers / scsi / scsi_scan . c <nl> ppp drivers / scsi / scsi_scan . c <nl> static int scsi_report_lun_scan ( struct scsi_target * starget , int bflags , <nl> out_err : <nl> kfree ( lun_data ); <nl> out : <nl> - scsi_device_put ( sdev ); <nl> if ( scsi_device_created ( sdev )) <nl> /* <nl> * the sdev we used didn ' t appear in the report luns scan <nl> */ <nl> __scsi_remove_device ( sdev ); <nl> + scsi_device_put ( sdev ); <nl> return ret ; <nl> } <nl> 
mmm drivers / scsi / megaraid / megaraid_sas_base . c <nl> ppp drivers / scsi / megaraid / megaraid_sas_base . c <nl> int megasas_alloc_cmds ( struct megasas_instance * instance ) <nl> if ( megasas_create_frame_pool ( instance )) { <nl> dev_printk ( KERN_DEBUG , & instance -> pdev -> dev , " Error creating frame DMA pool \ n "); <nl> megasas_free_cmds ( instance ); <nl> + return - ENOMEM ; <nl> } <nl>  <nl> return 0 ;
mmm net / core / pktgen . c <nl> ppp net / core / pktgen . c <nl> static void spin ( struct pktgen_dev * pkt_dev , ktime_t spin_until ) <nl> hrtimer_set_expires (& t . timer , spin_until ); <nl>  <nl> remaining = ktime_to_ns ( hrtimer_expires_remaining (& t . timer )); <nl> - if ( remaining <= 0 ) { <nl> - pkt_dev -> next_tx = ktime_add_ns ( spin_until , pkt_dev -> delay ); <nl> - return ; <nl> - } <nl> + if ( remaining <= 0 ) <nl> + goto out ; <nl>  <nl> start_time = ktime_get (); <nl> if ( remaining < 100000 ) { <nl> static void spin ( struct pktgen_dev * pkt_dev , ktime_t spin_until ) <nl> } <nl>  <nl> pkt_dev -> idle_acc += ktime_to_ns ( ktime_sub ( end_time , start_time )); <nl> + out : <nl> pkt_dev -> next_tx = ktime_add_ns ( spin_until , pkt_dev -> delay ); <nl> + destroy_hrtimer_on_stack (& t . timer ); <nl> } <nl>  <nl> static inline void set_pkt_overhead ( struct pktgen_dev * pkt_dev )
mmm net / ipv4 / netfilter / ipt_recent . c <nl> ppp net / ipv4 / netfilter / ipt_recent . c <nl> static int ip_recent_ctrl ( struct file * file , const char __user * input , unsigned <nl> curr_table -> table [ count ]. last_seen = 0 ; <nl> curr_table -> table [ count ]. addr = 0 ; <nl> curr_table -> table [ count ]. ttl = 0 ; <nl> - memset ( curr_table -> table [ count ]. last_pkts , 0 , ip_pkt_list_tot * sizeof ( u_int32_t )); <nl> + memset ( curr_table -> table [ count ]. last_pkts , 0 , ip_pkt_list_tot * sizeof ( unsigned long )); <nl> curr_table -> table [ count ]. oldest_pkt = 0 ; <nl> curr_table -> table [ count ]. time_pos = 0 ; <nl> curr_table -> time_info [ count ]. position = count ; <nl> match ( const struct sk_buff * skb , <nl> location = time_info [ curr_table -> time_pos ]. position ; <nl> hash_table [ r_list [ location ]. hash_entry ] = - 1 ; <nl> hash_table [ hash_result ] = location ; <nl> - memset ( r_list [ location ]. last_pkts , 0 , ip_pkt_list_tot * sizeof ( u_int32_t )); <nl> + memset ( r_list [ location ]. last_pkts , 0 , ip_pkt_list_tot * sizeof ( unsigned long )); <nl> r_list [ location ]. time_pos = curr_table -> time_pos ; <nl> r_list [ location ]. addr = addr ; <nl> r_list [ location ]. ttl = ttl ; <nl> match ( const struct sk_buff * skb , <nl> r_list [ location ]. last_seen = 0 ; <nl> r_list [ location ]. addr = 0 ; <nl> r_list [ location ]. ttl = 0 ; <nl> - memset ( r_list [ location ]. last_pkts , 0 , ip_pkt_list_tot * sizeof ( u_int32_t )); <nl> + memset ( r_list [ location ]. last_pkts , 0 , ip_pkt_list_tot * sizeof ( unsigned long )); <nl> r_list [ location ]. oldest_pkt = 0 ; <nl> ans = ! info -> invert ; <nl> } <nl> checkentry ( const char * tablename , <nl> memset ( curr_table -> table , 0 , sizeof ( struct recent_ip_list )* ip_list_tot ); <nl> # ifdef DEBUG <nl> if ( debug ) printk ( KERN_INFO RECENT_NAME ": checkentry : Allocating % d for pkt_list .\ n ", <nl> - sizeof ( u_int32_t )* ip_pkt_list_tot * ip_list_tot ); <nl> + sizeof ( unsigned long )* ip_pkt_list_tot * ip_list_tot ); <nl> # endif <nl>  <nl> - hold = vmalloc ( sizeof ( u_int32_t )* ip_pkt_list_tot * ip_list_tot ); <nl> + hold = vmalloc ( sizeof ( unsigned long )* ip_pkt_list_tot * ip_list_tot ); <nl> # ifdef DEBUG <nl> if ( debug ) printk ( KERN_INFO RECENT_NAME ": checkentry : After pkt_list allocation .\ n "); <nl> # endif
mmm tools / perf / arch / x86 / util / intel - bts . c <nl> ppp tools / perf / arch / x86 / util / intel - bts . c <nl> struct auxtrace_record * intel_bts_recording_init ( int * err ) <nl> if (! intel_bts_pmu ) <nl> return NULL ; <nl>  <nl> + if ( setenv (" JITDUMP_USE_ARCH_TIMESTAMP ", " 1 ", 1 )) { <nl> + * err = - errno ; <nl> + return NULL ; <nl> + } <nl> + <nl> btsr = zalloc ( sizeof ( struct intel_bts_recording )); <nl> if (! btsr ) { <nl> * err = - ENOMEM ;mmm tools / perf / arch / x86 / util / intel - pt . c <nl> ppp tools / perf / arch / x86 / util / intel - pt . c <nl> struct auxtrace_record * intel_bts_recording_init ( int * err ) <nl> if (! intel_bts_pmu ) <nl> return NULL ; <nl>  <nl> + if ( setenv (" JITDUMP_USE_ARCH_TIMESTAMP ", " 1 ", 1 )) { <nl> + * err = - errno ; <nl> + return NULL ; <nl> + } <nl> + <nl> btsr = zalloc ( sizeof ( struct intel_bts_recording )); <nl> if (! btsr ) { <nl> * err = - ENOMEM ; <nl> struct auxtrace_record * intel_pt_recording_init ( int * err ) <nl> if (! intel_pt_pmu ) <nl> return NULL ; <nl>  <nl> + if ( setenv (" JITDUMP_USE_ARCH_TIMESTAMP ", " 1 ", 1 )) { <nl> + * err = - errno ; <nl> + return NULL ; <nl> + } <nl> + <nl> ptr = zalloc ( sizeof ( struct intel_pt_recording )); <nl> if (! ptr ) { <nl> * err = - ENOMEM ;
mmm net / xfrm / xfrm_input . c <nl> ppp net / xfrm / xfrm_input . c <nl> EXPORT_SYMBOL ( xfrm_prepare_input ); <nl>  <nl> int xfrm_input ( struct sk_buff * skb , int nexthdr , __be32 spi , int encap_type ) <nl> { <nl> + struct net * net = dev_net ( skb -> dev ); <nl> int err ; <nl> __be32 seq ; <nl> struct xfrm_state * x ; <nl> int xfrm_input ( struct sk_buff * skb , int nexthdr , __be32 spi , int encap_type ) <nl> goto drop ; <nl> } <nl>  <nl> - x = xfrm_state_lookup (& init_net , daddr , spi , nexthdr , family ); <nl> + x = xfrm_state_lookup ( net , daddr , spi , nexthdr , family ); <nl> if ( x == NULL ) { <nl> XFRM_INC_STATS ( LINUX_MIB_XFRMINNOSTATES ); <nl> xfrm_audit_state_notfound ( skb , family , spi , seq );
mmm drivers / virt / vboxguest / vboxguest_linux . c <nl> ppp drivers / virt / vboxguest / vboxguest_linux . c <nl> static long vbg_misc_device_ioctl ( struct file * filp , unsigned int req , <nl> if (! buf ) <nl> return - ENOMEM ; <nl>  <nl> - if ( copy_from_user ( buf , ( void *) arg , hdr . size_in )) { <nl> + *(( struct vbg_ioctl_hdr *) buf ) = hdr ; <nl> + if ( copy_from_user ( buf + sizeof ( hdr ), ( void *) arg + sizeof ( hdr ), <nl> + hdr . size_in - sizeof ( hdr ))) { <nl> ret = - EFAULT ; <nl> goto out ; <nl> }
mmm net / bridge / br_multicast . c <nl> ppp net / bridge / br_multicast . c <nl> static int br_multicast_ipv4_rcv ( struct net_bridge * br , <nl> if ( unlikely ( ip_fast_csum (( u8 *) iph , iph -> ihl ))) <nl> return - EINVAL ; <nl>  <nl> - if ( iph -> protocol != IPPROTO_IGMP ) <nl> + if ( iph -> protocol != IPPROTO_IGMP ) { <nl> + if (( iph -> daddr & IGMP_LOCAL_GROUP_MASK ) != IGMP_LOCAL_GROUP ) <nl> + BR_INPUT_SKB_CB ( skb )-> mrouters_only = 1 ; <nl> return 0 ; <nl> + } <nl>  <nl> len = ntohs ( iph -> tot_len ); <nl> if ( skb -> len < len || len < ip_hdrlen ( skb ))
mmm drivers / net / wireless / wl12xx / scan . c <nl> ppp drivers / net / wireless / wl12xx / scan . c <nl> wl12xx_scan_sched_scan_ssid_list ( struct wl1271 * wl , <nl> return - ENOMEM ; <nl>  <nl> while (( cmd -> n_ssids < req -> n_ssids ) && ssid ) { <nl> - if ( ssid -> ssid_len == 0 ) <nl> + if ( ssid -> ssid_len == 0 ) { <nl> wildcard = 1 ; <nl> - cmd -> ssids [ cmd -> n_ssids ]. type = SCAN_SSID_TYPE_HIDDEN ; <nl> + cmd -> ssids [ cmd -> n_ssids ]. type = SCAN_SSID_TYPE_PUBLIC ; <nl> + } else { <nl> + cmd -> ssids [ cmd -> n_ssids ]. type = SCAN_SSID_TYPE_HIDDEN ; <nl> + } <nl> cmd -> ssids [ cmd -> n_ssids ]. len = ssid -> ssid_len ; <nl> memcpy ( cmd -> ssids [ cmd -> n_ssids ]. ssid , ssid -> ssid , <nl> ssid -> ssid_len );
mmm net / ipv4 / fib_hash . c <nl> ppp net / ipv4 / fib_hash . c <nl> static int fn_hash_insert ( struct fib_table * tb , struct fib_config * cfg ) <nl> struct fib_info * fi_drop ; <nl> u8 state ; <nl>  <nl> + if ( fi -> fib_treeref > 1 ) <nl> + goto out ; <nl> + <nl> write_lock_bh (& fib_hash_lock ); <nl> fi_drop = fa -> fa_info ; <nl> fa -> fa_info = fi ;
mmm kernel / printk / printk . c <nl> ppp kernel / printk / printk . c <nl> EXPORT_SYMBOL ( vprintk_emit ); <nl>  <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> { <nl> - return vprintk_emit ( 0 , LOGLEVEL_DEFAULT , NULL , 0 , fmt , args ); <nl> + return vprintk_func ( fmt , args ); <nl> } <nl> EXPORT_SYMBOL ( vprintk ); <nl> 
mmm drivers / gpu / drm / drm_prime . c <nl> ppp drivers / gpu / drm / drm_prime . c <nl> int drm_gem_prime_fd_to_handle ( struct drm_device * dev , <nl> get_dma_buf ( dma_buf ); <nl> } <nl>  <nl> - /* drm_gem_handle_create_tail unlocks dev -> object_name_lock . */ <nl> + /* _handle_create_tail unconditionally unlocks dev -> object_name_lock . */ <nl> ret = drm_gem_handle_create_tail ( file_priv , obj , handle ); <nl> drm_gem_object_unreference_unlocked ( obj ); <nl> if ( ret ) <nl> int drm_gem_prime_fd_to_handle ( struct drm_device * dev , <nl>  <nl> ret = drm_prime_add_buf_handle (& file_priv -> prime , <nl> dma_buf , * handle ); <nl> + mutex_unlock (& file_priv -> prime . lock ); <nl> if ( ret ) <nl> goto fail ; <nl>  <nl> - mutex_unlock (& file_priv -> prime . lock ); <nl> - <nl> dma_buf_put ( dma_buf ); <nl>  <nl> return 0 ; <nl> int drm_gem_prime_fd_to_handle ( struct drm_device * dev , <nl> * to detach .. which seems ok .. <nl> */ <nl> drm_gem_handle_delete ( file_priv , * handle ); <nl> + dma_buf_put ( dma_buf ); <nl> + return ret ; <nl> + <nl> out_unlock : <nl> mutex_unlock (& dev -> object_name_lock ); <nl> out_put : <nl> - dma_buf_put ( dma_buf ); <nl> mutex_unlock (& file_priv -> prime . lock ); <nl> + dma_buf_put ( dma_buf ); <nl> return ret ; <nl> } <nl> EXPORT_SYMBOL ( drm_gem_prime_fd_to_handle );
mmm drivers / usb / core / config . c <nl> ppp drivers / usb / core / config . c <nl> static int usb_parse_configuration ( struct usb_device * dev , int cfgidx , <nl>  <nl> } else if ( header -> bDescriptorType == <nl> USB_DT_INTERFACE_ASSOCIATION ) { <nl> + struct usb_interface_assoc_descriptor * d ; <nl> + <nl> + d = ( struct usb_interface_assoc_descriptor *) header ; <nl> + if ( d -> bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE ) { <nl> + dev_warn ( ddev , <nl> + " config % d has an invalid interface association descriptor of length % d , skipping \ n ", <nl> + cfgno , d -> bLength ); <nl> + continue ; <nl> + } <nl> + <nl> if ( iad_num == USB_MAXIADS ) { <nl> dev_warn ( ddev , " found more Interface " <nl> " Association Descriptors " <nl> " than allocated for in " <nl> " configuration % d \ n ", cfgno ); <nl> } else { <nl> - config -> intf_assoc [ iad_num ] = <nl> - ( struct usb_interface_assoc_descriptor <nl> - *) header ; <nl> + config -> intf_assoc [ iad_num ] = d ; <nl> iad_num ++; <nl> } <nl> mmm include / uapi / linux / usb / ch9 . h <nl> ppp include / uapi / linux / usb / ch9 . h <nl> static int usb_parse_configuration ( struct usb_device * dev , int cfgidx , <nl>  <nl> } else if ( header -> bDescriptorType == <nl> USB_DT_INTERFACE_ASSOCIATION ) { <nl> + struct usb_interface_assoc_descriptor * d ; <nl> + <nl> + d = ( struct usb_interface_assoc_descriptor *) header ; <nl> + if ( d -> bLength < USB_DT_INTERFACE_ASSOCIATION_SIZE ) { <nl> + dev_warn ( ddev , <nl> + " config % d has an invalid interface association descriptor of length % d , skipping \ n ", <nl> + cfgno , d -> bLength ); <nl> + continue ; <nl> + } <nl> + <nl> if ( iad_num == USB_MAXIADS ) { <nl> dev_warn ( ddev , " found more Interface " <nl> " Association Descriptors " <nl> " than allocated for in " <nl> " configuration % d \ n ", cfgno ); <nl> } else { <nl> - config -> intf_assoc [ iad_num ] = <nl> - ( struct usb_interface_assoc_descriptor <nl> - *) header ; <nl> + config -> intf_assoc [ iad_num ] = d ; <nl> iad_num ++; <nl> } <nl>  <nl> struct usb_interface_assoc_descriptor { <nl> __u8 iFunction ; <nl> } __attribute__ (( packed )); <nl>  <nl> +# define USB_DT_INTERFACE_ASSOCIATION_SIZE 8 <nl>  <nl> /*-------------------------------------------------------------------------*/ <nl> 
mmm fs / nfs / dir . c <nl> ppp fs / nfs / dir . c <nl> nfs_access_calc_mask ( u32 access_result ) <nl>  <nl> void nfs_access_set_mask ( struct nfs_access_entry * entry , u32 access_result ) <nl> { <nl> - entry -> mask = nfs_access_calc_mask ( access_result ); <nl> + entry -> mask = access_result ; <nl> } <nl> EXPORT_SYMBOL_GPL ( nfs_access_set_mask ); <nl>  <nl> static int nfs_do_access ( struct inode * inode , struct rpc_cred * cred , int mask ) <nl> { <nl> struct nfs_access_entry cache ; <nl> bool may_block = ( mask & MAY_NOT_BLOCK ) == 0 ; <nl> + int cache_mask ; <nl> int status ; <nl>  <nl> trace_nfs_access_enter ( inode ); <nl> static int nfs_do_access ( struct inode * inode , struct rpc_cred * cred , int mask ) <nl> goto out ; <nl>  <nl> /* Be clever : ask server to check for all possible rights */ <nl> - cache . mask = MAY_EXEC | MAY_WRITE | MAY_READ ; <nl> + cache . mask = NFS_MAY_LOOKUP | NFS_MAY_EXECUTE <nl> + | NFS_MAY_WRITE | NFS_MAY_READ ; <nl> cache . cred = cred ; <nl> cache . jiffies = jiffies ; <nl> status = NFS_PROTO ( inode )-> access ( inode , & cache ); <nl> static int nfs_do_access ( struct inode * inode , struct rpc_cred * cred , int mask ) <nl> } <nl> nfs_access_add_cache ( inode , & cache ); <nl> out_cached : <nl> - if (( mask & ~ cache . mask & ( MAY_READ | MAY_WRITE | MAY_EXEC )) != 0 ) <nl> + cache_mask = nfs_access_calc_mask ( cache . mask ); <nl> + if (( mask & ~ cache_mask & ( MAY_READ | MAY_WRITE | MAY_EXEC )) != 0 ) <nl> status = - EACCES ; <nl> out : <nl> trace_nfs_access_exit ( inode , status );mmm include / linux / nfs_fs . h <nl> ppp include / linux / nfs_fs . h <nl> nfs_access_calc_mask ( u32 access_result ) <nl>  <nl> void nfs_access_set_mask ( struct nfs_access_entry * entry , u32 access_result ) <nl> { <nl> - entry -> mask = nfs_access_calc_mask ( access_result ); <nl> + entry -> mask = access_result ; <nl> } <nl> EXPORT_SYMBOL_GPL ( nfs_access_set_mask ); <nl>  <nl> static int nfs_do_access ( struct inode * inode , struct rpc_cred * cred , int mask ) <nl> { <nl> struct nfs_access_entry cache ; <nl> bool may_block = ( mask & MAY_NOT_BLOCK ) == 0 ; <nl> + int cache_mask ; <nl> int status ; <nl>  <nl> trace_nfs_access_enter ( inode ); <nl> static int nfs_do_access ( struct inode * inode , struct rpc_cred * cred , int mask ) <nl> goto out ; <nl>  <nl> /* Be clever : ask server to check for all possible rights */ <nl> - cache . mask = MAY_EXEC | MAY_WRITE | MAY_READ ; <nl> + cache . mask = NFS_MAY_LOOKUP | NFS_MAY_EXECUTE <nl> + | NFS_MAY_WRITE | NFS_MAY_READ ; <nl> cache . cred = cred ; <nl> cache . jiffies = jiffies ; <nl> status = NFS_PROTO ( inode )-> access ( inode , & cache ); <nl> static int nfs_do_access ( struct inode * inode , struct rpc_cred * cred , int mask ) <nl> } <nl> nfs_access_add_cache ( inode , & cache ); <nl> out_cached : <nl> - if (( mask & ~ cache . mask & ( MAY_READ | MAY_WRITE | MAY_EXEC )) != 0 ) <nl> + cache_mask = nfs_access_calc_mask ( cache . mask ); <nl> + if (( mask & ~ cache_mask & ( MAY_READ | MAY_WRITE | MAY_EXEC )) != 0 ) <nl> status = - EACCES ; <nl> out : <nl> trace_nfs_access_exit ( inode , status ); <nl> struct nfs_access_entry { <nl> struct list_head lru ; <nl> unsigned long jiffies ; <nl> struct rpc_cred * cred ; <nl> - int mask ; <nl> + __u32 mask ; <nl> struct rcu_head rcu_head ; <nl> }; <nl> 
mmm drivers / media / video / videobuf2 - memops . c <nl> ppp drivers / media / video / videobuf2 - memops . c <nl> void vb2_put_vma ( struct vm_area_struct * vma ) <nl>  <nl> kfree ( vma ); <nl> } <nl> + EXPORT_SYMBOL_GPL ( vb2_put_vma ); <nl>  <nl> /** <nl> * vb2_get_contig_userptr () - lock physically contiguous userspace mapped memory <nl> int vb2_get_contig_userptr ( unsigned long vaddr , unsigned long size , <nl> up_read (& mm -> mmap_sem ); <nl> return ret ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( vb2_get_contig_userptr ); <nl>  <nl> /** <nl> * vb2_mmap_pfn_range () - map physical pages to userspace <nl> int vb2_mmap_pfn_range ( struct vm_area_struct * vma , unsigned long paddr , <nl>  <nl> return 0 ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( vb2_mmap_pfn_range ); <nl>  <nl> /** <nl> * vb2_common_vm_open () - increase refcount of the vma
mmm drivers / net / wireless / rt2x00 / rt2x00ht . c <nl> ppp drivers / net / wireless / rt2x00 / rt2x00ht . c <nl> void rt2x00ht_create_tx_descriptor ( struct queue_entry * entry , <nl> txdesc -> mpdu_density = 0 ; <nl>  <nl> txdesc -> ba_size = 7 ; /* FIXME : What value is needed ? */ <nl> - txdesc -> stbc = 0 ; /* FIXME : What value is needed ? */ <nl> + <nl> + txdesc -> stbc = <nl> + ( tx_info -> flags & IEEE80211_TX_CTL_STBC ) >> IEEE80211_TX_CTL_STBC_SHIFT ; <nl>  <nl> txdesc -> mcs = rt2x00_get_rate_mcs ( hwrate -> mcs ); <nl> if ( txrate -> flags & IEEE80211_TX_RC_USE_SHORT_PREAMBLE )
mmm drivers / vhost / vhost . c <nl> ppp drivers / vhost / vhost . c <nl> static int translate_desc ( struct vhost_dev * dev , u64 addr , u32 len , <nl> } <nl> _iov = iov + ret ; <nl> size = reg -> memory_size - addr + reg -> guest_phys_addr ; <nl> - _iov -> iov_len = min (( u64 ) len , size ); <nl> + _iov -> iov_len = min (( u64 ) len - s , size ); <nl> _iov -> iov_base = ( void __user *)( unsigned long ) <nl> ( reg -> userspace_addr + addr - reg -> guest_phys_addr ); <nl> s += size ;
mmm drivers / usb / serial / console . c <nl> ppp drivers / usb / serial / console . c <nl> static struct console usbcons = { <nl>  <nl> void usb_serial_console_disconnect ( struct usb_serial * serial ) <nl> { <nl> - if ( serial -> port [ 0 ] == usbcons_info . port ) { <nl> + if ( serial -> port [ 0 ] && serial -> port [ 0 ] == usbcons_info . port ) { <nl> usb_serial_console_exit (); <nl> usb_serial_put ( serial ); <nl> }
mmm drivers / gpu / drm / nouveau / nouveau_display . c <nl> ppp drivers / gpu / drm / nouveau / nouveau_display . c <nl> nouveau_framebuffer_init ( struct drm_device * dev , <nl> return - EINVAL ; <nl> } <nl>  <nl> + if ( nvbo -> tile_flags & NOUVEAU_GEM_TILE_NONCONTIG ) { <nl> + NV_ERROR ( drm , " framebuffer requires contiguous bo \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( nv_device ( drm -> device )-> chipset == 0x50 ) <nl> nv_fb -> r_format |= ( tile_flags << 8 ); <nl> 
mmm drivers / mmc / host / atmel - mci . c <nl> ppp drivers / mmc / host / atmel - mci . c <nl> struct atmel_mci { <nl> void __iomem * regs ; <nl>  <nl> struct scatterlist * sg ; <nl> + unsigned int sg_len ; <nl> unsigned int pio_offset ; <nl> unsigned int * buffer ; <nl> unsigned int buf_size ; <nl> static u32 atmci_prepare_data ( struct atmel_mci * host , struct mmc_data * data ) <nl> data -> error = - EINPROGRESS ; <nl>  <nl> host -> sg = data -> sg ; <nl> + host -> sg_len = data -> sg_len ; <nl> host -> data = data ; <nl> host -> data_chan = NULL ; <nl>  <nl> static void atmci_read_data_pio ( struct atmel_mci * host ) <nl> if ( offset == sg -> length ) { <nl> flush_dcache_page ( sg_page ( sg )); <nl> host -> sg = sg = sg_next ( sg ); <nl> - if (! sg ) <nl> + host -> sg_len --; <nl> + if (! sg || ! host -> sg_len ) <nl> goto done ; <nl>  <nl> offset = 0 ; <nl> static void atmci_read_data_pio ( struct atmel_mci * host ) <nl>  <nl> flush_dcache_page ( sg_page ( sg )); <nl> host -> sg = sg = sg_next ( sg ); <nl> - if (! sg ) <nl> + host -> sg_len --; <nl> + if (! sg || ! host -> sg_len ) <nl> goto done ; <nl>  <nl> offset = 4 - remaining ; <nl> static void atmci_write_data_pio ( struct atmel_mci * host ) <nl> nbytes += 4 ; <nl> if ( offset == sg -> length ) { <nl> host -> sg = sg = sg_next ( sg ); <nl> - if (! sg ) <nl> + host -> sg_len --; <nl> + if (! sg || ! host -> sg_len ) <nl> goto done ; <nl>  <nl> offset = 0 ; <nl> static void atmci_write_data_pio ( struct atmel_mci * host ) <nl> nbytes += remaining ; <nl>  <nl> host -> sg = sg = sg_next ( sg ); <nl> - if (! sg ) { <nl> + host -> sg_len --; <nl> + if (! sg || ! host -> sg_len ) { <nl> atmci_writel ( host , ATMCI_TDR , value ); <nl> goto done ; <nl> }
mmm drivers / staging / batman - adv / aggregation . c <nl> ppp drivers / staging / batman - adv / aggregation . c <nl> static bool can_aggregate_with ( struct batman_packet * new_batman_packet , <nl> * interface only - we still can aggregate */ <nl> if (( directlink ) && <nl> ( new_batman_packet -> ttl == 1 ) && <nl> - ( forw_packet -> if_incoming == if_incoming )) <nl> + ( forw_packet -> if_incoming == if_incoming ) && <nl> + <nl> + /* packets from direct neighbors or <nl> + * own secondary interface packets <nl> + * (= secondary interface packets in general ) */ <nl> + ( batman_packet -> flags & DIRECTLINK || <nl> + ( forw_packet -> own && <nl> + forw_packet -> if_incoming -> if_num != 0 ))) <nl> return true ; <nl> - <nl> } <nl>  <nl> return false ; <nl> void add_bat_packet_to_list ( unsigned char * packet_buff , int packet_len , <nl> * later on <nl> */ <nl> if ((! own_packet ) && <nl> - ( atomic_read (& bat_priv -> aggregation_enabled ))) <nl> + ( atomic_read (& aggregation_enabled ))) <nl> send_time += msecs_to_jiffies ( MAX_AGGREGATION_MS ); <nl>  <nl> new_aggregated_packet ( packet_buff , packet_len ,
mmm drivers / usb / storage / uas . c <nl> ppp drivers / usb / storage / uas . c <nl> static int uas_probe ( struct usb_interface * intf , const struct usb_device_id * id ) <nl>  <nl> shost -> max_cmd_len = 16 + 252 ; <nl> shost -> max_id = 1 ; <nl> + shost -> max_lun = 256 ; <nl> + shost -> max_channel = 0 ; <nl> shost -> sg_tablesize = udev -> bus -> sg_tablesize ; <nl>  <nl> devinfo -> intf = intf ;
mmm fs / nfs / pnfs . c <nl> ppp fs / nfs / pnfs . c <nl> void pnfs_error_mark_layout_for_return ( struct inode * inode , <nl> bool return_now = false ; <nl>  <nl> spin_lock (& inode -> i_lock ); <nl> + if (! pnfs_layout_is_valid ( lo )) { <nl> + spin_unlock (& inode -> i_lock ); <nl> + return ; <nl> + } <nl> pnfs_set_plh_return_info ( lo , range . iomode , 0 ); <nl> /* Block LAYOUTGET */ <nl> set_bit ( NFS_LAYOUT_RETURN , & lo -> plh_flags );
mmm tools / perf / util / thread_map . c <nl> ppp tools / perf / util / thread_map . c <nl> struct thread_map * thread_map__new_by_uid ( uid_t uid ) <nl> { <nl> DIR * proc ; <nl> int max_threads = 32 , items , i ; <nl> - char path [ 256 ]; <nl> + char path [ NAME_MAX + 1 + 6 ]; <nl> struct dirent * dirent , ** namelist = NULL ; <nl> struct thread_map * threads = thread_map__alloc ( max_threads ); <nl> 
mmm drivers / iommu / arm - smmu - v3 . c <nl> ppp drivers / iommu / arm - smmu - v3 . c <nl> arm_smmu_iova_to_phys ( struct iommu_domain * domain , dma_addr_t iova ) <nl> struct arm_smmu_domain * smmu_domain = to_smmu_domain ( domain ); <nl> struct io_pgtable_ops * ops = smmu_domain -> pgtbl_ops ; <nl>  <nl> + if ( domain -> type == IOMMU_DOMAIN_IDENTITY ) <nl> + return iova ; <nl> + <nl> if (! ops ) <nl> return 0 ; <nl> mmm drivers / iommu / arm - smmu . c <nl> ppp drivers / iommu / arm - smmu . c <nl> arm_smmu_iova_to_phys ( struct iommu_domain * domain , dma_addr_t iova ) <nl> struct arm_smmu_domain * smmu_domain = to_smmu_domain ( domain ); <nl> struct io_pgtable_ops * ops = smmu_domain -> pgtbl_ops ; <nl>  <nl> + if ( domain -> type == IOMMU_DOMAIN_IDENTITY ) <nl> + return iova ; <nl> + <nl> if (! ops ) <nl> return 0 ; <nl>  <nl> static phys_addr_t arm_smmu_iova_to_phys ( struct iommu_domain * domain , <nl> struct arm_smmu_domain * smmu_domain = to_smmu_domain ( domain ); <nl> struct io_pgtable_ops * ops = smmu_domain -> pgtbl_ops ; <nl>  <nl> + if ( domain -> type == IOMMU_DOMAIN_IDENTITY ) <nl> + return iova ; <nl> + <nl> if (! ops ) <nl> return 0 ; <nl> 
mmm net / mac80211 / mesh . c <nl> ppp net / mac80211 / mesh . c <nl> void mesh_mgmt_ies_add ( struct sk_buff * skb , struct ieee80211_sub_if_data * sdata ) <nl> } <nl> } <nl>  <nl> + if ( sband -> band == IEEE80211_BAND_2GHZ ) { <nl> + pos = skb_put ( skb , 2 + 1 ); <nl> + * pos ++ = WLAN_EID_DS_PARAMS ; <nl> + * pos ++ = 1 ; <nl> + * pos ++ = ieee80211_frequency_to_channel ( local -> hw . conf . channel -> center_freq ); <nl> + } <nl> + <nl> pos = skb_put ( skb , 2 + sdata -> u . mesh . mesh_id_len ); <nl> * pos ++ = WLAN_EID_MESH_ID ; <nl> * pos ++ = sdata -> u . mesh . mesh_id_len ;
mmm drivers / gpu / drm / i915 / intel_dp . c <nl> ppp drivers / gpu / drm / i915 / intel_dp . c <nl> intel_edp_init_dpcd ( struct intel_dp * intel_dp ) <nl> /* Read the eDP Display control capabilities registers */ <nl> if (( intel_dp -> dpcd [ DP_EDP_CONFIGURATION_CAP ] & DP_DPCD_DISPLAY_CONTROL_CAPABLE ) && <nl> drm_dp_dpcd_read (& intel_dp -> aux , DP_EDP_DPCD_REV , <nl> - intel_dp -> edp_dpcd , sizeof ( intel_dp -> edp_dpcd ) == <nl> - sizeof ( intel_dp -> edp_dpcd ))) <nl> + intel_dp -> edp_dpcd , sizeof ( intel_dp -> edp_dpcd )) == <nl> + sizeof ( intel_dp -> edp_dpcd )) <nl> DRM_DEBUG_KMS (" EDP DPCD : %* ph \ n ", ( int ) sizeof ( intel_dp -> edp_dpcd ), <nl> intel_dp -> edp_dpcd ); <nl> 
mmm sound / soc / codecs / wm8993 . c <nl> ppp sound / soc / codecs / wm8993 . c <nl> /* <nl> * wm8993 . c -- WM8993 ALSA SoC audio driver <nl> * <nl> - * Copyright 2009 Wolfson Microelectronics plc <nl> + * Copyright 2009 , 2010 Wolfson Microelectronics plc <nl> * <nl> * Author : Mark Brown < broonie @ opensource . wolfsonmicro . com > <nl> * <nl> static int wm8993_i2c_probe ( struct i2c_client * i2c , <nl> codec -> private_data = wm8993 ; <nl>  <nl> wm8993 -> hubs_data . hp_startup_mode = 1 ; <nl> + wm8993 -> hubs_data . dcs_codes = - 2 ; <nl>  <nl> memcpy ( wm8993 -> reg_cache , wm8993_reg_defaults , <nl> sizeof ( wm8993 -> reg_cache ));
mmm drivers / infiniband / core / cma . c <nl> ppp drivers / infiniband / core / cma . c <nl> static struct rdma_id_private * cma_id_from_event ( struct ib_cm_id * cm_id , <nl> bind_list = cma_ps_find ( rdma_ps_from_service_id ( req . service_id ), <nl> cma_port_from_service_id ( req . service_id )); <nl> id_priv = cma_find_listener ( bind_list , cm_id , ib_event , & req , * net_dev ); <nl> + if ( IS_ERR ( id_priv )) { <nl> + dev_put (* net_dev ); <nl> + * net_dev = NULL ; <nl> + } <nl>  <nl> return id_priv ; <nl> }
mmm sound / aoa / core / snd - aoa - gpio - pmf . c <nl> ppp sound / aoa / core / snd - aoa - gpio - pmf . c <nl> static void pmf_gpio_set_ ## name ( struct gpio_runtime * rt , int on )\ <nl> \ <nl> if ( unlikely (! rt )) return ; \ <nl> rc = pmf_call_function ( rt -> node , # name "- mute ", & args ); \ <nl> - if ( rc ) \ <nl> + if ( rc && rc != - ENODEV ) \ <nl> printk ( KERN_WARNING " pmf_gpio_set_ " # name \ <nl> " failed , rc : % d \ n ", rc ); \ <nl> rt -> implementation_private &= ~( 1 << bit ); \
mmm drivers / mtd / ubi / fastmap . c <nl> ppp drivers / mtd / ubi / fastmap . c <nl> static int count_fastmap_pebs ( struct ubi_attach_info * ai ) <nl> list_for_each_entry ( aeb , & ai -> free , u . list ) <nl> n ++; <nl>  <nl> - ubi_rb_for_each_entry ( rb1 , av , & ai -> volumes , rb ) <nl> + ubi_rb_for_each_entry ( rb1 , av , & ai -> volumes , rb ) <nl> ubi_rb_for_each_entry ( rb2 , aeb , & av -> root , u . rb ) <nl> n ++; <nl> 
mmm net / ipv6 / udp . c <nl> ppp net / ipv6 / udp . c <nl> int udpv6_recvmsg ( struct sock * sk , struct msghdr * msg , size_t len , <nl> } <nl> unlock_sock_fast ( sk , slow ); <nl>  <nl> - if ( noblock ) <nl> - return - EAGAIN ; <nl> - <nl> - /* starting over for a new packet */ <nl> + /* starting over for a new packet , but check if we need to yield */ <nl> + cond_resched (); <nl> msg -> msg_flags &= ~ MSG_TRUNC ; <nl> goto try_again ; <nl> }mmm net / ipv4 / udp . c <nl> ppp net / ipv4 / udp . c <nl> int udpv6_recvmsg ( struct sock * sk , struct msghdr * msg , size_t len , <nl> } <nl> unlock_sock_fast ( sk , slow ); <nl>  <nl> - if ( noblock ) <nl> - return - EAGAIN ; <nl> - <nl> - /* starting over for a new packet */ <nl> + /* starting over for a new packet , but check if we need to yield */ <nl> + cond_resched (); <nl> msg -> msg_flags &= ~ MSG_TRUNC ; <nl> goto try_again ; <nl> } <nl> int udp_recvmsg ( struct sock * sk , struct msghdr * msg , size_t len , int noblock , <nl> } <nl> unlock_sock_fast ( sk , slow ); <nl>  <nl> - if ( noblock ) <nl> - return - EAGAIN ; <nl> - <nl> - /* starting over for a new packet */ <nl> + /* starting over for a new packet , but check if we need to yield */ <nl> + cond_resched (); <nl> msg -> msg_flags &= ~ MSG_TRUNC ; <nl> goto try_again ; <nl> }
mmm fs / btrfs / inode . c <nl> ppp fs / btrfs / inode . c <nl> static noinline int cow_file_range ( struct inode * inode , <nl> if ( IS_ERR ( trans )) { <nl> extent_clear_unlock_delalloc ( inode , <nl> & BTRFS_I ( inode )-> io_tree , <nl> - start , end , NULL , <nl> + start , end , locked_page , <nl> EXTENT_CLEAR_UNLOCK_PAGE | <nl> EXTENT_CLEAR_UNLOCK | <nl> EXTENT_CLEAR_DELALLOC | <nl> static noinline int cow_file_range ( struct inode * inode , <nl> out_unlock : <nl> extent_clear_unlock_delalloc ( inode , <nl> & BTRFS_I ( inode )-> io_tree , <nl> - start , end , NULL , <nl> + start , end , locked_page , <nl> EXTENT_CLEAR_UNLOCK_PAGE | <nl> EXTENT_CLEAR_UNLOCK | <nl> EXTENT_CLEAR_DELALLOC |
mmm ipc / msg . c <nl> ppp ipc / msg . c <nl> long do_msgsnd ( int msqid , long mtype , void __user * mtext , <nl> goto out_unlock1 ; <nl> } <nl>  <nl> + ipc_lock_object (& msq -> q_perm ); <nl> + <nl> for (;;) { <nl> struct msg_sender s ; <nl>  <nl> err = - EACCES ; <nl> if ( ipcperms ( ns , & msq -> q_perm , S_IWUGO )) <nl> - goto out_unlock1 ; <nl> + goto out_unlock0 ; <nl>  <nl> err = security_msg_queue_msgsnd ( msq , msg , msgflg ); <nl> if ( err ) <nl> - goto out_unlock1 ; <nl> + goto out_unlock0 ; <nl>  <nl> if ( msgsz + msq -> q_cbytes <= msq -> q_qbytes && <nl> 1 + msq -> q_qnum <= msq -> q_qbytes ) { <nl> long do_msgsnd ( int msqid , long mtype , void __user * mtext , <nl> /* queue full , wait : */ <nl> if ( msgflg & IPC_NOWAIT ) { <nl> err = - EAGAIN ; <nl> - goto out_unlock1 ; <nl> + goto out_unlock0 ; <nl> } <nl>  <nl> - ipc_lock_object (& msq -> q_perm ); <nl> ss_add ( msq , & s ); <nl>  <nl> if (! ipc_rcu_getref ( msq )) { <nl> long do_msgsnd ( int msqid , long mtype , void __user * mtext , <nl> goto out_unlock0 ; <nl> } <nl>  <nl> - ipc_unlock_object (& msq -> q_perm ); <nl> } <nl> - <nl> - ipc_lock_object (& msq -> q_perm ); <nl> msq -> q_lspid = task_tgid_vnr ( current ); <nl> msq -> q_stime = get_seconds (); <nl> 
mmm fs / namei . c <nl> ppp fs / namei . c <nl> int vfs_unlink ( struct inode * dir , struct dentry * dentry ) <nl> error = - EBUSY ; <nl> else { <nl> error = security_inode_unlink ( dir , dentry ); <nl> - if (! error ) <nl> + if (! error ) { <nl> error = dir -> i_op -> unlink ( dir , dentry ); <nl> + if (! error ) <nl> + dentry -> d_inode -> i_flags |= S_DEAD ; <nl> + } <nl> } <nl> mutex_unlock (& dentry -> d_inode -> i_mutex ); <nl>  <nl> static int vfs_rename_other ( struct inode * old_dir , struct dentry * old_dentry , <nl> else <nl> error = old_dir -> i_op -> rename ( old_dir , old_dentry , new_dir , new_dentry ); <nl> if (! error ) { <nl> + if ( target ) <nl> + target -> i_flags |= S_DEAD ; <nl> if (!( old_dir -> i_sb -> s_type -> fs_flags & FS_RENAME_DOES_D_MOVE )) <nl> d_move ( old_dentry , new_dentry ); <nl> }
mmm include / linux / sched . h <nl> ppp include / linux / sched . h <nl> struct sched_entity { <nl>  <nl> struct sched_rt_entity { <nl> struct list_head run_list ; <nl> - unsigned int time_slice ; <nl> unsigned long timeout ; <nl> + unsigned int time_slice ; <nl> int nr_cpus_allowed ; <nl>  <nl> struct sched_rt_entity * back ;
mmm drivers / pinctrl / sh - pfc / core . c <nl> ppp drivers / pinctrl / sh - pfc / core . c <nl> static int sh_pfc_ioremap ( struct sh_pfc * pfc , struct platform_device * pdev ) <nl> struct resource * res ; <nl> int k ; <nl>  <nl> - if ( pdev -> num_resources == 0 ) { <nl> - pfc -> num_windows = 0 ; <nl> - return 0 ; <nl> - } <nl> + if ( pdev -> num_resources == 0 ) <nl> + return - EINVAL ; <nl>  <nl> pfc -> window = devm_kzalloc ( pfc -> dev , pdev -> num_resources * <nl> sizeof (* pfc -> window ), GFP_NOWAIT ); <nl> static void __iomem * sh_pfc_phys_to_virt ( struct sh_pfc * pfc , <nl> unsigned long address ) <nl> { <nl> struct sh_pfc_window * window ; <nl> - int k ; <nl> + unsigned int i ; <nl>  <nl> /* scan through physical windows and convert address */ <nl> - for ( k = 0 ; k < pfc -> num_windows ; k ++) { <nl> - window = pfc -> window + k ; <nl> + for ( i = 0 ; i < pfc -> num_windows ; i ++) { <nl> + window = pfc -> window + i ; <nl>  <nl> if ( address < window -> phys ) <nl> continue ; <nl> static void __iomem * sh_pfc_phys_to_virt ( struct sh_pfc * pfc , <nl> return window -> virt + ( address - window -> phys ); <nl> } <nl>  <nl> - /* no windows defined , register must be 1 : 1 mapped virt : phys */ <nl> - return ( void __iomem *) address ; <nl> + BUG (); <nl> } <nl>  <nl> struct sh_pfc_pin * sh_pfc_get_pin ( struct sh_pfc * pfc , unsigned int pin )
mmm fs / btrfs / volumes . c <nl> ppp fs / btrfs / volumes . c <nl> int btrfs_balance ( struct btrfs_balance_control * bctl , <nl> mutex_lock (& fs_info -> balance_mutex ); <nl> atomic_dec (& fs_info -> balance_running ); <nl>  <nl> + if ( bctl -> sys . flags & BTRFS_BALANCE_ARGS_CONVERT ) { <nl> + fs_info -> num_tolerated_disk_barrier_failures = <nl> + btrfs_calc_num_tolerated_disk_barrier_failures ( fs_info ); <nl> + } <nl> + <nl> if ( bargs ) { <nl> memset ( bargs , 0 , sizeof (* bargs )); <nl> update_ioctl_balance_args ( fs_info , 0 , bargs ); <nl> int btrfs_balance ( struct btrfs_balance_control * bctl , <nl> __cancel_balance ( fs_info ); <nl> } <nl>  <nl> - if ( bctl -> sys . flags & BTRFS_BALANCE_ARGS_CONVERT ) { <nl> - fs_info -> num_tolerated_disk_barrier_failures = <nl> - btrfs_calc_num_tolerated_disk_barrier_failures ( fs_info ); <nl> - } <nl> - <nl> wake_up (& fs_info -> balance_wait_q ); <nl>  <nl> return ret ;
mmm net / ipv4 / netfilter / ipt_hashlimit . c <nl> ppp net / ipv4 / netfilter / ipt_hashlimit . c <nl> struct ipt_hashlimit_htable { <nl> /* used internally */ <nl> spinlock_t lock ; /* lock for list_head */ <nl> u_int32_t rnd ; /* random seed for hash */ <nl> + int rnd_initialized ; <nl> struct timer_list timer ; /* timer for gc */ <nl> atomic_t count ; /* number entries in table */ <nl>  <nl> __dsthash_alloc_init ( struct ipt_hashlimit_htable * ht , struct dsthash_dst * dst ) <nl>  <nl> /* initialize hash with random val at the time we allocate <nl> * the first hashtable entry */ <nl> - if (! ht -> rnd ) <nl> + if (! ht -> rnd_initialized ) { <nl> get_random_bytes (& ht -> rnd , 4 ); <nl> + ht -> rnd_initialized = 1 ; <nl> + } <nl>  <nl> if ( ht -> cfg . max && <nl> atomic_read (& ht -> count ) >= ht -> cfg . max ) { <nl> static int htable_create ( struct ipt_hashlimit_info * minfo ) <nl>  <nl> atomic_set (& hinfo -> count , 0 ); <nl> atomic_set (& hinfo -> use , 1 ); <nl> - hinfo -> rnd = 0 ; <nl> + hinfo -> rnd_initialized = 0 ; <nl> spin_lock_init (& hinfo -> lock ); <nl> hinfo -> pde = create_proc_entry ( minfo -> name , 0 , hashlimit_procdir ); <nl> if (! hinfo -> pde ) {
mmm sound / pci / hda / patch_realtek . c <nl> ppp sound / pci / hda / patch_realtek . c <nl> static void alc662_led_gpio1_mute_hook ( void * private_data , int enabled ) <nl> spec -> gpio_led ); <nl> } <nl>  <nl> +/* avoid D3 for keeping GPIO up */ <nl> + static unsigned int gpio_led_power_filter ( struct hda_codec * codec , <nl> + hda_nid_t nid , <nl> + unsigned int power_state ) <nl> +{ <nl> + struct alc_spec * spec = codec -> spec ; <nl> + if ( nid == codec -> afg && power_state == AC_PWRST_D3 && spec -> gpio_led ) <nl> + return AC_PWRST_D0 ; <nl> + return power_state ; <nl> +} <nl> + <nl> static void alc662_fixup_led_gpio1 ( struct hda_codec * codec , <nl> const struct hda_fixup * fix , int action ) <nl> { <nl> static void alc662_fixup_led_gpio1 ( struct hda_codec * codec , <nl> spec -> gen . vmaster_mute . hook = alc662_led_gpio1_mute_hook ; <nl> spec -> gpio_led = 0 ; <nl> snd_hda_add_verbs ( codec , gpio_init ); <nl> + codec -> power_filter = gpio_led_power_filter ; <nl> } <nl> } <nl> 
mmm lib / flex_array . c <nl> ppp lib / flex_array . c <nl> int flex_array_prealloc ( struct flex_array * fa , unsigned int start , <nl> unsigned int end ; <nl> struct flex_array_part * part ; <nl>  <nl> + if (! start && ! nr_elements ) <nl> + return 0 ; <nl> + if ( start >= fa -> total_nr_elements ) <nl> + return - ENOSPC ; <nl> + if (! nr_elements ) <nl> + return 0 ; <nl> + <nl> end = start + nr_elements - 1 ; <nl>  <nl> - if ( start >= fa -> total_nr_elements || end >= fa -> total_nr_elements ) <nl> + if ( end >= fa -> total_nr_elements ) <nl> return - ENOSPC ; <nl> if ( elements_fit_in_base ( fa )) <nl> return 0 ; <nl> int flex_array_shrink ( struct flex_array * fa ) <nl> int part_nr ; <nl> int ret = 0 ; <nl>  <nl> + if (! fa -> total_nr_elements ) <nl> + return 0 ; <nl> if ( elements_fit_in_base ( fa )) <nl> return ret ; <nl> for ( part_nr = 0 ; part_nr < FLEX_ARRAY_NR_BASE_PTRS ; part_nr ++) {
mmm drivers / net / cassini . c <nl> ppp drivers / net / cassini . c <nl> static int __devinit cas_init_one ( struct pci_dev * pdev , <nl> INIT_WORK (& cp -> reset_task , cas_reset_task ); <nl>  <nl> /* Default link parameters */ <nl> - if ( link_mode >= 0 && link_mode <= 6 ) <nl> + if ( link_mode >= 0 && link_mode < 6 ) <nl> cp -> link_cntl = link_modes [ link_mode ]; <nl> else <nl> cp -> link_cntl = BMCR_ANENABLE ;
mmm net / sunrpc / xprtrdma / verbs . c <nl> ppp net / sunrpc / xprtrdma / verbs . c <nl> rpcrdma_register_internal ( struct rpcrdma_ia * ia , void * va , int len , <nl> */ <nl> iov -> addr = ib_dma_map_single ( ia -> ri_id -> device , <nl> va , len , DMA_BIDIRECTIONAL ); <nl> + if ( ib_dma_mapping_error ( ia -> ri_id -> device , iov -> addr )) <nl> + return - ENOMEM ; <nl> + <nl> iov -> length = len ; <nl>  <nl> if ( ia -> ri_have_dma_lkey ) {
mmm net / sctp / sm_statefuns . c <nl> ppp net / sctp / sm_statefuns . c <nl> sctp_disposition_t sctp_sf_ootb ( struct net * net , <nl> return sctp_sf_violation_chunklen ( net , ep , asoc , type , arg , <nl> commands ); <nl>  <nl> + /* Report violation if chunk len overflows */ <nl> + ch_end = (( __u8 *) ch ) + SCTP_PAD4 ( ntohs ( ch -> length )); <nl> + if ( ch_end > skb_tail_pointer ( skb )) <nl> + return sctp_sf_violation_chunklen ( net , ep , asoc , type , arg , <nl> + commands ); <nl> + <nl> /* Now that we know we at least have a chunk header , <nl> * do things that are type appropriate . <nl> */ <nl> sctp_disposition_t sctp_sf_ootb ( struct net * net , <nl> } <nl> } <nl>  <nl> - /* Report violation if chunk len overflows */ <nl> - ch_end = (( __u8 *) ch ) + SCTP_PAD4 ( ntohs ( ch -> length )); <nl> - if ( ch_end > skb_tail_pointer ( skb )) <nl> - return sctp_sf_violation_chunklen ( net , ep , asoc , type , arg , <nl> - commands ); <nl> - <nl> ch = ( sctp_chunkhdr_t *) ch_end ; <nl> } while ( ch_end < skb_tail_pointer ( skb )); <nl> 
mmm drivers / net / wireless / ti / wlcore / tx . c <nl> ppp drivers / net / wireless / ti / wlcore / tx . c <nl> static int wl1271_prepare_tx_frame ( struct wl1271 * wl , struct wl12xx_vif * wlvif , <nl> is_wep = ( cipher == WLAN_CIPHER_SUITE_WEP40 ) || <nl> ( cipher == WLAN_CIPHER_SUITE_WEP104 ); <nl>  <nl> - if ( WARN_ON ( is_wep && wlvif -> default_key != idx )) { <nl> + if ( WARN_ON ( is_wep && wlvif && wlvif -> default_key != idx )) { <nl> ret = wl1271_set_default_wep_key ( wl , wlvif , idx ); <nl> if ( ret < 0 ) <nl> return ret ;
mmm drivers / gpu / drm / drm_crtc_helper . c <nl> ppp drivers / gpu / drm / drm_crtc_helper . c <nl> int drm_crtc_helper_set_config ( struct drm_mode_set * set ) <nl> int count = 0 , ro , fail = 0 ; <nl> struct drm_crtc_helper_funcs * crtc_funcs ; <nl> int ret = 0 ; <nl> + int i ; <nl>  <nl> DRM_DEBUG_KMS ("\ n "); <nl>  <nl> int drm_crtc_helper_set_config ( struct drm_mode_set * set ) <nl> if ( ret != 0 ) <nl> goto fail ; <nl> } <nl> + DRM_DEBUG_KMS (" Setting connector DPMS state to on \ n "); <nl> + for ( i = 0 ; i < set -> num_connectors ; i ++) { <nl> + DRM_DEBUG_KMS ("\ t [ CONNECTOR :% d :% s ] set DPMS on \ n ", set -> connectors [ i ]-> base . id , <nl> + drm_get_connector_name ( set -> connectors [ i ])); <nl> + set -> connectors [ i ]-> dpms = DRM_MODE_DPMS_ON ; <nl> + } <nl>  <nl> kfree ( save_connectors ); <nl> kfree ( save_encoders );
mmm arch / i386 / kernel / timers / timer_tsc . c <nl> ppp arch / i386 / kernel / timers / timer_tsc . c <nl> static unsigned long last_tsc_high ; /* msb 32 bits of Time Stamp Counter */ <nl> static unsigned long long monotonic_base ; <nl> static seqlock_t monotonic_lock = SEQLOCK_UNLOCKED ; <nl>  <nl> +/* Avoid compensating for lost ticks before TSCs are synched */ <nl> + static int detect_lost_ticks ; <nl> + static int __init start_lost_tick_compensation ( void ) <nl> +{ <nl> + detect_lost_ticks = 1 ; <nl> + return 0 ; <nl> +} <nl> + late_initcall ( start_lost_tick_compensation ); <nl> + <nl> /* convert from cycles ( 64bits ) => nanoseconds ( 64bits ) <nl> * basic equation : <nl> * ns = cycles / ( freq / ns_per_sec ) <nl> static void mark_offset_tsc_hpet ( void ) <nl>  <nl> /* lost tick compensation */ <nl> offset = hpet_readl ( HPET_T0_CMP ) - hpet_tick ; <nl> - if ( unlikely ((( offset - hpet_last ) > hpet_tick ) && ( hpet_last != 0 ))) { <nl> + if ( unlikely ((( offset - hpet_last ) > hpet_tick ) && ( hpet_last != 0 )) <nl> + && detect_lost_ticks ) { <nl> int lost_ticks = ( offset - hpet_last ) / hpet_tick ; <nl> jiffies_64 += lost_ticks ; <nl> } <nl> static void mark_offset_tsc ( void ) <nl> delta += delay_at_last_interrupt ; <nl> lost = delta /( 1000000 / HZ ); <nl> delay = delta %( 1000000 / HZ ); <nl> - if ( lost >= 2 ) { <nl> + if ( lost >= 2 && detect_lost_ticks ) { <nl> jiffies_64 += lost - 1 ; <nl>  <nl> /* sanity check to ensure we ' re not always losing ticks */
mmm include / xen / interface / memory . h <nl> ppp include / xen / interface / memory . h <nl> struct xen_memory_reservation { <nl> * OUT : GMFN bases of extents that were allocated <nl> * ( NB . This command also updates the mach_to_phys translation table ) <nl> */ <nl> - ulong extent_start ; <nl> + GUEST_HANDLE ( ulong ) extent_start ; <nl>  <nl> /* Number of extents , and size / alignment of each ( 2 ^ extent_order pages ). */ <nl> unsigned long nr_extents ; <nl> struct xen_memory_reservation { <nl> domid_t domid ; <nl>  <nl> }; <nl> + DEFINE_GUEST_HANDLE_STRUCT ( xen_memory_reservation ); <nl>  <nl> /* <nl> * Returns the maximum machine frame number of mapped RAM in this system . <nl> struct xen_machphys_mfn_list { <nl> * any large discontiguities in the machine address space , 2MB gaps in <nl> * the machphys table will be represented by an MFN base of zero . <nl> */ <nl> - ulong extent_start ; <nl> + GUEST_HANDLE ( ulong ) extent_start ; <nl>  <nl> /* <nl> * Number of extents written to the above array . This will be smaller <nl> struct xen_machphys_mfn_list { <nl> */ <nl> unsigned int nr_extents ; <nl> }; <nl> + DEFINE_GUEST_HANDLE_STRUCT ( xen_machphys_mfn_list ); <nl>  <nl> /* <nl> * Sets the GPFN at which a particular page appears in the specified guest ' s <nl> struct xen_add_to_physmap { <nl> /* GPFN where the source mapping page should appear . */ <nl> unsigned long gpfn ; <nl> }; <nl> + DEFINE_GUEST_HANDLE_STRUCT ( xen_add_to_physmap ); <nl>  <nl> /* <nl> * Translates a list of domain - specific GPFNs into MFNs . Returns a - ve error <nl> struct xen_translate_gpfn_list { <nl> unsigned long nr_gpfns ; <nl>  <nl> /* List of GPFNs to translate . */ <nl> - ulong gpfn_list ; <nl> + GUEST_HANDLE ( ulong ) gpfn_list ; <nl>  <nl> /* <nl> * Output list to contain MFN translations . May be the same as the input <nl> * list ( in which case each input GPFN is overwritten with the output MFN ). <nl> */ <nl> - ulong mfn_list ; <nl> + GUEST_HANDLE ( ulong ) mfn_list ; <nl> }; <nl> + DEFINE_GUEST_HANDLE_STRUCT ( xen_translate_gpfn_list ); <nl>  <nl> # endif /* __XEN_PUBLIC_MEMORY_H__ */
mmm drivers / staging / greybus / hd . c <nl> ppp drivers / staging / greybus / hd . c <nl> struct gb_host_device * gb_hd_create ( struct gb_hd_driver * driver , <nl> return ERR_PTR (- EINVAL ); <nl> } <nl>  <nl> - if ( num_cports == 0 || num_cports > CPORT_ID_MAX ) { <nl> + if ( num_cports == 0 || num_cports > CPORT_ID_MAX + 1 ) { <nl> dev_err ( parent , " Invalid number of CPorts : % zu \ n ", num_cports ); <nl> return ERR_PTR (- EINVAL ); <nl> }
mmm drivers / staging / comedi / drivers / das1800 . c <nl> ppp drivers / staging / comedi / drivers / das1800 . c <nl> static int das1800_attach ( struct comedi_device * dev , <nl> if ( dev -> irq & it -> options [ 2 ]) <nl> das1800_init_dma ( dev , it ); <nl>  <nl> - devpriv -> fifo_buf = kmalloc ( FIFO_SIZE * sizeof ( uint16_t ), GFP_KERNEL ); <nl> + devpriv -> fifo_buf = kmalloc_array ( FIFO_SIZE , sizeof ( uint16_t ), GFP_KERNEL ); <nl> if (! devpriv -> fifo_buf ) <nl> return - ENOMEM ; <nl> 
mmm fs / hfsplus / super . c <nl> ppp fs / hfsplus / super . c <nl> static int hfsplus_sync_fs ( struct super_block * sb , int wait ) <nl>  <nl> static void delayed_sync_fs ( struct work_struct * work ) <nl> { <nl> + int err ; <nl> struct hfsplus_sb_info * sbi ; <nl>  <nl> sbi = container_of ( work , struct hfsplus_sb_info , sync_work . work ); <nl> static void delayed_sync_fs ( struct work_struct * work ) <nl> sbi -> work_queued = 0 ; <nl> spin_unlock (& sbi -> work_lock ); <nl>  <nl> - hfsplus_sync_fs ( sbi -> alloc_file -> i_sb , 1 ); <nl> + err = hfsplus_sync_fs ( sbi -> alloc_file -> i_sb , 1 ); <nl> + if ( err ) <nl> + printk ( KERN_ERR " hfs : delayed sync fs err % d \ n ", err ); <nl> } <nl>  <nl> void hfsplus_mark_mdb_dirty ( struct super_block * sb )
mmm drivers / nvdimm / region_devs . c <nl> ppp drivers / nvdimm / region_devs . c <nl> int nvdimm_has_flush ( struct nd_region * nd_region ) <nl> { <nl> int i ; <nl>  <nl> - /* no nvdimm == flushing capability unknown */ <nl> - if ( nd_region -> ndr_mappings == 0 ) <nl> + /* no nvdimm or pmem api == flushing capability unknown */ <nl> + if ( nd_region -> ndr_mappings == 0 <nl> + || ! IS_ENABLED ( CONFIG_ARCH_HAS_PMEM_API )) <nl> return - ENXIO ; <nl>  <nl> for ( i = 0 ; i < nd_region -> ndr_mappings ; i ++) {
mmm arch / sh / boards / board - ap325rxa . c <nl> ppp arch / sh / boards / board - ap325rxa . c <nl> static int __init ap325rxa_devices_setup ( void ) <nl> } <nl> device_initcall ( ap325rxa_devices_setup ); <nl>  <nl> +/* Return the board specific boot mode pin configuration */ <nl> + static int ap325rxa_mode_pins ( void ) <nl> +{ <nl> + /* MD0 = 0 , MD1 = 0 , MD2 = 0 : Clock Mode 0 <nl> + * MD3 = 0 : 16 - bit Area0 Bus Width <nl> + * MD5 = 1 : Little Endian <nl> + * TSTMD = 1 , MD8 = 1 : Test Mode Disabled <nl> + */ <nl> + return MODE_PIN5 | MODE_PIN8 ; <nl> +} <nl> + <nl> static struct sh_machine_vector mv_ap325rxa __initmv = { <nl> . mv_name = " AP - 325RXA ", <nl> + . mv_mode_pins = ap325rxa_mode_pins , <nl> };
mmm samples / bpf / xdpsock_user . c <nl> ppp samples / bpf / xdpsock_user . c <nl> static void kick_tx ( int fd ) <nl> int ret ; <nl>  <nl> ret = sendto ( fd , NULL , 0 , MSG_DONTWAIT , NULL , 0 ); <nl> - if ( ret >= 0 || errno == ENOBUFS || errno == EAGAIN ) <nl> + if ( ret >= 0 || errno == ENOBUFS || errno == EAGAIN || errno == EBUSY ) <nl> return ; <nl> lassert ( 0 ); <nl> }
mmm drivers / input / joystick / iforce / iforce - packets . c <nl> ppp drivers / input / joystick / iforce / iforce - packets . c <nl> void iforce_process_packet ( struct iforce * iforce , u16 cmd , unsigned char * data ) <nl>  <nl> int iforce_get_id_packet ( struct iforce * iforce , char * packet ) <nl> { <nl> + int status ; <nl> + <nl> switch ( iforce -> bus ) { <nl>  <nl> case IFORCE_USB : <nl> int iforce_get_id_packet ( struct iforce * iforce , char * packet ) <nl> iforce -> cr . bRequest = packet [ 0 ]; <nl> iforce -> ctrl -> dev = iforce -> usbdev ; <nl>  <nl> - if ( usb_submit_urb ( iforce -> ctrl , GFP_ATOMIC )) <nl> + status = usb_submit_urb ( iforce -> ctrl , GFP_ATOMIC ); <nl> + if ( status ) { <nl> + err (" usb_submit_urb failed % d ", status ); <nl> return - 1 ; <nl> + } <nl>  <nl> wait_event_interruptible_timeout ( iforce -> wait , <nl> iforce -> ctrl -> status != - EINPROGRESS , HZ ); <nl>  <nl> if ( iforce -> ctrl -> status ) { <nl> + dbg (" iforce -> ctrl -> status = % d ", iforce -> ctrl -> status ); <nl> usb_unlink_urb ( iforce -> ctrl ); <nl> return - 1 ; <nl> } <nl> # else <nl> - err (" iforce_get_id_packet : iforce -> bus = USB !"); <nl> + dbg (" iforce_get_id_packet : iforce -> bus = USB !"); <nl> # endif <nl> break ; <nl> mmm drivers / input / joystick / iforce / iforce - main . c <nl> ppp drivers / input / joystick / iforce / iforce - main . c <nl> void iforce_process_packet ( struct iforce * iforce , u16 cmd , unsigned char * data ) <nl>  <nl> int iforce_get_id_packet ( struct iforce * iforce , char * packet ) <nl> { <nl> + int status ; <nl> + <nl> switch ( iforce -> bus ) { <nl>  <nl> case IFORCE_USB : <nl> int iforce_get_id_packet ( struct iforce * iforce , char * packet ) <nl> iforce -> cr . bRequest = packet [ 0 ]; <nl> iforce -> ctrl -> dev = iforce -> usbdev ; <nl>  <nl> - if ( usb_submit_urb ( iforce -> ctrl , GFP_ATOMIC )) <nl> + status = usb_submit_urb ( iforce -> ctrl , GFP_ATOMIC ); <nl> + if ( status ) { <nl> + err (" usb_submit_urb failed % d ", status ); <nl> return - 1 ; <nl> + } <nl>  <nl> wait_event_interruptible_timeout ( iforce -> wait , <nl> iforce -> ctrl -> status != - EINPROGRESS , HZ ); <nl>  <nl> if ( iforce -> ctrl -> status ) { <nl> + dbg (" iforce -> ctrl -> status = % d ", iforce -> ctrl -> status ); <nl> usb_unlink_urb ( iforce -> ctrl ); <nl> return - 1 ; <nl> } <nl> # else <nl> - err (" iforce_get_id_packet : iforce -> bus = USB !"); <nl> + dbg (" iforce_get_id_packet : iforce -> bus = USB !"); <nl> # endif <nl> break ; <nl>  <nl> int iforce_init_device ( struct iforce * iforce ) <nl>  <nl> /* <nl> * Disable spring , enable force feedback . <nl> - * FIXME : We should use iforce_set_autocenter () et al here . <nl> */ <nl> - <nl> - iforce_send_packet ( iforce , FF_CMD_AUTOCENTER , "\ 004 \ 000 "); <nl> + iforce_set_autocenter ( input_dev , 0 ); <nl>  <nl> /* <nl> * Find appropriate device entry
mmm crypto / crypto_user_stat . c <nl> ppp crypto / crypto_user_stat . c <nl> int crypto_reportstat ( struct sk_buff * in_skb , struct nlmsghdr * in_nlh , <nl> drop_alg : <nl> crypto_mod_put ( alg ); <nl>  <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> return nlmsg_unicast ( net -> crypto_nlsk , skb , NETLINK_CB ( in_skb ). portid ); <nl> }
mmm drivers / gpu / drm / bochs / bochs_fbdev . c <nl> ppp drivers / gpu / drm / bochs / bochs_fbdev . c <nl> void bochs_fbdev_fini ( struct bochs_device * bochs ) <nl> if ( bochs -> fb . initialized ) <nl> bochs_fbdev_destroy ( bochs ); <nl>  <nl> - drm_fb_helper_fini (& bochs -> fb . helper ); <nl> + if ( bochs -> fb . helper . fbdev ) <nl> + drm_fb_helper_fini (& bochs -> fb . helper ); <nl> + <nl> bochs -> fb . initialized = false ; <nl> }
mmm tools / perf / util / annotate . c <nl> ppp tools / perf / util / annotate . c <nl> int __annotation__scnprintf_samples_period ( struct annotation * notes , <nl> bool show_freq ) <nl> { <nl> const char * ev_name = perf_evsel__name ( evsel ); <nl> - char ref [ 30 ] = " show reference callgraph , "; <nl> + char buf [ 1024 ], ref [ 30 ] = " show reference callgraph , "; <nl> char sample_freq_str [ 64 ] = ""; <nl> unsigned long nr_samples = 0 ; <nl> int nr_members = 1 ; <nl> int __annotation__scnprintf_samples_period ( struct annotation * notes , <nl> char unit ; <nl> int i ; <nl>  <nl> - if ( perf_evsel__is_group_event ( evsel )) <nl> + if ( perf_evsel__is_group_event ( evsel )) { <nl> + perf_evsel__group_desc ( evsel , buf , sizeof ( buf )); <nl> + ev_name = buf ; <nl> nr_members = evsel -> nr_members ; <nl> + } <nl>  <nl> for ( i = 0 ; i < nr_members ; i ++) { <nl> struct sym_hist * ah = annotation__histogram ( notes , evsel -> idx + i );
mmm drivers / net / usb / rtl8150 . c <nl> ppp drivers / net / usb / rtl8150 . c <nl> static int read_mii_word ( rtl8150_t * dev , u8 phy , __u8 indx , u16 * reg ) <nl> get_registers ( dev , PHYCNT , 1 , data ); <nl> } while (( data [ 0 ] & PHY_GO ) && ( i ++ < MII_TIMEOUT )); <nl>  <nl> - if ( i < MII_TIMEOUT ) { <nl> + if ( i <= MII_TIMEOUT ) { <nl> get_registers ( dev , PHYDAT , 2 , data ); <nl> * reg = data [ 0 ] | ( data [ 1 ] << 8 ); <nl> return 0 ; <nl> static int write_mii_word ( rtl8150_t * dev , u8 phy , __u8 indx , u16 reg ) <nl> get_registers ( dev , PHYCNT , 1 , data ); <nl> } while (( data [ 0 ] & PHY_GO ) && ( i ++ < MII_TIMEOUT )); <nl>  <nl> - if ( i < MII_TIMEOUT ) <nl> + if ( i <= MII_TIMEOUT ) <nl> return 0 ; <nl> else <nl> return 1 ;
mmm tools / perf / util / intel - pt - decoder / intel - pt - decoder . c <nl> ppp tools / perf / util / intel - pt - decoder / intel - pt - decoder . c <nl> static const char * intel_pt_err_msgs [] = { <nl>  <nl> int intel_pt__strerror ( int code , char * buf , size_t buflen ) <nl> { <nl> - if ( code < 1 || code > INTEL_PT_ERR_MAX ) <nl> + if ( code < 1 || code >= INTEL_PT_ERR_MAX ) <nl> code = INTEL_PT_ERR_UNK ; <nl> strlcpy ( buf , intel_pt_err_msgs [ code ], buflen ); <nl> return 0 ;
mmm fs / ocfs2 / quota_local . c <nl> ppp fs / ocfs2 / quota_local . c <nl> static int ocfs2_local_free_info ( struct super_block * sb , int type ) <nl> int mark_clean = 1 , len ; <nl> int status ; <nl>  <nl> - /* At this point we know there are no more dquots and thus <nl> - * even if there ' s some sync in the pdflush queue , it won ' t <nl> - * find any dquots and return without doing anything */ <nl> - cancel_delayed_work_sync (& oinfo -> dqi_sync_work ); <nl> iput ( oinfo -> dqi_gqinode ); <nl> ocfs2_simple_drop_lockres ( OCFS2_SB ( sb ), & oinfo -> dqi_gqlock ); <nl> ocfs2_lock_res_free (& oinfo -> dqi_gqlock );mmm fs / ocfs2 / super . c <nl> ppp fs / ocfs2 / super . c <nl> static int ocfs2_local_free_info ( struct super_block * sb , int type ) <nl> int mark_clean = 1 , len ; <nl> int status ; <nl>  <nl> - /* At this point we know there are no more dquots and thus <nl> - * even if there ' s some sync in the pdflush queue , it won ' t <nl> - * find any dquots and return without doing anything */ <nl> - cancel_delayed_work_sync (& oinfo -> dqi_sync_work ); <nl> iput ( oinfo -> dqi_gqinode ); <nl> ocfs2_simple_drop_lockres ( OCFS2_SB ( sb ), & oinfo -> dqi_gqlock ); <nl> ocfs2_lock_res_free (& oinfo -> dqi_gqlock ); <nl> static void ocfs2_disable_quotas ( struct ocfs2_super * osb ) <nl> int type ; <nl> struct inode * inode ; <nl> struct super_block * sb = osb -> sb ; <nl> + struct ocfs2_mem_dqinfo * oinfo ; <nl>  <nl> /* We mostly ignore errors in this function because there ' s not much <nl> * we can do when we see them */ <nl> for ( type = 0 ; type < MAXQUOTAS ; type ++) { <nl> if (! sb_has_quota_loaded ( sb , type )) <nl> continue ; <nl> + /* Cancel periodic syncing before we grab dqonoff_mutex */ <nl> + oinfo = sb_dqinfo ( sb , type )-> dqi_priv ; <nl> + cancel_delayed_work_sync (& oinfo -> dqi_sync_work ); <nl> inode = igrab ( sb -> s_dquot . files [ type ]); <nl> /* Turn off quotas . This will remove all dquot structures from <nl> * memory and so they will be automatically synced to global
mmm drivers / gpu / drm / i915 / i915_gem . c <nl> ppp drivers / gpu / drm / i915 / i915_gem . c <nl> i915_gem_pwrite_ioctl ( struct drm_device * dev , void * data , <nl>  <nl> if ( obj -> gtt_space && <nl> obj -> cache_level == I915_CACHE_NONE && <nl> + obj -> tiling_mode == I915_TILING_NONE && <nl> obj -> map_and_fenceable && <nl> obj -> base . write_domain != I915_GEM_DOMAIN_CPU ) { <nl> ret = i915_gem_gtt_pwrite_fast ( dev , obj , args , file );
mmm net / bridge / br_mdb . c <nl> ppp net / bridge / br_mdb . c <nl> static int br_mdb_fill_info ( struct sk_buff * skb , struct netlink_callback * cb , <nl> port = p -> port ; <nl> if ( port ) { <nl> struct br_mdb_entry e ; <nl> + memset (& e , 0 , sizeof ( e )); <nl> e . ifindex = port -> dev -> ifindex ; <nl> e . state = p -> state ; <nl> if ( p -> addr . proto == htons ( ETH_P_IP )) <nl> static int br_mdb_dump ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> break ; <nl>  <nl> bpm = nlmsg_data ( nlh ); <nl> + memset ( bpm , 0 , sizeof (* bpm )); <nl> bpm -> ifindex = dev -> ifindex ; <nl> if ( br_mdb_fill_info ( skb , cb , dev ) < 0 ) <nl> goto out ; <nl> static int nlmsg_populate_mdb_fill ( struct sk_buff * skb , <nl> return - EMSGSIZE ; <nl>  <nl> bpm = nlmsg_data ( nlh ); <nl> + memset ( bpm , 0 , sizeof (* bpm )); <nl> bpm -> family = AF_BRIDGE ; <nl> bpm -> ifindex = dev -> ifindex ; <nl> nest = nla_nest_start ( skb , MDBA_MDB ); <nl> void br_mdb_notify ( struct net_device * dev , struct net_bridge_port * port , <nl> { <nl> struct br_mdb_entry entry ; <nl>  <nl> + memset (& entry , 0 , sizeof ( entry )); <nl> entry . ifindex = port -> dev -> ifindex ; <nl> entry . addr . proto = group -> proto ; <nl> entry . addr . u . ip4 = group -> u . ip4 ;
mmm mm / nommu . c <nl> ppp mm / nommu . c <nl> int __get_user_pages ( struct task_struct * tsk , struct mm_struct * mm , <nl> ( VM_MAYREAD | VM_MAYWRITE ) : ( VM_READ | VM_WRITE ); <nl>  <nl> for ( i = 0 ; i < nr_pages ; i ++) { <nl> - vma = find_vma ( mm , start ); <nl> + vma = find_extend_vma ( mm , start ); <nl> if (! vma ) <nl> goto finish_or_fault ; <nl>  <nl> EXPORT_SYMBOL ( find_vma ); <nl> */ <nl> struct vm_area_struct * find_extend_vma ( struct mm_struct * mm , unsigned long addr ) <nl> { <nl> - return find_vma ( mm , addr ); <nl> + return find_vma ( mm , addr & PAGE_MASK ); <nl> } <nl>  <nl> /*
mmm kernel / sched . c <nl> ppp kernel / sched . c <nl> asmlinkage void __sched schedule ( void ) <nl> } <nl> EXPORT_SYMBOL ( schedule ); <nl>  <nl> -# ifdef CONFIG_SMP <nl> +# ifdef CONFIG_MUTEX_SPIN_ON_OWNER <nl> /* <nl> * Look out ! " owner " is an entirely speculative pointer <nl> * access and not reliable .
mmm net / rds / rdma . c <nl> ppp net / rds / rdma . c <nl> int rds_rdma_extra_size ( struct rds_rdma_args * args ) <nl>  <nl> local_vec = ( struct rds_iovec __user *)( unsigned long ) args -> local_vec_addr ; <nl>  <nl> + if ( args -> nr_local == 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* figure out the number of pages in the vector */ <nl> for ( i = 0 ; i < args -> nr_local ; i ++) { <nl> if ( copy_from_user (& vec , & local_vec [ i ],
mmm sound / soc / sof / debug . c <nl> ppp sound / soc / sof / debug . c <nl> static ssize_t sof_dfsentry_write ( struct file * file , const char __user * buffer , <nl> */ <nl> dentry = file -> f_path . dentry ; <nl> if ( strcmp ( dentry -> d_name . name , " ipc_flood_count ") && <nl> - strcmp ( dentry -> d_name . name , " ipc_flood_duration_ms ")) <nl> - return - EINVAL ; <nl> + strcmp ( dentry -> d_name . name , " ipc_flood_duration_ms ")) { <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> + } <nl>  <nl> if (! strcmp ( dentry -> d_name . name , " ipc_flood_duration_ms ")) <nl> flood_duration_test = true ;
mmm drivers / hwmon / nct7802 . c <nl> ppp drivers / hwmon / nct7802 . c <nl> static int nct7802_read_fan_min ( struct nct7802_data * data , u8 reg_fan_low , <nl> ret = 0 ; <nl> else if ( ret ) <nl> ret = DIV_ROUND_CLOSEST ( 1350000U , ret ); <nl> + else <nl> + ret = 1350000U ; <nl> abort : <nl> mutex_unlock (& data -> access_lock ); <nl> return ret ; <nl> } <nl>  <nl> static int nct7802_write_fan_min ( struct nct7802_data * data , u8 reg_fan_low , <nl> - u8 reg_fan_high , unsigned int limit ) <nl> + u8 reg_fan_high , unsigned long limit ) <nl> { <nl> int err ; <nl>  <nl> static int nct7802_write_voltage ( struct nct7802_data * data , int nr , int index , <nl> int shift = 8 - REG_VOLTAGE_LIMIT_MSB_SHIFT [ index - 1 ][ nr ]; <nl> int err ; <nl>  <nl> + voltage = clamp_val ( voltage , 0 , 0x3ff * nct7802_vmul [ nr ]); <nl> voltage = DIV_ROUND_CLOSEST ( voltage , nct7802_vmul [ nr ]); <nl> - voltage = clamp_val ( voltage , 0 , 0x3ff ); <nl>  <nl> mutex_lock (& data -> access_lock ); <nl> err = regmap_write ( data -> regmap , <nl> static ssize_t store_temp ( struct device * dev , struct device_attribute * attr , <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> - val = clamp_val ( DIV_ROUND_CLOSEST ( val , 1000 ), - 128 , 127 ); <nl> + val = DIV_ROUND_CLOSEST ( clamp_val ( val , - 128000 , 127000 ), 1000 ); <nl>  <nl> err = regmap_write ( data -> regmap , nr , val & 0xff ); <nl> return err ? : count ;
mmm drivers / net / ethernet / broadcom / bcmsysport . c <nl> ppp drivers / net / ethernet / broadcom / bcmsysport . c <nl> static struct sk_buff * bcm_sysport_insert_tsb ( struct sk_buff * skb , <nl> u32 csum_info ; <nl> u8 ip_proto ; <nl> u16 csum_start ; <nl> - u16 ip_ver ; <nl> + __be16 ip_ver ; <nl>  <nl> /* Re - allocate SKB if needed */ <nl> if ( unlikely ( skb_headroom ( skb ) < sizeof (* tsb ))) { <nl> static struct sk_buff * bcm_sysport_insert_tsb ( struct sk_buff * skb , <nl> memset ( tsb , 0 , sizeof (* tsb )); <nl>  <nl> if ( skb -> ip_summed == CHECKSUM_PARTIAL ) { <nl> - ip_ver = htons ( skb -> protocol ); <nl> + ip_ver = skb -> protocol ; <nl> switch ( ip_ver ) { <nl> - case ETH_P_IP : <nl> + case htons ( ETH_P_IP ): <nl> ip_proto = ip_hdr ( skb )-> protocol ; <nl> break ; <nl> - case ETH_P_IPV6 : <nl> + case htons ( ETH_P_IPV6 ): <nl> ip_proto = ipv6_hdr ( skb )-> nexthdr ; <nl> break ; <nl> default : <nl> static struct sk_buff * bcm_sysport_insert_tsb ( struct sk_buff * skb , <nl>  <nl> if ( ip_proto == IPPROTO_TCP || ip_proto == IPPROTO_UDP ) { <nl> csum_info |= L4_LENGTH_VALID ; <nl> - if ( ip_proto == IPPROTO_UDP && ip_ver == ETH_P_IP ) <nl> + if ( ip_proto == IPPROTO_UDP && <nl> + ip_ver == htons ( ETH_P_IP )) <nl> csum_info |= L4_UDP ; <nl> } else { <nl> csum_info = 0 ;
mmm drivers / media / i2c / s5k5baf . c <nl> ppp drivers / media / i2c / s5k5baf . c <nl> static void s5k5baf_synchronize ( struct s5k5baf * state , int timeout , u16 addr ) <nl> static u16 * s5k5baf_fw_get_seq ( struct s5k5baf * state , u16 seq_id ) <nl> { <nl> struct s5k5baf_fw * fw = state -> fw ; <nl> - u16 * data = fw -> data + 2 * fw -> count ; <nl> + u16 * data ; <nl> int i ; <nl>  <nl> if ( fw == NULL ) <nl> return NULL ; <nl>  <nl> + data = fw -> data + 2 * fw -> count ; <nl> + <nl> for ( i = 0 ; i < fw -> count ; ++ i ) { <nl> if ( fw -> seq [ i ]. id == seq_id ) <nl> return data + fw -> seq [ i ]. offset ;
mmm drivers / mfd / max77686 . c <nl> ppp drivers / mfd / max77686 . c <nl> static struct mfd_cell max77686_devs [] = { <nl> { . name = " max77686 - pmic ", }, <nl> { . name = " max77686 - rtc ", }, <nl> + { . name = " max77686 - clk ", }, <nl> }; <nl>  <nl> static struct regmap_config max77686_regmap_config = {
mmm drivers / isdn / hardware / eicon / message . c <nl> ppp drivers / isdn / hardware / eicon / message . c <nl> static byte connect_res ( dword Id , word Number , DIVA_CAPI_ADAPTER * a , <nl> add_ai ( plci , & parms [ 5 ]); <nl> sig_req ( plci , REJECT , 0 ); <nl> } <nl> - else if ( Reject == 1 || Reject > 9 ) <nl> + else if ( Reject == 1 || Reject >= 9 ) <nl> { <nl> add_ai ( plci , & parms [ 5 ]); <nl> sig_req ( plci , HANGUP , 0 );
mmm drivers / media / video / cx231xx / cx231xx - cards . c <nl> ppp drivers / media / video / cx231xx / cx231xx - cards . c <nl> void cx231xx_pre_card_setup ( struct cx231xx * dev ) <nl> dev -> board . name , dev -> model ); <nl>  <nl> /* set the direction for GPIO pins */ <nl> - cx231xx_set_gpio_direction ( dev , dev -> board . tuner_gpio -> bit , 1 ); <nl> - cx231xx_set_gpio_value ( dev , dev -> board . tuner_gpio -> bit , 1 ); <nl> - cx231xx_set_gpio_direction ( dev , dev -> board . tuner_sif_gpio , 1 ); <nl> + if ( dev -> board . tuner_gpio ) { <nl> + cx231xx_set_gpio_direction ( dev , dev -> board . tuner_gpio -> bit , 1 ); <nl> + cx231xx_set_gpio_value ( dev , dev -> board . tuner_gpio -> bit , 1 ); <nl> + cx231xx_set_gpio_direction ( dev , dev -> board . tuner_sif_gpio , 1 ); <nl>  <nl> - /* request some modules if any required */ <nl> + /* request some modules if any required */ <nl>  <nl> - /* reset the Tuner */ <nl> - cx231xx_gpio_set ( dev , dev -> board . tuner_gpio ); <nl> + /* reset the Tuner */ <nl> + cx231xx_gpio_set ( dev , dev -> board . tuner_gpio ); <nl> + } <nl>  <nl> /* set the mode to Analog mode initially */ <nl> cx231xx_set_mode ( dev , CX231XX_ANALOG_MODE );
mmm drivers / net / wireless / ti / wlcore / debugfs . c <nl> ppp drivers / net / wireless / ti / wlcore / debugfs . c <nl> static ssize_t driver_state_read ( struct file * file , char __user * user_buf , <nl> DRIVER_STATE_PRINT_HEX ( chip . id ); <nl> DRIVER_STATE_PRINT_STR ( chip . fw_ver_str ); <nl> DRIVER_STATE_PRINT_STR ( chip . phy_fw_ver_str ); <nl> + DRIVER_STATE_PRINT_INT ( recovery_count ); <nl>  <nl> # undef DRIVER_STATE_PRINT_INT <nl> # undef DRIVER_STATE_PRINT_LONGmmm drivers / net / wireless / ti / wlcore / wlcore . h <nl> ppp drivers / net / wireless / ti / wlcore / wlcore . h <nl> static ssize_t driver_state_read ( struct file * file , char __user * user_buf , <nl> DRIVER_STATE_PRINT_HEX ( chip . id ); <nl> DRIVER_STATE_PRINT_STR ( chip . fw_ver_str ); <nl> DRIVER_STATE_PRINT_STR ( chip . phy_fw_ver_str ); <nl> + DRIVER_STATE_PRINT_INT ( recovery_count ); <nl>  <nl> # undef DRIVER_STATE_PRINT_INT <nl> # undef DRIVER_STATE_PRINT_LONG <nl> struct wl1271 { <nl>  <nl> bool enable_11a ; <nl>  <nl> + int recovery_count ; <nl> + <nl> /* Most recently reported noise in dBm */ <nl> s8 noise ; <nl> mmm drivers / net / wireless / ti / wlcore / main . c <nl> ppp drivers / net / wireless / ti / wlcore / main . c <nl> static ssize_t driver_state_read ( struct file * file , char __user * user_buf , <nl> DRIVER_STATE_PRINT_HEX ( chip . id ); <nl> DRIVER_STATE_PRINT_STR ( chip . fw_ver_str ); <nl> DRIVER_STATE_PRINT_STR ( chip . phy_fw_ver_str ); <nl> + DRIVER_STATE_PRINT_INT ( recovery_count ); <nl>  <nl> # undef DRIVER_STATE_PRINT_INT <nl> # undef DRIVER_STATE_PRINT_LONG <nl> struct wl1271 { <nl>  <nl> bool enable_11a ; <nl>  <nl> + int recovery_count ; <nl> + <nl> /* Most recently reported noise in dBm */ <nl> s8 noise ; <nl>  <nl> static void wlcore_print_recovery ( struct wl1271 * wl ) <nl> if ( ret < 0 ) <nl> return ; <nl>  <nl> - wl1271_info (" pc : 0x % x , hint_sts : 0x % 08x ", pc , hint_sts ); <nl> + wl1271_info (" pc : 0x % x , hint_sts : 0x % 08x count : % d ", <nl> + pc , hint_sts , ++ wl -> recovery_count ); <nl>  <nl> wlcore_set_partition ( wl , & wl -> ptable [ PART_WORK ]); <nl> } <nl> struct ieee80211_hw * wlcore_alloc_hw ( size_t priv_size , u32 aggr_buf_size , <nl> wl -> flags = 0 ; <nl> wl -> sg_enabled = true ; <nl> wl -> sleep_auth = WL1271_PSM_ILLEGAL ; <nl> + wl -> recovery_count = 0 ; <nl> wl -> hw_pg_ver = - 1 ; <nl> wl -> ap_ps_map = 0 ; <nl> wl -> ap_fw_ps_map = 0 ;
mmm sound / pci / rme9652 / hdspm . c <nl> ppp sound / pci / rme9652 / hdspm . c <nl> static void hdspm_set_dds_value ( struct hdspm * hdspm , int rate ) <nl> { <nl> u64 n ; <nl>  <nl> + if ( snd_BUG_ON ( rate <= 0 )) <nl> + return ; <nl> + <nl> if ( rate >= 112000 ) <nl> rate /= 4 ; <nl> else if ( rate >= 56000 ) <nl> static int hdspm_get_system_sample_rate ( struct hdspm * hdspm ) <nl> } else { <nl> /* slave mode , return external sample rate */ <nl> rate = hdspm_external_sample_rate ( hdspm ); <nl> + if (! rate ) <nl> + rate = hdspm -> system_sample_rate ; <nl> } <nl> } <nl>  <nl> static int snd_hdspm_put_system_sample_rate ( struct snd_kcontrol * kcontrol , <nl> ucontrol ) <nl> { <nl> struct hdspm * hdspm = snd_kcontrol_chip ( kcontrol ); <nl> + int rate = ucontrol -> value . integer . value [ 0 ]; <nl>  <nl> + if ( rate < 27000 || rate > 207000 ) <nl> + return - EINVAL ; <nl> hdspm_set_dds_value ( hdspm , ucontrol -> value . integer . value [ 0 ]); <nl> return 0 ; <nl> }
mmm drivers / infiniband / hw / nes / nes_cm . c <nl> ppp drivers / infiniband / hw / nes / nes_cm . c <nl> # include < net / neighbour . h > <nl> # include < net / route . h > <nl> # include < net / ip_fib . h > <nl> +# include < net / tcp . h > <nl>  <nl> # include " nes . h " <nl>  <nl> static int check_seq ( struct nes_cm_node * cm_node , struct tcphdr * tcph , <nl> rcv_wnd = cm_node -> tcp_cntxt . rcv_wnd ; <nl> if ( ack_seq != loc_seq_num ) <nl> err = 1 ; <nl> - else if (( seq + rcv_wnd ) < rcv_nxt ) <nl> + else if (! between ( seq , rcv_nxt , ( rcv_nxt + rcv_wnd ))) <nl> err = 1 ; <nl> if ( err ) { <nl> nes_debug ( NES_DBG_CM , "% s [% u ] create abort for cm_node =% p "
mmm drivers / i2c / i2c - core . c <nl> ppp drivers / i2c / i2c - core . c <nl> static int i2c_check_addr ( struct i2c_adapter * adapter , int addr ) <nl> int i2c_attach_client ( struct i2c_client * client ) <nl> { <nl> struct i2c_adapter * adapter = client -> adapter ; <nl> - int res = 0 ; <nl> + int res ; <nl> + <nl> + /* Check for address business */ <nl> + res = i2c_check_addr ( adapter , client -> addr ); <nl> + if ( res ) <nl> + return res ; <nl>  <nl> client -> dev . parent = & client -> adapter -> dev ; <nl> client -> dev . bus = & i2c_bus_type ;
mmm drivers / char / vt . c <nl> ppp drivers / char / vt . c <nl> static int con_open ( struct tty_struct * tty , struct file * filp ) <nl> tty -> winsize . ws_row = vc_cons [ currcons ]. d -> vc_rows ; <nl> tty -> winsize . ws_col = vc_cons [ currcons ]. d -> vc_cols ; <nl> } <nl> + if ( vc -> vc_utf ) <nl> + tty -> termios -> c_iflag |= IUTF8 ; <nl> + else <nl> + tty -> termios -> c_iflag &= ~ IUTF8 ; <nl> release_console_sem (); <nl> vcs_make_sysfs ( tty ); <nl> return ret ; <nl> int __init vty_init ( void ) <nl> console_driver -> minor_start = 1 ; <nl> console_driver -> type = TTY_DRIVER_TYPE_CONSOLE ; <nl> console_driver -> init_termios = tty_std_termios ; <nl> + if ( default_utf8 ) <nl> + console_driver -> init_termios . c_iflag |= IUTF8 ; <nl> console_driver -> flags = TTY_DRIVER_REAL_RAW | TTY_DRIVER_RESET_TERMIOS ; <nl> tty_set_operations ( console_driver , & con_ops ); <nl> if ( tty_register_driver ( console_driver ))
mmm drivers / staging / rtl8723au / core / rtw_cmd . c <nl> ppp drivers / staging / rtl8723au / core / rtw_cmd . c <nl> int rtw_disassoc_cmd23a ( struct rtw_adapter * padapter , u32 deauth_timeout_ms , <nl> } else { <nl> /* no need to enqueue , do the cmd hdl directly and <nl> free cmd parameter */ <nl> - if ( H2C_SUCCESS != disconnect_hdl23a ( padapter , ( u8 *) param )) <nl> + if ( disconnect_hdl23a ( padapter , ( u8 *) param ) != H2C_SUCCESS ) <nl> res = _FAIL ; <nl> kfree ( param ); <nl> }
mmm drivers / iio / accel / bmc150 - accel . c <nl> ppp drivers / iio / accel / bmc150 - accel . c <nl> static void bmc150_accel_unregister_triggers ( struct bmc150_accel_data * data , <nl> { <nl> int i ; <nl>  <nl> - for ( i = from ; i >= 0 ; i ++) { <nl> + for ( i = from ; i >= 0 ; i --) { <nl> if ( data -> triggers [ i ]. indio_trig ) { <nl> iio_trigger_unregister ( data -> triggers [ i ]. indio_trig ); <nl> data -> triggers [ i ]. indio_trig = NULL ;
mmm drivers / net / ethernet / intel / i40e / i40e_dcb_nl . c <nl> ppp drivers / net / ethernet / intel / i40e / i40e_dcb_nl . c <nl> void i40e_dcbnl_set_all ( struct i40e_vsi * vsi ) <nl> if (!( pf -> flags & I40E_FLAG_DCB_ENABLED )) <nl> return ; <nl>  <nl> + /* MFP mode but not an iSCSI PF so return */ <nl> + if (( pf -> flags & I40E_FLAG_MFP_ENABLED ) && !( pf -> hw . func_caps . iscsi )) <nl> + return ; <nl> + <nl> dcbxcfg = & hw -> local_dcbx_config ; <nl>  <nl> /* Set up all the App TLVs if DCBx is negotiated */ <nl> void i40e_dcbnl_flush_apps ( struct i40e_pf * pf , <nl> struct i40e_dcb_app_priority_table app ; <nl> int i ; <nl>  <nl> + /* MFP mode but not an iSCSI PF so return */ <nl> + if (( pf -> flags & I40E_FLAG_MFP_ENABLED ) && !( pf -> hw . func_caps . iscsi )) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < old_cfg -> numapps ; i ++) { <nl> app = old_cfg -> app [ i ]; <nl> /* The APP is not available anymore delete it */
mmm net / wireless / scan . c <nl> ppp net / wireless / scan . c <nl> struct cfg80211_bss * cfg80211_get_bss ( struct wiphy * wiphy , <nl> continue ; <nl> if ( channel && bss -> pub . channel != channel ) <nl> continue ; <nl> + if (! is_valid_ether_addr ( bss -> pub . bssid )) <nl> + continue ; <nl> /* Don ' t get expired BSS structs */ <nl> if ( time_after ( now , bss -> ts + IEEE80211_SCAN_RESULT_EXPIRE ) && <nl> ! atomic_read (& bss -> hold ))
mmm drivers / net / ethernet / amd / xgbe / xgbe - desc . c <nl> ppp drivers / net / ethernet / amd / xgbe / xgbe - desc . c <nl> static int xgbe_map_tx_skb ( struct xgbe_channel * channel , struct sk_buff * skb ) <nl> } <nl> } <nl>  <nl> - /* Save the skb address in the last entry */ <nl> + /* Save the skb address in the last entry . We always have some data <nl> + * that has been mapped so rdata is always advanced past the last <nl> + * piece of mapped data - use the entry pointed to by cur_index - 1 . <nl> + */ <nl> + rdata = XGBE_GET_DESC_DATA ( ring , cur_index - 1 ); <nl> rdata -> skb = skb ; <nl>  <nl> /* Save the number of descriptor entries used */
mmm sound / soc / intel / skylake / skl - topology . c <nl> ppp sound / soc / intel / skylake / skl - topology . c <nl> static int skl_manifest_load ( struct snd_soc_component * cmpnt , <nl> struct skl * skl = ebus_to_skl ( ebus ); <nl> int ret = 0 ; <nl>  <nl> + /* proceed only if we have private data defined */ <nl> + if ( manifest -> priv . size == 0 ) <nl> + return 0 ; <nl> + <nl> minfo = & skl -> skl_sst -> manifest ; <nl>  <nl> skl_tplg_get_manifest_data ( manifest , bus -> dev , minfo );
mmm drivers / staging / greybus / module . c <nl> ppp drivers / staging / greybus / module . c <nl> static struct attribute * module_attrs [] = { <nl> }; <nl> ATTRIBUTE_GROUPS ( module ); <nl>  <nl> - static void greybus_module_release ( struct device * dev ) <nl> + static void gb_module_release ( struct device * dev ) <nl> { <nl> struct gb_module * module = to_gb_module ( dev ); <nl>  <nl> static void greybus_module_release ( struct device * dev ) <nl>  <nl> struct device_type greybus_module_type = { <nl> . name = " greybus_module ", <nl> - . release = greybus_module_release , <nl> + . release = gb_module_release , <nl> }; <nl>  <nl> struct module_find {
mmm drivers / net / irda / nsc - ircc . c <nl> ppp drivers / net / irda / nsc - ircc . c <nl> static int nsc_ircc_probe_39x ( nsc_chip_t * chip , chipio_t * info ); <nl> static int nsc_ircc_init_108 ( nsc_chip_t * chip , chipio_t * info ); <nl> static int nsc_ircc_init_338 ( nsc_chip_t * chip , chipio_t * info ); <nl> static int nsc_ircc_init_39x ( nsc_chip_t * chip , chipio_t * info ); <nl> +# ifdef CONFIG_PNP <nl> static int nsc_ircc_pnp_probe ( struct pnp_dev * dev , const struct pnp_device_id * id ); <nl> +# endif <nl>  <nl> /* These are the known NSC chips */ <nl> static nsc_chip_t chips [] = { <nl> static const struct pnp_device_id nsc_ircc_pnp_table [] = { <nl> MODULE_DEVICE_TABLE ( pnp , nsc_ircc_pnp_table ); <nl>  <nl> static struct pnp_driver nsc_ircc_pnp_driver = { <nl> +# ifdef CONFIG_PNP <nl> . name = " nsc - ircc ", <nl> . id_table = nsc_ircc_pnp_table , <nl> . probe = nsc_ircc_pnp_probe , <nl> +# endif <nl> }; <nl>  <nl> /* Some prototypes */ <nl> static int nsc_ircc_probe_39x ( nsc_chip_t * chip , chipio_t * info ) <nl> return 0 ; <nl> } <nl>  <nl> +# ifdef CONFIG_PNP <nl> /* PNP probing */ <nl> static int nsc_ircc_pnp_probe ( struct pnp_dev * dev , const struct pnp_device_id * id ) <nl> { <nl> static int nsc_ircc_pnp_probe ( struct pnp_dev * dev , const struct pnp_device_id * i <nl>  <nl> return 0 ; <nl> } <nl> +# endif <nl>  <nl> /* <nl> * Function nsc_ircc_setup ( info )
mmm drivers / rtc / rtc - pcf85063 . c <nl> ppp drivers / rtc / rtc - pcf85063 . c <nl> static int pcf85063_probe ( struct i2c_client * client , <nl> const struct i2c_device_id * id ) <nl> { <nl> struct rtc_device * rtc ; <nl> + int err ; <nl>  <nl> dev_dbg (& client -> dev , "% s \ n ", __func__ ); <nl>  <nl> if (! i2c_check_functionality ( client -> adapter , I2C_FUNC_I2C )) <nl> return - ENODEV ; <nl>  <nl> + err = i2c_smbus_read_byte_data ( client , PCF85063_REG_CTRL1 ); <nl> + if ( err < 0 ) { <nl> + dev_err (& client -> dev , " RTC chip is not present \ n "); <nl> + return err ; <nl> + } <nl> + <nl> rtc = devm_rtc_device_register (& client -> dev , <nl> pcf85063_driver . driver . name , <nl> & pcf85063_rtc_ops , THIS_MODULE );
mmm drivers / net / virtio_net . c <nl> ppp drivers / net / virtio_net . c <nl> static int virtnet_set_channels ( struct net_device * dev , <nl> if ( channels -> rx_count || channels -> tx_count || channels -> other_count ) <nl> return - EINVAL ; <nl>  <nl> - if ( queue_pairs > vi -> max_queue_pairs ) <nl> + if ( queue_pairs > vi -> max_queue_pairs || queue_pairs == 0 ) <nl> return - EINVAL ; <nl>  <nl> get_online_cpus ();
mmm drivers / phy / phy - core . c <nl> ppp drivers / phy / phy - core . c <nl> struct phy * phy_create ( struct device * dev , const struct phy_ops * ops , <nl> if ( id < 0 ) { <nl> dev_err ( dev , " unable to get id \ n "); <nl> ret = id ; <nl> - goto err0 ; <nl> + goto err1 ; <nl> } <nl>  <nl> device_initialize (& phy -> dev ); <nl> struct phy * phy_create ( struct device * dev , const struct phy_ops * ops , <nl>  <nl> ret = dev_set_name (& phy -> dev , " phy -% s .% d ", dev_name ( dev ), id ); <nl> if ( ret ) <nl> - goto err1 ; <nl> + goto err2 ; <nl>  <nl> ret = device_add (& phy -> dev ); <nl> if ( ret ) <nl> - goto err1 ; <nl> + goto err2 ; <nl>  <nl> if ( pm_runtime_enabled ( dev )) { <nl> pm_runtime_enable (& phy -> dev ); <nl> struct phy * phy_create ( struct device * dev , const struct phy_ops * ops , <nl>  <nl> return phy ; <nl>  <nl> - err1 : <nl> + err2 : <nl> ida_remove (& phy_ida , phy -> id ); <nl> put_device (& phy -> dev ); <nl> + err1 : <nl> kfree ( phy ); <nl> - <nl> err0 : <nl> return ERR_PTR ( ret ); <nl> }
mmm drivers / clk / clk - stm32h7 . c <nl> ppp drivers / clk / clk - stm32h7 . c <nl> static void get_cfg_composite_div ( const struct composite_clk_gcfg * gcfg , <nl> mux_ops = div_ops = gate_ops = NULL ; <nl> mux_hw = div_hw = gate_hw = NULL ; <nl>  <nl> - if ( gcfg -> mux && gcfg -> mux ) { <nl> + if ( gcfg -> mux && cfg -> mux ) { <nl> mux = _get_cmux ( base + cfg -> mux -> offset , <nl> cfg -> mux -> shift , <nl> cfg -> mux -> width , <nl> static void get_cfg_composite_div ( const struct composite_clk_gcfg * gcfg , <nl> } <nl> } <nl>  <nl> - if ( gcfg -> gate && gcfg -> gate ) { <nl> + if ( gcfg -> gate && cfg -> gate ) { <nl> gate = _get_cgate ( base + cfg -> gate -> offset , <nl> cfg -> gate -> bit_idx , <nl> gcfg -> gate -> flags , lock );
mmm arch / powerpc / kernel / ptrace . c <nl> ppp arch / powerpc / kernel / ptrace . c <nl> static void flush_tmregs_to_thread ( struct task_struct * tsk ) <nl> * in the appropriate thread structures from live . <nl> */ <nl>  <nl> - if ( tsk != current ) <nl> + if ((! cpu_has_feature ( CPU_FTR_TM )) || ( tsk != current )) <nl> return ; <nl>  <nl> if ( MSR_TM_SUSPENDED ( mfmsr ())) {
mmm arch / arm / mach - omap2 / prcm - common . h <nl> ppp arch / arm / mach - omap2 / prcm - common . h <nl> # define OMAP24XX_EN_GPT1_MASK ( 1 << 0 ) <nl>  <nl> /* PM_WKST_WKUP , CM_IDLEST_WKUP shared bits */ <nl> -# define OMAP24XX_ST_GPIOS_SHIFT ( 1 << 2 ) <nl> -# define OMAP24XX_ST_GPIOS_MASK 2 <nl> -# define OMAP24XX_ST_GPT1_SHIFT ( 1 << 0 ) <nl> -# define OMAP24XX_ST_GPT1_MASK 0 <nl> +# define OMAP24XX_ST_GPIOS_SHIFT 2 <nl> +# define OMAP24XX_ST_GPIOS_MASK ( 1 << 2 ) <nl> +# define OMAP24XX_ST_GPT1_SHIFT 0 <nl> +# define OMAP24XX_ST_GPT1_MASK ( 1 << 0 ) <nl>  <nl> /* CM_IDLEST_MDM and PM_WKST_MDM shared bits */ <nl> -# define OMAP2430_ST_MDM_SHIFT ( 1 << 0 ) <nl> +# define OMAP2430_ST_MDM_SHIFT 0 <nl> +# define OMAP2430_ST_MDM_MASK ( 1 << 0 ) <nl>  <nl>  <nl> /* 3430 register bits shared between CM & PRM registers */
mmm drivers / net / ethernet / broadcom / bcmsysport . c <nl> ppp drivers / net / ethernet / broadcom / bcmsysport . c <nl> static int bcm_sysport_init_tx_ring ( struct bcm_sysport_priv * priv , <nl>  <nl> ring -> cbs = kcalloc ( size , sizeof ( struct bcm_sysport_cb ), GFP_KERNEL ); <nl> if (! ring -> cbs ) { <nl> + dma_free_coherent ( kdev , sizeof ( struct dma_desc ), <nl> + ring -> desc_cpu , ring -> desc_dma ); <nl> netif_err ( priv , hw , priv -> netdev , " CB allocation failed \ n "); <nl> return - ENOMEM ; <nl> }
mmm fs / fuse / dev . c <nl> ppp fs / fuse / dev . c <nl> static int fuse_notify_inval_entry ( struct fuse_conn * fc , unsigned int size , <nl> if ( outarg . namelen > FUSE_NAME_MAX ) <nl> goto err ; <nl>  <nl> + err = - EINVAL ; <nl> + if ( size != sizeof ( outarg ) + outarg . namelen + 1 ) <nl> + goto err ; <nl> + <nl> name . name = buf ; <nl> name . len = outarg . namelen ; <nl> err = fuse_copy_one ( cs , buf , outarg . namelen + 1 );
mmm drivers / net / ixgb / ixgb_main . c <nl> ppp drivers / net / ixgb / ixgb_main . c <nl> ixgb_unmap_and_free_tx_resource ( struct ixgb_adapter * adapter , <nl> pci_unmap_page ( pdev , buffer_info -> dma , buffer_info -> length , <nl> PCI_DMA_TODEVICE ); <nl>  <nl> + /* okay to call kfree_skb here instead of kfree_skb_any because <nl> + * this is never called in interrupt context */ <nl> if ( buffer_info -> skb ) <nl> - dev_kfree_skb_any ( buffer_info -> skb ); <nl> + dev_kfree_skb ( buffer_info -> skb ); <nl>  <nl> buffer_info -> skb = NULL ; <nl> buffer_info -> dma = 0 ; <nl> ixgb_xmit_frame ( struct sk_buff * skb , struct net_device * netdev ) <nl> } <nl>  <nl> if ( skb -> len <= 0 ) { <nl> - dev_kfree_skb_any ( skb ); <nl> + dev_kfree_skb ( skb ); <nl> return 0 ; <nl> } <nl>  <nl> ixgb_xmit_frame ( struct sk_buff * skb , struct net_device * netdev ) <nl>  <nl> tso = ixgb_tso ( adapter , skb ); <nl> if ( tso < 0 ) { <nl> - dev_kfree_skb_any ( skb ); <nl> + dev_kfree_skb ( skb ); <nl> return NETDEV_TX_OK ; <nl> } <nl> 
mmm net / rds / ib . c <nl> ppp net / rds / ib . c <nl> static int rds_ib_laddr_check ( __be32 addr ) <nl> ret = rdma_bind_addr ( cm_id , ( struct sockaddr *)& sin ); <nl> /* due to this , we will claim to support iWARP devices unless we <nl> check node_type . */ <nl> - if ( ret || cm_id -> device -> node_type != RDMA_NODE_IB_CA ) <nl> + if ( ret || ! cm_id -> device || <nl> + cm_id -> device -> node_type != RDMA_NODE_IB_CA ) <nl> ret = - EADDRNOTAVAIL ; <nl>  <nl> rdsdebug (" addr % pI4 ret % d node type % d \ n ",
mmm drivers / gpu / drm / virtio / virtgpu_object . c <nl> ppp drivers / gpu / drm / virtio / virtgpu_object . c <nl> static int virtio_gpu_object_shmem_init ( struct virtio_gpu_device * vgdev , <nl> * since virtio_gpu doesn ' t support dma - buf import from other devices . <nl> */ <nl> shmem -> pages = drm_gem_shmem_get_sg_table (& bo -> base ); <nl> - if (! shmem -> pages ) { <nl> + if ( IS_ERR ( shmem -> pages )) { <nl> drm_gem_shmem_unpin (& bo -> base ); <nl> - return - EINVAL ; <nl> + return PTR_ERR ( shmem -> pages ); <nl> } <nl>  <nl> if ( use_dma_api ) {
mmm mm / bootmem . c <nl> ppp mm / bootmem . c <nl> static void __init __free_pages_memory ( unsigned long start , unsigned long end ) <nl> end_aligned = end & ~( BITS_PER_LONG - 1 ); <nl>  <nl> if ( end_aligned <= start_aligned ) { <nl> -# if 1 <nl> - printk ( KERN_DEBUG " % lx - % lx \ n ", start , end ); <nl> -# endif <nl> for ( i = start ; i < end ; i ++) <nl> __free_pages_bootmem ( pfn_to_page ( i ), 0 ); <nl>  <nl> return ; <nl> } <nl>  <nl> -# if 1 <nl> - printk ( KERN_DEBUG " % lx % lx - % lx % lx \ n ", <nl> - start , start_aligned , end_aligned , end ); <nl> -# endif <nl> for ( i = start ; i < start_aligned ; i ++) <nl> __free_pages_bootmem ( pfn_to_page ( i ), 0 ); <nl>  <nl> void __init free_bootmem_node ( pg_data_t * pgdat , unsigned long physaddr , <nl> { <nl> # ifdef CONFIG_NO_BOOTMEM <nl> free_early ( physaddr , physaddr + size ); <nl> -# if 0 <nl> - printk ( KERN_DEBUG " free % lx % lx \ n ", physaddr , size ); <nl> -# endif <nl> # else <nl> unsigned long start , end ; <nl>  <nl> void __init free_bootmem ( unsigned long addr , unsigned long size ) <nl> { <nl> # ifdef CONFIG_NO_BOOTMEM <nl> free_early ( addr , addr + size ); <nl> -# if 0 <nl> - printk ( KERN_DEBUG " free % lx % lx \ n ", addr , size ); <nl> -# endif <nl> # else <nl> unsigned long start , end ; <nl> 
mmm drivers / net / ethernet / arc / emac_main . c <nl> ppp drivers / net / ethernet / arc / emac_main . c <nl> static void arc_emac_tx_clean ( struct net_device * ndev ) <nl> struct sk_buff * skb = tx_buff -> skb ; <nl> unsigned int info = le32_to_cpu ( txbd -> info ); <nl>  <nl> - if (( info & FOR_EMAC ) || ! txbd -> data ) <nl> + if (( info & FOR_EMAC ) || ! txbd -> data || ! skb ) <nl> break ; <nl>  <nl> if ( unlikely ( info & ( DROP | DEFR | LTCL | UFLO ))) { <nl> static void arc_emac_tx_clean ( struct net_device * ndev ) <nl>  <nl> txbd -> data = 0 ; <nl> txbd -> info = 0 ; <nl> + tx_buff -> skb = NULL ; <nl>  <nl> * txbd_dirty = (* txbd_dirty + 1 ) % TX_BD_NUM ; <nl> } <nl> static int arc_emac_tx ( struct sk_buff * skb , struct net_device * ndev ) <nl> dma_unmap_addr_set (& priv -> tx_buff [* txbd_curr ], addr , addr ); <nl> dma_unmap_len_set (& priv -> tx_buff [* txbd_curr ], len , len ); <nl>  <nl> - priv -> tx_buff [* txbd_curr ]. skb = skb ; <nl> priv -> txbd [* txbd_curr ]. data = cpu_to_le32 ( addr ); <nl>  <nl> /* Make sure pointer to data buffer is set */ <nl> static int arc_emac_tx ( struct sk_buff * skb , struct net_device * ndev ) <nl>  <nl> * info = cpu_to_le32 ( FOR_EMAC | FIRST_OR_LAST_MASK | len ); <nl>  <nl> + /* Make sure info word is set */ <nl> + wmb (); <nl> + <nl> + priv -> tx_buff [* txbd_curr ]. skb = skb ; <nl> + <nl> /* Increment index to point to the next BD */ <nl> * txbd_curr = (* txbd_curr + 1 ) % TX_BD_NUM ; <nl> 
mmm drivers / tty / tty_io . c <nl> ppp drivers / tty / tty_io . c <nl> static int tty_open ( struct inode * inode , struct file * filp ) <nl> if ( IS_ERR ( tty )) { <nl> tty_unlock (); <nl> mutex_unlock (& tty_mutex ); <nl> + tty_driver_kref_put ( driver ); <nl> return PTR_ERR ( tty ); <nl> } <nl> }
mmm drivers / video / fbdev / ssd1307fb . c <nl> ppp drivers / video / fbdev / ssd1307fb . c <nl> static int ssd1307fb_probe ( struct i2c_client * client , <nl> snprintf ( bl_name , sizeof ( bl_name ), " ssd1307fb % d ", info -> node ); <nl> bl = backlight_device_register ( bl_name , & client -> dev , par , <nl> & ssd1307fb_bl_ops , NULL ); <nl> - bl -> props . brightness = par -> contrast ; <nl> - bl -> props . max_brightness = MAX_CONTRAST ; <nl> - info -> bl_dev = bl ; <nl> - <nl> if ( IS_ERR ( bl )) { <nl> dev_err (& client -> dev , " unable to register backlight device : % ld \ n ", <nl> PTR_ERR ( bl )); <nl> goto bl_init_error ; <nl> } <nl> + <nl> + bl -> props . brightness = par -> contrast ; <nl> + bl -> props . max_brightness = MAX_CONTRAST ; <nl> + info -> bl_dev = bl ; <nl> + <nl> dev_info (& client -> dev , " fb % d : % s framebuffer device registered , using % d bytes of video memory \ n ", info -> node , info -> fix . id , vmem_size ); <nl>  <nl> return 0 ;
mmm drivers / staging / ozwpan / ozcdev . c <nl> ppp drivers / staging / ozwpan / ozcdev . c <nl> static ssize_t oz_cdev_write ( struct file * filp , const char __user * buf , <nl> struct oz_app_hdr * app_hdr ; <nl> struct oz_serial_ctx * ctx ; <nl>  <nl> + if ( count > sizeof ( ei -> data ) - sizeof (* elt ) - sizeof (* app_hdr )) <nl> + return - EINVAL ; <nl> + <nl> spin_lock_bh (& g_cdev . lock ); <nl> pd = g_cdev . active_pd ; <nl> if ( pd )
mmm drivers / rpmsg / virtio_rpmsg_bus . c <nl> ppp drivers / rpmsg / virtio_rpmsg_bus . c <nl> static int rpmsg_probe ( struct virtio_device * vdev ) <nl>  <nl> err = rpmsg_ns_register_device ( rpdev_ns ); <nl> if ( err ) <nl> - goto free_vch ; <nl> + /* vch will be free in virtio_rpmsg_release_device () */ <nl> + goto free_ctrldev ; <nl> } <nl>  <nl> /* <nl> static int rpmsg_probe ( struct virtio_device * vdev ) <nl>  <nl> return 0 ; <nl>  <nl> - free_vch : <nl> - kfree ( vch ); <nl> free_ctrldev : <nl> rpmsg_virtio_del_ctrl_dev ( rpdev_ctrl ); <nl> free_coherent :
mmm arch / score / include / asm / uaccess . h <nl> ppp arch / score / include / asm / uaccess . h <nl> do { \ <nl> __get_user_asm ( val , " lw ", ptr ); \ <nl> break ; \ <nl> case 8 : \ <nl> - if (( copy_from_user (( void *)& val , ptr , 8 )) == 0 ) \ <nl> + if ( __copy_from_user (( void *)& val , ptr , 8 ) == 0 ) \ <nl> __gu_err = 0 ; \ <nl> else \ <nl> __gu_err = - EFAULT ; \ <nl> do { \ <nl> \ <nl> if ( likely ( access_ok ( VERIFY_READ , __gu_ptr , size ))) \ <nl> __get_user_common (( x ), size , __gu_ptr ); \ <nl> + else \ <nl> + ( x ) = 0 ; \ <nl> \ <nl> __gu_err ; \ <nl> }) <nl> do { \ <nl> " 2 :\ n " \ <nl> ". section . fixup ,\" ax \"\ n " \ <nl> " 3 : li % 0 , % 4 \ n " \ <nl> + " li % 1 , 0 \ n " \ <nl> " j 2b \ n " \ <nl> ". previous \ n " \ <nl> ". section __ex_table ,\" a \"\ n " \
mmm drivers / staging / lustre / lustre / llite / lproc_llite . c <nl> ppp drivers / staging / lustre / lustre / llite / lproc_llite . c <nl> static ssize_t ll_max_cached_mb_seq_write ( struct file * file , <nl> return - ERANGE ; <nl> } <nl>  <nl> - if ( sbi -> ll_dt_exp == NULL ) <nl> - return - ENODEV ; <nl> - <nl> spin_lock (& sbi -> ll_lock ); <nl> diff = pages_number - cache -> ccc_lru_max ; <nl> spin_unlock (& sbi -> ll_lock ); <nl> static ssize_t ll_max_cached_mb_seq_write ( struct file * file , <nl> if ( diff <= 0 ) <nl> break ; <nl>  <nl> + if ( sbi -> ll_dt_exp == NULL ) { /* being initialized */ <nl> + rc = - ENODEV ; <nl> + break ; <nl> + } <nl> + <nl> /* difficult - have to ask OSCs to drop LRU slots . */ <nl> tmp = diff << 1 ; <nl> rc = obd_set_info_async ( NULL , sbi -> ll_dt_exp ,
mmm arch / x86 / kvm / x86 . c <nl> ppp arch / x86 / kvm / x86 . c <nl> int kvm_set_msr_common ( struct kvm_vcpu * vcpu , struct msr_data * msr_info ) <nl> /* ... but clean it before doing the actual write */ <nl> vcpu -> arch . time_offset = data & ~( PAGE_MASK | 1 ); <nl>  <nl> + /* Check that the address is 32 - byte aligned . */ <nl> + if ( vcpu -> arch . time_offset & <nl> + ( sizeof ( struct pvclock_vcpu_time_info ) - 1 )) <nl> + break ; <nl> + <nl> vcpu -> arch . time_page = <nl> gfn_to_page ( vcpu -> kvm , data >> PAGE_SHIFT ); <nl> 
mmm drivers / staging / media / msi3101 / sdr - msi3101 . c <nl> ppp drivers / staging / media / msi3101 / sdr - msi3101 . c <nl> static int msi3101_g_fmt_sdr_cap ( struct file * file , void * priv , <nl> dev_dbg (& s -> udev -> dev , "% s : pixelformat fourcc % 4 . 4s \ n ", __func__ , <nl> ( char *)& s -> pixelformat ); <nl>  <nl> + memset ( f -> fmt . sdr . reserved , 0 , sizeof ( f -> fmt . sdr . reserved )); <nl> f -> fmt . sdr . pixelformat = s -> pixelformat ; <nl>  <nl> return 0 ; <nl> static int msi3101_s_fmt_sdr_cap ( struct file * file , void * priv , <nl> if ( vb2_is_busy ( q )) <nl> return - EBUSY ; <nl>  <nl> + memset ( f -> fmt . sdr . reserved , 0 , sizeof ( f -> fmt . sdr . reserved )); <nl> for ( i = 0 ; i < NUM_FORMATS ; i ++) { <nl> if ( formats [ i ]. pixelformat == f -> fmt . sdr . pixelformat ) { <nl> s -> pixelformat = f -> fmt . sdr . pixelformat ; <nl> static int msi3101_try_fmt_sdr_cap ( struct file * file , void * priv , <nl> dev_dbg (& s -> udev -> dev , "% s : pixelformat fourcc % 4 . 4s \ n ", __func__ , <nl> ( char *)& f -> fmt . sdr . pixelformat ); <nl>  <nl> + memset ( f -> fmt . sdr . reserved , 0 , sizeof ( f -> fmt . sdr . reserved )); <nl> for ( i = 0 ; i < NUM_FORMATS ; i ++) { <nl> if ( formats [ i ]. pixelformat == f -> fmt . sdr . pixelformat ) <nl> return 0 ; <nl> static int msi3101_g_frequency ( struct file * file , void * priv , <nl> f -> frequency = s -> f_adc ; <nl> ret = 0 ; <nl> } else if ( f -> tuner == 1 ) { <nl> + f -> type = V4L2_TUNER_RF ; <nl> ret = v4l2_subdev_call ( s -> v4l2_subdev , tuner , g_frequency , f ); <nl> } else { <nl> ret = - EINVAL ;
mmm drivers / pcmcia / pcmcia_ioctl . c <nl> ppp drivers / pcmcia / pcmcia_ioctl . c <nl> static int ds_open ( struct inode * inode , struct file * file ) <nl> socket_t i = iminor ( inode ); <nl> struct pcmcia_socket * s ; <nl> user_info_t * user ; <nl> + static int warning_printed = 0 ; <nl>  <nl> ds_dbg ( 0 , " ds_open ( socket % d )\ n ", i ); <nl>  <nl> static int ds_open ( struct inode * inode , struct file * file ) <nl> s -> user = user ; <nl> file -> private_data = user ; <nl>  <nl> + if (! warning_printed ) { <nl> + printk ( KERN_INFO " pcmcia : Detected deprecated PCMCIA ioctl " <nl> + " usage .\ n "); <nl> + printk ( KERN_INFO " pcmcia : This interface will soon be removed from " <nl> + " the kernel ; please expect breakage unless you upgrade " <nl> + " to new tools .\ n "); <nl> + printk ( KERN_INFO " pcmcia : see http :// www . kernel . org / pub / linux /" <nl> + " utils / kernel / pcmcia / pcmcia . html for details .\ n "); <nl> + warning_printed = 1 ; <nl> + } <nl> + <nl> if ( s -> pcmcia_state . present ) <nl> queue_event ( user , CS_EVENT_CARD_INSERTION ); <nl> return 0 ;
mmm fs / proc / base . c <nl> ppp fs / proc / base . c <nl> static void proc_flush_task_mnt ( struct vfsmount * mnt , pid_t pid , pid_t tgid ) <nl> dput ( dentry ); <nl> } <nl>  <nl> + if ( pid == tgid ) <nl> + return ; <nl> + <nl> name . name = buf ; <nl> name . len = snprintf ( buf , sizeof ( buf ), "% d ", tgid ); <nl> leader = d_hash_and_lookup ( mnt -> mnt_root , & name );
mmm drivers / gpu / drm / i915 / i915_irq . c <nl> ppp drivers / gpu / drm / i915 / i915_irq . c <nl> void intel_irq_init ( struct drm_device * dev ) <nl> dev -> driver -> get_vblank_counter = gm45_get_vblank_counter ; <nl> } <nl>  <nl> - <nl> - dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + if ( drm_core_check_feature ( dev , DRIVER_MODESET )) <nl> + dev -> driver -> get_vblank_timestamp = i915_get_vblank_timestamp ; <nl> + else <nl> + dev -> driver -> get_vblank_timestamp = NULL ; <nl> dev -> driver -> get_scanout_position = i915_get_crtc_scanoutpos ; <nl>  <nl> if ( IS_IVYBRIDGE ( dev )) {
mmm drivers / net / ethernet / xilinx / ll_temac_main . c <nl> ppp drivers / net / ethernet / xilinx / ll_temac_main . c <nl> temac_start_xmit ( struct sk_buff * skb , struct net_device * ndev ) <nl> smp_mb (); <nl>  <nl> /* Space might have just been freed - check again */ <nl> - if ( temac_check_tx_bd_space ( lp , num_frag )) <nl> + if ( temac_check_tx_bd_space ( lp , num_frag + 1 )) <nl> return NETDEV_TX_BUSY ; <nl>  <nl> netif_wake_queue ( ndev );
mmm net / ipv4 / tcp_ipv4 . c <nl> ppp net / ipv4 / tcp_ipv4 . c <nl> static void tcp_v4_send_reset ( struct sock * sk , struct sk_buff * skb ) <nl> if ( th -> rst ) <nl> return ; <nl>  <nl> - if ( skb_rtable ( skb )-> rt_type != RTN_LOCAL ) <nl> + /* If sk not NULL , it means we did a successful lookup and incoming <nl> + * route had to be correct . prequeue might have dropped our dst . <nl> + */ <nl> + if (! sk && skb_rtable ( skb )-> rt_type != RTN_LOCAL ) <nl> return ; <nl>  <nl> /* Swap the send and the receive . */mmm net / ipv6 / tcp_ipv6 . c <nl> ppp net / ipv6 / tcp_ipv6 . c <nl> static void tcp_v4_send_reset ( struct sock * sk , struct sk_buff * skb ) <nl> if ( th -> rst ) <nl> return ; <nl>  <nl> - if ( skb_rtable ( skb )-> rt_type != RTN_LOCAL ) <nl> + /* If sk not NULL , it means we did a successful lookup and incoming <nl> + * route had to be correct . prequeue might have dropped our dst . <nl> + */ <nl> + if (! sk && skb_rtable ( skb )-> rt_type != RTN_LOCAL ) <nl> return ; <nl>  <nl> /* Swap the send and the receive . */ <nl> static void tcp_v6_send_reset ( struct sock * sk , struct sk_buff * skb ) <nl> if ( th -> rst ) <nl> return ; <nl>  <nl> - if (! ipv6_unicast_destination ( skb )) <nl> + /* If sk not NULL , it means we did a successful lookup and incoming <nl> + * route had to be correct . prequeue might have dropped our dst . <nl> + */ <nl> + if (! sk && ! ipv6_unicast_destination ( skb )) <nl> return ; <nl>  <nl> # ifdef CONFIG_TCP_MD5SIG
mmm fs / ocfs2 / dlm / dlmunlock . c <nl> ppp fs / ocfs2 / dlm / dlmunlock . c <nl> int dlm_unlock_lock_handler ( struct o2net_msg * msg , u32 len , void * data , <nl> } <nl>  <nl> if (! dlm_grab ( dlm )) <nl> - return DLM_REJECTED ; <nl> + return DLM_FORWARD ; <nl>  <nl> mlog_bug_on_msg (! dlm_domain_fully_joined ( dlm ), <nl> " Domain % s not fully joined !\ n ", dlm -> name );mmm fs / ocfs2 / dlm / dlmmaster . c <nl> ppp fs / ocfs2 / dlm / dlmmaster . c <nl> int dlm_unlock_lock_handler ( struct o2net_msg * msg , u32 len , void * data , <nl> } <nl>  <nl> if (! dlm_grab ( dlm )) <nl> - return DLM_REJECTED ; <nl> + return DLM_FORWARD ; <nl>  <nl> mlog_bug_on_msg (! dlm_domain_fully_joined ( dlm ), <nl> " Domain % s not fully joined !\ n ", dlm -> name ); <nl> int dlm_migrate_request_handler ( struct o2net_msg * msg , u32 len , void * data , <nl> int ret = 0 ; <nl>  <nl> if (! dlm_grab ( dlm )) <nl> - return - EINVAL ; <nl> + return 0 ; <nl>  <nl> name = migrate -> name ; <nl> namelen = migrate -> namelen ;
mmm drivers / net / wireless / realtek / rtlwifi / btcoexist / halbtcoutsrc . h <nl> ppp drivers / net / wireless / realtek / rtlwifi / btcoexist / halbtcoutsrc . h <nl> enum btc_set_type { <nl> BTC_SET_ACT_GET_BT_RSSI , <nl> BTC_SET_ACT_AGGREGATE_CTRL , <nl> BTC_SET_ACT_ANTPOSREGRISTRY_CTRL , <nl> + BTC_SET_MIMO_PS_MODE , <nl>  <nl> /********* for 1Ant **********/ <nl> /* type bool */ <nl> enum btc_set_type { <nl> BTC_SET_ACT_POST_NORMAL_LPS , <nl> BTC_SET_ACT_INC_FORCE_EXEC_PWR_CMD_CNT , <nl> BTC_SET_ACT_DISABLE_LOW_POWER , <nl> + BTC_SET_BL_BT_LNA_CONSTRAIN_LEVEL , <nl> BTC_SET_ACT_UPDATE_RAMASK , <nl> BTC_SET_ACT_SEND_MIMO_PS , <nl> /* BT Coex related */ <nl> struct btc_coexist { <nl>  <nl> void (* btc_set_bt_reg )( void * btc_context , u8 reg_type , u32 offset , <nl> u32 value ); <nl> + u32 (* btc_get_bt_reg )( void * btc_context , u8 reg_type , u32 offset ); <nl> u32 (* btc_get_bt_coex_supported_feature )( void * btcoexist ); <nl> u32 (* btc_get_bt_coex_supported_version )( void * btcoexist ); <nl> u32 (* btc_get_bt_phydm_version )( void * btcoexist );mmm drivers / net / wireless / realtek / rtlwifi / btcoexist / halbtcoutsrc . c <nl> ppp drivers / net / wireless / realtek / rtlwifi / btcoexist / halbtcoutsrc . c <nl> enum btc_set_type { <nl> BTC_SET_ACT_GET_BT_RSSI , <nl> BTC_SET_ACT_AGGREGATE_CTRL , <nl> BTC_SET_ACT_ANTPOSREGRISTRY_CTRL , <nl> + BTC_SET_MIMO_PS_MODE , <nl>  <nl> /********* for 1Ant **********/ <nl> /* type bool */ <nl> enum btc_set_type { <nl> BTC_SET_ACT_POST_NORMAL_LPS , <nl> BTC_SET_ACT_INC_FORCE_EXEC_PWR_CMD_CNT , <nl> BTC_SET_ACT_DISABLE_LOW_POWER , <nl> + BTC_SET_BL_BT_LNA_CONSTRAIN_LEVEL , <nl> BTC_SET_ACT_UPDATE_RAMASK , <nl> BTC_SET_ACT_SEND_MIMO_PS , <nl> /* BT Coex related */ <nl> struct btc_coexist { <nl>  <nl> void (* btc_set_bt_reg )( void * btc_context , u8 reg_type , u32 offset , <nl> u32 value ); <nl> + u32 (* btc_get_bt_reg )( void * btc_context , u8 reg_type , u32 offset ); <nl> u32 (* btc_get_bt_coex_supported_feature )( void * btcoexist ); <nl> u32 (* btc_get_bt_coex_supported_version )( void * btcoexist ); <nl> u32 (* btc_get_bt_phydm_version )( void * btcoexist ); <nl> static bool halbtc_get ( void * void_btcoexist , u8 get_type , void * out_buf ) <nl> case BTC_GET_BL_IS_ASUS_8723B : <nl> * bool_tmp = false ; <nl> break ; <nl> + case BTC_GET_BL_RF4CE_CONNECTED : <nl> + * bool_tmp = false ; <nl> + break ; <nl> case BTC_GET_S4_WIFI_RSSI : <nl> * s32_tmp = halbtc_get_wifi_rssi ( rtlpriv ); <nl> break ; <nl> static void halbtc_display_dbg_msg ( void * bt_context , u8 disp_type , <nl> } <nl> } <nl>  <nl> + static u32 halbtc_get_bt_reg ( void * btc_context , u8 reg_type , u32 offset ) <nl> +{ <nl> + return 0 ; <nl> +} <nl> + <nl> static bool halbtc_under_ips ( struct btc_coexist * btcoexist ) <nl> { <nl> struct rtl_priv * rtlpriv = btcoexist -> adapter ; <nl> bool exhalbtc_initlize_variables ( struct rtl_priv * rtlpriv ) <nl> btcoexist -> btc_get = halbtc_get ; <nl> btcoexist -> btc_set = halbtc_set ; <nl> btcoexist -> btc_set_bt_reg = halbtc_set_bt_reg ; <nl> + btcoexist -> btc_get_bt_reg = halbtc_get_bt_reg ; <nl>  <nl> btcoexist -> bt_info . bt_ctrl_buf_size = false ; <nl> btcoexist -> bt_info . agg_buf_size = 5 ;
mmm fs / nfsd / nfs4idmap . c <nl> ppp fs / nfsd / nfs4idmap . c <nl> nfsd_map_name_to_uid ( struct svc_rqst * rqstp , const char * name , size_t namelen , <nl> { <nl> __be32 status ; <nl> u32 id = - 1 ; <nl> + <nl> + if ( name == NULL || namelen == 0 ) <nl> + return nfserr_inval ; <nl> + <nl> status = do_name_to_id ( rqstp , IDMAP_TYPE_USER , name , namelen , & id ); <nl> * uid = make_kuid (& init_user_ns , id ); <nl> if (! uid_valid (* uid )) <nl> nfsd_map_name_to_gid ( struct svc_rqst * rqstp , const char * name , size_t namelen , <nl> { <nl> __be32 status ; <nl> u32 id = - 1 ; <nl> + <nl> + if ( name == NULL || namelen == 0 ) <nl> + return nfserr_inval ; <nl> + <nl> status = do_name_to_id ( rqstp , IDMAP_TYPE_GROUP , name , namelen , & id ); <nl> * gid = make_kgid (& init_user_ns , id ); <nl> if (! gid_valid (* gid ))
mmm drivers / platform / x86 / thinkpad_acpi . c <nl> ppp drivers / platform / x86 / thinkpad_acpi . c <nl> static int volume_alsa_mute_put ( struct snd_kcontrol * kcontrol , <nl> return volume_alsa_set_mute (! ucontrol -> value . integer . value [ 0 ]); <nl> } <nl>  <nl> - static struct snd_kcontrol_new volume_alsa_control_vol = { <nl> + static struct snd_kcontrol_new volume_alsa_control_vol __initdata = { <nl> . iface = SNDRV_CTL_ELEM_IFACE_MIXER , <nl> . name = " Console Playback Volume ", <nl> . index = 0 , <nl> static struct snd_kcontrol_new volume_alsa_control_vol = { <nl> . get = volume_alsa_vol_get , <nl> }; <nl>  <nl> - static struct snd_kcontrol_new volume_alsa_control_mute = { <nl> + static struct snd_kcontrol_new volume_alsa_control_mute __initdata = { <nl> . iface = SNDRV_CTL_ELEM_IFACE_MIXER , <nl> . name = " Console Playback Switch ", <nl> . index = 0 ,
mmm drivers / cpufreq / exynos - cpufreq . c <nl> ppp drivers / cpufreq / exynos - cpufreq . c <nl> static int exynos_cpufreq_scale ( unsigned int target_freq ) <nl> if ( ret ) { <nl> pr_err ("% s : failed to set cpu voltage to % d \ n ", <nl> __func__ , arm_volt ); <nl> - goto out ; <nl> + freqs . new = freqs . old ; <nl> + goto post_notify ; <nl> } <nl> } <nl>  <nl> static int exynos_cpufreq_scale ( unsigned int target_freq ) <nl> if ( ret ) { <nl> pr_err ("% s : failed to set cpu voltage to % d \ n ", <nl> __func__ , safe_arm_volt ); <nl> - goto out ; <nl> + freqs . new = freqs . old ; <nl> + goto post_notify ; <nl> } <nl> } <nl>  <nl> exynos_info -> set_freq ( old_index , index ); <nl>  <nl> + post_notify : <nl> cpufreq_notify_transition ( policy , & freqs , CPUFREQ_POSTCHANGE ); <nl>  <nl> + if ( ret ) <nl> + goto out ; <nl> + <nl> /* When the new frequency is lower than current frequency */ <nl> if (( freqs . new < freqs . old ) || <nl> (( freqs . new > freqs . old ) && safe_arm_volt )) {
mmm kernel / signal . c <nl> ppp kernel / signal . c <nl> enum siginfo_layout siginfo_layout ( int sig , int si_code ) <nl> [ SIGSEGV ] = { NSIGSEGV , SIL_FAULT }, <nl> [ SIGBUS ] = { NSIGBUS , SIL_FAULT }, <nl> [ SIGTRAP ] = { NSIGTRAP , SIL_FAULT }, <nl> -# if defined ( SIGMET ) && defined ( NSIGEMT ) <nl> +# if defined ( SIGEMT ) && defined ( NSIGEMT ) <nl> [ SIGEMT ] = { NSIGEMT , SIL_FAULT }, <nl> # endif <nl> [ SIGCHLD ] = { NSIGCHLD , SIL_CHLD },
mmm drivers / xen / events . c <nl> ppp drivers / xen / events . c <nl> int evtchn_get ( unsigned int evtchn ) <nl> struct irq_info * info ; <nl> int err = - ENOENT ; <nl>  <nl> + if ( evtchn >= NR_EVENT_CHANNELS ) <nl> + return - EINVAL ; <nl> + <nl> mutex_lock (& irq_mapping_update_lock ); <nl>  <nl> irq = evtchn_to_irq [ evtchn ];
mmm drivers / staging / rtl8192e / rtl8192e / rtl_core . c <nl> ppp drivers / staging / rtl8192e / rtl8192e / rtl_core . c <nl> static void _rtl92e_if_silent_reset ( struct net_device * dev ) <nl> } <nl> } <nl>  <nl> - static void rtl819x_update_rxcounts ( struct r8192_priv * priv , u32 * TotalRxBcnNum , <nl> + static void _rtl92e_update_rxcounts ( struct r8192_priv * priv , u32 * TotalRxBcnNum , <nl> u32 * TotalRxDataNum ) <nl> { <nl> u16 SlotIndex ; <nl> static void rtl819x_watchdog_wqcallback ( void * data ) <nl> u32 TotalRxBcnNum = 0 ; <nl> u32 TotalRxDataNum = 0 ; <nl>  <nl> - rtl819x_update_rxcounts ( priv , & TotalRxBcnNum , & TotalRxDataNum ); <nl> + _rtl92e_update_rxcounts ( priv , & TotalRxBcnNum , & TotalRxDataNum ); <nl>  <nl> if (( TotalRxBcnNum + TotalRxDataNum ) == 0 ) <nl> priv -> check_roaming_cnt ++;
mmm drivers / scsi / qla2xxx / qla_iocb . c <nl> ppp drivers / scsi / qla2xxx / qla_iocb . c <nl> qla2x00_start_scsi ( srb_t * sp ) <nl> __constant_cpu_to_le16 ( CF_SIMPLE_TAG ); <nl> break ; <nl> } <nl> + } else { <nl> + cmd_pkt -> control_flags = __constant_cpu_to_le16 ( CF_SIMPLE_TAG ); <nl> } <nl>  <nl> /* Load SCSI command packet . */ <nl> qla24xx_build_scsi_crc_2_iocbs ( srb_t * sp , struct cmd_type_crc_2 * cmd_pkt , <nl> fcp_cmnd -> task_attribute = TSK_ORDERED ; <nl> break ; <nl> default : <nl> - fcp_cmnd -> task_attribute = 0 ; <nl> + fcp_cmnd -> task_attribute = TSK_SIMPLE ; <nl> break ; <nl> } <nl> } else { <nl> - fcp_cmnd -> task_attribute = 0 ; <nl> + fcp_cmnd -> task_attribute = TSK_SIMPLE ; <nl> } <nl>  <nl> cmd_pkt -> fcp_rsp_dseg_len = 0 ; /* Let response come in status iocb */ <nl> qla24xx_start_scsi ( srb_t * sp ) <nl> case ORDERED_QUEUE_TAG : <nl> cmd_pkt -> task = TSK_ORDERED ; <nl> break ; <nl> + default : <nl> + cmd_pkt -> task = TSK_SIMPLE ; <nl> + break ; <nl> } <nl> + } else { <nl> + cmd_pkt -> task = TSK_SIMPLE ; <nl> } <nl>  <nl> /* Load SCSI command packet . */
mmm drivers / gpu / drm / i915 / intel_display . c <nl> ppp drivers / gpu / drm / i915 / intel_display . c <nl> bool intel_crtc_active ( struct drm_crtc * crtc ) <nl> * <nl> * We can ditch the crtc -> primary -> fb check as soon as we can <nl> * properly reconstruct framebuffers . <nl> + * <nl> + * FIXME : The intel_crtc -> active here should be switched to <nl> + * crtc -> state -> active once we have proper CRTC states wired up <nl> + * for atomic . <nl> */ <nl> - return intel_crtc -> active && crtc -> primary -> fb && <nl> + return intel_crtc -> active && crtc -> primary -> state -> fb && <nl> intel_crtc -> config -> base . adjusted_mode . crtc_clock ; <nl> } <nl> 
mmm drivers / usb / otg / twl4030 - usb . c <nl> ppp drivers / usb / otg / twl4030 - usb . c <nl> static int __exit twl4030_usb_remove ( struct platform_device * pdev ) <nl> /* disable complete OTG block */ <nl> twl4030_usb_clear_bits ( twl , POWER_CTRL , POWER_CTRL_OTG_ENAB ); <nl>  <nl> - twl4030_phy_power ( twl , 0 ); <nl> + if (! twl -> asleep ) <nl> + twl4030_phy_power ( twl , 0 ); <nl> regulator_put ( twl -> usb1v5 ); <nl> regulator_put ( twl -> usb1v8 ); <nl> regulator_put ( twl -> usb3v1 );
mmm drivers / iommu / amd_iommu . c <nl> ppp drivers / iommu / amd_iommu . c <nl> static void dma_ops_domain_free ( struct dma_ops_domain * dom ) <nl>  <nl> free_pagetable (& dom -> domain ); <nl>  <nl> + if ( dom -> domain . id ) <nl> + domain_id_free ( dom -> domain . id ); <nl> + <nl> kfree ( dom ); <nl> } <nl> 
mmm drivers / gpu / drm / drm_modeset_lock . c <nl> ppp drivers / gpu / drm / drm_modeset_lock . c <nl> void drm_modeset_acquire_init ( struct drm_modeset_acquire_ctx * ctx , <nl> uint32_t flags ) <nl> { <nl> + memset ( ctx , 0 , sizeof (* ctx )); <nl> ww_acquire_init (& ctx -> ww_ctx , & crtc_ww_class ); <nl> INIT_LIST_HEAD (& ctx -> locked ); <nl> }
mmm drivers / usb / phy / phy - rcar - gen2 - usb . c <nl> ppp drivers / usb / phy / phy - rcar - gen2 - usb . c <nl> static int rcar_gen2_usb_phy_probe ( struct platform_device * pdev ) <nl> priv -> phy . shutdown = rcar_gen2_usb_phy_shutdown ; <nl> priv -> phy . set_suspend = rcar_gen2_usb_phy_set_suspend ; <nl>  <nl> - retval = usb_add_phy (& priv -> phy , USB_PHY_TYPE_USB2 ); <nl> + retval = usb_add_phy_dev (& priv -> phy ); <nl> if ( retval < 0 ) { <nl> dev_err ( dev , " Failed to add USB phy \ n "); <nl> return retval ;
mmm drivers / i2c / busses / scx200_acb . c <nl> ppp drivers / i2c / busses / scx200_acb . c <nl> static s32 scx200_acb_smbus_xfer ( struct i2c_adapter * adapter , <nl> buffer = ( u8 *)& cur_word ; <nl> break ; <nl>  <nl> - case I2C_SMBUS_BLOCK_DATA : <nl> + case I2C_SMBUS_I2C_BLOCK_DATA : <nl> + if ( rw == I2C_SMBUS_READ ) <nl> + data -> block [ 0 ] = I2C_SMBUS_BLOCK_MAX ; /* For now */ <nl> len = data -> block [ 0 ]; <nl> + if ( len == 0 || len > I2C_SMBUS_BLOCK_MAX ) <nl> + return - EINVAL ; <nl> buffer = & data -> block [ 1 ]; <nl> break ; <nl>  <nl> static u32 scx200_acb_func ( struct i2c_adapter * adapter ) <nl> { <nl> return I2C_FUNC_SMBUS_QUICK | I2C_FUNC_SMBUS_BYTE | <nl> I2C_FUNC_SMBUS_BYTE_DATA | I2C_FUNC_SMBUS_WORD_DATA | <nl> - I2C_FUNC_SMBUS_BLOCK_DATA ; <nl> + I2C_FUNC_SMBUS_I2C_BLOCK ; <nl> } <nl>  <nl> /* For now , we only handle combined mode ( smbus ) */
mmm net / ipv6 / addrconf . c <nl> ppp net / ipv6 / addrconf . c <nl> static inline void ipv6_store_devconf ( struct ipv6_devconf * cnf , <nl> # endif <nl> array [ DEVCONF_DISABLE_IPV6 ] = cnf -> disable_ipv6 ; <nl> array [ DEVCONF_ACCEPT_DAD ] = cnf -> accept_dad ; <nl> + array [ DEVCONF_FORCE_TLLAO ] = cnf -> force_tllao ; <nl> } <nl>  <nl> static inline size_t inet6_if_nlmsg_size ( void )
mmm drivers / media / video / videodev . c <nl> ppp drivers / media / video / videodev . c <nl> EXPORT_SYMBOL ( video_ioctl2 ); <nl> static int get_index ( struct video_device * vdev , int num ) <nl> { <nl> u32 used = 0 ; <nl> + const unsigned max_index = sizeof ( used ) * 8 - 1 ; <nl> int i ; <nl>  <nl> - if ( num >= 32 ) { <nl> + /* Currently a single v4l driver instance cannot create more than <nl> + 32 devices . <nl> + Increase to u64 or an array of u32 if more are needed . */ <nl> + if ( num > max_index ) { <nl> printk ( KERN_ERR " videodev : % s num is too large \ n ", __func__ ); <nl> return - EINVAL ; <nl> } <nl> static int get_index ( struct video_device * vdev , int num ) <nl> } <nl>  <nl> i = ffz ( used ); <nl> - return i >= 32 ? - ENFILE : i ; <nl> + return i > max_index ? - ENFILE : i ; <nl> } <nl>  <nl> static const struct file_operations video_fops ;
mmm fs / namei . c <nl> ppp fs / namei . c <nl> struct file * do_filp_open ( int dfd , const char * pathname , <nl> mutex_lock (& dir -> d_inode -> i_mutex ); <nl> path . dentry = lookup_hash (& nd ); <nl> path . mnt = nd . path . mnt ; <nl> - __putname ( nd . last . name ); <nl> filp = do_last (& nd , & path , open_flag , flag , acc_mode , mode , <nl> pathname , dir , & is_link ); <nl> + __putname ( nd . last . name ); <nl> if ( is_link ) <nl> goto do_link ; <nl> if ( nd . root . mnt )
mmm tools / perf / builtin - trace . c <nl> ppp tools / perf / builtin - trace . c <nl> static int trace__read_syscall_info ( struct trace * trace , int id ) <nl>  <nl> sc -> args = sc -> tp_format -> format . fields ; <nl> sc -> nr_args = sc -> tp_format -> format . nr_fields ; <nl> - /* drop nr field - not relevant here ; does not exist on older kernels */ <nl> - if ( sc -> args && strcmp ( sc -> args -> name , " nr ") == 0 ) { <nl> + /* <nl> + * We need to check and discard the first variable ' __syscall_nr ' <nl> + * or ' nr ' that mean the syscall number . It is needless here . <nl> + * So drop ' __syscall_nr ' or ' nr ' field but does not exist on older kernels . <nl> + */ <nl> + if ( sc -> args && (! strcmp ( sc -> args -> name , " __syscall_nr ") || ! strcmp ( sc -> args -> name , " nr "))) { <nl> sc -> args = sc -> args -> next ; <nl> -- sc -> nr_args ; <nl> }
mmm fs / xfs / libxfs / xfs_ialloc . c <nl> ppp fs / xfs / libxfs / xfs_ialloc . c <nl> xfs_dialloc_ag_inobt ( <nl>  <nl> /* free inodes to the left ? */ <nl> if ( useleft && trec . ir_freecount ) { <nl> - rec = trec ; <nl> xfs_btree_del_cursor ( cur , XFS_BTREE_NOERROR ); <nl> cur = tcur ; <nl>  <nl> pag -> pagl_leftrec = trec . ir_startino ; <nl> pag -> pagl_rightrec = rec . ir_startino ; <nl> pag -> pagl_pagino = pagino ; <nl> + rec = trec ; <nl> goto alloc_inode ; <nl> } <nl> 
mmm drivers / net / wireless / mwifiex / cfg80211 . c <nl> ppp drivers / net / wireless / mwifiex / cfg80211 . c <nl> static int mwifiex_cfg80211_set_bitrate_mask ( struct wiphy * wiphy , <nl> struct mwifiex_private * priv = mwifiex_netdev_get_priv ( dev ); <nl> u16 bitmap_rates [ MAX_BITMAP_RATES_SIZE ]; <nl> enum ieee80211_band band ; <nl> + struct mwifiex_adapter * adapter = priv -> adapter ; <nl>  <nl> if (! priv -> media_connected ) { <nl> - dev_err ( priv -> adapter -> dev , <nl> + dev_err ( adapter -> dev , <nl> " Can not set Tx data rate in disconnected state \ n "); <nl> return - EINVAL ; <nl> } <nl> static int mwifiex_cfg80211_set_bitrate_mask ( struct wiphy * wiphy , <nl>  <nl> /* Fill HT MCS rates */ <nl> bitmap_rates [ 2 ] = mask -> control [ band ]. ht_mcs [ 0 ]; <nl> - if ( priv -> adapter -> hw_dev_mcs_support == HT_STREAM_2X2 ) <nl> + if ( adapter -> hw_dev_mcs_support == HT_STREAM_2X2 ) <nl> bitmap_rates [ 2 ] |= mask -> control [ band ]. ht_mcs [ 1 ] << 8 ; <nl>  <nl> + /* Fill VHT MCS rates */ <nl> + if ( adapter -> fw_api_ver == MWIFIEX_FW_V15 ) { <nl> + bitmap_rates [ 10 ] = mask -> control [ band ]. vht_mcs [ 0 ]; <nl> + if ( adapter -> hw_dev_mcs_support == HT_STREAM_2X2 ) <nl> + bitmap_rates [ 11 ] = mask -> control [ band ]. vht_mcs [ 1 ]; <nl> + } <nl> + <nl> return mwifiex_send_cmd ( priv , HostCmd_CMD_TX_RATE_CFG , <nl> HostCmd_ACT_GEN_SET , 0 , bitmap_rates , true ); <nl> }
mmm drivers / serial / imx . c <nl> ppp drivers / serial / imx . c <nl> static int serial_imx_probe ( struct platform_device * pdev ) <nl> if ( pdata && ( pdata -> flags & IMXUART_HAVE_RTSCTS )) <nl> sport -> have_rtscts = 1 ; <nl>  <nl> - if ( pdata -> init ) <nl> - pdata -> init ( pdev ); <nl> + if ( pdata -> init ) { <nl> + ret = pdata -> init ( pdev ); <nl> + if ( ret ) <nl> + goto clkput ; <nl> + } <nl>  <nl> uart_add_one_port (& imx_reg , & sport -> port ); <nl> platform_set_drvdata ( pdev , & sport -> port ); <nl>  <nl> return 0 ; <nl> + clkput : <nl> + clk_put ( sport -> clk ); <nl> + clk_disable ( sport -> clk ); <nl> unmap : <nl> iounmap ( sport -> port . membase ); <nl> free :
mmm net / mac80211 / mlme . c <nl> ppp net / mac80211 / mlme . c <nl> static bool ieee80211_sta_wmm_params ( struct ieee80211_local * local , <nl> params [ ac ]. acm = acm ; <nl> params [ ac ]. uapsd = uapsd ; <nl>  <nl> - if ( params [ ac ]. cw_min > params [ ac ]. cw_max ) { <nl> + if ( params -> cw_min == 0 || <nl> + params [ ac ]. cw_min > params [ ac ]. cw_max ) { <nl> sdata_info ( sdata , <nl> " AP has invalid WMM params ( CWmin / max =% d /% d for ACI % d ), using defaults \ n ", <nl> params [ ac ]. cw_min , params [ ac ]. cw_max , aci );
mmm fs / nfsd / nfs4state . c <nl> ppp fs / nfsd / nfs4state . c <nl> static bool svc_rqst_integrity_protected ( struct svc_rqst * rqstp ) <nl> struct svc_cred * cr = & rqstp -> rq_cred ; <nl> u32 service ; <nl>  <nl> + if (! cr -> cr_gss_mech ) <nl> + return false ; <nl> service = gss_pseudoflavor_to_service ( cr -> cr_gss_mech , cr -> cr_flavor ); <nl> return service == RPC_GSS_SVC_INTEGRITY || <nl> service == RPC_GSS_SVC_PRIVACY ;
mmm drivers / mfd / lm3533 - core . c <nl> ppp drivers / mfd / lm3533 - core . c <nl> static bool lm3533_readable_register ( struct device * dev , unsigned int reg ) <nl> static bool lm3533_volatile_register ( struct device * dev , unsigned int reg ) <nl> { <nl> switch ( reg ) { <nl> - case 0x34 : /* zone */ <nl> + case 0x34 ... 0x36 : /* zone */ <nl> case 0x37 ... 0x38 : /* adc */ <nl> case 0xb0 ... 0xb1 : /* fault */ <nl> return true ;
mmm drivers / char / pcmcia / cm4000_cs . c <nl> ppp drivers / char / pcmcia / cm4000_cs . c <nl> static int cmm_ioctl ( struct inode * inode , struct file * filp , unsigned int cmd , <nl> dev_link_t * link ; <nl> int size ; <nl> int rc ; <nl> + void __user * argp = ( void __user *) arg ; <nl> # ifdef PCMCIA_DEBUG <nl> char * ioctl_names [ CM_IOC_MAXNR + 1 ] = { <nl> [ _IOC_NR ( CM_IOCGSTATUS )] " CM_IOCGSTATUS ", <nl> static int cmm_ioctl ( struct inode * inode , struct file * filp , unsigned int cmd , <nl> _IOC_DIR ( cmd ), _IOC_READ , _IOC_WRITE , size , cmd ); <nl>  <nl> if ( _IOC_DIR ( cmd ) & _IOC_READ ) { <nl> - if (! access_ok ( VERIFY_WRITE , ( void *) arg , size )) <nl> + if (! access_ok ( VERIFY_WRITE , argp , size )) <nl> return - EFAULT ; <nl> } <nl> if ( _IOC_DIR ( cmd ) & _IOC_WRITE ) { <nl> - if (! access_ok ( VERIFY_READ , ( void *) arg , size )) <nl> + if (! access_ok ( VERIFY_READ , argp , size )) <nl> return - EFAULT ; <nl> } <nl>  <nl> static int cmm_ioctl ( struct inode * inode , struct file * filp , unsigned int cmd , <nl> status |= CM_NO_READER ; <nl> if ( test_bit ( IS_BAD_CARD , & dev -> flags )) <nl> status |= CM_BAD_CARD ; <nl> - if ( copy_to_user (( int *) arg , & status , sizeof ( int ))) <nl> + if ( copy_to_user ( argp , & status , sizeof ( int ))) <nl> return - EFAULT ; <nl> } <nl> return 0 ; <nl> case CM_IOCGATR : <nl> DEBUGP ( 4 , dev , "... in CM_IOCGATR \ n "); <nl> { <nl> - struct atreq * atreq = ( struct atreq *) arg ; <nl> + struct atreq __user * atreq = argp ; <nl> int tmp ; <nl> /* allow nonblocking io and being interrupted */ <nl> if ( wait_event_interruptible <nl> static int cmm_ioctl ( struct inode * inode , struct file * filp , unsigned int cmd , <nl> { <nl> struct ptsreq krnptsreq ; <nl>  <nl> - if ( copy_from_user (& krnptsreq , ( struct ptsreq *) arg , <nl> + if ( copy_from_user (& krnptsreq , argp , <nl> sizeof ( struct ptsreq ))) <nl> return - EFAULT ; <nl>  <nl> static int cmm_ioctl ( struct inode * inode , struct file * filp , unsigned int cmd , <nl> int old_pc_debug = 0 ; <nl>  <nl> old_pc_debug = pc_debug ; <nl> - if ( copy_from_user (& pc_debug , ( int *) arg , sizeof ( int ))) <nl> + if ( copy_from_user (& pc_debug , argp , sizeof ( int ))) <nl> return - EFAULT ; <nl>  <nl> if ( old_pc_debug != pc_debug )
mmm drivers / char / virtio_console . c <nl> ppp drivers / char / virtio_console . c <nl> static int put_chars ( u32 vtermno , const char * buf , int count ) <nl> { <nl> struct port * port ; <nl> struct scatterlist sg [ 1 ]; <nl> + void * data ; <nl> + int ret ; <nl>  <nl> if ( unlikely ( early_put_chars )) <nl> return early_put_chars ( vtermno , buf , count ); <nl> static int put_chars ( u32 vtermno , const char * buf , int count ) <nl> if (! port ) <nl> return - EPIPE ; <nl>  <nl> - sg_init_one ( sg , buf , count ); <nl> - return __send_to_port ( port , sg , 1 , count , ( void *) buf , false ); <nl> + data = kmemdup ( buf , count , GFP_ATOMIC ); <nl> + if (! data ) <nl> + return - ENOMEM ; <nl> + <nl> + sg_init_one ( sg , data , count ); <nl> + ret = __send_to_port ( port , sg , 1 , count , data , false ); <nl> + kfree ( data ); <nl> + return ret ; <nl> } <nl>  <nl> /*
mmm net / ipv4 / netfilter / nf_nat_snmp_basic_main . c <nl> ppp net / ipv4 / netfilter / nf_nat_snmp_basic_main . c <nl> static void fast_csum ( struct snmp_ctx * ctx , unsigned char offset ) <nl> int snmp_version ( void * context , size_t hdrlen , unsigned char tag , <nl> const void * data , size_t datalen ) <nl> { <nl> + if ( datalen != 1 ) <nl> + return - EINVAL ; <nl> if (*( unsigned char *) data > 1 ) <nl> return - ENOTSUPP ; <nl> return 1 ; <nl> int snmp_helper ( void * context , size_t hdrlen , unsigned char tag , <nl> const void * data , size_t datalen ) <nl> { <nl> struct snmp_ctx * ctx = ( struct snmp_ctx *) context ; <nl> - __be32 * pdata = ( __be32 *) data ; <nl> + __be32 * pdata ; <nl>  <nl> + if ( datalen != 4 ) <nl> + return - EINVAL ; <nl> + pdata = ( __be32 *) data ; <nl> if (* pdata == ctx -> from ) { <nl> pr_debug ("% s : % pI4 to % pI4 \ n ", __func__ , <nl> ( void *)& ctx -> from , ( void *)& ctx -> to );
mmm drivers / dma / k3dma . c <nl> ppp drivers / dma / k3dma . c <nl> static struct dma_chan * k3_of_dma_simple_xlate ( struct of_phandle_args * dma_spec , <nl> struct k3_dma_dev * d = ofdma -> of_dma_data ; <nl> unsigned int request = dma_spec -> args [ 0 ]; <nl>  <nl> - if ( request > d -> dma_requests ) <nl> + if ( request >= d -> dma_requests ) <nl> return NULL ; <nl>  <nl> return dma_get_slave_channel (&( d -> chans [ request ]. vc . chan ));
mmm net / bluetooth / sco . c <nl> ppp net / bluetooth / sco . c <nl> static int sco_sock_getsockopt_old ( struct socket * sock , int optname , char __user <nl> break ; <nl> } <nl>  <nl> + memset (& cinfo , 0 , sizeof ( cinfo )); <nl> cinfo . hci_handle = sco_pi ( sk )-> conn -> hcon -> handle ; <nl> memcpy ( cinfo . dev_class , sco_pi ( sk )-> conn -> hcon -> dev_class , 3 ); <nl> 
mmm drivers / gpu / drm / i915 / i915_gem . c <nl> ppp drivers / gpu / drm / i915 / i915_gem . c <nl> i915_gem_busy_ioctl ( struct drm_device * dev , void * data , <nl> } <nl>  <nl> obj_priv = obj -> driver_private ; <nl> - args -> busy = obj_priv -> active ; <nl> + /* Don ' t count being on the flushing list against the object being <nl> + * done . Otherwise , a buffer left on the flushing list but not getting <nl> + * flushed ( because nobody ' s flushing that domain ) won ' t ever return <nl> + * unbusy and get reused by libdrm ' s bo cache . The other expected <nl> + * consumer of this interface , OpenGL ' s occlusion queries , also specs <nl> + * that the objects get unbusy " eventually " without any interference . <nl> + */ <nl> + args -> busy = obj_priv -> active && obj_priv -> last_rendering_seqno != 0 ; <nl>  <nl> drm_gem_object_unreference ( obj ); <nl> mutex_unlock (& dev -> struct_mutex );
mmm mm / slab . c <nl> ppp mm / slab . c <nl> union freelist_init_state { <nl> unsigned int pos ; <nl> unsigned int * list ; <nl> unsigned int count ; <nl> - unsigned int rand ; <nl> }; <nl> struct rnd_state rnd_state ; <nl> }; <nl> static bool freelist_state_initialize ( union freelist_init_state * state , <nl> } else { <nl> state -> list = cachep -> random_seq ; <nl> state -> count = count ; <nl> - state -> pos = 0 ; <nl> - state -> rand = rand ; <nl> + state -> pos = rand % count ; <nl> ret = true ; <nl> } <nl> return ret ; <nl> static bool freelist_state_initialize ( union freelist_init_state * state , <nl> /* Get the next entry on the list and randomize it using a random shift */ <nl> static freelist_idx_t next_random_slot ( union freelist_init_state * state ) <nl> { <nl> - return ( state -> list [ state -> pos ++] + state -> rand ) % state -> count ; <nl> + if ( state -> pos >= state -> count ) <nl> + state -> pos = 0 ; <nl> + return state -> list [ state -> pos ++]; <nl> } <nl>  <nl> /* Swap two freelist entries */
mmm drivers / mfd / palmas . c <nl> ppp drivers / mfd / palmas . c <nl> static int palmas_i2c_probe ( struct i2c_client * i2c , <nl> ret = - ENOMEM ; <nl> goto err ; <nl> } <nl> + palmas -> i2c_clients [ i ]-> dev . of_node = of_node_get ( node ); <nl> } <nl> palmas -> regmap [ i ] = devm_regmap_init_i2c ( palmas -> i2c_clients [ i ], <nl> & palmas_regmap_config [ i ]);
mmm net / tipc / socket . c <nl> ppp net / tipc / socket . c <nl> static int get_name ( struct socket * sock , struct sockaddr * uaddr , <nl> * socket state flags set <nl> * ------------ --------- <nl> * unconnected no read flags <nl> - * no write flags <nl> + * POLLOUT if port is not congested <nl> * <nl> * connecting POLLIN / POLLRDNORM if ACK / NACK in rx queue <nl> * no write flags <nl> static unsigned int poll ( struct file * file , struct socket * sock , <nl> sock_poll_wait ( file , sk_sleep ( sk ), wait ); <nl>  <nl> switch (( int ) sock -> state ) { <nl> + case SS_UNCONNECTED : <nl> + if (! tipc_sk_port ( sk )-> congested ) <nl> + mask |= POLLOUT ; <nl> + break ; <nl> case SS_READY : <nl> case SS_CONNECTED : <nl> if (! tipc_sk_port ( sk )-> congested )
mmm net / netfilter / nft_meta . c <nl> ppp net / netfilter / nft_meta . c <nl> void nft_meta_get_eval ( const struct nft_expr * expr , <nl> dest -> data [ 0 ] = out -> group ; <nl> break ; <nl> case NFT_META_CGROUP : <nl> - if ( skb -> sk == NULL ) <nl> - break ; <nl> - <nl> + if ( skb -> sk == NULL || ! sk_fullsock ( skb -> sk )) <nl> + goto err ; <nl> dest -> data [ 0 ] = skb -> sk -> sk_classid ; <nl> break ; <nl> default :
mmm drivers / target / target_core_sbc . c <nl> ppp drivers / target / target_core_sbc . c <nl> sbc_parse_cdb ( struct se_cmd * cmd , struct sbc_ops * ops ) <nl> break ; <nl> case VERIFY : <nl> size = 0 ; <nl> + sectors = transport_get_sectors_10 ( cdb ); <nl> + cmd -> t_task_lba = transport_lba_32 ( cdb ); <nl> cmd -> execute_cmd = sbc_emulate_noop ; <nl> - break ; <nl> + goto check_lba ; <nl> case REZERO_UNIT : <nl> case SEEK_6 : <nl> case SEEK_10 :
mmm include / linux / rtnetlink . h <nl> ppp include / linux / rtnetlink . h <nl> extern void __rta_fill ( struct sk_buff * skb , int attrtype , int attrlen , const voi <nl> ( skb )-> len ; }) <nl>  <nl> # define RTA_NEST_CANCEL ( skb , start ) \ <nl> -({ skb_trim ( skb , ( unsigned char *) ( start ) - ( skb )-> data ); \ <nl> +({ if ( start ) \ <nl> + skb_trim ( skb , ( unsigned char *) ( start ) - ( skb )-> data ); \ <nl> - 1 ; }) <nl>  <nl> # define RTA_GET_U32 ( rta ) \
mmm drivers / net / ethernet / qlogic / qlcnic / qlcnic_sriov_common . c <nl> ppp drivers / net / ethernet / qlogic / qlcnic / qlcnic_sriov_common . c <nl> static int qlcnic_sriov_get_vf_acl ( struct qlcnic_adapter * adapter ) <nl> struct qlcnic_cmd_args cmd ; <nl> int ret = 0 ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( cmd )); <nl> ret = qlcnic_sriov_alloc_bc_mbx_args (& cmd , QLCNIC_BC_CMD_GET_ACL ); <nl> if ( ret ) <nl> return ret ; <nl> static int qlcnic_sriov_channel_cfg_cmd ( struct qlcnic_adapter * adapter , u8 cmd_o <nl> struct qlcnic_vf_info * vf = & adapter -> ahw -> sriov -> vf_info [ 0 ]; <nl> int ret ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( cmd )); <nl> if ( qlcnic_sriov_alloc_bc_mbx_args (& cmd , cmd_op )) <nl> return - ENOMEM ; <nl>  <nl> int qlcnic_sriov_cfg_vf_guest_vlan ( struct qlcnic_adapter * adapter , <nl> struct qlcnic_cmd_args cmd ; <nl> int ret ; <nl>  <nl> + memset (& cmd , 0 , sizeof ( cmd )); <nl> if ( vid == 0 ) <nl> return 0 ; <nl> 
mmm drivers / staging / media / lirc / lirc_sir . c <nl> ppp drivers / staging / media / lirc / lirc_sir . c <nl> static int init_hardware ( void ) <nl> spin_lock_irqsave (& hardware_lock , flags ); <nl> /* reset UART */ <nl> # ifdef LIRC_ON_SA1100 <nl> -# ifdef CONFIG_SA1100_BITSY <nl> - if ( machine_is_bitsy ()) { <nl> - pr_info (" Power on IR module \ n "); <nl> - set_bitsy_egpio ( EGPIO_BITSY_IR_ON ); <nl> - } <nl> -# endif <nl> # ifdef CONFIG_SA1100_COLLIE <nl> sa1100_irda_set_power_collie ( 3 ); /* power on */ <nl> # endif <nl> static void drop_hardware ( void ) <nl> Ser2UTCR3 = sr . utcr3 ; <nl>  <nl> Ser2HSCR0 = sr . hscr0 ; <nl> -# ifdef CONFIG_SA1100_BITSY <nl> - if ( machine_is_bitsy ()) <nl> - clr_bitsy_egpio ( EGPIO_BITSY_IR_ON ); <nl> -# endif <nl> # ifdef CONFIG_SA1100_COLLIE <nl> sa1100_irda_set_power_collie ( 0 ); /* power off */ <nl> # endif
mmm drivers / hid / hid - magicmouse . c <nl> ppp drivers / hid / hid - magicmouse . c <nl> static int magicmouse_raw_event ( struct hid_device * hdev , <nl> if ( size < 4 || (( size - 4 ) % 9 ) != 0 ) <nl> return 0 ; <nl> npoints = ( size - 4 ) / 9 ; <nl> + if ( npoints > 15 ) { <nl> + hid_warn ( hdev , " invalid size value (% d ) for TRACKPAD_REPORT_ID \ n ", <nl> + size ); <nl> + return 0 ; <nl> + } <nl> msc -> ntouches = 0 ; <nl> for ( ii = 0 ; ii < npoints ; ii ++) <nl> magicmouse_emit_touch ( msc , ii , data + ii * 9 + 4 ); <nl> static int magicmouse_raw_event ( struct hid_device * hdev , <nl> if ( size < 6 || (( size - 6 ) % 8 ) != 0 ) <nl> return 0 ; <nl> npoints = ( size - 6 ) / 8 ; <nl> + if ( npoints > 15 ) { <nl> + hid_warn ( hdev , " invalid size value (% d ) for MOUSE_REPORT_ID \ n ", <nl> + size ); <nl> + return 0 ; <nl> + } <nl> msc -> ntouches = 0 ; <nl> for ( ii = 0 ; ii < npoints ; ii ++) <nl> magicmouse_emit_touch ( msc , ii , data + ii * 8 + 6 );
mmm drivers / usb / serial / cypress_m8 . c <nl> ppp drivers / usb / serial / cypress_m8 . c <nl> static int cypress_generic_port_probe ( struct usb_serial_port * port ) <nl> struct usb_serial * serial = port -> serial ; <nl> struct cypress_private * priv ; <nl>  <nl> + if (! port -> interrupt_out_urb || ! port -> interrupt_in_urb ) { <nl> + dev_err (& port -> dev , " required endpoint is missing \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> priv = kzalloc ( sizeof ( struct cypress_private ), GFP_KERNEL ); <nl> if (! priv ) <nl> return - ENOMEM ; <nl> static int cypress_open ( struct tty_struct * tty , struct usb_serial_port * port ) <nl> cypress_set_termios ( tty , port , & priv -> tmp_termios ); <nl>  <nl> /* setup the port and start reading from the device */ <nl> - if (! port -> interrupt_in_urb ) { <nl> - dev_err (& port -> dev , "% s - interrupt_in_urb is empty !\ n ", <nl> - __func__ ); <nl> - return - 1 ; <nl> - } <nl> - <nl> usb_fill_int_urb ( port -> interrupt_in_urb , serial -> dev , <nl> usb_rcvintpipe ( serial -> dev , port -> interrupt_in_endpointAddress ), <nl> port -> interrupt_in_urb -> transfer_buffer ,
mmm drivers / staging / rtl8192e / rtllib_softmac . c <nl> ppp drivers / staging / rtl8192e / rtllib_softmac . c <nl> int rtllib_wpa_supplicant_ioctl ( struct rtllib_device * ieee , struct iw_point * p , <nl> } <nl> EXPORT_SYMBOL ( rtllib_wpa_supplicant_ioctl ); <nl>  <nl> - void rtllib_MgntDisconnectIBSS ( struct rtllib_device * rtllib ) <nl> + static void rtllib_MgntDisconnectIBSS ( struct rtllib_device * rtllib ) <nl> { <nl> u8 OpMode ; <nl> u8 i ; <nl> void rtllib_MgntDisconnectIBSS ( struct rtllib_device * rtllib ) <nl>  <nl> } <nl>  <nl> - void rtllib_MlmeDisassociateRequest ( struct rtllib_device * rtllib , u8 * asSta , <nl> + static void rtllib_MlmeDisassociateRequest ( struct rtllib_device * rtllib , u8 * asSta , <nl> u8 asRsn ) <nl> { <nl> u8 i ; <nl> void rtllib_MlmeDisassociateRequest ( struct rtllib_device * rtllib , u8 * asSta , <nl>  <nl> } <nl>  <nl> - void <nl> + static void <nl> rtllib_MgntDisconnectAP ( <nl> struct rtllib_device * rtllib , <nl> u8 asRsn
mmm tools / perf / util / ui / browsers / hists . c <nl> ppp tools / perf / util / ui / browsers / hists . c <nl> int hists__browse ( struct hists * self , const char * helpline , const char * ev_name ) <nl>  <nl> switch ( key ) { <nl> case ' a ': <nl> - if ( browser -> selection -> map == NULL && <nl> + if ( browser -> selection -> map == NULL || <nl> browser -> selection -> map -> dso -> annotate_warned ) <nl> continue ; <nl> goto do_annotate ;
mmm net / core / ethtool . c <nl> ppp net / core / ethtool . c <nl> static int ethtool_get_eeprom ( struct net_device * dev , void __user * useraddr ) <nl> bytes_remaining -= eeprom . len ; <nl> } <nl>  <nl> + eeprom . len = userbuf - ( useraddr + sizeof ( eeprom )); <nl> + eeprom . offset -= eeprom . len ; <nl> + if ( copy_to_user ( useraddr , & eeprom , sizeof ( eeprom ))) <nl> + ret = - EFAULT ; <nl> + <nl> kfree ( data ); <nl> return ret ; <nl> }
mmm drivers / mfd / pm8921 - core . c <nl> ppp drivers / mfd / pm8921 - core . c <nl> static const struct regmap_config ssbi_regmap_config = { <nl> . reg_write = ssbi_reg_write <nl> }; <nl>  <nl> + static const struct of_device_id pm8921_id_table [] = { <nl> + { . compatible = " qcom , pm8058 ", }, <nl> + { . compatible = " qcom , pm8921 ", }, <nl> + { } <nl> +}; <nl> + MODULE_DEVICE_TABLE ( of , pm8921_id_table ); <nl> + <nl> static int pm8921_probe ( struct platform_device * pdev ) <nl> { <nl> struct pm8921 * pmic ; <nl> static struct platform_driver pm8921_driver = { <nl> . driver = { <nl> . name = " pm8921 - core ", <nl> . owner = THIS_MODULE , <nl> + . of_match_table = pm8921_id_table , <nl> }, <nl> }; <nl> 
mmm drivers / tty / serial / 8250 / 8250_of . c <nl> ppp drivers / tty / serial / 8250 / 8250_of . c <nl> static int of_platform_serial_setup ( struct platform_device * ofdev , <nl> port -> line = ret ; <nl>  <nl> port -> irq = irq_of_parse_and_map ( np , 0 ); <nl> + if (! port -> irq ) { <nl> + ret = - EPROBE_DEFER ; <nl> + goto err_unprepare ; <nl> + } <nl>  <nl> info -> rst = devm_reset_control_get_optional_shared (& ofdev -> dev , NULL ); <nl> if ( IS_ERR ( info -> rst )) {
mmm net / netfilter / nfnetlink . c <nl> ppp net / netfilter / nfnetlink . c <nl> static void nfnetlink_rcv_batch ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl> nlh = nlmsg_hdr ( skb ); <nl> err = 0 ; <nl>  <nl> - if ( nlmsg_len ( nlh ) < sizeof ( struct nfgenmsg ) || <nl> - skb -> len < nlh -> nlmsg_len ) { <nl> - err = - EINVAL ; <nl> - goto ack ; <nl> + if ( nlh -> nlmsg_len < NLMSG_HDRLEN || <nl> + skb -> len < nlh -> nlmsg_len || <nl> + nlmsg_len ( nlh ) < sizeof ( struct nfgenmsg )) { <nl> + nfnl_err_reset (& err_list ); <nl> + status |= NFNL_BATCH_FAILURE ; <nl> + goto done ; <nl> } <nl>  <nl> /* Only requests are handled by the kernel */
mmm drivers / infiniband / core / mad . c <nl> ppp drivers / infiniband / core / mad . c <nl> static int validate_mad ( struct ib_mad * mad , u32 qp_num ) <nl> return valid ; <nl> } <nl>  <nl> - static int is_data_mad ( struct ib_mad_agent_private * mad_agent_priv , <nl> + static int is_rmpp_data_mad ( struct ib_mad_agent_private * mad_agent_priv , <nl> struct ib_mad_hdr * mad_hdr ) <nl> { <nl> struct ib_rmpp_mad * rmpp_mad ; <nl> ib_find_send_mad ( struct ib_mad_agent_private * mad_agent_priv , <nl> * been notified that the send has completed <nl> */ <nl> list_for_each_entry ( wr , & mad_agent_priv -> send_list , agent_list ) { <nl> - if ( is_data_mad ( mad_agent_priv , wr -> send_buf . mad ) && <nl> + if ( is_rmpp_data_mad ( mad_agent_priv , wr -> send_buf . mad ) && <nl> wr -> tid == mad -> mad_hdr . tid && <nl> wr -> timeout && <nl> rcv_has_same_class ( wr , wc ) && <nl> find_send_wr ( struct ib_mad_agent_private * mad_agent_priv , <nl>  <nl> list_for_each_entry ( mad_send_wr , & mad_agent_priv -> send_list , <nl> agent_list ) { <nl> - if ( is_data_mad ( mad_agent_priv , mad_send_wr -> send_buf . mad ) && <nl> + if ( is_rmpp_data_mad ( mad_agent_priv , <nl> + mad_send_wr -> send_buf . mad ) && <nl> & mad_send_wr -> send_buf == send_buf ) <nl> return mad_send_wr ; <nl> }
mmm drivers / net / wireless / b43 / main . c <nl> ppp drivers / net / wireless / b43 / main . c <nl> static int b43_wireless_core_init ( struct b43_wldev * dev ) <nl> b43_set_phytxctl_defaults ( dev ); <nl>  <nl> /* Minimum Contention Window */ <nl> - if ( phy -> type == B43_PHYTYPE_B ) { <nl> + if ( phy -> type == B43_PHYTYPE_B ) <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0x1F ); <nl> - } else { <nl> + else <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MINCONT , 0xF ); <nl> - } <nl> /* Maximum Contention Window */ <nl> b43_shm_write16 ( dev , B43_SHM_SCRATCH , B43_SHM_SC_MAXCONT , 0x3FF ); <nl> 
mmm include / asm - i386 / tsc . h <nl> ppp include / asm - i386 / tsc . h <nl> static __always_inline cycles_t get_cycles_sync ( void ) <nl> unsigned long long ret ; <nl> unsigned eax ; <nl>  <nl> + /* <nl> + * Use RDTSCP if possible ; it is guaranteed to be synchronous <nl> + * and doesn ' t cause a VMEXIT on Hypervisors <nl> + */ <nl> + alternative_io ( ASM_NOP3 , ". byte 0x0f , 0x01 , 0xf9 ", X86_FEATURE_RDTSCP , <nl> + "= A " ( ret ), " 0 " ( 0ULL ) : " ecx ", " memory "); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* <nl> * Don ' t do an additional sync on CPUs where we know <nl> * RDTSC is already synchronous :
mmm scripts / mod / modpost . c <nl> ppp scripts / mod / modpost . c <nl> static const struct sectioncheck * section_mismatch ( <nl> int elems = sizeof ( sectioncheck ) / sizeof ( struct sectioncheck ); <nl> const struct sectioncheck * check = & sectioncheck [ 0 ]; <nl>  <nl> + /* <nl> + * The target section could be the SHT_NUL section when we ' re <nl> + * handling relocations to un - resolved symbols , trying to match it <nl> + * doesn ' t make much sense and causes build failures on parisc and <nl> + * mn10300 architectures . <nl> + */ <nl> + if (* tosec == '\ 0 ') <nl> + return NULL ; <nl> + <nl> for ( i = 0 ; i < elems ; i ++) { <nl> if ( match ( fromsec , check -> fromsec )) { <nl> if ( check -> bad_tosec [ 0 ] && match ( tosec , check -> bad_tosec ))
mmm drivers / net / ethernet / qlogic / qed / qed_dcbx . c <nl> ppp drivers / net / ethernet / qlogic / qed / qed_dcbx . c <nl> qed_dcbx_set_pfc_data ( struct qed_hwfn * p_hwfn , <nl> if ( p_params -> pfc . prio [ i ]) <nl> pfc_map |= BIT ( i ); <nl>  <nl> + * pfc &= ~ DCBX_PFC_PRI_EN_BITMAP_MASK ; <nl> * pfc |= ( pfc_map << DCBX_PFC_PRI_EN_BITMAP_SHIFT ); <nl>  <nl> DP_VERBOSE ( p_hwfn , QED_MSG_DCB , " pfc = 0x % x \ n ", * pfc ); <nl> qed_dcbx_set_app_data ( struct qed_hwfn * p_hwfn , <nl>  <nl> for ( i = 0 ; i < DCBX_MAX_APP_PROTOCOL ; i ++) { <nl> entry = & p_app -> app_pri_tbl [ i ]. entry ; <nl> + * entry = 0 ; <nl> if ( ieee ) { <nl> * entry &= ~( DCBX_APP_SF_IEEE_MASK | DCBX_APP_SF_MASK ); <nl> switch ( p_params -> app_entry [ i ]. sf_ieee ) { <nl> int qed_dcbx_get_config_params ( struct qed_hwfn * p_hwfn , <nl> return - ENOMEM ; <nl> } <nl>  <nl> + memset ( dcbx_info , 0 , sizeof (* dcbx_info )); <nl> rc = qed_dcbx_query_params ( p_hwfn , dcbx_info , QED_DCBX_OPERATIONAL_MIB ); <nl> if ( rc ) { <nl> kfree ( dcbx_info ); <nl> static struct qed_dcbx_get * qed_dcbnl_get_dcbx ( struct qed_hwfn * hwfn , <nl> return NULL ; <nl> } <nl>  <nl> + memset ( dcbx_info , 0 , sizeof (* dcbx_info )); <nl> if ( qed_dcbx_query_params ( hwfn , dcbx_info , type )) { <nl> kfree ( dcbx_info ); <nl> return NULL ;
mmm drivers / clk / clk - gpio . c <nl> ppp drivers / clk / clk - gpio . c <nl> static struct clk * of_clk_gpio_delayed_register_get ( <nl> num_parents = of_clk_get_parent_count ( data -> node ); <nl>  <nl> parent_names = kcalloc ( num_parents , sizeof ( char *), GFP_KERNEL ); <nl> - if (! parent_names ) <nl> - return ERR_PTR (- ENOMEM ); <nl> + if (! parent_names ) { <nl> + clk = ERR_PTR (- ENOMEM ); <nl> + goto out ; <nl> + } <nl>  <nl> for ( i = 0 ; i < num_parents ; i ++) <nl> parent_names [ i ] = of_clk_get_parent_name ( data -> node , i );
mmm drivers / mailbox / mtk - cmdq - mailbox . c <nl> ppp drivers / mailbox / mtk - cmdq - mailbox . c <nl> static void __exit cmdq_drv_exit ( void ) <nl>  <nl> subsys_initcall ( cmdq_drv_init ); <nl> module_exit ( cmdq_drv_exit ); <nl> + <nl> + MODULE_LICENSE (" GPL v2 ");
mmm arch / powerpc / mm / hash_utils_64 . c <nl> ppp arch / powerpc / mm / hash_utils_64 . c <nl> void hash__setup_initial_memory_limit ( phys_addr_t first_memblock_base , <nl> * non - virtualized 64 - bit hash MMU systems don ' t have a limitation <nl> * on real mode access . <nl> * <nl> - * We also clamp it to 1G to avoid some funky things <nl> - * such as RTAS bugs etc ... <nl> + * For guests on platforms before POWER9 , we clamp the it limit to 1G <nl> + * to avoid some funky things such as RTAS bugs etc ... <nl> */ <nl> if (! early_cpu_has_feature ( CPU_FTR_HVMODE )) { <nl> - ppc64_rma_size = min_t ( u64 , first_memblock_size , 0x40000000 ); <nl> + ppc64_rma_size = first_memblock_size ; <nl> + if (! early_cpu_has_feature ( CPU_FTR_ARCH_300 )) <nl> + ppc64_rma_size = min_t ( u64 , ppc64_rma_size , 0x40000000 ); <nl>  <nl> /* Finally limit subsequent allocations */ <nl> memblock_set_current_limit ( ppc64_rma_size );
mmm arch / arm64 / kernel / sys . c <nl> ppp arch / arm64 / kernel / sys . c <nl> asmlinkage long sys_rt_sigreturn_wrapper ( void ); <nl> * The sys_call_table array must be 4K aligned to be accessible from <nl> * kernel / entry . S . <nl> */ <nl> - void * sys_call_table [ __NR_syscalls ] __aligned ( 4096 ) = { <nl> + void * const sys_call_table [ __NR_syscalls ] __aligned ( 4096 ) = { <nl> [ 0 ... __NR_syscalls - 1 ] = sys_ni_syscall , <nl> # include < asm / unistd . h > <nl> };
mmm fs / xfs / xfs_vnodeops . c <nl> ppp fs / xfs / xfs_vnodeops . c <nl> xfs_zero_remaining_bytes ( <nl> bp = xfs_buf_get_noaddr ( mp -> m_sb . sb_blocksize , <nl> XFS_IS_REALTIME_INODE ( ip ) ? <nl> mp -> m_rtdev_targp : mp -> m_ddev_targp ); <nl> + if (! bp ) <nl> + return XFS_ERROR ( ENOMEM ); <nl>  <nl> for ( offset = startoff ; offset <= endoff ; offset = lastoffset + 1 ) { <nl> offset_fsb = XFS_B_TO_FSBT ( mp , offset );
mmm lib / rhashtable . c <nl> ppp lib / rhashtable . c <nl> int rhashtable_walk_start_check ( struct rhashtable_iter * iter ) <nl> skip ++; <nl> if ( list == iter -> list ) { <nl> iter -> p = p ; <nl> - skip = skip ; <nl> + iter -> skip = skip ; <nl> goto found ; <nl> } <nl> }
mmm drivers / net / ethernet / sun / sunvnet . c <nl> ppp drivers / net / ethernet / sun / sunvnet . c <nl> vnet_select_queue ( struct net_device * dev , struct sk_buff * skb , <nl> struct vnet * vp = netdev_priv ( dev ); <nl> struct vnet_port * port = __tx_port_find ( vp , skb ); <nl>  <nl> + if ( port == NULL ) <nl> + return 0 ; <nl> return port -> q_index ; <nl> } <nl> 
mmm drivers / char / hw_random / omap - rng . c <nl> ppp drivers / char / hw_random / omap - rng . c <nl> static struct hwrng omap_rng_ops = { <nl>  <nl> static int __devinit omap_rng_probe ( struct platform_device * pdev ) <nl> { <nl> - struct resource * res , * mem ; <nl> + struct resource * res ; <nl> int ret ; <nl>  <nl> /* <nl> static int __devinit omap_rng_probe ( struct platform_device * pdev ) <nl> if (! res ) <nl> return - ENOENT ; <nl>  <nl> - mem = request_mem_region ( res -> start , resource_size ( res ), <nl> - pdev -> name ); <nl> - if ( mem == NULL ) { <nl> + if (! request_mem_region ( res -> start , resource_size ( res ), pdev -> name )) { <nl> ret = - EBUSY ; <nl> goto err_region ; <nl> } <nl>  <nl> - dev_set_drvdata (& pdev -> dev , mem ); <nl> + dev_set_drvdata (& pdev -> dev , res ); <nl> rng_base = ioremap ( res -> start , resource_size ( res )); <nl> if (! rng_base ) { <nl> ret = - ENOMEM ; <nl> static int __devinit omap_rng_probe ( struct platform_device * pdev ) <nl> iounmap ( rng_base ); <nl> rng_base = NULL ; <nl> err_ioremap : <nl> - release_resource ( mem ); <nl> + release_mem_region ( res -> start , resource_size ( res )); <nl> err_region : <nl> if ( cpu_is_omap24xx ()) { <nl> clk_disable ( rng_ick ); <nl> static int __devinit omap_rng_probe ( struct platform_device * pdev ) <nl>  <nl> static int __exit omap_rng_remove ( struct platform_device * pdev ) <nl> { <nl> - struct resource * mem = dev_get_drvdata (& pdev -> dev ); <nl> + struct resource * res = dev_get_drvdata (& pdev -> dev ); <nl>  <nl> hwrng_unregister (& omap_rng_ops ); <nl>  <nl> static int __exit omap_rng_remove ( struct platform_device * pdev ) <nl> clk_put ( rng_ick ); <nl> } <nl>  <nl> - release_resource ( mem ); <nl> + release_mem_region ( res -> start , resource_size ( res )); <nl> rng_base = NULL ; <nl>  <nl> return 0 ;
mmm lib / kobject_uevent . c <nl> ppp lib / kobject_uevent . c <nl> int kobject_uevent_env ( struct kobject * kobj , enum kobject_action action , <nl> /* keys passed in from the caller */ <nl> if ( envp_ext ) { <nl> for ( i = 0 ; envp_ext [ i ]; i ++) { <nl> - retval = add_uevent_var ( env , envp_ext [ i ]); <nl> + retval = add_uevent_var ( env , "% s ", envp_ext [ i ]); <nl> if ( retval ) <nl> goto exit ; <nl> }
mmm drivers / net / ethernet / intel / i40e / i40e_txrx . c <nl> ppp drivers / net / ethernet / intel / i40e / i40e_txrx . c <nl> int i40e_napi_poll ( struct napi_struct * napi , int budget ) <nl> ring -> arm_wb = false ; <nl> } <nl>  <nl> + /* Handle case where we are called by netpoll with a budget of 0 */ <nl> + if ( budget <= 0 ) <nl> + goto tx_only ; <nl> + <nl> /* We attempt to distribute budget to each Rx queue fairly , but don ' t <nl> * allow the budget to go below 1 because that would exit polling early . <nl> */ <nl> int i40e_napi_poll ( struct napi_struct * napi , int budget ) <nl>  <nl> /* If work not completed , return budget and polling will return */ <nl> if (! clean_complete ) { <nl> + tx_only : <nl> if ( arm_wb ) <nl> i40e_force_wb ( vsi , q_vector ); <nl> return budget ;mmm drivers / net / ethernet / intel / i40evf / i40e_txrx . c <nl> ppp drivers / net / ethernet / intel / i40evf / i40e_txrx . c <nl> int i40e_napi_poll ( struct napi_struct * napi , int budget ) <nl> ring -> arm_wb = false ; <nl> } <nl>  <nl> + /* Handle case where we are called by netpoll with a budget of 0 */ <nl> + if ( budget <= 0 ) <nl> + goto tx_only ; <nl> + <nl> /* We attempt to distribute budget to each Rx queue fairly , but don ' t <nl> * allow the budget to go below 1 because that would exit polling early . <nl> */ <nl> int i40e_napi_poll ( struct napi_struct * napi , int budget ) <nl>  <nl> /* If work not completed , return budget and polling will return */ <nl> if (! clean_complete ) { <nl> + tx_only : <nl> if ( arm_wb ) <nl> i40e_force_wb ( vsi , q_vector ); <nl> return budget ; <nl> int i40evf_napi_poll ( struct napi_struct * napi , int budget ) <nl> ring -> arm_wb = false ; <nl> } <nl>  <nl> + /* Handle case where we are called by netpoll with a budget of 0 */ <nl> + if ( budget <= 0 ) <nl> + goto tx_only ; <nl> + <nl> /* We attempt to distribute budget to each Rx queue fairly , but don ' t <nl> * allow the budget to go below 1 because that would exit polling early . <nl> */ <nl> int i40evf_napi_poll ( struct napi_struct * napi , int budget ) <nl>  <nl> /* If work not completed , return budget and polling will return */ <nl> if (! clean_complete ) { <nl> + tx_only : <nl> if ( arm_wb ) <nl> i40evf_force_wb ( vsi , q_vector ); <nl> return budget ;
mmm kernel / power / snapshot . c <nl> ppp kernel / power / snapshot . c <nl> static struct page * saveable_highmem_page ( struct zone * zone , unsigned long pfn ) <nl> PageReserved ( page )) <nl> return NULL ; <nl>  <nl> + if ( page_is_guard ( page )) <nl> + return NULL ; <nl> + <nl> return page ; <nl> } <nl>  <nl> static struct page * saveable_page ( struct zone * zone , unsigned long pfn ) <nl> && (! kernel_page_present ( page ) || pfn_is_nosave ( pfn ))) <nl> return NULL ; <nl>  <nl> + if ( page_is_guard ( page )) <nl> + return NULL ; <nl> + <nl> return page ; <nl> } <nl> 
mmm arch / x86 / kvm / lapic . c <nl> ppp arch / x86 / kvm / lapic . c <nl> int kvm_x2apic_msr_write ( struct kvm_vcpu * vcpu , u32 msr , u64 data ) <nl> if (! irqchip_in_kernel ( vcpu -> kvm ) || ! apic_x2apic_mode ( apic )) <nl> return 1 ; <nl>  <nl> + if ( reg == APIC_ICR2 ) <nl> + return 1 ; <nl> + <nl> /* if this is ICR write vector before command */ <nl> if ( msr == 0x830 ) <nl> apic_reg_write ( apic , APIC_ICR2 , ( u32 )( data >> 32 )); <nl> int kvm_x2apic_msr_read ( struct kvm_vcpu * vcpu , u32 msr , u64 * data ) <nl> if (! irqchip_in_kernel ( vcpu -> kvm ) || ! apic_x2apic_mode ( apic )) <nl> return 1 ; <nl>  <nl> + if ( reg == APIC_DFR || reg == APIC_ICR2 ) { <nl> + apic_debug (" KVM_APIC_READ : read x2apic reserved register % x \ n ", <nl> + reg ); <nl> + return 1 ; <nl> + } <nl> + <nl> if ( apic_reg_read ( apic , reg , 4 , & low )) <nl> return 1 ; <nl> if ( msr == 0x830 )
mmm arch / blackfin / mach - bf609 / include / mach / pm . h <nl> ppp arch / blackfin / mach - bf609 / include / mach / pm . h <nl> # define __MACH_BF609_PM_H__ <nl>  <nl> # include < linux / suspend . h > <nl> +# include < linux / platform_device . h > <nl>  <nl> extern int bfin609_pm_enter ( suspend_state_t state ); <nl> extern int bf609_pm_prepare ( void ); <nl> void bf609_hibernate ( void ); <nl> void bfin_sec_raise_irq ( unsigned int sid ); <nl> void coreb_enable ( void ); <nl>  <nl> - int bf609_nor_flash_init ( void ); <nl> - void bf609_nor_flash_exit ( void ); <nl> + int bf609_nor_flash_init ( struct platform_device * pdev ); <nl> + void bf609_nor_flash_exit ( struct platform_device * pdev ); <nl> # endifmmm arch / blackfin / mach - bf609 / boards / ezkit . c <nl> ppp arch / blackfin / mach - bf609 / boards / ezkit . c <nl> # define __MACH_BF609_PM_H__ <nl>  <nl> # include < linux / suspend . h > <nl> +# include < linux / platform_device . h > <nl>  <nl> extern int bfin609_pm_enter ( suspend_state_t state ); <nl> extern int bf609_pm_prepare ( void ); <nl> void bf609_hibernate ( void ); <nl> void bfin_sec_raise_irq ( unsigned int sid ); <nl> void coreb_enable ( void ); <nl>  <nl> - int bf609_nor_flash_init ( void ); <nl> - void bf609_nor_flash_exit ( void ); <nl> + int bf609_nor_flash_init ( struct platform_device * pdev ); <nl> + void bf609_nor_flash_exit ( struct platform_device * pdev ); <nl> # endif <nl> int bf609_nor_flash_init ( struct platform_device * pdev ) <nl> { <nl> # define CONFIG_SMC_GCTL_VAL 0x00000010 <nl>  <nl> - if (! devm_pinctrl_get_select_default (& pdev -> dev )) <nl> - return - EBUSY ; <nl> bfin_write32 ( SMC_GCTL , CONFIG_SMC_GCTL_VAL ); <nl> bfin_write32 ( SMC_B0CTL , 0x01002011 ); <nl> bfin_write32 ( SMC_B0TIM , 0x08170977 ); <nl> int bf609_nor_flash_init ( struct platform_device * pdev ) <nl>  <nl> void bf609_nor_flash_exit ( struct platform_device * pdev ) <nl> { <nl> - devm_pinctrl_put ( pdev -> dev . pins -> p ); <nl> bfin_write32 ( SMC_GCTL , 0 ); <nl> } <nl> mmm arch / blackfin / mach - bf609 / pm . c <nl> ppp arch / blackfin / mach - bf609 / pm . c <nl> # define __MACH_BF609_PM_H__ <nl>  <nl> # include < linux / suspend . h > <nl> +# include < linux / platform_device . h > <nl>  <nl> extern int bfin609_pm_enter ( suspend_state_t state ); <nl> extern int bf609_pm_prepare ( void ); <nl> void bf609_hibernate ( void ); <nl> void bfin_sec_raise_irq ( unsigned int sid ); <nl> void coreb_enable ( void ); <nl>  <nl> - int bf609_nor_flash_init ( void ); <nl> - void bf609_nor_flash_exit ( void ); <nl> + int bf609_nor_flash_init ( struct platform_device * pdev ); <nl> + void bf609_nor_flash_exit ( struct platform_device * pdev ); <nl> # endif <nl> int bf609_nor_flash_init ( struct platform_device * pdev ) <nl> { <nl> # define CONFIG_SMC_GCTL_VAL 0x00000010 <nl>  <nl> - if (! devm_pinctrl_get_select_default (& pdev -> dev )) <nl> - return - EBUSY ; <nl> bfin_write32 ( SMC_GCTL , CONFIG_SMC_GCTL_VAL ); <nl> bfin_write32 ( SMC_B0CTL , 0x01002011 ); <nl> bfin_write32 ( SMC_B0TIM , 0x08170977 ); <nl> int bf609_nor_flash_init ( struct platform_device * pdev ) <nl>  <nl> void bf609_nor_flash_exit ( struct platform_device * pdev ) <nl> { <nl> - devm_pinctrl_put ( pdev -> dev . pins -> p ); <nl> bfin_write32 ( SMC_GCTL , 0 ); <nl> } <nl>  <nl> static struct bfin_cpu_pm_fns bf609_cpu_pm = { <nl> # if defined ( CONFIG_MTD_PHYSMAP ) || defined ( CONFIG_MTD_PHYSMAP_MODULE ) <nl> static int smc_pm_syscore_suspend ( void ) <nl> { <nl> - bf609_nor_flash_exit (); <nl> + bf609_nor_flash_exit ( NULL ); <nl> return 0 ; <nl> } <nl>  <nl> static void smc_pm_syscore_resume ( void ) <nl> { <nl> - bf609_nor_flash_init (); <nl> + bf609_nor_flash_init ( NULL ); <nl> } <nl>  <nl> static struct syscore_ops smc_pm_syscore_ops = {
mmm arch / metag / kernel / perf / perf_event . c <nl> ppp arch / metag / kernel / perf / perf_event . c <nl> int metag_pmu_event_set_period ( struct perf_event * event , <nl> if ( left > ( s64 ) metag_pmu -> max_period ) <nl> left = metag_pmu -> max_period ; <nl>  <nl> - if ( metag_pmu -> write ) <nl> - metag_pmu -> write ( idx , ( u64 )(- left ) & MAX_PERIOD ); <nl> + if ( metag_pmu -> write ) { <nl> + local64_set (& hwc -> prev_count , -( s32 ) left ); <nl> + metag_pmu -> write ( idx , - left & MAX_PERIOD ); <nl> + } <nl>  <nl> perf_event_update_userpage ( event ); <nl>  <nl> static void metag_pmu_enable_counter ( struct hw_perf_event * event , int idx ) <nl> * set to a specific value that needs preserving . <nl> */ <nl> tmp |= metag_in32 ( PERF_COUNT ( idx )) & 0x00ffffff ; <nl> + else <nl> + /* <nl> + * Older cores reset the counter on write , so prev_count needs <nl> + * resetting too so we can calculate a correct delta . <nl> + */ <nl> + local64_set (& event -> prev_count , 0 ); <nl>  <nl> metag_out32 ( tmp , PERF_COUNT ( idx )); <nl> unlock :
mmm net / bluetooth / hci_core . c <nl> ppp net / bluetooth / hci_core . c <nl> static int hci_dev_do_open ( struct hci_dev * hdev ) <nl> * be able to determine if there is a public address <nl> * or not . <nl> * <nl> + * In case of user channel usage , it is not important <nl> + * if a public address or static random address is <nl> + * available . <nl> + * <nl> * This check is only valid for BR / EDR controllers <nl> * since AMP controllers do not have an address . <nl> */ <nl> - if ( hdev -> dev_type == HCI_BREDR && <nl> + if (! test_bit ( HCI_USER_CHANNEL , & hdev -> dev_flags ) && <nl> + hdev -> dev_type == HCI_BREDR && <nl> ! bacmp (& hdev -> bdaddr , BDADDR_ANY ) && <nl> ! bacmp (& hdev -> static_addr , BDADDR_ANY )) { <nl> ret = - EADDRNOTAVAIL ;
mmm kernel / cgroup . c <nl> ppp kernel / cgroup . c <nl> static void cgroup_enable_task_cg_lists ( void ) <nl> } <nl>  <nl> void cgroup_iter_start ( struct cgroup * cgrp , struct cgroup_iter * it ) <nl> + __acquires ( css_set_lock ) <nl> { <nl> /* <nl> * The first time anyone tries to iterate across a cgroup , <nl> struct task_struct * cgroup_iter_next ( struct cgroup * cgrp , <nl> } <nl>  <nl> void cgroup_iter_end ( struct cgroup * cgrp , struct cgroup_iter * it ) <nl> + __releases ( css_set_lock ) <nl> { <nl> read_unlock (& css_set_lock ); <nl> }
mmm drivers / media / video / cx23885 / cx23885 - cards . c <nl> ppp drivers / media / video / cx23885 / cx23885 - cards . c <nl> int cx23885_tuner_callback ( void * priv , int component , int command , int arg ) <nl> struct cx23885_dev * dev = port -> dev ; <nl> u32 bitmask = 0 ; <nl>  <nl> - if ( command == XC2028_RESET_CLK ) <nl> + if (( command == XC2028_RESET_CLK ) || ( command == XC2028_I2C_FLUSH )) <nl> return 0 ; <nl>  <nl> if ( command != 0 ) {
mmm sound / soc / generic / simple - card . c <nl> ppp sound / soc / generic / simple - card . c <nl> static int asoc_simple_card_probe ( struct platform_device * pdev ) <nl> snd_soc_card_set_drvdata ( card , priv ); <nl>  <nl> ret = devm_snd_soc_register_card ( dev , card ); <nl> - if ( ret >= 0 ) <nl> - return ret ; <nl> + if ( ret < 0 ) <nl> + goto err ; <nl> + <nl> + return 0 ; <nl> err : <nl> asoc_simple_card_clean_reference ( card ); <nl> 
mmm drivers / net / can / usb / ems_usb . c <nl> ppp drivers / net / can / usb / ems_usb . c <nl> static netdev_tx_t ems_usb_start_xmit ( struct sk_buff * skb , struct net_device * ne <nl>  <nl> usb_unanchor_urb ( urb ); <nl> usb_free_coherent ( dev -> udev , size , buf , urb -> transfer_dma ); <nl> - dev_kfree_skb ( skb ); <nl>  <nl> atomic_dec (& dev -> active_tx_urbs ); <nl> 
mmm sound / soc / img / img - parallel - out . c <nl> ppp sound / soc / img / img - parallel - out . c <nl> static int img_prl_out_set_fmt ( struct snd_soc_dai * dai , unsigned int fmt ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + pm_runtime_get_sync ( prl -> dev ); <nl> reg = img_prl_out_readl ( prl , IMG_PRL_OUT_CTL ); <nl> reg = ( reg & ~ IMG_PRL_OUT_CTL_EDGE_MASK ) | control_set ; <nl> img_prl_out_writel ( prl , reg , IMG_PRL_OUT_CTL ); <nl> + pm_runtime_put ( prl -> dev ); <nl>  <nl> return 0 ; <nl> }
mmm drivers / net / forcedeth . c <nl> ppp drivers / net / forcedeth . c <nl> static struct ethtool_ops ops = { <nl> . get_regs_len = nv_get_regs_len , <nl> . get_regs = nv_get_regs , <nl> . nway_reset = nv_nway_reset , <nl> + . get_perm_addr = ethtool_op_get_perm_addr , <nl> }; <nl>  <nl> static int nv_open ( struct net_device * dev ) <nl> static int __devinit nv_probe ( struct pci_dev * pci_dev , const struct pci_device_i <nl> dev -> dev_addr [ 3 ] = ( np -> orig_mac [ 0 ] >> 16 ) & 0xff ; <nl> dev -> dev_addr [ 4 ] = ( np -> orig_mac [ 0 ] >> 8 ) & 0xff ; <nl> dev -> dev_addr [ 5 ] = ( np -> orig_mac [ 0 ] >> 0 ) & 0xff ; <nl> + memcpy ( dev -> perm_addr , dev -> dev_addr , dev -> addr_len ); <nl>  <nl> - if (! is_valid_ether_addr ( dev -> dev_addr )) { <nl> + if (! is_valid_ether_addr ( dev -> perm_addr )) { <nl> /* <nl> * Bad mac address . At least one bios sets the mac address <nl> * to 01 : 23 : 45 : 67 : 89 : ab
mmm arch / x86 / xen / mmu . c <nl> ppp arch / x86 / xen / mmu . c <nl> xmaddr_t arbitrary_virt_to_machine ( void * vaddr ) <nl> } <nl> EXPORT_SYMBOL_GPL ( arbitrary_virt_to_machine ); <nl>  <nl> - void xen_flush_tlb_all ( void ) <nl> + static void xen_flush_tlb_all ( void ) <nl> { <nl> struct mmuext_op * op ; <nl> struct multicall_space mcs ;
mmm kernel / auditsc . c <nl> ppp kernel / auditsc . c <nl> void __audit_syscall_entry ( int major , unsigned long a1 , unsigned long a2 , <nl> context -> argv [ 2 ] = a3 ; <nl> context -> argv [ 3 ] = a4 ; <nl> context -> serial = 0 ; <nl> - context -> ctime = current_kernel_time64 (); <nl> context -> in_syscall = 1 ; <nl> context -> current_state = state ; <nl> context -> ppid = 0 ; <nl> + ktime_get_coarse_ts64 (& context -> ctime ); <nl> } <nl>  <nl> /**mmm kernel / audit . c <nl> ppp kernel / audit . c <nl> void __audit_syscall_entry ( int major , unsigned long a1 , unsigned long a2 , <nl> context -> argv [ 2 ] = a3 ; <nl> context -> argv [ 3 ] = a4 ; <nl> context -> serial = 0 ; <nl> - context -> ctime = current_kernel_time64 (); <nl> context -> in_syscall = 1 ; <nl> context -> current_state = state ; <nl> context -> ppid = 0 ; <nl> + ktime_get_coarse_ts64 (& context -> ctime ); <nl> } <nl>  <nl> /** <nl> static inline void audit_get_stamp ( struct audit_context * ctx , <nl> struct timespec64 * t , unsigned int * serial ) <nl> { <nl> if (! ctx || ! auditsc_get_stamp ( ctx , t , serial )) { <nl> - * t = current_kernel_time64 (); <nl> + ktime_get_coarse_ts64 ( t ); <nl> * serial = audit_serial (); <nl> } <nl> }
mmm drivers / nvme / target / admin - cmd . c <nl> ppp drivers / nvme / target / admin - cmd . c <nl> static void nvmet_execute_identify_ctrl ( struct nvmet_req * req ) <nl> id -> vid = 0 ; <nl> id -> ssvid = 0 ; <nl>  <nl> + memset ( id -> sn , ' ', sizeof ( id -> sn )); <nl> bin2hex ( id -> sn , & ctrl -> subsys -> serial , <nl> min ( sizeof ( ctrl -> subsys -> serial ), sizeof ( id -> sn ) / 2 )); <nl> memcpy_and_pad ( id -> mn , sizeof ( id -> mn ), model , sizeof ( model ) - 1 , ' ');
mmm arch / arm / plat - mxc / devices . c <nl> ppp arch / arm / plat - mxc / devices . c <nl> struct platform_device * __init imx_add_platform_device_dmamask ( <nl> ret = platform_device_add ( pdev ); <nl> if ( ret ) { <nl> err : <nl> + if ( dmamask ) <nl> + kfree ( pdev -> dev . dma_mask ); <nl> platform_device_put ( pdev ); <nl> return ERR_PTR ( ret ); <nl> }
mmm net / llc / af_llc . c <nl> ppp net / llc / af_llc . c <nl> static int llc_ui_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> int target ; /* Read at least this many bytes */ <nl> long timeo ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl> copied = - ENOTCONN ; <nl> if ( unlikely ( sk -> sk_type == SOCK_STREAM && sk -> sk_state == TCP_LISTEN ))
mmm kernel / pid . c <nl> ppp kernel / pid . c <nl> static int alloc_pidmap ( struct pid_namespace * pid_ns ) <nl> return - 1 ; <nl> } <nl>  <nl> - int next_pidmap ( struct pid_namespace * pid_ns , int last ) <nl> + int next_pidmap ( struct pid_namespace * pid_ns , unsigned int last ) <nl> { <nl> int offset ; <nl> struct pidmap * map , * end ; <nl>  <nl> + if ( last >= PID_MAX_LIMIT ) <nl> + return - 1 ; <nl> + <nl> offset = ( last + 1 ) & BITS_PER_PAGE_MASK ; <nl> map = & pid_ns -> pidmap [( last + 1 )/ BITS_PER_PAGE ]; <nl> end = & pid_ns -> pidmap [ PIDMAP_ENTRIES ];mmm include / linux / pid . h <nl> ppp include / linux / pid . h <nl> static int alloc_pidmap ( struct pid_namespace * pid_ns ) <nl> return - 1 ; <nl> } <nl>  <nl> - int next_pidmap ( struct pid_namespace * pid_ns , int last ) <nl> + int next_pidmap ( struct pid_namespace * pid_ns , unsigned int last ) <nl> { <nl> int offset ; <nl> struct pidmap * map , * end ; <nl>  <nl> + if ( last >= PID_MAX_LIMIT ) <nl> + return - 1 ; <nl> + <nl> offset = ( last + 1 ) & BITS_PER_PAGE_MASK ; <nl> map = & pid_ns -> pidmap [( last + 1 )/ BITS_PER_PAGE ]; <nl> end = & pid_ns -> pidmap [ PIDMAP_ENTRIES ]; <nl> extern struct pid * find_vpid ( int nr ); <nl> */ <nl> extern struct pid * find_get_pid ( int nr ); <nl> extern struct pid * find_ge_pid ( int nr , struct pid_namespace *); <nl> - int next_pidmap ( struct pid_namespace * pid_ns , int last ); <nl> + int next_pidmap ( struct pid_namespace * pid_ns , unsigned int last ); <nl>  <nl> extern struct pid * alloc_pid ( struct pid_namespace * ns ); <nl> extern void free_pid ( struct pid * pid );
mmm kernel / perf_event . c <nl> ppp kernel / perf_event . c <nl> static int perf_mmap ( struct file * file , struct vm_area_struct * vma ) <nl> long user_extra , extra ; <nl> int ret = 0 ; <nl>  <nl> + /* <nl> + * Don ' t allow mmap () of inherited per - task counters . This would <nl> + * create a performance issue due to all children writing to the <nl> + * same buffer . <nl> + */ <nl> + if ( event -> cpu == - 1 && event -> attr . inherit ) <nl> + return - EINVAL ; <nl> + <nl> if (!( vma -> vm_flags & VM_SHARED )) <nl> return - EINVAL ; <nl> 
mmm drivers / staging / typec / tcpm . c <nl> ppp drivers / staging / typec / tcpm . c <nl> static void _tcpm_cc_change ( struct tcpm_port * port , enum typec_cc_status cc1 , <nl> break ; <nl>  <nl> case SRC_TRY : <nl> - tcpm_set_state ( port , SRC_TRY_DEBOUNCE , 0 ); <nl> + if ( tcpm_port_is_source ( port )) <nl> + tcpm_set_state ( port , SRC_TRY_DEBOUNCE , 0 ); <nl> break ; <nl> case SRC_TRY_DEBOUNCE : <nl> tcpm_set_state ( port , SRC_TRY , 0 );
mmm drivers / gpu / drm / gma500 / mdfld_dsi_pkg_sender . c <nl> ppp drivers / gpu / drm / gma500 / mdfld_dsi_pkg_sender . c <nl> int mdfld_dsi_send_gen_short ( struct mdfld_dsi_pkg_sender * sender , u8 param0 , <nl> unsigned long flags ; <nl> u8 data_type ; <nl>  <nl> - if (! sender || param_num < 0 || param_num > 2 ) { <nl> + if (! sender || param_num > 2 ) { <nl> DRM_ERROR (" Invalid parameter \ n "); <nl> return - EINVAL ; <nl> }
mmm drivers / staging / lustre / lustre / llite / file . c <nl> ppp drivers / staging / lustre / lustre / llite / file . c <nl> int ll_fid2path ( struct inode * inode , void __user * arg ) <nl> if ( get_user ( pathlen , & gfin -> gf_pathlen )) <nl> return - EFAULT ; <nl>  <nl> + if ( pathlen > PATH_MAX ) <nl> + return - EINVAL ; <nl> + <nl> outsize = sizeof (* gfout ) + pathlen ; <nl>  <nl> OBD_ALLOC ( gfout , outsize );
mmm net / mac80211 / iface . c <nl> ppp net / mac80211 / iface . c <nl> static void ieee80211_iface_work ( struct work_struct * work ) <nl> if ( sta ) { <nl> u16 last_seq ; <nl>  <nl> - last_seq = le16_to_cpu ( <nl> - sta -> last_seq_ctrl [ rx_agg -> tid ]); <nl> + last_seq = IEEE80211_SEQ_TO_SN ( le16_to_cpu ( <nl> + sta -> last_seq_ctrl [ rx_agg -> tid ])); <nl>  <nl> __ieee80211_start_rx_ba_session ( sta , <nl> 0 , 0 ,
mmm arch / x86 / kvm / svm / nested . c <nl> ppp arch / x86 / kvm / svm / nested . c <nl> void recalc_intercepts ( struct vcpu_svm * svm ) <nl> /* If SMI is not intercepted , ignore guest SMI intercept as well */ <nl> if (! intercept_smi ) <nl> vmcb_clr_intercept ( c , INTERCEPT_SMI ); <nl> + <nl> + vmcb_set_intercept ( c , INTERCEPT_VMLOAD ); <nl> + vmcb_set_intercept ( c , INTERCEPT_VMSAVE ); <nl> } <nl>  <nl> static void copy_vmcb_control_area ( struct vmcb_control_area * dst ,
mmm net / bridge / br_multicast . c <nl> ppp net / bridge / br_multicast . c <nl> static void br_multicast_del_pg ( struct net_bridge * br , <nl> del_timer (& p -> timer ); <nl> call_rcu_bh (& p -> rcu , br_multicast_free_pg ); <nl>  <nl> - if (! mp -> ports && ! mp -> mglist && <nl> + if (! mp -> ports && ! mp -> mglist && mp -> timer_armed && <nl> netif_running ( br -> dev )) <nl> mod_timer (& mp -> timer , jiffies ); <nl> mmm net / bridge / br_mdb . c <nl> ppp net / bridge / br_mdb . c <nl> static void br_multicast_del_pg ( struct net_bridge * br , <nl> del_timer (& p -> timer ); <nl> call_rcu_bh (& p -> rcu , br_multicast_free_pg ); <nl>  <nl> - if (! mp -> ports && ! mp -> mglist && <nl> + if (! mp -> ports && ! mp -> mglist && mp -> timer_armed && <nl> netif_running ( br -> dev )) <nl> mod_timer (& mp -> timer , jiffies ); <nl>  <nl> static int __br_mdb_del ( struct net_bridge * br , struct br_mdb_entry * entry ) <nl> call_rcu_bh (& p -> rcu , br_multicast_free_pg ); <nl> err = 0 ; <nl>  <nl> - if (! mp -> ports && ! mp -> mglist && <nl> + if (! mp -> ports && ! mp -> mglist && mp -> timer_armed && <nl> netif_running ( br -> dev )) <nl> mod_timer (& mp -> timer , jiffies ); <nl> break ;
mmm drivers / net / ethernet / mellanox / mlx5 / core / health . c <nl> ppp drivers / net / ethernet / mellanox / mlx5 / core / health . c <nl> mlx5_fw_fatal_reporter_dump ( struct devlink_health_reporter * reporter , <nl> return - ENOMEM ; <nl> err = mlx5_crdump_collect ( dev , cr_data ); <nl> if ( err ) <nl> - return err ; <nl> + goto free_data ; <nl>  <nl> if ( priv_ctx ) { <nl> struct mlx5_fw_reporter_ctx * fw_reporter_ctx = priv_ctx ;
mmm drivers / net / can / usb / mcba_usb . c <nl> ppp drivers / net / can / usb / mcba_usb . c <nl> static void mcba_usb_read_bulk_callback ( struct urb * urb ) <nl>  <nl> case - ENOENT : <nl> case - EPIPE : <nl> + case - EPROTO : <nl> case - ESHUTDOWN : <nl> return ; <nl> 
mmm net / x25 / af_x25 . c <nl> ppp net / x25 / af_x25 . c <nl> int x25_rx_call_request ( struct sk_buff * skb , struct x25_neigh * nb , <nl> goto out_clear_request ; <nl> skb_pull ( skb , len ); <nl>  <nl> + /* <nl> + * Ensure that the amount of call user data is valid . <nl> + */ <nl> + if ( skb -> len > X25_MAX_CUD_LEN ) <nl> + goto out_clear_request ; <nl> + <nl> /* <nl> * Find a listener for the particular address / cud pair . <nl> */mmm net / x25 / x25_in . c <nl> ppp net / x25 / x25_in . c <nl> int x25_rx_call_request ( struct sk_buff * skb , struct x25_neigh * nb , <nl> goto out_clear_request ; <nl> skb_pull ( skb , len ); <nl>  <nl> + /* <nl> + * Ensure that the amount of call user data is valid . <nl> + */ <nl> + if ( skb -> len > X25_MAX_CUD_LEN ) <nl> + goto out_clear_request ; <nl> + <nl> /* <nl> * Find a listener for the particular address / cud pair . <nl> */ <nl> static int x25_state1_machine ( struct sock * sk , struct sk_buff * skb , int frametyp <nl> * Copy any Call User Data . <nl> */ <nl> if ( skb -> len > 0 ) { <nl> + if ( skb -> len > X25_MAX_CUD_LEN ) <nl> + goto out_clear ; <nl> + <nl> skb_copy_from_linear_data ( skb , <nl> x25 -> calluserdata . cuddata , <nl> skb -> len );
mmm net / netrom / af_netrom . c <nl> ppp net / netrom / af_netrom . c <nl> static int nr_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> } <nl>  <nl> if ( sax != NULL ) { <nl> - memset ( sax , 0 , sizeof ( sax )); <nl> + memset ( sax , 0 , sizeof (* sax )); <nl> sax -> sax25_family = AF_NETROM ; <nl> skb_copy_from_linear_data_offset ( skb , 7 , sax -> sax25_call . ax25_call , <nl> AX25_ADDR_LEN );
mmm drivers / video / fbdev / vt8500lcdfb . c <nl> ppp drivers / video / fbdev / vt8500lcdfb . c <nl> static int vt8500lcd_remove ( struct platform_device * pdev ) <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> release_mem_region ( res -> start , resource_size ( res )); <nl>  <nl> - kfree ( fbi ); <nl> - <nl> return 0 ; <nl> } <nl> 
mmm drivers / staging / frontier / alphatrack . c <nl> ppp drivers / staging / frontier / alphatrack . c <nl> static void usb_alphatrack_disconnect ( struct usb_interface * intf ) <nl> mutex_unlock (& dev -> mtx ); <nl> usb_alphatrack_delete ( dev ); <nl> } else { <nl> + atomic_set (& dev -> writes_pending , 0 ); <nl> dev -> intf = NULL ; <nl> mutex_unlock (& dev -> mtx ); <nl> } <nl>  <nl> - atomic_set (& dev -> writes_pending , 0 ); <nl> mutex_unlock (& disconnect_mutex ); <nl>  <nl> dev_info (& intf -> dev , " Alphatrack Surface #% d now disconnected \ n ",
mmm mm / oom_kill . c <nl> ppp mm / oom_kill . c <nl> static void dump_tasks ( const struct mem_cgroup * mem ) <nl> continue ; <nl> } <nl>  <nl> - printk ( KERN_INFO "[% 5d ] % 5d % 5d % 8lu % 8lu % 3d % 3d % s \ n ", <nl> + printk ( KERN_INFO "[% 5d ] % 5d % 5d % 8lu % 8lu % 3u % 3d % s \ n ", <nl> task -> pid , __task_cred ( task )-> uid , task -> tgid , <nl> task -> mm -> total_vm , get_mm_rss ( task -> mm ), <nl> - ( int ) task_cpu ( task ), task -> signal -> oom_adj , p -> comm ); <nl> + task_cpu ( task ), task -> signal -> oom_adj , task -> comm ); <nl> task_unlock ( task ); <nl> } <nl> }
mmm drivers / net / ethernet / cirrus / ep93xx_eth . c <nl> ppp drivers / net / ethernet / cirrus / ep93xx_eth . c <nl> static void ep93xx_free_buffers ( struct ep93xx_priv * ep ) <nl> struct device * dev = ep -> dev -> dev . parent ; <nl> int i ; <nl>  <nl> + if (! ep -> descs ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < RX_QUEUE_ENTRIES ; i ++) { <nl> dma_addr_t d ; <nl>  <nl> static void ep93xx_free_buffers ( struct ep93xx_priv * ep ) <nl>  <nl> dma_free_coherent ( dev , sizeof ( struct ep93xx_descs ), ep -> descs , <nl> ep -> descs_dma_addr ); <nl> + ep -> descs = NULL ; <nl> } <nl>  <nl> static int ep93xx_alloc_buffers ( struct ep93xx_priv * ep )
mmm fs / ocfs2 / dlm / dlmthread . c <nl> ppp fs / ocfs2 / dlm / dlmthread . c <nl> static int dlm_purge_lockres ( struct dlm_ctxt * dlm , <nl> res -> lockname . name , master ); <nl>  <nl> if (! master ) { <nl> + /* drop spinlock ... retake below */ <nl> + spin_unlock (& dlm -> spinlock ); <nl> + <nl> spin_lock (& res -> spinlock ); <nl> /* This ensures that clear refmap is sent after the set */ <nl> __dlm_wait_on_lockres_flags ( res , DLM_LOCK_RES_SETREF_INPROG ); <nl> spin_unlock (& res -> spinlock ); <nl> - /* drop spinlock to do messaging , retake below */ <nl> - spin_unlock (& dlm -> spinlock ); <nl> + <nl> /* clear our bit from the master ' s refmap , ignore errors */ <nl> ret = dlm_drop_lockres_ref ( dlm , res ); <nl> if ( ret < 0 ) {
mmm drivers / net / wireless / ath / ath10k / wmi - tlv . c <nl> ppp drivers / net / wireless / ath / ath10k / wmi - tlv . c <nl> ath10k_wmi_tlv_op_gen_start_scan ( struct ath10k * ar , <nl> bssid_len = arg -> n_bssids * sizeof ( struct wmi_mac_addr ); <nl> ie_len = roundup ( arg -> ie_len , 4 ); <nl> len = ( sizeof (* tlv ) + sizeof (* cmd )) + <nl> - ( arg -> n_channels ? sizeof (* tlv ) + chan_len : 0 ) + <nl> - ( arg -> n_ssids ? sizeof (* tlv ) + ssid_len : 0 ) + <nl> - ( arg -> n_bssids ? sizeof (* tlv ) + bssid_len : 0 ) + <nl> - ( arg -> ie_len ? sizeof (* tlv ) + ie_len : 0 ); <nl> + sizeof (* tlv ) + chan_len + <nl> + sizeof (* tlv ) + ssid_len + <nl> + sizeof (* tlv ) + bssid_len + <nl> + sizeof (* tlv ) + ie_len ; <nl>  <nl> skb = ath10k_wmi_alloc_skb ( ar , len ); <nl> if (! skb )
mmm include / linux / types . h <nl> ppp include / linux / types . h <nl> typedef __s64 int64_t ; <nl> # endif <nl>  <nl> /* this is a special 64bit data type that is 8 - byte aligned */ <nl> -# define aligned_u64 unsigned long long __attribute__ (( aligned ( 8 ))) <nl> +# define aligned_u64 __u64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_be64 __be64 __attribute__ (( aligned ( 8 ))) <nl> # define aligned_le64 __le64 __attribute__ (( aligned ( 8 ))) <nl> 
mmm drivers / mfd / mfd - core . c <nl> ppp drivers / mfd / mfd - core . c <nl> static int mfd_add_device ( struct platform_device * parent , <nl> if ( ret ) <nl> goto fail_device ; <nl>  <nl> - memzero ( res , sizeof ( res )); <nl> + memset ( res , 0 , sizeof ( res )); <nl> for ( r = 0 ; r < cell -> num_resources ; r ++) { <nl> res [ r ]. name = cell -> resources [ r ]. name ; <nl> res [ r ]. flags = cell -> resources [ r ]. flags ;
mmm drivers / gpu / drm / nouveau / nouveau_connector . c <nl> ppp drivers / gpu / drm / nouveau / nouveau_connector . c <nl> nouveau_connector_set_encoder ( struct drm_connector * connector , <nl> return ; <nl> nv_connector -> detected_encoder = nv_encoder ; <nl>  <nl> + if ( dev_priv -> card_type >= NV_50 ) { <nl> + connector -> interlace_allowed = true ; <nl> + connector -> doublescan_allowed = true ; <nl> + } else <nl> if ( nv_encoder -> dcb -> type == OUTPUT_LVDS || <nl> nv_encoder -> dcb -> type == OUTPUT_TMDS ) { <nl> connector -> doublescan_allowed = false ;
mmm kernel / events / uprobes . c <nl> ppp kernel / events / uprobes . c <nl> int uprobe_write_opcode ( struct mm_struct * mm , unsigned long vaddr , <nl>  <nl> retry : <nl> /* Read the page with vaddr into memory */ <nl> - ret = get_user_pages_remote ( NULL , mm , vaddr , 1 , FOLL_FORCE , & old_page , <nl> - & vma , NULL ); <nl> + ret = get_user_pages_remote ( NULL , mm , vaddr , 1 , <nl> + FOLL_FORCE | FOLL_SPLIT , & old_page , & vma , NULL ); <nl> if ( ret <= 0 ) <nl> return ret ; <nl> 
mmm crypto / async_tx / async_pq . c <nl> ppp crypto / async_tx / async_pq . c <nl> async_syndrome_val ( struct page ** blocks , unsigned int offset , int disks , <nl>  <nl> dma_set_unmap ( tx , unmap ); <nl> async_tx_submit ( chan , tx , submit ); <nl> - <nl> - return tx ; <nl> } else { <nl> struct page * p_src = P ( blocks , disks ); <nl> struct page * q_src = Q ( blocks , disks ); <nl> async_syndrome_val ( struct page ** blocks , unsigned int offset , int disks , <nl> submit -> cb_param = cb_param_orig ; <nl> submit -> flags = flags_orig ; <nl> async_tx_sync_epilog ( submit ); <nl> - <nl> - return NULL ; <nl> + tx = NULL ; <nl> } <nl> + dmaengine_unmap_put ( unmap ); <nl> + <nl> + return tx ; <nl> } <nl> EXPORT_SYMBOL_GPL ( async_syndrome_val ); <nl> 
mmm drivers / net / wireless / b43 / dma . h <nl> ppp drivers / net / wireless / b43 / dma . h <nl> struct b43_dmadesc_generic { <nl> /* DMA engine tuning knobs */ <nl> # define B43_TXRING_SLOTS 256 <nl> # define B43_RXRING_SLOTS 64 <nl> -# define B43_DMA0_RX_BUFFERSIZE IEEE80211_MAX_FRAME_LEN <nl> +# define B43_DMA0_RX_BUFFERSIZE ( B43_DMA0_RX_FRAMEOFFSET + IEEE80211_MAX_FRAME_LEN ) <nl>  <nl> /* Pointer poison */ <nl> # define B43_DMA_PTR_POISON (( void *) ERR_PTR (- ENOMEM ))mmm drivers / net / wireless / b43 / dma . c <nl> ppp drivers / net / wireless / b43 / dma . c <nl> struct b43_dmadesc_generic { <nl> /* DMA engine tuning knobs */ <nl> # define B43_TXRING_SLOTS 256 <nl> # define B43_RXRING_SLOTS 64 <nl> -# define B43_DMA0_RX_BUFFERSIZE IEEE80211_MAX_FRAME_LEN <nl> +# define B43_DMA0_RX_BUFFERSIZE ( B43_DMA0_RX_FRAMEOFFSET + IEEE80211_MAX_FRAME_LEN ) <nl>  <nl> /* Pointer poison */ <nl> # define B43_DMA_PTR_POISON (( void *) ERR_PTR (- ENOMEM )) <nl> static void dma_rx ( struct b43_dmaring * ring , int * slot ) <nl> dmaaddr = meta -> dmaaddr ; <nl> goto drop_recycle_buffer ; <nl> } <nl> - if ( unlikely ( len > ring -> rx_buffersize )) { <nl> + if ( unlikely ( len + ring -> frameoffset > ring -> rx_buffersize )) { <nl> /* The data did not fit into one descriptor buffer <nl> * and is split over multiple buffers . <nl> * This should never happen , as we try to allocate buffers
mmm fs / f2fs / super . c <nl> ppp fs / f2fs / super . c <nl> static int f2fs_fill_super ( struct super_block * sb , void * data , int silent ) <nl> int n = ( i == META ) ? 1 : NR_TEMP_TYPE ; <nl> int j ; <nl>  <nl> - sbi -> write_io [ i ] = f2fs_kmalloc ( sbi , <nl> - n * sizeof ( struct f2fs_bio_info ), <nl> - GFP_KERNEL ); <nl> + sbi -> write_io [ i ] = <nl> + f2fs_kmalloc ( sbi , <nl> + array_size ( n , <nl> + sizeof ( struct f2fs_bio_info )), <nl> + GFP_KERNEL ); <nl> if (! sbi -> write_io [ i ]) { <nl> err = - ENOMEM ; <nl> goto free_options ;
mmm net / mac80211 / ibss . c <nl> ppp net / mac80211 / ibss . c <nl> struct sta_info * ieee80211_ibss_add_sta ( struct ieee80211_sub_if_data * sdata , <nl> if (! sta ) <nl> return NULL ; <nl>  <nl> + sta -> last_rx = jiffies ; <nl> set_sta_flags ( sta , WLAN_STA_AUTHORIZED ); <nl>  <nl> /* make sure mandatory rates are always added */
mmm fs / xfs / xfs_da_btree . c <nl> ppp fs / xfs / xfs_da_btree . c <nl> xfs_da3_fixhashpath ( <nl> node = blk -> bp -> b_addr ; <nl> dp -> d_ops -> node_hdr_from_disk (& nodehdr , node ); <nl> btree = dp -> d_ops -> node_tree_p ( node ); <nl> - if ( be32_to_cpu ( btree -> hashval ) == lasthash ) <nl> + if ( be32_to_cpu ( btree [ blk -> index ]. hashval ) == lasthash ) <nl> break ; <nl> blk -> hashval = lasthash ; <nl> btree [ blk -> index ]. hashval = cpu_to_be32 ( lasthash );
mmm drivers / media / media - device . c <nl> ppp drivers / media / media - device . c <nl> static long __media_device_enum_links ( struct media_device * mdev , <nl>  <nl> for ( p = 0 ; p < entity -> num_pads ; p ++) { <nl> struct media_pad_desc pad ; <nl> + <nl> + memset (& pad , 0 , sizeof ( pad )); <nl> media_device_kpad_to_upad (& entity -> pads [ p ], & pad ); <nl> if ( copy_to_user (& links -> pads [ p ], & pad , sizeof ( pad ))) <nl> return - EFAULT ; <nl> static long __media_device_enum_links ( struct media_device * mdev , <nl> if ( entity -> links [ l ]. source -> entity != entity ) <nl> continue ; <nl>  <nl> + memset (& link , 0 , sizeof ( link )); <nl> media_device_kpad_to_upad ( entity -> links [ l ]. source , <nl> & link . source ); <nl> media_device_kpad_to_upad ( entity -> links [ l ]. sink ,
mmm drivers / scsi / scsi_transport_iscsi . c <nl> ppp drivers / scsi / scsi_transport_iscsi . c <nl> iscsi_if_rx ( struct sk_buff * skb ) <nl> uint32_t group ; <nl>  <nl> nlh = nlmsg_hdr ( skb ); <nl> - if ( nlh -> nlmsg_len < sizeof (* nlh ) || <nl> + if ( nlh -> nlmsg_len < sizeof (* nlh ) + sizeof (* ev ) || <nl> skb -> len < nlh -> nlmsg_len ) { <nl> break ; <nl> }
mmm net / sctp / auth . c <nl> ppp net / sctp / auth . c <nl> static struct sctp_auth_bytes * sctp_auth_create_key ( __u32 key_len , gfp_t gfp ) <nl> struct sctp_auth_bytes * key ; <nl>  <nl> /* Verify that we are not going to overflow INT_MAX */ <nl> - if (( INT_MAX - key_len ) < sizeof ( struct sctp_auth_bytes )) <nl> + if ( key_len > ( INT_MAX - sizeof ( struct sctp_auth_bytes ))) <nl> return NULL ; <nl>  <nl> /* Allocate the shared key */
mmm include / linux / kvm_host . h <nl> ppp include / linux / kvm_host . h <nl> static inline struct kvm_vcpu * kvm_get_vcpu_by_id ( struct kvm * kvm , int id ) <nl> struct kvm_vcpu * vcpu ; <nl> int i ; <nl>  <nl> + if ( id < 0 || id >= KVM_MAX_VCPUS ) <nl> + return NULL ; <nl> + vcpu = kvm_get_vcpu ( kvm , id ); <nl> + if ( vcpu && vcpu -> vcpu_id == id ) <nl> + return vcpu ; <nl> kvm_for_each_vcpu ( i , vcpu , kvm ) <nl> if ( vcpu -> vcpu_id == id ) <nl> return vcpu ;
mmm drivers / gpu / drm / i915 / gvt / handlers . c <nl> ppp drivers / gpu / drm / i915 / gvt / handlers . c <nl> static int bxt_phy_ctl_family_write ( struct intel_vgpu * vgpu , <nl> u32 v = *( u32 *) p_data ; <nl> u32 data = v & COMMON_RESET_DIS ? BXT_PHY_LANE_ENABLED : 0 ; <nl>  <nl> - vgpu_vreg ( vgpu , _BXT_PHY_CTL_DDI_A ) = data ; <nl> - vgpu_vreg ( vgpu , _BXT_PHY_CTL_DDI_B ) = data ; <nl> - vgpu_vreg ( vgpu , _BXT_PHY_CTL_DDI_C ) = data ; <nl> + switch ( offset ) { <nl> + case _PHY_CTL_FAMILY_EDP : <nl> + vgpu_vreg ( vgpu , _BXT_PHY_CTL_DDI_A ) = data ; <nl> + break ; <nl> + case _PHY_CTL_FAMILY_DDI : <nl> + vgpu_vreg ( vgpu , _BXT_PHY_CTL_DDI_B ) = data ; <nl> + vgpu_vreg ( vgpu , _BXT_PHY_CTL_DDI_C ) = data ; <nl> + break ; <nl> + } <nl>  <nl> vgpu_vreg ( vgpu , offset ) = v ; <nl> 
mmm drivers / net / ethernet / ibm / ibmvnic . c <nl> ppp drivers / net / ethernet / ibm / ibmvnic . c <nl> static int reset_one_sub_crq_queue ( struct ibmvnic_adapter * adapter , <nl> scrq -> irq = 0 ; <nl> } <nl>  <nl> - memset ( scrq -> msgs , 0 , 2 * PAGE_SIZE ); <nl> + memset ( scrq -> msgs , 0 , 4 * PAGE_SIZE ); <nl> scrq -> cur = 0 ; <nl>  <nl> rc = h_reg_sub_crq ( adapter -> vdev -> unit_address , scrq -> msg_token ,
mmm drivers / net / ethernet / mellanox / mlx5 / core / fpga / conn . c <nl> ppp drivers / net / ethernet / mellanox / mlx5 / core / fpga / conn . c <nl> static int mlx5_fpga_conn_create_cq ( struct mlx5_fpga_conn * conn , int cq_size ) <nl> } <nl>  <nl> err = mlx5_vector2eqn ( mdev , smp_processor_id (), & eqn , & irqn ); <nl> - if ( err ) <nl> + if ( err ) { <nl> + kvfree ( in ); <nl> goto err_cqwq ; <nl> + } <nl>  <nl> cqc = MLX5_ADDR_OF ( create_cq_in , in , cq_context ); <nl> MLX5_SET ( cqc , cqc , log_cq_size , ilog2 ( cq_size ));
mmm net / bluetooth / sco . c <nl> ppp net / bluetooth / sco . c <nl> static int sco_sock_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> test_bit ( BT_SK_DEFER_SETUP , & bt_sk ( sk )-> flags )) { <nl> hci_conn_accept ( pi -> conn -> hcon , 0 ); <nl> sk -> sk_state = BT_CONFIG ; <nl> + msg -> msg_namelen = 0 ; <nl>  <nl> release_sock ( sk ); <nl> return 0 ;
mmm net / dsa / dsa . c <nl> ppp net / dsa / dsa . c <nl> static int dsa_of_probe ( struct device * dev ) <nl> continue ; <nl>  <nl> cd -> sw_addr = be32_to_cpup ( sw_addr ); <nl> - if ( cd -> sw_addr > PHY_MAX_ADDR ) <nl> + if ( cd -> sw_addr >= PHY_MAX_ADDR ) <nl> continue ; <nl>  <nl> if (! of_property_read_u32 ( child , " eeprom - length ", & eeprom_len ))
mmm kernel / sched / core . c <nl> ppp kernel / sched / core . c <nl> void partition_sched_domains ( int ndoms_new , cpumask_var_t doms_new [], <nl> ; <nl> } <nl>  <nl> + n = ndoms_cur ; <nl> if ( doms_new == NULL ) { <nl> - ndoms_cur = 0 ; <nl> + n = 0 ; <nl> doms_new = & fallback_doms ; <nl> cpumask_andnot ( doms_new [ 0 ], cpu_active_mask , cpu_isolated_map ); <nl> WARN_ON_ONCE ( dattr_new ); <nl> void partition_sched_domains ( int ndoms_new , cpumask_var_t doms_new [], <nl>  <nl> /* Build new domains */ <nl> for ( i = 0 ; i < ndoms_new ; i ++) { <nl> - for ( j = 0 ; j < ndoms_cur && ! new_topology ; j ++) { <nl> + for ( j = 0 ; j < n && ! new_topology ; j ++) { <nl> if ( cpumask_equal ( doms_new [ i ], doms_cur [ j ]) <nl> && dattrs_equal ( dattr_new , i , dattr_cur , j )) <nl> goto match2 ;
mmm fs / btrfs / super . c <nl> ppp fs / btrfs / super . c <nl> static int btrfs_show_options ( struct seq_file * seq , struct dentry * dentry ) <nl> seq_puts ( seq , ", fatal_errors = panic "); <nl> if ( info -> commit_interval != BTRFS_DEFAULT_COMMIT_INTERVAL ) <nl> seq_printf ( seq , ", commit =% d ", info -> commit_interval ); <nl> + seq_printf ( seq , ", subvolid =% llu ", <nl> + BTRFS_I ( d_inode ( dentry ))-> root -> root_key . objectid ); <nl> + seq_puts ( seq , ", subvol ="); <nl> + seq_dentry ( seq , dentry , " \ t \ n \\"); <nl> return 0 ; <nl> } <nl> mmm fs / seq_file . c <nl> ppp fs / seq_file . c <nl> static int btrfs_show_options ( struct seq_file * seq , struct dentry * dentry ) <nl> seq_puts ( seq , ", fatal_errors = panic "); <nl> if ( info -> commit_interval != BTRFS_DEFAULT_COMMIT_INTERVAL ) <nl> seq_printf ( seq , ", commit =% d ", info -> commit_interval ); <nl> + seq_printf ( seq , ", subvolid =% llu ", <nl> + BTRFS_I ( d_inode ( dentry ))-> root -> root_key . objectid ); <nl> + seq_puts ( seq , ", subvol ="); <nl> + seq_dentry ( seq , dentry , " \ t \ n \\"); <nl> return 0 ; <nl> } <nl>  <nl> int seq_dentry ( struct seq_file * m , struct dentry * dentry , const char * esc ) <nl>  <nl> return res ; <nl> } <nl> + EXPORT_SYMBOL ( seq_dentry ); <nl>  <nl> static void * single_start ( struct seq_file * p , loff_t * pos ) <nl> {
mmm drivers / mmc / core / sdio_cis . c <nl> ppp drivers / mmc / core / sdio_cis . c <nl> static int sdio_read_cis ( struct mmc_card * card , struct sdio_func * func ) <nl> if ( tpl_code == 0xff ) <nl> break ; <nl>  <nl> + /* null entries have no link field or data */ <nl> + if ( tpl_code == 0x00 ) <nl> + continue ; <nl> + <nl> ret = mmc_io_rw_direct ( card , 0 , 0 , ptr ++, 0 , & tpl_link ); <nl> if ( ret ) <nl> break ;
mmm net / rds / ib_send . c <nl> ppp net / rds / ib_send . c <nl> int rds_ib_xmit ( struct rds_connection * conn , struct rds_message * rm , <nl> if ( credit_alloc < work_alloc ) { <nl> rds_ib_ring_unalloc (& ic -> i_send_ring , work_alloc - credit_alloc ); <nl> work_alloc = credit_alloc ; <nl> - flow_controlled ++; <nl> + flow_controlled = 1 ; <nl> } <nl> if ( work_alloc == 0 ) { <nl> set_bit ( RDS_LL_SEND_FULL , & conn -> c_flags ); <nl> int rds_ib_xmit ( struct rds_connection * conn , struct rds_message * rm , <nl> /* <nl> * Update adv_credits since we reset the ACK_REQUIRED bit . <nl> */ <nl> - rds_ib_send_grab_credits ( ic , 0 , & posted , 1 , RDS_MAX_ADV_CREDIT - adv_credits ); <nl> - adv_credits += posted ; <nl> - BUG_ON ( adv_credits > 255 ); <nl> + if ( ic -> i_flowctl ) { <nl> + rds_ib_send_grab_credits ( ic , 0 , & posted , 1 , RDS_MAX_ADV_CREDIT - adv_credits ); <nl> + adv_credits += posted ; <nl> + BUG_ON ( adv_credits > 255 ); <nl> + } <nl> } <nl>  <nl> /* Sometimes you want to put a fence between an RDMA <nl> int rds_ib_xmit ( struct rds_connection * conn , struct rds_message * rm , <nl> /* <nl> * Always signal the last one if we ' re stopping due to flow control . <nl> */ <nl> - if ( flow_controlled && i == ( work_alloc - 1 )) <nl> + if ( ic -> i_flowctl && flow_controlled && i == ( work_alloc - 1 )) <nl> send -> s_wr . send_flags |= IB_SEND_SIGNALED | IB_SEND_SOLICITED ; <nl>  <nl> rdsdebug (" send % p wr % p num_sge % u next % p \ n ", send , <nl> & send -> s_wr , send -> s_wr . num_sge , send -> s_wr . next ); <nl>  <nl> - if ( adv_credits ) { <nl> + if ( ic -> i_flowctl && adv_credits ) { <nl> struct rds_header * hdr = & ic -> i_send_hdrs [ pos ]; <nl>  <nl> /* add credit and redo the header checksum */
mmm fs / binfmt_elf . c <nl> ppp fs / binfmt_elf . c <nl> static int fill_thread_core_info ( struct elf_thread_core_info * t , <nl> for ( i = 1 ; i < view -> n ; ++ i ) { <nl> const struct user_regset * regset = & view -> regsets [ i ]; <nl> do_thread_regset_writeback ( t -> task , regset ); <nl> - if ( regset -> core_note_type && <nl> + if ( regset -> core_note_type && regset -> get && <nl> (! regset -> active || regset -> active ( t -> task , regset ))) { <nl> int ret ; <nl> size_t size = regset -> n * regset -> size ;mmm include / linux / regset . h <nl> ppp include / linux / regset . h <nl> static int fill_thread_core_info ( struct elf_thread_core_info * t , <nl> for ( i = 1 ; i < view -> n ; ++ i ) { <nl> const struct user_regset * regset = & view -> regsets [ i ]; <nl> do_thread_regset_writeback ( t -> task , regset ); <nl> - if ( regset -> core_note_type && <nl> + if ( regset -> core_note_type && regset -> get && <nl> (! regset -> active || regset -> active ( t -> task , regset ))) { <nl> int ret ; <nl> size_t size = regset -> n * regset -> size ; <nl> static inline int copy_regset_to_user ( struct task_struct * target , <nl> { <nl> const struct user_regset * regset = & view -> regsets [ setno ]; <nl>  <nl> + if (! regset -> get ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> if (! access_ok ( VERIFY_WRITE , data , size )) <nl> return - EIO ; <nl>  <nl> static inline int copy_regset_from_user ( struct task_struct * target , <nl> { <nl> const struct user_regset * regset = & view -> regsets [ setno ]; <nl>  <nl> + if (! regset -> set ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> if (! access_ok ( VERIFY_READ , data , size )) <nl> return - EIO ; <nl> 
mmm arch / x86 / kernel / msr . c <nl> ppp arch / x86 / kernel / msr . c <nl> static int msr_open ( struct inode * inode , struct file * file ) <nl> unsigned int cpu ; <nl> struct cpuinfo_x86 * c ; <nl>  <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> + <nl> cpu = iminor ( file -> f_path . dentry -> d_inode ); <nl> if ( cpu >= nr_cpu_ids || ! cpu_online ( cpu )) <nl> return - ENXIO ; /* No such CPU */
mmm arch / x86_64 / kernel / aperture . c <nl> ppp arch / x86_64 / kernel / aperture . c <nl> static u32 __init allocate_aperture ( void ) <nl> printk (" Cannot allocate aperture memory hole (% p ,% uK )\ n ", <nl> p , aper_size >> 10 ); <nl> if ( p ) <nl> - free_bootmem_node ( nd0 , ( unsigned long ) p , aper_size ); <nl> + free_bootmem_node ( nd0 , __pa ( p ), aper_size ); <nl> return 0 ; <nl> } <nl> printk (" Mapping aperture over % d KB of RAM @ % lx \ n ",
mmm include / linux / vt_kern . h <nl> ppp include / linux / vt_kern . h <nl> int vty_init ( const struct file_operations * console_fops ); <nl>  <nl> static inline bool vt_force_oops_output ( struct vc_data * vc ) <nl> { <nl> - if ( oops_in_progress && vc -> vc_panic_force_write ) <nl> + if ( oops_in_progress && vc -> vc_panic_force_write && panic_timeout >= 0 ) <nl> return true ; <nl> return false ; <nl> }
mmm arch / arm / kernel / perf_event . c <nl> ppp arch / arm / kernel / perf_event . c <nl> validate_event ( struct pmu_hw_events * hw_events , <nl> struct arm_pmu * armpmu = to_arm_pmu ( event -> pmu ); <nl> struct pmu * leader_pmu = event -> group_leader -> pmu ; <nl>  <nl> + if ( is_software_event ( event )) <nl> + return 1 ; <nl> + <nl> if ( event -> pmu != leader_pmu || event -> state < PERF_EVENT_STATE_OFF ) <nl> return 1 ; <nl> 
mmm include / trace / events / asoc . h <nl> ppp include / trace / events / asoc . h <nl> TRACE_EVENT ( snd_soc_dapm_output_path , <nl> __assign_str ( pname , path -> name ? path -> name : DAPM_DIRECT ); <nl> __assign_str ( psname , path -> sink -> name ); <nl> __entry -> path_connect = path -> connect ; <nl> - __entry -> path_sink = ( int ) path -> sink ; <nl> + __entry -> path_sink = ( long ) path -> sink ; <nl> ), <nl>  <nl> TP_printk ("% c % s -> % s -> % s \ n ", <nl> TRACE_EVENT ( snd_soc_dapm_input_path , <nl> __assign_str ( pname , path -> name ? path -> name : DAPM_DIRECT ); <nl> __assign_str ( psname , path -> source -> name ); <nl> __entry -> path_connect = path -> connect ; <nl> - __entry -> path_source = ( int ) path -> source ; <nl> + __entry -> path_source = ( long ) path -> source ; <nl> ), <nl>  <nl> TP_printk ("% c % s <- % s <- % s \ n ",
mmm drivers / media / video / au0828 / au0828 . h <nl> ppp drivers / media / video / au0828 / au0828 . h <nl> struct au0828_dev { <nl> unsigned int frame_count ; <nl> int ctrl_freq ; <nl> int input_type ; <nl> + int std_set_in_tuner_core ; <nl> unsigned int ctrl_input ; <nl> enum au0828_dev_state dev_state ; <nl> enum au0828_stream_state stream_state ;mmm drivers / media / video / au0828 / au0828 - video . c <nl> ppp drivers / media / video / au0828 / au0828 - video . c <nl> struct au0828_dev { <nl> unsigned int frame_count ; <nl> int ctrl_freq ; <nl> int input_type ; <nl> + int std_set_in_tuner_core ; <nl> unsigned int ctrl_input ; <nl> enum au0828_dev_state dev_state ; <nl> enum au0828_stream_state stream_state ; <nl> static int vidioc_s_std ( struct file * file , void * priv , v4l2_std_id * norm ) <nl> buffer , which is currently hardcoded at 720x480 */ <nl>  <nl> v4l2_device_call_all (& dev -> v4l2_dev , 0 , core , s_std , * norm ); <nl> + dev -> std_set_in_tuner_core = 1 ; <nl> return 0 ; <nl> } <nl>  <nl> static int vidioc_s_frequency ( struct file * file , void * priv , <nl>  <nl> dev -> ctrl_freq = freq -> frequency ; <nl>  <nl> + if ( dev -> std_set_in_tuner_core == 0 ) { <nl> + /* If we ' ve never sent the standard in tuner core , do so now . We <nl> + don ' t do this at device probe because we don ' t want to incur <nl> + the cost of a firmware load */ <nl> + v4l2_device_call_all (& dev -> v4l2_dev , 0 , core , s_std , <nl> + dev -> vdev -> tvnorms ); <nl> + dev -> std_set_in_tuner_core = 1 ; <nl> + } <nl> + <nl> v4l2_device_call_all (& dev -> v4l2_dev , 0 , tuner , s_frequency , freq ); <nl>  <nl> au0828_analog_stream_reset ( dev );
mmm drivers / gpu / drm / i915 / intel_dp . c <nl> ppp drivers / gpu / drm / i915 / intel_dp . c <nl> static void intel_dp_prepare ( struct intel_encoder * encoder , <nl> trans_dp &= ~ TRANS_DP_ENH_FRAMING ; <nl> I915_WRITE ( TRANS_DP_CTL ( crtc -> pipe ), trans_dp ); <nl> } else { <nl> - if (! HAS_PCH_SPLIT ( dev_priv ) && ! IS_VALLEYVIEW ( dev_priv ) && <nl> - ! IS_CHERRYVIEW ( dev_priv ) && <nl> - pipe_config -> limited_color_range ) <nl> + if ( IS_G4X ( dev_priv ) && pipe_config -> limited_color_range ) <nl> intel_dp -> DP |= DP_COLOR_RANGE_16_235 ; <nl>  <nl> if ( adjusted_mode -> flags & DRM_MODE_FLAG_PHSYNC ) <nl> static void intel_dp_get_config ( struct intel_encoder * encoder , <nl>  <nl> pipe_config -> base . adjusted_mode . flags |= flags ; <nl>  <nl> - if (! HAS_PCH_SPLIT ( dev_priv ) && ! IS_VALLEYVIEW ( dev_priv ) && <nl> - ! IS_CHERRYVIEW ( dev_priv ) && tmp & DP_COLOR_RANGE_16_235 ) <nl> + if ( IS_G4X ( dev_priv ) && tmp & DP_COLOR_RANGE_16_235 ) <nl> pipe_config -> limited_color_range = true ; <nl>  <nl> pipe_config -> lane_count =
mmm arch / x86 / kernel / cpu / perf_event_intel . c <nl> ppp arch / x86 / kernel / cpu / perf_event_intel . c <nl> __init int intel_pmu_init ( void ) <nl> if ( version > 1 ) <nl> x86_pmu . num_counters_fixed = max (( int ) edx . split . num_counters_fixed , 3 ); <nl>  <nl> - /* <nl> - * v2 and above have a perf capabilities MSR <nl> - */ <nl> - if ( version > 1 ) { <nl> + if ( boot_cpu_has ( X86_FEATURE_PDCM )) { <nl> u64 capabilities ; <nl>  <nl> rdmsrl ( MSR_IA32_PERF_CAPABILITIES , capabilities );
mmm fs / ext4 / namei . c <nl> ppp fs / ext4 / namei . c <nl> int ext4_orphan_add ( handle_t * handle , struct inode * inode ) <nl> struct ext4_iloc iloc ; <nl> int err = 0 , rc ; <nl>  <nl> - if (! ext4_handle_valid ( handle )) <nl> + if (! EXT4_SB ( sb )-> s_journal ) <nl> return 0 ; <nl>  <nl> mutex_lock (& EXT4_SB ( sb )-> s_orphan_lock ); <nl> int ext4_orphan_del ( handle_t * handle , struct inode * inode ) <nl> struct ext4_iloc iloc ; <nl> int err = 0 ; <nl>  <nl> - /* ext4_handle_valid () assumes a valid handle_t pointer */ <nl> - if ( handle && ! ext4_handle_valid ( handle )) <nl> + if (! EXT4_SB ( inode -> i_sb )-> s_journal ) <nl> return 0 ; <nl>  <nl> mutex_lock (& EXT4_SB ( inode -> i_sb )-> s_orphan_lock ); <nl> int ext4_orphan_del ( handle_t * handle , struct inode * inode ) <nl> * transaction handle with which to update the orphan list on <nl> * disk , but we still need to remove the inode from the linked <nl> * list in memory . */ <nl> - if ( sbi -> s_journal && ! handle ) <nl> + if (! handle ) <nl> goto out ; <nl>  <nl> err = ext4_reserve_inode_write ( handle , inode , & iloc );
mmm drivers / net / irda / w83977af_ir . c <nl> ppp drivers / net / irda / w83977af_ir . c <nl> static netdev_tx_t w83977af_hard_xmit ( struct sk_buff * skb , <nl>  <nl> mtt = irda_get_mtt ( skb ); <nl> pr_debug ("% s (% ld ), mtt =% d \ n ", __func__ , jiffies , mtt ); <nl> - if ( mtt ) <nl> + if ( mtt > 1000 ) <nl> + mdelay ( mtt / 1000 ); <nl> + else if ( mtt ) <nl> udelay ( mtt ); <nl>  <nl> /* Enable DMA interrupt */
mmm drivers / net / ixgbe / ixgbe_main . c <nl> ppp drivers / net / ixgbe / ixgbe_main . c <nl> static inline void map_vector_to_rxq ( struct ixgbe_adapter * a , int v_idx , <nl>  <nl> set_bit ( r_idx , q_vector -> rxr_idx ); <nl> q_vector -> rxr_count ++; <nl> - a -> rx_ring [ r_idx ]. v_idx = 1 << v_idx ; <nl> + a -> rx_ring [ r_idx ]. v_idx = ( u64 ) 1 << v_idx ; <nl> } <nl>  <nl> static inline void map_vector_to_txq ( struct ixgbe_adapter * a , int v_idx , <nl> static inline void map_vector_to_txq ( struct ixgbe_adapter * a , int v_idx , <nl>  <nl> set_bit ( t_idx , q_vector -> txr_idx ); <nl> q_vector -> txr_count ++; <nl> - a -> tx_ring [ t_idx ]. v_idx = 1 << v_idx ; <nl> + a -> tx_ring [ t_idx ]. v_idx = ( u64 ) 1 << v_idx ; <nl> } <nl>  <nl> /** <nl> static void ixgbe_watchdog ( unsigned long data ) <nl> int i ; <nl>  <nl> for ( i = 0 ; i < adapter -> num_msix_vectors - NON_Q_VECTORS ; i ++) <nl> - eics |= ( 1 << i ); <nl> + eics |= (( u64 ) 1 << i ); <nl>  <nl> /* Cause software interrupt to ensure rx rings are cleaned */ <nl> switch ( hw -> mac . type ) {
mmm drivers / net / wireless / iwlwifi / mvm / fw . c <nl> ppp drivers / net / wireless / iwlwifi / mvm / fw . c <nl> int iwl_mvm_rx_card_state_notif ( struct iwl_mvm * mvm , <nl> ( flags & CT_KILL_CARD_DISABLED ) ? <nl> " Reached " : " Not reached "); <nl>  <nl> - if ( flags & CARD_DISABLED_MSK ) <nl> - iwl_write32 ( mvm -> trans , CSR_UCODE_DRV_GP1_SET , <nl> - CSR_UCODE_DRV_GP1_BIT_CMD_BLOCKED ); <nl> - <nl> return 0 ; <nl> } <nl> 
mmm drivers / mtd / nand / diskonchip . c <nl> ppp drivers / mtd / nand / diskonchip . c <nl> static int doc_ecc_decode ( struct rs_control * rs , uint8_t * data , uint8_t * ecc ) <nl> uint8_t parity ; <nl> uint16_t ds [ 4 ], s [ 5 ], tmp , errval [ 8 ], syn [ 4 ]; <nl>  <nl> + memset ( syn , 0 , sizeof ( syn )); <nl> /* Convert the ecc bytes into words */ <nl> ds [ 0 ] = (( ecc [ 4 ] & 0xff ) >> 0 ) | (( ecc [ 5 ] & 0x03 ) << 8 ); <nl> ds [ 1 ] = (( ecc [ 5 ] & 0xfc ) >> 2 ) | (( ecc [ 2 ] & 0x0f ) << 6 ); <nl> static int doc_ecc_decode ( struct rs_control * rs , uint8_t * data , uint8_t * ecc ) <nl> s [ i ] ^= rs -> alpha_to [ rs_modnn ( rs , tmp + ( FCR + i ) * j )]; <nl> } <nl>  <nl> - /* Calc s [ i ] = s [ i ] / alpha ^( v + i ) */ <nl> + /* Calc syn [ i ] = s [ i ] / alpha ^( v + i ) */ <nl> for ( i = 0 ; i < NROOTS ; i ++) { <nl> - if ( syn [ i ]) <nl> + if ( s [ i ]) <nl> syn [ i ] = rs_modnn ( rs , rs -> index_of [ s [ i ]] + ( NN - FCR - i )); <nl> } <nl> /* Call the decoder library */
mmm fs / xfs / xfs_super . c <nl> ppp fs / xfs / xfs_super . c <nl> xfs_fs_fill_super ( <nl> out_close_devices : <nl> xfs_close_devices ( mp ); <nl> out_free_fsname : <nl> + sb -> s_fs_info = NULL ; <nl> xfs_free_fsname ( mp ); <nl> kfree ( mp ); <nl> out : <nl> xfs_fs_put_super ( <nl> { <nl> struct xfs_mount * mp = XFS_M ( sb ); <nl>  <nl> + /* if -> fill_super failed , we have no mount to tear down */ <nl> + if (! sb -> s_fs_info ) <nl> + return ; <nl> + <nl> xfs_notice ( mp , " Unmounting Filesystem "); <nl> xfs_filestream_unmount ( mp ); <nl> xfs_unmountfs ( mp ); <nl> xfs_fs_put_super ( <nl> xfs_destroy_percpu_counters ( mp ); <nl> xfs_destroy_mount_workqueues ( mp ); <nl> xfs_close_devices ( mp ); <nl> + <nl> + sb -> s_fs_info = NULL ; <nl> xfs_free_fsname ( mp ); <nl> kfree ( mp ); <nl> } <nl> xfs_fs_nr_cached_objects ( <nl> struct super_block * sb , <nl> struct shrink_control * sc ) <nl> { <nl> + /* Paranoia : catch incorrect calls during mount setup or teardown */ <nl> + if ( WARN_ON_ONCE (! sb -> s_fs_info )) <nl> + return 0 ; <nl> return xfs_reclaim_inodes_count ( XFS_M ( sb )); <nl> } <nl> 
mmm drivers / net / ethernet / netronome / nfp / bpf / cmsg . c <nl> ppp drivers / net / ethernet / netronome / nfp / bpf / cmsg . c <nl> nfp_bpf_cmsg_wait_reply ( struct nfp_app_bpf * bpf , enum nfp_bpf_cmsg_type type , <nl> int tag ) <nl> { <nl> struct sk_buff * skb ; <nl> - int err ; <nl> + int i , err ; <nl> + <nl> + for ( i = 0 ; i < 50 ; i ++) { <nl> + udelay ( 4 ); <nl> + skb = nfp_bpf_reply ( bpf , tag ); <nl> + if ( skb ) <nl> + return skb ; <nl> + } <nl>  <nl> err = wait_event_interruptible_timeout ( bpf -> cmsg_wq , <nl> skb = nfp_bpf_reply ( bpf , tag ),
mmm block / blk - cgroup . c <nl> ppp block / blk - cgroup . c <nl> static int blkcg_print_stat ( struct seq_file * sf , void * v ) <nl> struct cftype blkcg_files [] = { <nl> { <nl> . name = " stat ", <nl> + . flags = CFTYPE_NOT_ON_ROOT , <nl> . seq_show = blkcg_print_stat , <nl> }, <nl> { } /* terminate */
mmm net / bridge / netfilter / ebt_ulog . c <nl> ppp net / bridge / netfilter / ebt_ulog . c <nl> static void ebt_ulog_packet ( struct net * net , unsigned int hooknr , <nl> ub -> qlen ++; <nl>  <nl> pm = nlmsg_data ( nlh ); <nl> + memset ( pm , 0 , sizeof (* pm )); <nl>  <nl> /* Fill in the ulog data */ <nl> pm -> version = EBT_ULOG_VERSION ; <nl> static void ebt_ulog_packet ( struct net * net , unsigned int hooknr , <nl> pm -> hook = hooknr ; <nl> if ( uloginfo -> prefix != NULL ) <nl> strcpy ( pm -> prefix , uloginfo -> prefix ); <nl> - else <nl> - *( pm -> prefix ) = '\ 0 '; <nl>  <nl> if ( in ) { <nl> strcpy ( pm -> physindev , in -> name ); <nl> static void ebt_ulog_packet ( struct net * net , unsigned int hooknr , <nl> strcpy ( pm -> indev , br_port_get_rcu ( in )-> br -> dev -> name ); <nl> else <nl> strcpy ( pm -> indev , in -> name ); <nl> - } else <nl> - pm -> indev [ 0 ] = pm -> physindev [ 0 ] = '\ 0 '; <nl> + } <nl>  <nl> if ( out ) { <nl> /* If out exists , then out is a bridge port */ <nl> strcpy ( pm -> physoutdev , out -> name ); <nl> /* rcu_read_lock () ed by nf_hook_slow */ <nl> strcpy ( pm -> outdev , br_port_get_rcu ( out )-> br -> dev -> name ); <nl> - } else <nl> - pm -> outdev [ 0 ] = pm -> physoutdev [ 0 ] = '\ 0 '; <nl> + } <nl>  <nl> if ( skb_copy_bits ( skb , - ETH_HLEN , pm -> data , copy_len ) < 0 ) <nl> BUG ();
mmm kernel / sched . c <nl> ppp kernel / sched . c <nl> static int __build_sched_domains ( const cpumask_t * cpu_map , <nl> error : <nl> free_sched_groups ( cpu_map , tmpmask ); <nl> SCHED_CPUMASK_FREE (( void *) allmasks ); <nl> + kfree ( rd ); <nl> return - ENOMEM ; <nl> # endif <nl> }
mmm security / keys / keyring . c <nl> ppp security / keys / keyring . c <nl> void __key_link_end ( struct key * keyring , <nl> if ( index_key -> type == & key_type_keyring ) <nl> up_write (& keyring_serialise_link_sem ); <nl>  <nl> - if ( edit && ! edit -> dead_leaf ) { <nl> - key_payload_reserve ( keyring , <nl> - keyring -> datalen - KEYQUOTA_LINK_BYTES ); <nl> + if ( edit ) { <nl> + if (! edit -> dead_leaf ) { <nl> + key_payload_reserve ( keyring , <nl> + keyring -> datalen - KEYQUOTA_LINK_BYTES ); <nl> + } <nl> assoc_array_cancel_edit ( edit ); <nl> } <nl> up_write (& keyring -> sem );
mmm net / ipv4 / ip_sockglue . c <nl> ppp net / ipv4 / ip_sockglue . c <nl> static void ip_cmsg_recv_checksum ( struct msghdr * msg , struct sk_buff * skb , <nl> if ( skb -> ip_summed != CHECKSUM_COMPLETE ) <nl> return ; <nl>  <nl> - if ( offset != 0 ) <nl> - csum = csum_sub ( csum , <nl> - csum_partial ( skb_transport_header ( skb ) + tlen , <nl> - offset , 0 )); <nl> + if ( offset != 0 ) { <nl> + int tend_off = skb_transport_offset ( skb ) + tlen ; <nl> + csum = csum_sub ( csum , skb_checksum ( skb , tend_off , offset , 0 )); <nl> + } <nl>  <nl> put_cmsg ( msg , SOL_IP , IP_CHECKSUM , sizeof ( __wsum ), & csum ); <nl> }
mmm arch / arm / mach - pxa / pxa25x . c <nl> ppp arch / arm / mach - pxa / pxa25x . c <nl> void __init pxa26x_init_irq ( void ) <nl> } <nl> # endif <nl>  <nl> + void __init pxa25x_dt_init_irq ( void ) <nl> +{ <nl> + if ( IS_ENABLED ( CONFIG_OF )) <nl> + pxa_dt_irq_init ( pxa25x_set_wake ); <nl> +} <nl> + <nl> static struct map_desc pxa25x_io_desc [] __initdata = { <nl> { /* Mem Ctl */ <nl> . virtual = ( unsigned long ) SMEMC_VIRT ,mmm arch / arm / mach - pxa / generic . h <nl> ppp arch / arm / mach - pxa / generic . h <nl> void __init pxa26x_init_irq ( void ) <nl> } <nl> # endif <nl>  <nl> + void __init pxa25x_dt_init_irq ( void ) <nl> +{ <nl> + if ( IS_ENABLED ( CONFIG_OF )) <nl> + pxa_dt_irq_init ( pxa25x_set_wake ); <nl> +} <nl> + <nl> static struct map_desc pxa25x_io_desc [] __initdata = { <nl> { /* Mem Ctl */ <nl> . virtual = ( unsigned long ) SMEMC_VIRT , <nl> extern void pxa_timer_init ( void ); <nl>  <nl> # define pxa25x_handle_irq icip_handle_irq <nl> extern int __init pxa25x_clocks_init ( void ); <nl> + extern void __init pxa25x_dt_init_irq ( void ); <nl> extern void __init pxa25x_init_irq ( void ); <nl> extern void __init pxa25x_map_io ( void ); <nl> extern void __init pxa26x_init_irq ( void );
mmm drivers / net / ethernet / marvell / skge . c <nl> ppp drivers / net / ethernet / marvell / skge . c <nl> static int skge_down ( struct net_device * dev ) <nl> struct skge_hw * hw = skge -> hw ; <nl> int port = skge -> port ; <nl>  <nl> - if ( skge -> mem == NULL ) <nl> + if (! skge -> mem ) <nl> return 0 ; <nl>  <nl> netif_info ( skge , ifdown , skge -> netdev , " disabling interface \ n ");
mmm net / ipv6 / fib6_rules . c <nl> ppp net / ipv6 / fib6_rules . c <nl> static bool fib6_rule_suppress ( struct fib_rule * rule , struct fib_lookup_arg * arg <nl> return false ; <nl>  <nl> suppress_route : <nl> - ip6_rt_put ( rt ); <nl> + if (!( arg -> flags & FIB_LOOKUP_NOREF )) <nl> + ip6_rt_put ( rt ); <nl> return true ; <nl> } <nl> 
mmm drivers / media / video / omap3isp / ispcsiphy . c <nl> ppp drivers / media / video / omap3isp / ispcsiphy . c <nl> int omap3isp_csiphy_acquire ( struct isp_csiphy * phy ) <nl> if ( rval < 0 ) <nl> goto done ; <nl>  <nl> - omap3isp_csi2_reset ( phy -> csi2 ); <nl> + rval = omap3isp_csi2_reset ( phy -> csi2 ); <nl> + if ( rval < 0 ) <nl> + goto done ; <nl>  <nl> csiphy_dphy_config ( phy ); <nl> csiphy_lanes_config ( phy );
mmm drivers / media / dvb / frontends / ds3000 . c <nl> ppp drivers / media / dvb / frontends / ds3000 . c <nl> static int ds3000_writeFW ( struct ds3000_state * state , int reg , <nl> struct i2c_msg msg ; <nl> u8 * buf ; <nl>  <nl> - buf = kmalloc ( 3 , GFP_KERNEL ); <nl> + buf = kmalloc ( 33 , GFP_KERNEL ); <nl> if ( buf == NULL ) { <nl> printk ( KERN_ERR " Unable to kmalloc \ n "); <nl> ret = - ENOMEM ; <nl> static int ds3000_writeFW ( struct ds3000_state * state , int reg , <nl> msg . addr = state -> config -> demod_address ; <nl> msg . flags = 0 ; <nl> msg . buf = buf ; <nl> - msg . len = 3 ; <nl> + msg . len = 33 ; <nl>  <nl> - for ( i = 0 ; i < len ; i += 2 ) { <nl> - memcpy ( buf + 1 , data + i , 2 ); <nl> + for ( i = 0 ; i < len ; i += 32 ) { <nl> + memcpy ( buf + 1 , data + i , 32 ); <nl>  <nl> dprintk ("% s : write reg 0x % 02x , len = % d \ n ", __func__ , reg , len ); <nl> 
mmm drivers / gpu / drm / i915 / i915_debugfs . c <nl> ppp drivers / gpu / drm / i915 / i915_debugfs . c <nl> static int i915_ppgtt_info ( struct seq_file * m , void * data ) <nl> task = get_pid_task ( file -> pid , PIDTYPE_PID ); <nl> if (! task ) { <nl> ret = - ESRCH ; <nl> - goto out_put ; <nl> + goto out_unlock ; <nl> } <nl> seq_printf ( m , "\ nproc : % s \ n ", task -> comm ); <nl> put_task_struct ( task ); <nl> idr_for_each (& file_priv -> context_idr , per_file_ctx , <nl> ( void *)( unsigned long ) m ); <nl> } <nl> + out_unlock : <nl> mutex_unlock (& dev -> filelist_mutex ); <nl>  <nl> - out_put : <nl> intel_runtime_pm_put ( dev_priv ); <nl> mutex_unlock (& dev -> struct_mutex ); <nl> 
mmm drivers / pci / hotplug / cpqphp_ctrl . c <nl> ppp drivers / pci / hotplug / cpqphp_ctrl . c <nl> static struct pci_resource * get_max_resource ( struct pci_resource ** head , u32 siz <nl> temp = temp -> next ; <nl> } <nl>  <nl> - temp -> next = max -> next ; <nl> + if ( temp ) <nl> + temp -> next = max -> next ; <nl> } <nl>  <nl> max -> next = NULL ;
mmm arch / arm64 / include / asm / pgtable . h <nl> ppp arch / arm64 / include / asm / pgtable . h <nl> extern pgprot_t phys_mem_access_prot ( struct file * file , unsigned long pfn , <nl> # define pmd_sect ( pmd ) (( pmd_val ( pmd ) & PMD_TYPE_MASK ) == \ <nl> PMD_TYPE_SECT ) <nl>  <nl> -# ifdef CONFIG_ARM64_64K_PAGES <nl> +# if defined ( CONFIG_ARM64_64K_PAGES ) || CONFIG_PGTABLE_LEVELS < 3 <nl> # define pud_sect ( pud ) ( 0 ) <nl> # define pud_table ( pud ) ( 1 ) <nl> # else
mmm drivers / usb / serial / visor . c <nl> ppp drivers / usb / serial / visor . c <nl> static int treo_attach ( struct usb_serial * serial ) <nl> ( serial -> num_interrupt_in == 0 )) <nl> return 0 ; <nl>  <nl> + if ( serial -> num_bulk_in < 2 || serial -> num_interrupt_in < 2 ) { <nl> + dev_err (& serial -> interface -> dev , " missing endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> /* <nl> * It appears that Treos and Kyoceras want to use the <nl> * 1st bulk in endpoint to communicate with the 2nd bulk out endpoint ,
mmm drivers / gpu / drm / i915 / i915_debugfs . c <nl> ppp drivers / gpu / drm / i915 / i915_debugfs . c <nl> static int i915_display_info ( struct seq_file * m , void * unused ) <nl> x , y , crtc -> cursor_addr , <nl> yesno ( active )); <nl> } <nl> + <nl> + seq_printf ( m , "\ tunderrun reporting : cpu =% s pch =% s \ n ", <nl> + yesno (! crtc -> cpu_fifo_underrun_disabled ), <nl> + yesno (! crtc -> pch_fifo_underrun_disabled )); <nl> } <nl>  <nl> seq_printf ( m , "\ n ");
mmm fs / partitions / ldm . c <nl> ppp fs / partitions / ldm . c <nl> static bool ldm_frag_add ( const u8 * data , int size , struct list_head * frags ) <nl>  <nl> list_add_tail (& f -> list , frags ); <nl> found : <nl> + if ( rec >= f -> num ) { <nl> + ldm_error (" REC value (% d ) exceeds NUM value (% d )", rec , f -> num ); <nl> + return false ; <nl> + } <nl> + <nl> if ( f -> map & ( 1 << rec )) { <nl> ldm_error (" Duplicate VBLK , part % d .", rec ); <nl> f -> map &= 0x7F ; /* Mark the group as broken */
mmm drivers / gpu / drm / amd / powerplay / eventmgr / eventtasks . c <nl> ppp drivers / gpu / drm / amd / powerplay / eventmgr / eventtasks . c <nl> int pem_task_create_user_performance_state ( struct pp_eventmgr * eventmgr , struct <nl> int pem_task_initialize_thermal_controller ( struct pp_eventmgr * eventmgr , struct pem_event_data * event_data ) <nl> { <nl> struct PP_TemperatureRange range ; <nl> + <nl> range . max = TEMP_RANGE_MAX ; <nl> range . min = TEMP_RANGE_MIN ; <nl>  <nl> - return phm_start_thermal_controller ( eventmgr -> hwmgr , & range ); <nl> + if ( eventmgr == NULL || eventmgr -> platform_descriptor == NULL ) <nl> + return - EINVAL ; <nl> + <nl> + if ( phm_cap_enabled ( eventmgr -> platform_descriptor -> platformCaps , PHM_PlatformCaps_ThermalController )) <nl> + return phm_start_thermal_controller ( eventmgr -> hwmgr , & range ); <nl> + <nl> + return 0 ; <nl> } <nl>  <nl> int pem_task_uninitialize_thermal_controller ( struct pp_eventmgr * eventmgr , struct pem_event_data * event_data )
mmm drivers / net / ethernet / intel / igb / igb_main . c <nl> ppp drivers / net / ethernet / intel / igb / igb_main . c <nl> static void igb_reset_q_vector ( struct igb_adapter * adapter , int v_idx ) <nl> { <nl> struct igb_q_vector * q_vector = adapter -> q_vector [ v_idx ]; <nl>  <nl> + /* Coming from igb_set_interrupt_capability , the vectors are not yet <nl> + * allocated . So , q_vector is NULL so we should stop here . <nl> + */ <nl> + if (! q_vector ) <nl> + return ; <nl> + <nl> if ( q_vector -> tx . ring ) <nl> adapter -> tx_ring [ q_vector -> tx . ring -> queue_index ] = NULL ; <nl> 
mmm drivers / hwmon / xgene - hwmon . c <nl> ppp drivers / hwmon / xgene - hwmon . c <nl> static int xgene_hwmon_remove ( struct platform_device * pdev ) <nl> { <nl> struct xgene_hwmon_dev * ctx = platform_get_drvdata ( pdev ); <nl>  <nl> + cancel_work_sync (& ctx -> workq ); <nl> hwmon_device_unregister ( ctx -> hwmon_dev ); <nl> kfifo_free (& ctx -> async_msg_fifo ); <nl> if ( acpi_disabled )
mmm net / netfilter / nf_conntrack_netlink . c <nl> ppp net / netfilter / nf_conntrack_netlink . c <nl> ctnetlink_create_conntrack ( struct nlattr * cda [], <nl> rcu_read_lock (); <nl> helper = __nf_ct_helper_find ( rtuple ); <nl> if ( helper ) { <nl> - help = nf_ct_helper_ext_add ( ct , GFP_KERNEL ); <nl> + help = nf_ct_helper_ext_add ( ct , GFP_ATOMIC ); <nl> if ( help == NULL ) { <nl> rcu_read_unlock (); <nl> err = - ENOMEM ;
mmm drivers / scsi / storvsc_drv . c <nl> ppp drivers / scsi / storvsc_drv . c <nl> static int storvsc_queuecommand ( struct Scsi_Host * host , struct scsi_cmnd * scmnd ) <nl> vm_srb -> data_in = READ_TYPE ; <nl> vm_srb -> win8_extension . srb_flags |= SRB_FLAGS_DATA_IN ; <nl> break ; <nl> - default : <nl> + case DMA_NONE : <nl> vm_srb -> data_in = UNKNOWN_TYPE ; <nl> vm_srb -> win8_extension . srb_flags |= SRB_FLAGS_NO_DATA_TRANSFER ; <nl> break ; <nl> + default : <nl> + /* <nl> + * This is DMA_BIDIRECTIONAL or something else we are never <nl> + * supposed to see here . <nl> + */ <nl> + WARN ( 1 , " Unexpected data direction : % d \ n ", <nl> + scmnd -> sc_data_direction ); <nl> + return - EINVAL ; <nl> } <nl>  <nl> 
mmm drivers / bluetooth / btusb . c <nl> ppp drivers / bluetooth / btusb . c <nl> static const struct usb_device_id blacklist_table [] = { <nl> { USB_DEVICE ( 0x16d3 , 0x0002 ), <nl> . driver_info = BTUSB_SNIFFER | BTUSB_BROKEN_ISOC }, <nl>  <nl> + /* Marvell Bluetooth devices */ <nl> + { USB_DEVICE ( 0x1286 , 0x2044 ), . driver_info = BTUSB_MARVELL }, <nl> + { USB_DEVICE ( 0x1286 , 0x2046 ), . driver_info = BTUSB_MARVELL }, <nl> + <nl> /* Intel Bluetooth device */ <nl> { USB_DEVICE ( 0x8087 , 0x07dc ), . driver_info = BTUSB_INTEL }, <nl> { USB_DEVICE ( 0x8087 , 0x0a2a ), . driver_info = BTUSB_INTEL }, <nl> { USB_DEVICE ( 0x8087 , 0x0a2b ), . driver_info = BTUSB_INTEL_NEW }, <nl>  <nl> - /* Marvell device */ <nl> - { USB_DEVICE ( 0x1286 , 0x2044 ), . driver_info = BTUSB_MARVELL }, <nl> - { USB_DEVICE ( 0x1286 , 0x2046 ), . driver_info = BTUSB_MARVELL }, <nl>  <nl> { } /* Terminating entry */ <nl> };
mmm drivers / net / ethernet / freescale / fman / mac . c <nl> ppp drivers / net / ethernet / freescale / fman / mac . c <nl> static int mac_probe ( struct platform_device * _of_dev ) <nl> priv -> fixed_link -> duplex = phy -> duplex ; <nl> priv -> fixed_link -> pause = phy -> pause ; <nl> priv -> fixed_link -> asym_pause = phy -> asym_pause ; <nl> + <nl> + put_device (& phy -> mdio . dev ); <nl> } <nl>  <nl> err = mac_dev -> init ( mac_dev );
mmm drivers / infiniband / core / ucma . c <nl> ppp drivers / infiniband / core / ucma . c <nl> static struct ucma_multicast * ucma_alloc_multicast ( struct ucma_context * ctx ) <nl> return NULL ; <nl>  <nl> mutex_lock (& mut ); <nl> - mc -> id = idr_alloc (& multicast_idr , mc , 0 , 0 , GFP_KERNEL ); <nl> + mc -> id = idr_alloc (& multicast_idr , NULL , 0 , 0 , GFP_KERNEL ); <nl> mutex_unlock (& mut ); <nl> if ( mc -> id < 0 ) <nl> goto error ; <nl> static ssize_t ucma_process_join ( struct ucma_file * file , <nl> goto err3 ; <nl> } <nl>  <nl> + mutex_lock (& mut ); <nl> + idr_replace (& multicast_idr , mc , mc -> id ); <nl> + mutex_unlock (& mut ); <nl> + <nl> mutex_unlock (& file -> mut ); <nl> ucma_put_ctx ( ctx ); <nl> return 0 ;
mmm drivers / media / dvb / ttpci / av7110_ca . c <nl> ppp drivers / media / dvb / ttpci / av7110_ca . c <nl> static int dvb_ca_ioctl ( struct file * file , unsigned int cmd , void * parg ) <nl> { <nl> ca_slot_info_t * info =( ca_slot_info_t *) parg ; <nl>  <nl> - if ( info -> num > 1 ) <nl> + if ( info -> num < 0 || info -> num > 1 ) <nl> return - EINVAL ; <nl> av7110 -> ci_slot [ info -> num ]. num = info -> num ; <nl> av7110 -> ci_slot [ info -> num ]. type = FW_CI_LL_SUPPORT ( av7110 -> arm_app ) ?
mmm drivers / usb / serial / visor . c <nl> ppp drivers / usb / serial / visor . c <nl> static int clie_5_attach ( struct usb_serial * serial ) <nl> */ <nl>  <nl> /* some sanity check */ <nl> - if ( serial -> num_ports < 2 ) <nl> - return - 1 ; <nl> + if ( serial -> num_bulk_out < 2 ) { <nl> + dev_err (& serial -> interface -> dev , " missing bulk out endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl>  <nl> /* port 0 now uses the modified endpoint Address */ <nl> port = serial -> port [ 0 ];
mmm drivers / cpufreq / cpufreq . c <nl> ppp drivers / cpufreq / cpufreq . c <nl> static void cpufreq_policy_free ( struct cpufreq_policy * policy ) <nl>  <nl> static void update_policy_cpu ( struct cpufreq_policy * policy , unsigned int cpu ) <nl> { <nl> + if ( cpu == policy -> cpu ) <nl> + return ; <nl> + <nl> policy -> last_cpu = policy -> cpu ; <nl> policy -> cpu = cpu ; <nl> 
mmm drivers / sbus / char / envctrl . c <nl> ppp drivers / sbus / char / envctrl . c <nl> static int kenvctrld ( void * __unused ) <nl> return - ENODEV ; <nl> } <nl>  <nl> - poll_interval = 5 * HZ ; /* TODO env_mon_interval */ <nl> + poll_interval = 5000 ; /* TODO env_mon_interval */ <nl>  <nl> daemonize (" kenvctrld "); <nl> allow_signal ( SIGKILL ); <nl> static int kenvctrld ( void * __unused ) <nl>  <nl> printk ( KERN_INFO " envctrl : % s starting ...\ n ", current -> comm ); <nl> for (;;) { <nl> - current -> state = TASK_INTERRUPTIBLE ; <nl> - schedule_timeout ( poll_interval ); <nl> - <nl> - if ( signal_pending ( current )) <nl> + if ( msleep_interruptible ( poll_interval )) <nl> break ; <nl>  <nl> for ( whichcpu = 0 ; whichcpu < ENVCTRL_MAX_CPU ; ++ whichcpu ) {
mmm tools / perf / util / probe - event . c <nl> ppp tools / perf / util / probe - event . c <nl> static struct map * kernel_get_module_map ( const char * module ) <nl> module = " kernel "; <nl>  <nl> for ( pos = maps__first ( maps ); pos ; pos = map__next ( pos )) { <nl> + /* short_name is "[ module ]" */ <nl> if ( strncmp ( pos -> dso -> short_name + 1 , module , <nl> - pos -> dso -> short_name_len - 2 ) == 0 ) { <nl> + pos -> dso -> short_name_len - 2 ) == 0 && <nl> + module [ pos -> dso -> short_name_len - 2 ] == '\ 0 ') { <nl> return pos ; <nl> } <nl> }
mmm drivers / media / i2c / s5c73m3 / s5c73m3 - core . c <nl> ppp drivers / media / i2c / s5c73m3 / s5c73m3 - core . c <nl> static int s5c73m3_probe ( struct i2c_client * client , <nl> state -> oif_pads [ OIF_ISP_PAD ]. flags = MEDIA_PAD_FL_SINK ; <nl> state -> oif_pads [ OIF_JPEG_PAD ]. flags = MEDIA_PAD_FL_SINK ; <nl> state -> oif_pads [ OIF_SOURCE_PAD ]. flags = MEDIA_PAD_FL_SOURCE ; <nl> - oif_sd -> entity . function = MEDIA_ENT_F_V4L2_SUBDEV_UNKNOWN ; <nl> + oif_sd -> entity . function = MEDIA_ENT_F_PROC_VIDEO_SCALER ; <nl>  <nl> ret = media_entity_pads_init (& oif_sd -> entity , OIF_NUM_PADS , <nl> state -> oif_pads );
mmm drivers / ide / legacy / falconide . c <nl> ppp drivers / ide / legacy / falconide . c <nl> # include < asm / atariints . h > <nl> # include < asm / atari_stdma . h > <nl>  <nl> +# define DRV_NAME " falconide " <nl>  <nl> /* <nl> * Base of the IDE interface <nl> static int __init falconide_init ( void ) <nl>  <nl> printk ( KERN_INFO " ide : Falcon IDE controller \ n "); <nl>  <nl> + if (! request_mem_region ( ATA_HD_BASE , 0x40 , DRV_NAME )) { <nl> + printk ( KERN_ERR "% s : resources busy \ n ", DRV_NAME ); <nl> + return - EBUSY ; <nl> + } <nl> + <nl> falconide_setup_ports (& hw ); <nl>  <nl> hwif = ide_find_port (); <nl> static int __init falconide_init ( void ) <nl>  <nl> ide_init_port_data ( hwif , index ); <nl> ide_init_port_hw ( hwif , & hw ); <nl> + hwif -> mmio = 1 ; <nl>  <nl> ide_get_lock ( NULL , NULL ); <nl> ide_device_add ( idx , NULL );
mmm kernel / time / timekeeping . c <nl> ppp kernel / time / timekeeping . c <nl> static struct timespec timekeeping_suspend_time ; <nl> */ <nl> static void __timekeeping_inject_sleeptime ( struct timespec * delta ) <nl> { <nl> + if (! timespec_valid ( delta )) { <nl> + printk ( KERN_WARN " __timekeeping_inject_sleeptime : Invalid " <nl> + " sleep delta value !\ n "); <nl> + return ; <nl> + } <nl> + <nl> xtime = timespec_add ( xtime , * delta ); <nl> wall_to_monotonic = timespec_sub ( wall_to_monotonic , * delta ); <nl> total_sleep_time = timespec_add ( total_sleep_time , * delta );
mmm net / mac80211 / cfg . c <nl> ppp net / mac80211 / cfg . c <nl> static int ieee80211_cfg_get_channel ( struct wiphy * wiphy , <nl> struct cfg80211_chan_def * chandef ) <nl> { <nl> struct ieee80211_sub_if_data * sdata = IEEE80211_WDEV_TO_SUB_IF ( wdev ); <nl> + struct ieee80211_local * local = wiphy_priv ( wiphy ); <nl> struct ieee80211_chanctx_conf * chanctx_conf ; <nl> int ret = - ENODATA ; <nl>  <nl> rcu_read_lock (); <nl> - chanctx_conf = rcu_dereference ( sdata -> vif . chanctx_conf ); <nl> - if ( chanctx_conf ) { <nl> - * chandef = chanctx_conf -> def ; <nl> + if ( local -> use_chanctx ) { <nl> + chanctx_conf = rcu_dereference ( sdata -> vif . chanctx_conf ); <nl> + if ( chanctx_conf ) { <nl> + * chandef = chanctx_conf -> def ; <nl> + ret = 0 ; <nl> + } <nl> + } else if ( local -> open_count == local -> monitors ) { <nl> + * chandef = local -> monitor_chandef ; <nl> ret = 0 ; <nl> } <nl> rcu_read_unlock ();
mmm net / rds / tcp . c <nl> ppp net / rds / tcp . c <nl> static void rds_tcp_kill_sock ( struct net * net ) <nl> list_for_each_entry_safe ( tc , _tc , & rds_tcp_conn_list , t_tcp_node ) { <nl> struct net * c_net = read_pnet (& tc -> t_cpath -> cp_conn -> c_net ); <nl>  <nl> - if ( net != c_net || ! tc -> t_sock ) <nl> + if ( net != c_net ) <nl> continue ; <nl> if (! list_has_conn (& tmp_list , tc -> t_cpath -> cp_conn )) { <nl> list_move_tail (& tc -> t_tcp_node , & tmp_list );
mmm drivers / net / ethernet / intel / ixgbe / ixgbe_dcb_82599 . c <nl> ppp drivers / net / ethernet / intel / ixgbe / ixgbe_dcb_82599 . c <nl> s32 ixgbe_dcb_config_pfc_82599 ( struct ixgbe_hw * hw , u8 pfc_en , u8 * prio_tc ) <nl> reg |= IXGBE_MFLCN_DPF ; <nl>  <nl> /* <nl> - * X540 supports per TC Rx priority flow control . So <nl> - * clear all TCs and only enable those that should be <nl> + * X540 & X550 supports per TC Rx priority flow control . <nl> + * So clear all TCs and only enable those that should be <nl> * enabled . <nl> */ <nl> reg &= ~( IXGBE_MFLCN_RPFCE_MASK | IXGBE_MFLCN_RFCE ); <nl>  <nl> - if ( hw -> mac . type == ixgbe_mac_X540 ) <nl> + if ( hw -> mac . type >= ixgbe_mac_X540 ) <nl> reg |= pfc_en << IXGBE_MFLCN_RPFCE_SHIFT ; <nl>  <nl> if ( pfc_en )
mmm drivers / scsi / aic94xx / aic94xx_scb . c <nl> ppp drivers / scsi / aic94xx / aic94xx_scb . c <nl> static void escb_tasklet_complete ( struct asd_ascb * ascb , <nl> tc_abort = le16_to_cpu ( tc_abort ); <nl>  <nl> list_for_each_entry_safe ( a , b , & asd_ha -> seq . pend_q , list ) { <nl> - struct sas_task * task = ascb -> uldd_task ; <nl> + struct sas_task * task = a -> uldd_task ; <nl> + <nl> + if ( a -> tc_index != tc_abort ) <nl> + continue ; <nl>  <nl> - if ( task && a -> tc_index == tc_abort ) { <nl> + if ( task ) { <nl> failed_dev = task -> dev ; <nl> sas_task_abort ( task ); <nl> - break ; <nl> + } else { <nl> + ASD_DPRINTK (" R_T_A for non TASK scb 0x % x \ n ", <nl> + a -> scb -> header . opcode ); <nl> } <nl> + break ; <nl> } <nl>  <nl> if (! failed_dev ) { <nl> static void escb_tasklet_complete ( struct asd_ascb * ascb , <nl> * that the EH will wake up and do something . <nl> */ <nl> list_for_each_entry_safe ( a , b , & asd_ha -> seq . pend_q , list ) { <nl> - struct sas_task * task = ascb -> uldd_task ; <nl> + struct sas_task * task = a -> uldd_task ; <nl>  <nl> if ( task && <nl> task -> dev == failed_dev &&
mmm arch / powerpc / platforms / cell / spufs / sched . c <nl> ppp arch / powerpc / platforms / cell / spufs / sched . c <nl> void spuctx_switch_state ( struct spu_context * ctx , <nl> node = spu -> node ; <nl> if ( old_state == SPU_UTIL_USER ) <nl> atomic_dec (& cbe_spu_info [ node ]. busy_spus ); <nl> - if ( new_state == SPU_UTIL_USER ); <nl> + if ( new_state == SPU_UTIL_USER ) <nl> atomic_inc (& cbe_spu_info [ node ]. busy_spus ); <nl> } <nl> }
mmm fs / f2fs / segment . c <nl> ppp fs / f2fs / segment . c <nl> static void __init_discard_policy ( struct f2fs_sb_info * sbi , <nl> dpolicy -> min_interval = DEF_MIN_DISCARD_ISSUE_TIME ; <nl> dpolicy -> max_interval = DEF_MAX_DISCARD_ISSUE_TIME ; <nl> dpolicy -> io_aware = true ; <nl> + dpolicy -> sync = false ; <nl> if ( utilization ( sbi ) > DEF_DISCARD_URGENT_UTIL ) { <nl> dpolicy -> granularity = 1 ; <nl> dpolicy -> max_interval = DEF_MIN_DISCARD_ISSUE_TIME ;
mmm kernel / sysctl . c <nl> ppp kernel / sysctl . c <nl> static int do_proc_dointvec_jiffies_conv ( int * negp , unsigned long * lvalp , <nl> int write , void * data ) <nl> { <nl> if ( write ) { <nl> + if (* lvalp > LONG_MAX / HZ ) <nl> + return 1 ; <nl> * valp = * negp ? -(* lvalp * HZ ) : (* lvalp * HZ ); <nl> } else { <nl> int val = * valp ; <nl> static int do_proc_dointvec_userhz_jiffies_conv ( int * negp , unsigned long * lvalp , <nl> int write , void * data ) <nl> { <nl> if ( write ) { <nl> + if ( USER_HZ < HZ && * lvalp > ( LONG_MAX / HZ ) * USER_HZ ) <nl> + return 1 ; <nl> * valp = clock_t_to_jiffies (* negp ? -* lvalp : * lvalp ); <nl> } else { <nl> int val = * valp ;
mmm arch / sh / kernel / idle . c <nl> ppp arch / sh / kernel / idle . c <nl> void cpu_idle ( void ) <nl> local_irq_disable (); <nl> /* Don ' t trace irqs off for idle */ <nl> stop_critical_timings (); <nl> - if ( cpuidle_call_idle ()) <nl> + if ( cpuidle_idle_call ()) <nl> pm_idle (); <nl> /* <nl> * Sanity check to ensure that pm_idle () returnsmmm arch / arm / kernel / process . c <nl> ppp arch / arm / kernel / process . c <nl> void cpu_idle ( void ) <nl> local_irq_disable (); <nl> /* Don ' t trace irqs off for idle */ <nl> stop_critical_timings (); <nl> - if ( cpuidle_call_idle ()) <nl> + if ( cpuidle_idle_call ()) <nl> pm_idle (); <nl> /* <nl> * Sanity check to ensure that pm_idle () returns <nl> void cpu_idle ( void ) <nl> cpu_relax (); <nl> } else { <nl> stop_critical_timings (); <nl> - if ( cpuidle_call_idle ()) <nl> + if ( cpuidle_idle_call ()) <nl> pm_idle (); <nl> start_critical_timings (); <nl> /*
mmm crypto / shash . c <nl> ppp crypto / shash . c <nl> int shash_ahash_finup ( struct ahash_request * req , struct shash_desc * desc ) <nl> struct crypto_hash_walk walk ; <nl> int nbytes ; <nl>  <nl> - for ( nbytes = crypto_hash_walk_first ( req , & walk ); nbytes > 0 ; <nl> - nbytes = crypto_hash_walk_done (& walk , nbytes )) <nl> + nbytes = crypto_hash_walk_first ( req , & walk ); <nl> + if (! nbytes ) <nl> + return crypto_shash_final ( desc , req -> result ); <nl> + <nl> + do { <nl> nbytes = crypto_hash_walk_last (& walk ) ? <nl> crypto_shash_finup ( desc , walk . data , nbytes , <nl> req -> result ) : <nl> crypto_shash_update ( desc , walk . data , nbytes ); <nl> + nbytes = crypto_hash_walk_done (& walk , nbytes ); <nl> + } while ( nbytes > 0 ); <nl>  <nl> return nbytes ; <nl> }
mmm drivers / mmc / host / meson - gx - mmc . c <nl> ppp drivers / mmc / host / meson - gx - mmc . c <nl> static int meson_mmc_probe ( struct platform_device * pdev ) <nl> } <nl>  <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if (! irq ) { <nl> + if ( irq <= 0 ) { <nl> dev_err (& pdev -> dev , " failed to get interrupt resource .\ n "); <nl> ret = - EINVAL ; <nl> goto free_host ;
mmm fs / btrfs / ioctl . c <nl> ppp fs / btrfs / ioctl . c <nl> int btrfs_defrag_file ( struct inode * inode , struct file * file , <nl>  <nl> defrag_count += ret ; <nl> balance_dirty_pages_ratelimited_nr ( inode -> i_mapping , ret ); <nl> - i += ret ; <nl>  <nl> if ( newer_than ) { <nl> if ( newer_off == ( u64 )- 1 ) <nl> int btrfs_defrag_file ( struct inode * inode , struct file * file , <nl> break ; <nl> } <nl> } else { <nl> - i ++; <nl> + if ( ret > 0 ) <nl> + i += ret ; <nl> + else <nl> + i ++; <nl> } <nl> } <nl> 
mmm kernel / time / timekeeping . c <nl> ppp kernel / time / timekeeping . c <nl> static void tk_setup_internals ( struct timekeeper * tk , struct clocksource * clock ) <nl> tk -> cycle_interval = interval ; <nl>  <nl> /* Go back from cycles -> shifted ns */ <nl> - tk -> xtime_interval = ( u64 ) interval * clock -> mult ; <nl> + tk -> xtime_interval = interval * clock -> mult ; <nl> tk -> xtime_remainder = ntpinterval - tk -> xtime_interval ; <nl> - tk -> raw_interval = <nl> - (( u64 ) interval * clock -> mult ) >> clock -> shift ; <nl> + tk -> raw_interval = ( interval * clock -> mult ) >> clock -> shift ; <nl>  <nl> /* if changing clocks , convert xtime_nsec shift units */ <nl> if ( old_clock ) {
mmm arch / x86 / kvm / svm . c <nl> ppp arch / x86 / kvm / svm . c <nl> static void init_vmcb ( struct vcpu_svm * svm ) <nl> set_exception_intercept ( svm , UD_VECTOR ); <nl> set_exception_intercept ( svm , MC_VECTOR ); <nl> set_exception_intercept ( svm , AC_VECTOR ); <nl> + set_exception_intercept ( svm , DB_VECTOR ); <nl>  <nl> set_intercept ( svm , INTERCEPT_INTR ); <nl> set_intercept ( svm , INTERCEPT_NMI ); <nl> static void svm_set_segment ( struct kvm_vcpu * vcpu , <nl> mark_dirty ( svm -> vmcb , VMCB_SEG ); <nl> } <nl>  <nl> - static void update_db_bp_intercept ( struct kvm_vcpu * vcpu ) <nl> + static void update_bp_intercept ( struct kvm_vcpu * vcpu ) <nl> { <nl> struct vcpu_svm * svm = to_svm ( vcpu ); <nl>  <nl> - clr_exception_intercept ( svm , DB_VECTOR ); <nl> clr_exception_intercept ( svm , BP_VECTOR ); <nl>  <nl> - if ( svm -> nmi_singlestep ) <nl> - set_exception_intercept ( svm , DB_VECTOR ); <nl> - <nl> if ( vcpu -> guest_debug & KVM_GUESTDBG_ENABLE ) { <nl> - if ( vcpu -> guest_debug & <nl> - ( KVM_GUESTDBG_SINGLESTEP | KVM_GUESTDBG_USE_HW_BP )) <nl> - set_exception_intercept ( svm , DB_VECTOR ); <nl> if ( vcpu -> guest_debug & KVM_GUESTDBG_USE_SW_BP ) <nl> set_exception_intercept ( svm , BP_VECTOR ); <nl> } else <nl> static int db_interception ( struct vcpu_svm * svm ) <nl> if (!( svm -> vcpu . guest_debug & KVM_GUESTDBG_SINGLESTEP )) <nl> svm -> vmcb -> save . rflags &= <nl> ~( X86_EFLAGS_TF | X86_EFLAGS_RF ); <nl> - update_db_bp_intercept (& svm -> vcpu ); <nl> } <nl>  <nl> if ( svm -> vcpu . guest_debug & <nl> static void enable_nmi_window ( struct kvm_vcpu * vcpu ) <nl> */ <nl> svm -> nmi_singlestep = true ; <nl> svm -> vmcb -> save . rflags |= ( X86_EFLAGS_TF | X86_EFLAGS_RF ); <nl> - update_db_bp_intercept ( vcpu ); <nl> } <nl>  <nl> static int svm_set_tss_addr ( struct kvm * kvm , unsigned int addr ) <nl> static struct kvm_x86_ops svm_x86_ops = { <nl> . vcpu_load = svm_vcpu_load , <nl> . vcpu_put = svm_vcpu_put , <nl>  <nl> - . update_db_bp_intercept = update_db_bp_intercept , <nl> + . update_db_bp_intercept = update_bp_intercept , <nl> . get_msr = svm_get_msr , <nl> . set_msr = svm_set_msr , <nl> . get_segment_base = svm_get_segment_base ,
mmm drivers / staging / comedi / comedi_fops . c <nl> ppp drivers / staging / comedi / comedi_fops . c <nl> static void comedi_set_subdevice_runflags ( struct comedi_subdevice * s , <nl> } <nl>  <nl> static int do_cmd_ioctl ( struct comedi_device * dev , <nl> - struct comedi_cmd __user * cmd , void * file ) <nl> + struct comedi_cmd __user * arg , void * file ) <nl> { <nl> struct comedi_cmd user_cmd ; <nl> struct comedi_subdevice * s ; <nl> static int do_cmd_ioctl ( struct comedi_device * dev , <nl> int ret = 0 ; <nl> unsigned int __user * chanlist_saver = NULL ; <nl>  <nl> - if ( copy_from_user (& user_cmd , cmd , sizeof ( struct comedi_cmd ))) { <nl> + if ( copy_from_user (& user_cmd , arg , sizeof ( struct comedi_cmd ))) { <nl> DPRINTK (" bad cmd address \ n "); <nl> return - EFAULT ; <nl> } <nl> static int do_cmd_ioctl ( struct comedi_device * dev , <nl> /* restore chanlist pointer before copying back */ <nl> user_cmd . chanlist = chanlist_saver ; <nl> user_cmd . data = NULL ; <nl> - if ( copy_to_user ( cmd , & user_cmd , sizeof ( struct comedi_cmd ))) { <nl> + if ( copy_to_user ( arg , & user_cmd , sizeof ( struct comedi_cmd ))) { <nl> DPRINTK (" fault writing cmd \ n "); <nl> ret = - EFAULT ; <nl> goto cleanup ;
mmm net / ipv4 / ip_fragment . c <nl> ppp net / ipv4 / ip_fragment . c <nl> static int ip_frag_reasm ( struct ipq * qp , struct sk_buff * prev , <nl> skb_morph ( head , qp -> q . fragments ); <nl> head -> next = qp -> q . fragments -> next ; <nl>  <nl> - kfree_skb ( qp -> q . fragments ); <nl> + consume_skb ( qp -> q . fragments ); <nl> qp -> q . fragments = head ; <nl> } <nl> 
mmm include / asm - m68k / sun3mmu . h <nl> ppp include / asm - m68k / sun3mmu . h <nl> # ifndef __SUN3_MMU_H__ <nl> # define __SUN3_MMU_H__ <nl>  <nl> +# include < linux / types . h > <nl> # include < asm / movs . h > <nl> # include < asm / sun3 - head . h > <nl>  <nl> static inline void sun3_put_context ( unsigned char c ) <nl> return ; <nl> } <nl>  <nl> - extern void * sun3_ioremap ( unsigned long phys , unsigned long size , <nl> + extern void __iomem * sun3_ioremap ( unsigned long phys , unsigned long size , <nl> unsigned long type ); <nl>  <nl> extern int sun3_map_test ( unsigned long addr , char * val );
mmm drivers / usb / serial / spcp8x5 . c <nl> ppp drivers / usb / serial / spcp8x5 . c <nl> static int spcp8x5_probe ( struct usb_serial * serial , <nl> return 0 ; <nl> } <nl>  <nl> + static int spcp8x5_attach ( struct usb_serial * serial ) <nl> +{ <nl> + unsigned char num_ports = serial -> num_ports ; <nl> + <nl> + if ( serial -> num_bulk_in < num_ports || <nl> + serial -> num_bulk_out < num_ports ) { <nl> + dev_err (& serial -> interface -> dev , " missing endpoints \ n "); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> + return 0 ; <nl> +} <nl> + <nl> static int spcp8x5_port_probe ( struct usb_serial_port * port ) <nl> { <nl> const struct usb_device_id * id = usb_get_serial_data ( port -> serial ); <nl> static struct usb_serial_driver spcp8x5_device = { <nl> . tiocmget = spcp8x5_tiocmget , <nl> . tiocmset = spcp8x5_tiocmset , <nl> . probe = spcp8x5_probe , <nl> + . attach = spcp8x5_attach , <nl> . port_probe = spcp8x5_port_probe , <nl> . port_remove = spcp8x5_port_remove , <nl> };
mmm drivers / kvm / kvm_main . c <nl> ppp drivers / kvm / kvm_main . c <nl> static long kvm_dev_ioctl ( struct file * filp , <nl> num_msrs_to_save * sizeof ( u32 ))) <nl> goto out ; <nl> r = 0 ; <nl> + break ; <nl> } <nl> default : <nl> ;
mmm drivers / idle / i7300_idle . c <nl> ppp drivers / idle / i7300_idle . c <nl> static u8 i7300_idle_thrtctl_saved ; <nl> static u8 i7300_idle_thrtlow_saved ; <nl> static u32 i7300_idle_mc_saved ; <nl>  <nl> - static cpumask_t idle_cpumask ; <nl> + static cpumask_var_t idle_cpumask ; <nl> static ktime_t start_ktime ; <nl> static unsigned long avg_idle_us ; <nl>  <nl> static int i7300_idle_notifier ( struct notifier_block * nb , unsigned long val , <nl> spin_lock_irqsave (& i7300_idle_lock , flags ); <nl> if ( val == IDLE_START ) { <nl>  <nl> - cpu_set ( smp_processor_id (), idle_cpumask ); <nl> + cpumask_set_cpu ( smp_processor_id (), idle_cpumask ); <nl>  <nl> - if ( cpus_weight ( idle_cpumask ) != num_online_cpus ()) <nl> + if ( cpumask_weight ( idle_cpumask ) != num_online_cpus ()) <nl> goto end ; <nl>  <nl> now_ktime = ktime_get (); <nl> static int i7300_idle_notifier ( struct notifier_block * nb , unsigned long val , <nl> i7300_idle_ioat_start (); <nl>  <nl> } else if ( val == IDLE_END ) { <nl> - cpu_clear ( smp_processor_id (), idle_cpumask ); <nl> - if ( cpus_weight ( idle_cpumask ) == ( num_online_cpus () - 1 )) { <nl> + cpumask_clear_cpu ( smp_processor_id (), idle_cpumask ); <nl> + if ( cpumask_weight ( idle_cpumask ) == ( num_online_cpus () - 1 )) { <nl> /* First CPU coming out of idle */ <nl> u64 idle_duration_us ; <nl>  <nl> struct debugfs_file_info { <nl> static int __init i7300_idle_init ( void ) <nl> { <nl> spin_lock_init (& i7300_idle_lock ); <nl> - cpus_clear ( idle_cpumask ); <nl> total_us = 0 ; <nl>  <nl> if ( i7300_idle_platform_probe (& fbd_dev , & ioat_dev , forceload )) <nl> static int __init i7300_idle_init ( void ) <nl> if ( i7300_idle_ioat_init ()) <nl> return - ENODEV ; <nl>  <nl> + if (! zalloc_cpumask_var (& idle_cpumask , GFP_KERNEL )) <nl> + return - ENOMEM ; <nl> + <nl> debugfs_dir = debugfs_create_dir (" i7300_idle ", NULL ); <nl> if ( debugfs_dir ) { <nl> int i = 0 ; <nl> static int __init i7300_idle_init ( void ) <nl> static void __exit i7300_idle_exit ( void ) <nl> { <nl> idle_notifier_unregister (& i7300_idle_nb ); <nl> + free_cpumask_var ( idle_cpumask ); <nl>  <nl> if ( debugfs_dir ) { <nl> int i = 0 ;
mmm drivers / pci / probe . c <nl> ppp drivers / pci / probe . c <nl> static int __pci_read_base ( struct pci_dev * dev , enum pci_bar_type type , <nl> if (! sz64 ) <nl> goto fail ; <nl>  <nl> - if (( BITS_PER_LONG < 64 ) && ( sz64 > 0x100000000ULL )) { <nl> + if (( sizeof ( resource_size_t ) < 8 ) && ( sz64 > 0x100000000ULL )) { <nl> dev_err (& dev -> dev , " can ' t handle 64 - bit BAR \ n "); <nl> goto fail ; <nl> - } else if (( BITS_PER_LONG < 64 ) && l ) { <nl> + } else if (( sizeof ( resource_size_t ) < 8 ) && l ) { <nl> /* Address above 32 - bit boundary ; disable the BAR */ <nl> pci_write_config_dword ( dev , pos , 0 ); <nl> pci_write_config_dword ( dev , pos + 4 , 0 );
mmm drivers / rtc / rtc - isl1208 . c <nl> ppp drivers / rtc / rtc - isl1208 . c <nl> isl1208_i2c_set_time ( struct i2c_client * client , struct rtc_time const * tm ) <nl> int sr ; <nl> u8 regs [ ISL1208_RTC_SECTION_LEN ] = { 0 , }; <nl>  <nl> + /* The clock has an 8 bit wide bcd - coded register ( they never learn ) <nl> + * for the year . tm_year is an offset from 1900 and we are interested <nl> + * in the 2000 - 2099 range , so any value less than 100 is invalid . <nl> + */ <nl> + if ( tm -> tm_year < 100 ) <nl> + return - EINVAL ; <nl> + <nl> regs [ ISL1208_REG_SC ] = bin2bcd ( tm -> tm_sec ); <nl> regs [ ISL1208_REG_MN ] = bin2bcd ( tm -> tm_min ); <nl> regs [ ISL1208_REG_HR ] = bin2bcd ( tm -> tm_hour ) | ISL1208_REG_HR_MIL ;
mmm arch / x86 / math - emu / fpu_entry . c <nl> ppp arch / x86 / math - emu / fpu_entry . c <nl> asmlinkage void math_emulate ( long arg ) <nl> entry_sel_off . offset = FPU_ORIG_EIP ; <nl> entry_sel_off . selector = FPU_CS ; <nl> entry_sel_off . opcode = ( byte1 << 8 ) | FPU_modrm ; <nl> + entry_sel_off . empty = 0 ; <nl>  <nl> FPU_rm = FPU_modrm & 7 ; <nl> 
mmm drivers / cpufreq / imx6q - cpufreq . c <nl> ppp drivers / cpufreq / imx6q - cpufreq . c <nl> static struct clk * step_clk ; <nl> static struct clk * pll2_pfd2_396m_clk ; <nl>  <nl> static struct device * cpu_dev ; <nl> + static bool free_opp ; <nl> static struct cpufreq_frequency_table * freq_table ; <nl> static unsigned int transition_latency ; <nl>  <nl> static int imx6q_cpufreq_probe ( struct platform_device * pdev ) <nl> goto put_reg ; <nl> } <nl>  <nl> + /* Because we have added the OPPs here , we must free them */ <nl> + free_opp = true ; <nl> + <nl> num = dev_pm_opp_get_opp_count ( cpu_dev ); <nl> if ( num < 0 ) { <nl> ret = num ; <nl> dev_err ( cpu_dev , " no OPP table is found : % d \ n ", ret ); <nl> - goto put_reg ; <nl> + goto out_free_opp ; <nl> } <nl> } <nl>  <nl> static int imx6q_cpufreq_probe ( struct platform_device * pdev ) <nl>  <nl> free_freq_table : <nl> dev_pm_opp_free_cpufreq_table ( cpu_dev , & freq_table ); <nl> + out_free_opp : <nl> + if ( free_opp ) <nl> + of_free_opp_table ( cpu_dev ); <nl> put_reg : <nl> if (! IS_ERR ( arm_reg )) <nl> regulator_put ( arm_reg ); <nl> static int imx6q_cpufreq_remove ( struct platform_device * pdev ) <nl> { <nl> cpufreq_unregister_driver (& imx6q_cpufreq_driver ); <nl> dev_pm_opp_free_cpufreq_table ( cpu_dev , & freq_table ); <nl> + if ( free_opp ) <nl> + of_free_opp_table ( cpu_dev ); <nl> regulator_put ( arm_reg ); <nl> if (! IS_ERR ( pu_reg )) <nl> regulator_put ( pu_reg );
mmm drivers / video / sunxvr500 . c <nl> ppp drivers / video / sunxvr500 . c <nl> static int __devinit e3d_pci_register ( struct pci_dev * pdev , <nl> if ( err < 0 ) { <nl> printk ( KERN_ERR " e3d : Could not register framebuffer % s \ n ", <nl> pci_name ( pdev )); <nl> - goto err_unmap_fb ; <nl> + goto err_free_cmap ; <nl> } <nl>  <nl> return 0 ; <nl>  <nl> + err_free_cmap : <nl> + fb_dealloc_cmap (& info -> cmap ); <nl> + <nl> err_unmap_fb : <nl> iounmap ( ep -> fb_base ); <nl>  <nl> static void __devexit e3d_pci_unregister ( struct pci_dev * pdev ) <nl> pci_release_region ( pdev , 0 ); <nl> pci_release_region ( pdev , 1 ); <nl>  <nl> + fb_dealloc_cmap (& info -> cmap ); <nl> framebuffer_release ( info ); <nl>  <nl> pci_disable_device ( pdev );
mmm net / core / sock . c <nl> ppp net / core / sock . c <nl> struct sk_buff * sock_alloc_send_pskb ( struct sock * sk , unsigned long header_len , <nl> gfp_t gfp_mask ; <nl> long timeo ; <nl> int err ; <nl> + int npages = ( data_len + ( PAGE_SIZE - 1 )) >> PAGE_SHIFT ; <nl> + <nl> + err = - EMSGSIZE ; <nl> + if ( npages > MAX_SKB_FRAGS ) <nl> + goto failure ; <nl>  <nl> gfp_mask = sk -> sk_allocation ; <nl> if ( gfp_mask & __GFP_WAIT ) <nl> struct sk_buff * sock_alloc_send_pskb ( struct sock * sk , unsigned long header_len , <nl> if ( atomic_read (& sk -> sk_wmem_alloc ) < sk -> sk_sndbuf ) { <nl> skb = alloc_skb ( header_len , gfp_mask ); <nl> if ( skb ) { <nl> - int npages ; <nl> int i ; <nl>  <nl> /* No pages , we ' re done ... */ <nl> if (! data_len ) <nl> break ; <nl>  <nl> - npages = ( data_len + ( PAGE_SIZE - 1 )) >> PAGE_SHIFT ; <nl> skb -> truesize += data_len ; <nl> skb_shinfo ( skb )-> nr_frags = npages ; <nl> for ( i = 0 ; i < npages ; i ++) {
mmm drivers / gpu / drm / radeon / radeon_device . c <nl> ppp drivers / gpu / drm / radeon / radeon_device . c <nl> int radeon_suspend_kms ( struct drm_device * dev , bool suspend , <nl> radeon_agp_suspend ( rdev ); <nl>  <nl> pci_save_state ( dev -> pdev ); <nl> - if ( freeze && rdev -> family >= CHIP_R600 ) { <nl> + if ( freeze && rdev -> family >= CHIP_CEDAR ) { <nl> rdev -> asic -> asic_reset ( rdev , true ); <nl> pci_restore_state ( dev -> pdev ); <nl> } else if ( suspend ) {
mmm fs / dlm / ast . c <nl> ppp fs / dlm / ast . c <nl> void dlm_add_cb ( struct dlm_lkb * lkb , uint32_t flags , int mode , int status , <nl>  <nl> spin_lock (& dlm_cb_seq_spin ); <nl> new_seq = ++ dlm_cb_seq ; <nl> + if (! dlm_cb_seq ) <nl> + new_seq = ++ dlm_cb_seq ; <nl> spin_unlock (& dlm_cb_seq_spin ); <nl>  <nl> if ( lkb -> lkb_flags & DLM_IFL_USER ) {
mmm drivers / staging / bcm / nvm . c <nl> ppp drivers / staging / bcm / nvm . c <nl> int BcmGetSectionValEndOffset ( struct bcm_mini_adapter * Adapter , enum bcm_flash2x <nl> case CONTROL_SECTION : <nl> /* Not Clear So Putting failure . confirm and fix it . */ <nl> SectEndOffset = STATUS_FAILURE ; <nl> + break ; <nl> case ISO_IMAGE1_PART2 : <nl> if ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End != UNINIT_PTR_IN_CS ) <nl> SectEndOffset = ( Adapter -> psFlash2xCSInfo -> OffsetISOImage1Part2End );
mmm drivers / dma / at_hdmac . c <nl> ppp drivers / dma / at_hdmac . c <nl> static int at_dma_remove ( struct platform_device * pdev ) <nl>  <nl> /* Disable interrupts */ <nl> atc_disable_chan_irq ( atdma , chan -> chan_id ); <nl> - tasklet_disable (& atchan -> tasklet ); <nl>  <nl> tasklet_kill (& atchan -> tasklet ); <nl> list_del (& chan -> device_node );
mmm net / ipv6 / netfilter / ip6_tables . c <nl> ppp net / ipv6 / netfilter / ip6_tables . c <nl> static int get_info ( struct net * net , void __user * user , <nl> private = & tmp ; <nl> } <nl> # endif <nl> + memset (& info , 0 , sizeof ( info )); <nl> info . valid_hooks = t -> valid_hooks ; <nl> memcpy ( info . hook_entry , private -> hook_entry , <nl> sizeof ( info . hook_entry ));
mmm arch / x86 / include / asm / mmu_context . h <nl> ppp arch / x86 / include / asm / mmu_context . h <nl> static inline int init_new_context ( struct task_struct * tsk , <nl> mm -> context . execute_only_pkey = - 1 ; <nl> } <nl> # endif <nl> - init_new_context_ldt ( tsk , mm ); <nl> - <nl> - return 0 ; <nl> + return init_new_context_ldt ( tsk , mm ); <nl> } <nl> static inline void destroy_context ( struct mm_struct * mm ) <nl> {
mmm drivers / staging / omapdrm / omap_gem_dmabuf . c <nl> ppp drivers / staging / omapdrm / omap_gem_dmabuf . c <nl> static struct sg_table * omap_gem_map_dma_buf ( <nl> /* this should be after _get_paddr () to ensure we have pages attached */ <nl> omap_gem_dma_sync ( obj , dir ); <nl>  <nl> - out : <nl> - if ( ret ) <nl> - return ERR_PTR ( ret ); <nl> return sg ; <nl> + out : <nl> + kfree ( sg ); <nl> + return ERR_PTR ( ret ); <nl> } <nl>  <nl> static void omap_gem_unmap_dma_buf ( struct dma_buf_attachment * attachment ,
mmm drivers / gpu / drm / i915 / intel_panel . c <nl> ppp drivers / gpu / drm / i915 / intel_panel . c <nl> cnp_setup_backlight ( struct intel_connector * connector , enum pipe unused ) <nl> u32 pwm_ctl , val ; <nl>  <nl> /* <nl> - * CNP has the BXT implementation of backlight , but with only <nl> - * one controller . Future platforms could have multiple controllers <nl> - * so let ' s make this extensible and prepared for the future . <nl> + * CNP has the BXT implementation of backlight , but with only one <nl> + * controller . TODO : ICP has multiple controllers but we only use <nl> + * controller 0 for now . <nl> */ <nl> panel -> backlight . controller = 0 ; <nl>  <nl> intel_panel_init_backlight_funcs ( struct intel_panel * panel ) <nl> panel -> backlight . set = bxt_set_backlight ; <nl> panel -> backlight . get = bxt_get_backlight ; <nl> panel -> backlight . hz_to_pwm = bxt_hz_to_pwm ; <nl> - } else if ( HAS_PCH_CNP ( dev_priv )) { <nl> + } else if ( HAS_PCH_CNP ( dev_priv ) || HAS_PCH_ICP ( dev_priv )) { <nl> panel -> backlight . setup = cnp_setup_backlight ; <nl> panel -> backlight . enable = cnp_enable_backlight ; <nl> panel -> backlight . disable = cnp_disable_backlight ;
mmm net / ipv4 / tcp . c <nl> ppp net / ipv4 / tcp . c <nl> ssize_t tcp_splice_read ( struct socket * sock , loff_t * ppos , <nl> ret = - EAGAIN ; <nl> break ; <nl> } <nl> + /* if __tcp_splice_read () got nothing while we have <nl> + * an skb in receive queue , we do not want to loop . <nl> + * This might happen with URG data . <nl> + */ <nl> + if (! skb_queue_empty (& sk -> sk_receive_queue )) <nl> + break ; <nl> sk_wait_data ( sk , & timeo , NULL ); <nl> if ( signal_pending ( current )) { <nl> ret = sock_intr_errno ( timeo );
mmm drivers / net / ethernet / renesas / ravb_main . c <nl> ppp drivers / net / ethernet / renesas / ravb_main . c <nl> static int ravb_close ( struct net_device * ndev ) <nl> priv -> phydev = NULL ; <nl> } <nl>  <nl> - if ( priv -> chip_id == RCAR_GEN3 ) <nl> + if ( priv -> chip_id != RCAR_GEN2 ) { <nl> + free_irq ( priv -> tx_irqs [ RAVB_NC ], ndev ); <nl> + free_irq ( priv -> rx_irqs [ RAVB_NC ], ndev ); <nl> + free_irq ( priv -> tx_irqs [ RAVB_BE ], ndev ); <nl> + free_irq ( priv -> rx_irqs [ RAVB_BE ], ndev ); <nl> free_irq ( priv -> emac_irq , ndev ); <nl> + } <nl> free_irq ( ndev -> irq , ndev ); <nl>  <nl> napi_disable (& priv -> napi [ RAVB_NC ]);
mmm drivers / net / wireless / realtek / rtl8xxxu / rtl8xxxu . c <nl> ppp drivers / net / wireless / realtek / rtl8xxxu / rtl8xxxu . c <nl> static int rtl8192eu_parse_efuse ( struct rtl8xxxu_priv * priv ) <nl> raw [ i + 6 ], raw [ i + 7 ]); <nl> } <nl> } <nl> + /* <nl> + * Temporarily disable 8192eu support <nl> + */ <nl> + return - EINVAL ; <nl> return 0 ; <nl> } <nl> 
mmm arch / x86 / crypto / aesni - intel_glue . c <nl> ppp arch / x86 / crypto / aesni - intel_glue . c <nl> static int __driver_rfc4106_decrypt ( struct aead_request * req ) <nl> src = kmalloc ( req -> cryptlen + req -> assoclen , GFP_ATOMIC ); <nl> if (! src ) <nl> return - ENOMEM ; <nl> - assoc = ( src + req -> cryptlen + auth_tag_len ); <nl> + assoc = ( src + req -> cryptlen ); <nl> scatterwalk_map_and_copy ( src , req -> src , 0 , req -> cryptlen , 0 ); <nl> scatterwalk_map_and_copy ( assoc , req -> assoc , 0 , <nl> req -> assoclen , 0 ); <nl> static int __driver_rfc4106_decrypt ( struct aead_request * req ) <nl> scatterwalk_done (& src_sg_walk , 0 , 0 ); <nl> scatterwalk_done (& assoc_sg_walk , 0 , 0 ); <nl> } else { <nl> - scatterwalk_map_and_copy ( dst , req -> dst , 0 , req -> cryptlen , 1 ); <nl> + scatterwalk_map_and_copy ( dst , req -> dst , 0 , tempCipherLen , 1 ); <nl> kfree ( src ); <nl> } <nl> return retval ;
mmm drivers / net / igb / igb_main . c <nl> ppp drivers / net / igb / igb_main . c <nl> int igb_set_spd_dplx ( struct igb_adapter * adapter , u16 spddplx ) <nl>  <nl> mac -> autoneg = 0 ; <nl>  <nl> + /* Fiber NIC ' s only allow 1000 Gbps Full duplex */ <nl> + if (( adapter -> hw . phy . media_type == e1000_media_type_internal_serdes ) && <nl> + spddplx != ( SPEED_1000 + DUPLEX_FULL )) { <nl> + dev_err (& pdev -> dev , " Unsupported Speed / Duplex configuration \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> switch ( spddplx ) { <nl> case SPEED_10 + DUPLEX_HALF : <nl> mac -> forced_speed_duplex = ADVERTISE_10_HALF ;
mmm net / kcm / kcmsock . c <nl> ppp net / kcm / kcmsock . c <nl> static int kcm_sendmsg ( struct socket * sock , struct msghdr * msg , size_t len ) <nl> } else { <nl> /* Message not complete , save state */ <nl> partial_message : <nl> - kcm -> seq_skb = head ; <nl> - kcm_tx_msg ( head )-> last_skb = skb ; <nl> + if ( head ) { <nl> + kcm -> seq_skb = head ; <nl> + kcm_tx_msg ( head )-> last_skb = skb ; <nl> + } <nl> } <nl>  <nl> KCM_STATS_ADD ( kcm -> stats . tx_bytes , copied );
mmm drivers / spi / spi - orion . c <nl> ppp drivers / spi / spi - orion . c <nl> static int orion_spi_transfer_one_message ( struct spi_master * master , <nl> goto msg_done ; <nl>  <nl> list_for_each_entry ( t , & m -> transfers , transfer_list ) { <nl> - /* make sure buffer length is even when working in 16 <nl> - * bit mode */ <nl> - if (( t -> bits_per_word == 16 ) && ( t -> len & 1 )) { <nl> - dev_err (& spi -> dev , <nl> - " message rejected : " <nl> - " odd data length % d while in 16 bit mode \ n ", <nl> - t -> len ); <nl> - status = - EIO ; <nl> - goto msg_done ; <nl> - } <nl> - <nl> if ( par_override || t -> speed_hz || t -> bits_per_word ) { <nl> par_override = 1 ; <nl> status = orion_spi_setup_transfer ( spi , t );
mmm net / wireless / scan . c <nl> ppp net / wireless / scan . c <nl> static int cmp_ies ( u8 num , u8 * ies1 , size_t len1 , u8 * ies2 , size_t len2 ) <nl>  <nl> if (! ie1 && ! ie2 ) <nl> return 0 ; <nl> - if (! ie1 ) <nl> + if (! ie1 || ! ie2 ) <nl> return - 1 ; <nl>  <nl> r = memcmp ( ie1 + 2 , ie2 + 2 , min ( ie1 [ 1 ], ie2 [ 1 ])); <nl> static bool is_mesh ( struct cfg80211_bss * a , <nl> ie = find_ie ( WLAN_EID_MESH_CONFIG , <nl> a -> information_elements , <nl> a -> len_information_elements ); <nl> + if (! ie ) <nl> + return false ; <nl> if ( ie [ 1 ] != IEEE80211_MESH_CONFIG_LEN ) <nl> return false ; <nl> 
mmm fs / f2fs / data . c <nl> ppp fs / f2fs / data . c <nl> static int f2fs_write_begin ( struct file * file , struct address_space * mapping , <nl>  <nl> /* check inline_data */ <nl> ipage = get_node_page ( sbi , inode -> i_ino ); <nl> - if ( IS_ERR ( ipage )) <nl> + if ( IS_ERR ( ipage )) { <nl> + err = PTR_ERR ( ipage ); <nl> goto unlock_fail ; <nl> + } <nl>  <nl> set_new_dnode (& dn , inode , ipage , ipage , 0 ); <nl> 
mmm fs / namespace . c <nl> ppp fs / namespace . c <nl> struct vfsmount * collect_mounts ( struct path * path ) <nl> { <nl> struct mount * tree ; <nl> namespace_lock (); <nl> - tree = copy_tree ( real_mount ( path -> mnt ), path -> dentry , <nl> - CL_COPY_ALL | CL_PRIVATE ); <nl> + if (! check_mnt ( real_mount ( path -> mnt ))) <nl> + tree = ERR_PTR (- EINVAL ); <nl> + else <nl> + tree = copy_tree ( real_mount ( path -> mnt ), path -> dentry , <nl> + CL_COPY_ALL | CL_PRIVATE ); <nl> namespace_unlock (); <nl> if ( IS_ERR ( tree )) <nl> return ERR_CAST ( tree );
mmm drivers / net / wireless / intel / iwlwifi / mvm / sta . c <nl> ppp drivers / net / wireless / intel / iwlwifi / mvm / sta . c <nl> int iwl_mvm_remove_sta_key ( struct iwl_mvm * mvm , <nl>  <nl> /* Get the station from the mvm local station table */ <nl> mvm_sta = iwl_mvm_get_key_sta ( mvm , vif , sta ); <nl> + if (! mvm_sta ) { <nl> + IWL_ERR ( mvm , " Failed to find station \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + sta_id = mvm_sta -> sta_id ; <nl>  <nl> IWL_DEBUG_WEP ( mvm , " mvm remove dynamic key : idx =% d sta =% d \ n ", <nl> keyconf -> keyidx , sta_id ); <nl> int iwl_mvm_remove_sta_key ( struct iwl_mvm * mvm , <nl> return 0 ; <nl> } <nl>  <nl> - sta_id = mvm_sta -> sta_id ; <nl> - <nl> ret = __iwl_mvm_remove_sta_key ( mvm , sta_id , keyconf , mcast ); <nl> if ( ret ) <nl> return ret ;
mmm sound / pci / hda / hda_generic . c <nl> ppp sound / pci / hda / hda_generic . c <nl> static int fill_and_eval_dacs ( struct hda_codec * codec , <nl> memset ( spec -> multiout . extra_out_nid , 0 , sizeof ( spec -> multiout . extra_out_nid )); <nl> spec -> multi_ios = 0 ; <nl> snd_array_free (& spec -> paths ); <nl> + <nl> + /* clear path indices */ <nl> + memset ( spec -> out_paths , 0 , sizeof ( spec -> out_paths )); <nl> + memset ( spec -> hp_paths , 0 , sizeof ( spec -> hp_paths )); <nl> + memset ( spec -> speaker_paths , 0 , sizeof ( spec -> speaker_paths )); <nl> + memset ( spec -> aamix_out_paths , 0 , sizeof ( spec -> aamix_out_paths )); <nl> + memset ( spec -> digout_paths , 0 , sizeof ( spec -> digout_paths )); <nl> + memset ( spec -> loopback_paths , 0 , sizeof ( spec -> loopback_paths )); <nl> + memset (& spec -> digin_path , 0 , sizeof ( spec -> digin_path )); <nl> + <nl> badness = 0 ; <nl>  <nl> /* fill hard - wired DACs first */
mmm drivers / watchdog / imx2_wdt . c <nl> ppp drivers / watchdog / imx2_wdt . c <nl> +// SPDX - License - Identifier : GPL - 2 . 0 <nl> /* <nl> * Watchdog driver for IMX2 and later processors <nl> * <nl> * some parts adapted by similar drivers from Darius Augulis and Vladimir <nl> * Zapolskiy , additional improvements by Wim Van Sebroeck . <nl> * <nl> - * This program is free software ; you can redistribute it and / or modify it <nl> - * under the terms of the GNU General Public License version 2 as published by <nl> - * the Free Software Foundation . <nl> - * <nl> * NOTE : MX1 has a slightly different Watchdog than MX2 and later : <nl> * <nl> * MX1 : MX2 +:
mmm drivers / gpio / gpio - pch . c <nl> ppp drivers / gpio / gpio - pch . c <nl> static void pch_gpio_setup ( struct pch_gpio * chip ) <nl> static int pch_irq_type ( struct irq_data * d , unsigned int type ) <nl> { <nl> u32 im ; <nl> - u32 * im_reg ; <nl> + u32 __iomem * im_reg ; <nl> u32 ien ; <nl> u32 im_pos ; <nl> int ch ;
mmm drivers / media / dvb - frontends / ascot2e . c <nl> ppp drivers / media / dvb - frontends / ascot2e . c <nl> static int ascot2e_write_regs ( struct ascot2e_priv * priv , <nl> } <nl> }; <nl>  <nl> - if ( len + 1 >= sizeof ( buf )) { <nl> + if ( len + 1 > sizeof ( buf )) { <nl> dev_warn (& priv -> i2c -> dev ," wr reg =% 04x : len =% d is too big !\ n ", <nl> reg , len + 1 ); <nl> return - E2BIG ;
mmm drivers / staging / iio / dac / ad5686 . c <nl> ppp drivers / staging / iio / dac / ad5686 . c <nl> static int ad5686_write_raw ( struct iio_dev * indio_dev , <nl>  <nl> switch ( mask ) { <nl> case 0 : <nl> - if ( val > ( 1 << chan -> scan_type . realbits )) <nl> + if ( val > ( 1 << chan -> scan_type . realbits ) || val < 0 ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& indio_dev -> mlock );
mmm net / sched / sch_netem . c <nl> ppp net / sched / sch_netem . c <nl> static struct sk_buff * netem_dequeue ( struct Qdisc * sch ) <nl>  <nl> /* if more time remaining ? */ <nl> if ( cb -> time_to_send <= psched_get_time ()) { <nl> - skb = qdisc_dequeue_tail ( sch ); <nl> - if ( unlikely (! skb )) <nl> - goto qdisc_dequeue ; <nl> + __skb_unlink ( skb , & sch -> q ); <nl> + sch -> qstats . backlog -= qdisc_pkt_len ( skb ); <nl>  <nl> # ifdef CONFIG_NET_CLS_ACT <nl> /* <nl> static struct sk_buff * netem_dequeue ( struct Qdisc * sch ) <nl> qdisc_watchdog_schedule (& q -> watchdog , cb -> time_to_send ); <nl> } <nl>  <nl> - qdisc_dequeue : <nl> if ( q -> qdisc ) { <nl> skb = q -> qdisc -> ops -> dequeue ( q -> qdisc ); <nl> if ( skb )
mmm fs / nfsd / nfsctl . c <nl> ppp fs / nfsd / nfsctl . c <nl> static ssize_t write_ports ( struct file * file , char * buf , size_t size ) <nl> /* Decrease the count , but don ' t shutdown the <nl> * the service <nl> */ <nl> + lock_kernel (); <nl> nfsd_serv -> sv_nrthreads --; <nl> + unlock_kernel (); <nl> } <nl> return err ; <nl> }
mmm block / blk - mq . c <nl> ppp block / blk - mq . c <nl> static void blk_mq_usage_counter_release ( struct percpu_ref * ref ) <nl> */ <nl> void blk_mq_freeze_queue ( struct request_queue * q ) <nl> { <nl> + bool freeze ; <nl> + <nl> spin_lock_irq ( q -> queue_lock ); <nl> - q -> mq_freeze_depth ++; <nl> + freeze = ! q -> mq_freeze_depth ++; <nl> spin_unlock_irq ( q -> queue_lock ); <nl>  <nl> - percpu_ref_kill (& q -> mq_usage_counter ); <nl> - blk_mq_run_queues ( q , false ); <nl> + if ( freeze ) { <nl> + percpu_ref_kill (& q -> mq_usage_counter ); <nl> + blk_mq_run_queues ( q , false ); <nl> + } <nl> wait_event ( q -> mq_freeze_wq , percpu_ref_is_zero (& q -> mq_usage_counter )); <nl> } <nl>  <nl> static void blk_mq_unfreeze_queue ( struct request_queue * q ) <nl> { <nl> - bool wake = false ; <nl> + bool wake ; <nl>  <nl> spin_lock_irq ( q -> queue_lock ); <nl> wake = !-- q -> mq_freeze_depth ;
mmm fs / dcache . c <nl> ppp fs / dcache . c <nl> static int prepend_path ( const struct path * path , <nl>  <nl> if ( dentry == vfsmnt -> mnt_root || IS_ROOT ( dentry )) { <nl> struct mount * parent = ACCESS_ONCE ( mnt -> mnt_parent ); <nl> + /* Escaped ? */ <nl> + if ( dentry != vfsmnt -> mnt_root ) { <nl> + bptr = * buffer ; <nl> + blen = * buflen ; <nl> + error = 3 ; <nl> + break ; <nl> + } <nl> /* Global root ? */ <nl> if ( mnt != parent ) { <nl> dentry = ACCESS_ONCE ( mnt -> mnt_mountpoint );
mmm net / core / rtnetlink . c <nl> ppp net / core / rtnetlink . c <nl> static int rtnl_fill_statsinfo ( struct sk_buff * skb , struct net_device * dev , <nl> return - EMSGSIZE ; <nl>  <nl> ifsm = nlmsg_data ( nlh ); <nl> + ifsm -> family = PF_UNSPEC ; <nl> + ifsm -> pad1 = 0 ; <nl> + ifsm -> pad2 = 0 ; <nl> ifsm -> ifindex = dev -> ifindex ; <nl> ifsm -> filter_mask = filter_mask ; <nl> 
mmm drivers / crypto / ccp / ccp - crypto - aes - cmac . c <nl> ppp drivers / crypto / ccp / ccp - crypto - aes - cmac . c <nl> static int ccp_aes_cmac_import ( struct ahash_request * req , const void * in ) <nl> /* ' in ' may not be aligned so memcpy to local variable */ <nl> memcpy (& state , in , sizeof ( state )); <nl>  <nl> + memset ( rctx , 0 , sizeof (* rctx )); <nl> rctx -> null_msg = state . null_msg ; <nl> memcpy ( rctx -> iv , state . iv , sizeof ( rctx -> iv )); <nl> rctx -> buf_count = state . buf_count ;mmm drivers / crypto / ccp / ccp - crypto - sha . c <nl> ppp drivers / crypto / ccp / ccp - crypto - sha . c <nl> static int ccp_aes_cmac_import ( struct ahash_request * req , const void * in ) <nl> /* ' in ' may not be aligned so memcpy to local variable */ <nl> memcpy (& state , in , sizeof ( state )); <nl>  <nl> + memset ( rctx , 0 , sizeof (* rctx )); <nl> rctx -> null_msg = state . null_msg ; <nl> memcpy ( rctx -> iv , state . iv , sizeof ( rctx -> iv )); <nl> rctx -> buf_count = state . buf_count ; <nl> static int ccp_sha_import ( struct ahash_request * req , const void * in ) <nl> /* ' in ' may not be aligned so memcpy to local variable */ <nl> memcpy (& state , in , sizeof ( state )); <nl>  <nl> + memset ( rctx , 0 , sizeof (* rctx )); <nl> rctx -> type = state . type ; <nl> rctx -> msg_bits = state . msg_bits ; <nl> rctx -> first = state . first ;
mmm drivers / net / wireless / brcm80211 / brcmfmac / dhd_linux . c <nl> ppp drivers / net / wireless / brcm80211 / brcmfmac / dhd_linux . c <nl> void brcmf_txflowblock ( struct device * dev , bool state ) <nl>  <nl> brcmf_dbg ( TRACE , " Enter \ n "); <nl>  <nl> - brcmf_fws_bus_blocked ( drvr , state ); <nl> - <nl> - for ( i = 0 ; i < BRCMF_MAX_IFS ; i ++) <nl> - brcmf_txflowblock_if ( drvr -> iflist [ i ], <nl> - BRCMF_NETIF_STOP_REASON_BLOCK_BUS , state ); <nl> + if ( brcmf_fws_fc_active ( drvr -> fws )) { <nl> + brcmf_fws_bus_blocked ( drvr , state ); <nl> + } else { <nl> + for ( i = 0 ; i < BRCMF_MAX_IFS ; i ++) <nl> + brcmf_txflowblock_if ( drvr -> iflist [ i ], <nl> + BRCMF_NETIF_STOP_REASON_BLOCK_BUS , <nl> + state ); <nl> + } <nl> } <nl>  <nl> void brcmf_rx_frames ( struct device * dev , struct sk_buff_head * skb_list )
mmm drivers / base / platform . c <nl> ppp drivers / base / platform . c <nl> int platform_device_add ( struct platform_device * pdev ) <nl> else <nl> dev_set_name (& pdev -> dev , pdev -> name ); <nl>  <nl> - pdev -> platform_data = pdev -> dev . platform_data ; <nl> + /* We will remove platform_data field from struct device <nl> + * if all platform devices pass its platform specific data <nl> + * from platform_device . The conversion is going to be a <nl> + * long time , so we allow the two cases coexist to make <nl> + * this kind of fix more easily */ <nl> + if ( pdev -> platform_data && pdev -> dev . platform_data ) { <nl> + printk ( KERN_ERR <nl> + "% s : use which platform_data ?\ n ", <nl> + dev_name (& pdev -> dev )); <nl> + } else if ( pdev -> platform_data ) { <nl> + pdev -> dev . platform_data = pdev -> platform_data ; <nl> + } else if ( pdev -> dev . platform_data ) { <nl> + pdev -> platform_data = pdev -> dev . platform_data ; <nl> + } <nl>  <nl> for ( i = 0 ; i < pdev -> num_resources ; i ++) { <nl> struct resource * p , * r = & pdev -> resource [ i ];
mmm kernel / futex . c <nl> ppp kernel / futex . c <nl> get_futex_key ( u32 __user * uaddr , int fshared , union futex_key * key , int rw ) <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> + page = compound_head ( page ); <nl> lock_page ( page ); <nl> if (! page -> mapping ) { <nl> unlock_page ( page );
mmm arch / arm / mach - exynos / pmu . c <nl> ppp arch / arm / mach - exynos / pmu . c <nl> static void exynos5_powerdown_conf ( enum sys_powerdown mode ) <nl> void exynos_sys_powerdown_conf ( enum sys_powerdown mode ) <nl> { <nl> unsigned int i ; <nl> + const struct exynos_pmu_data * pmu_data ; <nl> + <nl> + if (! pmu_context ) <nl> + return ; <nl>  <nl> - const struct exynos_pmu_data * pmu_data = pmu_context -> pmu_data ; <nl> + pmu_data = pmu_context -> pmu_data ; <nl>  <nl> if ( pmu_data -> powerdown_conf ) <nl> pmu_data -> powerdown_conf ( mode );
mmm sound / soc / soc - core . c <nl> ppp sound / soc / soc - core . c <nl> static void soc_init_codec_debugfs ( struct snd_soc_codec * codec ) <nl> { <nl> char codec_root [ 128 ]; <nl>  <nl> - snprintf ( codec_root , sizeof ( codec_root ), <nl> - "% s -% s ", dev_name ( codec -> socdev -> dev ), codec -> name ); <nl> + if ( codec -> dev ) <nl> + snprintf ( codec_root , sizeof ( codec_root ), <nl> + "% s .% s ", codec -> name , dev_name ( codec -> dev )); <nl> + else <nl> + snprintf ( codec_root , sizeof ( codec_root ), <nl> + "% s ", codec -> name ); <nl>  <nl> codec -> debugfs_codec_root = debugfs_create_dir ( codec_root , <nl> debugfs_root );
mmm arch / x86 / kvm / lapic . h <nl> ppp arch / x86 / kvm / lapic . h <nl> static inline bool kvm_apic_vid_enabled ( struct kvm * kvm ) <nl>  <nl> static inline bool kvm_apic_has_events ( struct kvm_vcpu * vcpu ) <nl> { <nl> - return vcpu -> arch . apic -> pending_events ; <nl> + return kvm_vcpu_has_lapic ( vcpu ) && vcpu -> arch . apic -> pending_events ; <nl> } <nl>  <nl> static inline bool kvm_lowest_prio_delivery ( struct kvm_lapic_irq * irq )
mmm drivers / acpi / acpica / evgpeblk . c <nl> ppp drivers / acpi / acpica / evgpeblk . c <nl> acpi_ev_initialize_gpe_block ( struct acpi_namespace_node * gpe_device , <nl>  <nl> gpe_index = ( i * ACPI_GPE_REGISTER_WIDTH ) + j ; <nl> gpe_event_info = & gpe_block -> event_info [ gpe_index ]; <nl> + gpe_number = gpe_index + gpe_block -> block_base_number ; <nl> + <nl> + /* <nl> + * If the GPE has already been enabled for runtime <nl> + * signaling , make sure it remains enabled , but do not <nl> + * increment its reference counter . <nl> + */ <nl> + if ( gpe_event_info -> runtime_count ) { <nl> + acpi_set_gpe ( gpe_device , gpe_number , <nl> + ACPI_GPE_ENABLE ); <nl> + gpe_enabled_count ++; <nl> + continue ; <nl> + } <nl>  <nl> if ( gpe_event_info -> flags & ACPI_GPE_CAN_WAKE ) { <nl> wake_gpe_count ++; <nl> acpi_ev_initialize_gpe_block ( struct acpi_namespace_node * gpe_device , <nl>  <nl> /* Enable this GPE */ <nl>  <nl> - gpe_number = gpe_index + gpe_block -> block_base_number ; <nl> status = acpi_enable_gpe ( gpe_device , gpe_number , <nl> ACPI_GPE_TYPE_RUNTIME ); <nl> if ( ACPI_FAILURE ( status )) {
mmm drivers / usb / gadget / file_storage . c <nl> ppp drivers / usb / gadget / file_storage . c <nl> static int __init check_parameters ( struct fsg_dev * fsg ) <nl> mod_data . protocol_type = USB_SC_SCSI ; <nl> mod_data . protocol_name = " Transparent SCSI "; <nl>  <nl> - if ( gadget_is_sh ( fsg -> gadget )) <nl> + /* Some peripheral controllers are known not to be able to <nl> + * halt bulk endpoints correctly . If one of them is present , <nl> + * disable stalls . <nl> + */ <nl> + if ( gadget_is_sh ( fsg -> gadget ) || gadget_is_at91 ( fsg -> gadget )) <nl> mod_data . can_stall = 0 ; <nl>  <nl> if ( mod_data . release == 0xffff ) { // Parameter wasn ' t set
mmm crypto / drbg . c <nl> ppp crypto / drbg . c <nl> static int drbg_generate_long ( struct drbg_state * drbg , <nl> if ( 0 >= tmplen ) <nl> return tmplen ; <nl> len += tmplen ; <nl> - } while ( slice > 0 ); <nl> + } while ( slice > 0 && ( len < buflen )); <nl> return len ; <nl> } <nl> 
mmm fs / btrfs / ioctl . c <nl> ppp fs / btrfs / ioctl . c <nl> static noinline int create_subvol ( struct btrfs_root * root , <nl> btrfs_record_root_in_trans ( trans , new_root ); <nl>  <nl> ret = btrfs_create_subvol_root ( trans , new_root , new_dirid ); <nl> + if ( ret ) { <nl> + /* We potentially lose an unused inode item here */ <nl> + goto fail ; <nl> + } <nl> + <nl> /* <nl> * insert the directory item <nl> */mmm fs / btrfs / inode . c <nl> ppp fs / btrfs / inode . c <nl> static noinline int create_subvol ( struct btrfs_root * root , <nl> btrfs_record_root_in_trans ( trans , new_root ); <nl>  <nl> ret = btrfs_create_subvol_root ( trans , new_root , new_dirid ); <nl> + if ( ret ) { <nl> + /* We potentially lose an unused inode item here */ <nl> + goto fail ; <nl> + } <nl> + <nl> /* <nl> * insert the directory item <nl> */ <nl> int btrfs_create_subvol_root ( struct btrfs_trans_handle * trans , <nl> btrfs_i_size_write ( inode , 0 ); <nl>  <nl> err = btrfs_update_inode ( trans , new_root , inode ); <nl> - BUG_ON ( err ); <nl>  <nl> iput ( inode ); <nl> - return 0 ; <nl> + return err ; <nl> } <nl>  <nl> struct inode * btrfs_alloc_inode ( struct super_block * sb )
mmm fs / btrfs / reada . c <nl> ppp fs / btrfs / reada . c <nl> static void __reada_start_machine ( struct btrfs_fs_info * fs_info ) <nl>  <nl> do { <nl> enqueued = 0 ; <nl> + mutex_lock (& fs_devices -> device_list_mutex ); <nl> list_for_each_entry ( device , & fs_devices -> devices , dev_list ) { <nl> if ( atomic_read (& device -> reada_in_flight ) < <nl> MAX_IN_FLIGHT ) <nl> enqueued += reada_start_machine_dev ( fs_info , <nl> device ); <nl> } <nl> + mutex_unlock (& fs_devices -> device_list_mutex ); <nl> total += enqueued ; <nl> } while ( enqueued && total < 10000 ); <nl> 
mmm drivers / atm / solos - pci . c <nl> ppp drivers / atm / solos - pci . c <nl> static void solos_bh ( unsigned long card_arg ) <nl> continue ; <nl> } <nl>  <nl> - skb = alloc_skb ( size + 1 , GFP_ATOMIC ); <nl> + /* Use netdev_alloc_skb () because it adds NET_SKB_PAD of <nl> + * headroom , and ensures we can route packets back out an <nl> + * Ethernet interface ( for example ) without having to <nl> + * reallocate . Adding NET_IP_ALIGN also ensures that both <nl> + * PPPoATM and PPPoEoBR2684 packets end up aligned . */ <nl> + skb = netdev_alloc_skb_ip_align ( NULL , size + 1 ); <nl> if (! skb ) { <nl> if ( net_ratelimit ()) <nl> dev_warn (& card -> dev -> dev , " Failed to allocate sk_buff for RX \ n "); <nl> static void solos_bh ( unsigned long card_arg ) <nl> /* Allocate RX skbs for any ports which need them */ <nl> if ( card -> using_dma && card -> atmdev [ port ] && <nl> ! card -> rx_skb [ port ]) { <nl> - struct sk_buff * skb = alloc_skb ( RX_DMA_SIZE , GFP_ATOMIC ); <nl> + /* Unlike the MMIO case ( qv ) we can ' t add NET_IP_ALIGN <nl> + * here ; the FPGA can only DMA to addresses which are <nl> + * aligned to 4 bytes . */ <nl> + struct sk_buff * skb = dev_alloc_skb ( RX_DMA_SIZE ); <nl> if ( skb ) { <nl> SKB_CB ( skb )-> dma_addr = <nl> dma_map_single (& card -> dev -> dev , skb -> data ,
mmm drivers / media / v4l2 - core / videobuf2 - core . c <nl> ppp drivers / media / v4l2 - core / videobuf2 - core . c <nl> int vb2_mmap ( struct vb2_queue * q , struct vm_area_struct * vma ) <nl> { <nl> unsigned long off = vma -> vm_pgoff << PAGE_SHIFT ; <nl> struct vb2_buffer * vb ; <nl> - unsigned int buffer , plane ; <nl> + unsigned int buffer = 0 , plane = 0 ; <nl> int ret ; <nl> unsigned long length ; <nl> 
mmm drivers / rtc / rtc - twl . c <nl> ppp drivers / rtc / rtc - twl . c <nl> static int set_rtc_irq_bit ( unsigned char bit ) <nl> unsigned char val ; <nl> int ret ; <nl>  <nl> + /* if the bit is set , return from here */ <nl> + if ( rtc_irq_bits & bit ) <nl> + return 0 ; <nl> + <nl> val = rtc_irq_bits | bit ; <nl> val &= ~ BIT_RTC_INTERRUPTS_REG_EVERY_M ; <nl> ret = twl_rtc_write_u8 ( val , REG_RTC_INTERRUPTS_REG ); <nl> static int mask_rtc_irq_bit ( unsigned char bit ) <nl> unsigned char val ; <nl> int ret ; <nl>  <nl> + /* if the bit is clear , return from here */ <nl> + if (!( rtc_irq_bits & bit )) <nl> + return 0 ; <nl> + <nl> val = rtc_irq_bits & ~ bit ; <nl> ret = twl_rtc_write_u8 ( val , REG_RTC_INTERRUPTS_REG ); <nl> if ( ret == 0 )
mmm drivers / net / wireless / ath / ath9k / htc_drv_beacon . c <nl> ppp drivers / net / wireless / ath / ath9k / htc_drv_beacon . c <nl> static void ath9k_htc_send_buffered ( struct ath9k_htc_priv * priv , <nl> } <nl>  <nl> tx_slot = ath9k_htc_tx_get_slot ( priv ); <nl> - if ( tx_slot != 0 ) { <nl> + if ( tx_slot < 0 ) { <nl> ath_dbg ( common , ATH_DBG_XMIT , " No free CAB slot \ n "); <nl> dev_kfree_skb_any ( skb ); <nl> goto next ;
mmm drivers / xen / xen - pciback / vpci . c <nl> ppp drivers / xen / xen - pciback / vpci . c <nl> static int __xen_pcibk_add_pci_dev ( struct xen_pcibk_device * pdev , <nl> /* Publish this device . */ <nl> if (! err ) <nl> err = publish_cb ( pdev , 0 , 0 , PCI_DEVFN ( slot , func ), devid ); <nl> + else <nl> + kfree ( dev_entry ); <nl>  <nl> out : <nl> return err ;
mmm drivers / target / iscsi / iscsi_target_parameters . c <nl> ppp drivers / target / iscsi / iscsi_target_parameters . c <nl> static int iscsi_add_notunderstood_response ( <nl> } <nl> INIT_LIST_HEAD (& extra_response -> er_list ); <nl>  <nl> - strncpy ( extra_response -> key , key , strlen ( key ) + 1 ); <nl> - strncpy ( extra_response -> value , NOTUNDERSTOOD , <nl> - strlen ( NOTUNDERSTOOD ) + 1 ); <nl> + strlcpy ( extra_response -> key , key , sizeof ( extra_response -> key )); <nl> + strlcpy ( extra_response -> value , NOTUNDERSTOOD , <nl> + sizeof ( extra_response -> value )); <nl>  <nl> list_add_tail (& extra_response -> er_list , <nl> & param_list -> extra_response_list ); <nl> int iscsi_decode_text_input ( <nl>  <nl> if ( phase & PHASE_SECURITY ) { <nl> if ( iscsi_check_for_auth_key ( key ) > 0 ) { <nl> - char * tmpptr = key + strlen ( key ); <nl> - * tmpptr = '='; <nl> kfree ( tmpbuf ); <nl> return 1 ; <nl> }mmm drivers / target / iscsi / iscsi_target_parameters . h <nl> ppp drivers / target / iscsi / iscsi_target_parameters . h <nl> static int iscsi_add_notunderstood_response ( <nl> } <nl> INIT_LIST_HEAD (& extra_response -> er_list ); <nl>  <nl> - strncpy ( extra_response -> key , key , strlen ( key ) + 1 ); <nl> - strncpy ( extra_response -> value , NOTUNDERSTOOD , <nl> - strlen ( NOTUNDERSTOOD ) + 1 ); <nl> + strlcpy ( extra_response -> key , key , sizeof ( extra_response -> key )); <nl> + strlcpy ( extra_response -> value , NOTUNDERSTOOD , <nl> + sizeof ( extra_response -> value )); <nl>  <nl> list_add_tail (& extra_response -> er_list , <nl> & param_list -> extra_response_list ); <nl> int iscsi_decode_text_input ( <nl>  <nl> if ( phase & PHASE_SECURITY ) { <nl> if ( iscsi_check_for_auth_key ( key ) > 0 ) { <nl> - char * tmpptr = key + strlen ( key ); <nl> - * tmpptr = '='; <nl> kfree ( tmpbuf ); <nl> return 1 ; <nl> } <nl> # ifndef ISCSI_PARAMETERS_H <nl> # define ISCSI_PARAMETERS_H <nl>  <nl> +# include < scsi / iscsi_proto . h > <nl> + <nl> struct iscsi_extra_response { <nl> - char key [ 64 ]; <nl> + char key [ KEY_MAXLEN ]; <nl> char value [ 32 ]; <nl> struct list_head er_list ; <nl> } ____cacheline_aligned ;
mmm net / netlink / genetlink . c <nl> ppp net / netlink / genetlink . c <nl> int genl_register_family ( struct genl_family * family ) <nl> start , end + 1 , GFP_KERNEL ); <nl> if ( family -> id < 0 ) { <nl> err = family -> id ; <nl> - goto errout_locked ; <nl> + goto errout_free ; <nl> } <nl>  <nl> err = genl_validate_assign_mc_groups ( family ); <nl> int genl_register_family ( struct genl_family * family ) <nl>  <nl> errout_remove : <nl> idr_remove (& genl_fam_idr , family -> id ); <nl> + errout_free : <nl> kfree ( family -> attrbuf ); <nl> errout_locked : <nl> genl_unlock_all ();
mmm sound / core / timer . c <nl> ppp sound / core / timer . c <nl> static int snd_timer_user_params ( struct file * file , <nl> if ( tu -> timeri -> flags & SNDRV_TIMER_IFLG_EARLY_EVENT ) { <nl> if ( tu -> tread ) { <nl> struct snd_timer_tread tread ; <nl> + memset (& tread , 0 , sizeof ( tread )); <nl> tread . event = SNDRV_TIMER_EVENT_EARLY ; <nl> tread . tstamp . tv_sec = 0 ; <nl> tread . tstamp . tv_nsec = 0 ;
mmm drivers / uwb / uwbd . c <nl> ppp drivers / uwb / uwbd . c <nl> static int uwbd ( void * param ) <nl> HZ ); <nl> if ( should_stop ) <nl> break ; <nl> - try_to_freeze (); <nl>  <nl> spin_lock_irqsave (& rc -> uwbd . event_list_lock , flags ); <nl> if (! list_empty (& rc -> uwbd . event_list )) {
mmm kernel / perf_event . c <nl> ppp kernel / perf_event . c <nl> inherit_event ( struct perf_event * parent_event , <nl> struct perf_event_context * child_ctx ) <nl> { <nl> struct perf_event * child_event ; <nl> + unsigned long flags ; <nl>  <nl> /* <nl> * Instead of creating recursive hierarchies of events , <nl> inherit_event ( struct perf_event * parent_event , <nl> /* <nl> * Link it up in the child ' s context : <nl> */ <nl> + raw_spin_lock_irqsave (& child_ctx -> lock , flags ); <nl> add_event_to_ctx ( child_event , child_ctx ); <nl> + raw_spin_unlock_irqrestore (& child_ctx -> lock , flags ); <nl>  <nl> /* <nl> * Get a reference to the parent filp - we will fput it
mmm drivers / hid / hid - sensor - hub . c <nl> ppp drivers / hid / hid - sensor - hub . c <nl> static int sensor_hub_probe ( struct hid_device * hdev , <nl> if ( name == NULL ) { <nl> hid_err ( hdev , " Failed MFD device name \ n "); <nl> ret = - ENOMEM ; <nl> + kfree ( hsdev ); <nl> goto err_no_mem ; <nl> } <nl> sd -> hid_sensor_hub_client_devs [
mmm include / net / mac80211 . h <nl> ppp include / net / mac80211 . h <nl> ieee80211_tx_info_clear_status ( struct ieee80211_tx_info * info ) <nl> * @ RX_FLAG_DECRYPTED : This frame was decrypted in hardware . <nl> * @ RX_FLAG_MMIC_STRIPPED : the Michael MIC is stripped off this frame , <nl> * verification has been done by the hardware . <nl> - * @ RX_FLAG_IV_STRIPPED : The IV / ICV are stripped from this frame . <nl> + * @ RX_FLAG_IV_STRIPPED : The IV and ICV are stripped from this frame . <nl> * If this flag is set , the stack cannot do any replay detection <nl> * hence the driver or hardware will have to do that . <nl> * @ RX_FLAG_PN_VALIDATED : Currently only valid for CCMP / GCMP frames , this <nl> ieee80211_tx_info_clear_status ( struct ieee80211_tx_info * info ) <nl> * @ RX_FLAG_ALLOW_SAME_PN : Allow the same PN as same packet before . <nl> * This is used for AMSDU subframes which can have the same PN as <nl> * the first subframe . <nl> + * @ RX_FLAG_ICV_STRIPPED : The ICV is stripped from this frame . CRC checking must <nl> + * be done in the hardware . <nl> */ <nl> enum mac80211_rx_flags { <nl> RX_FLAG_MMIC_ERROR = BIT ( 0 ), <nl> enum mac80211_rx_flags { <nl> RX_FLAG_RADIOTAP_VENDOR_DATA = BIT ( 31 ), <nl> RX_FLAG_MIC_STRIPPED = BIT_ULL ( 32 ), <nl> RX_FLAG_ALLOW_SAME_PN = BIT_ULL ( 33 ), <nl> + RX_FLAG_ICV_STRIPPED = BIT_ULL ( 34 ), <nl> }; <nl>  <nl> # define RX_FLAG_STBC_SHIFT 26mmm net / mac80211 / wpa . c <nl> ppp net / mac80211 / wpa . c <nl> ieee80211_tx_info_clear_status ( struct ieee80211_tx_info * info ) <nl> * @ RX_FLAG_DECRYPTED : This frame was decrypted in hardware . <nl> * @ RX_FLAG_MMIC_STRIPPED : the Michael MIC is stripped off this frame , <nl> * verification has been done by the hardware . <nl> - * @ RX_FLAG_IV_STRIPPED : The IV / ICV are stripped from this frame . <nl> + * @ RX_FLAG_IV_STRIPPED : The IV and ICV are stripped from this frame . <nl> * If this flag is set , the stack cannot do any replay detection <nl> * hence the driver or hardware will have to do that . <nl> * @ RX_FLAG_PN_VALIDATED : Currently only valid for CCMP / GCMP frames , this <nl> ieee80211_tx_info_clear_status ( struct ieee80211_tx_info * info ) <nl> * @ RX_FLAG_ALLOW_SAME_PN : Allow the same PN as same packet before . <nl> * This is used for AMSDU subframes which can have the same PN as <nl> * the first subframe . <nl> + * @ RX_FLAG_ICV_STRIPPED : The ICV is stripped from this frame . CRC checking must <nl> + * be done in the hardware . <nl> */ <nl> enum mac80211_rx_flags { <nl> RX_FLAG_MMIC_ERROR = BIT ( 0 ), <nl> enum mac80211_rx_flags { <nl> RX_FLAG_RADIOTAP_VENDOR_DATA = BIT ( 31 ), <nl> RX_FLAG_MIC_STRIPPED = BIT_ULL ( 32 ), <nl> RX_FLAG_ALLOW_SAME_PN = BIT_ULL ( 33 ), <nl> + RX_FLAG_ICV_STRIPPED = BIT_ULL ( 34 ), <nl> }; <nl>  <nl> # define RX_FLAG_STBC_SHIFT 26 <nl> ieee80211_crypto_tkip_decrypt ( struct ieee80211_rx_data * rx ) <nl> return RX_DROP_UNUSABLE ; <nl>  <nl> /* Trim ICV */ <nl> - skb_trim ( skb , skb -> len - IEEE80211_TKIP_ICV_LEN ); <nl> + if (!( status -> flag & RX_FLAG_ICV_STRIPPED )) <nl> + skb_trim ( skb , skb -> len - IEEE80211_TKIP_ICV_LEN ); <nl>  <nl> /* Remove IV */ <nl> memmove ( skb -> data + IEEE80211_TKIP_IV_LEN , skb -> data , hdrlen );mmm net / mac80211 / wep . c <nl> ppp net / mac80211 / wep . c <nl> ieee80211_tx_info_clear_status ( struct ieee80211_tx_info * info ) <nl> * @ RX_FLAG_DECRYPTED : This frame was decrypted in hardware . <nl> * @ RX_FLAG_MMIC_STRIPPED : the Michael MIC is stripped off this frame , <nl> * verification has been done by the hardware . <nl> - * @ RX_FLAG_IV_STRIPPED : The IV / ICV are stripped from this frame . <nl> + * @ RX_FLAG_IV_STRIPPED : The IV and ICV are stripped from this frame . <nl> * If this flag is set , the stack cannot do any replay detection <nl> * hence the driver or hardware will have to do that . <nl> * @ RX_FLAG_PN_VALIDATED : Currently only valid for CCMP / GCMP frames , this <nl> ieee80211_tx_info_clear_status ( struct ieee80211_tx_info * info ) <nl> * @ RX_FLAG_ALLOW_SAME_PN : Allow the same PN as same packet before . <nl> * This is used for AMSDU subframes which can have the same PN as <nl> * the first subframe . <nl> + * @ RX_FLAG_ICV_STRIPPED : The ICV is stripped from this frame . CRC checking must <nl> + * be done in the hardware . <nl> */ <nl> enum mac80211_rx_flags { <nl> RX_FLAG_MMIC_ERROR = BIT ( 0 ), <nl> enum mac80211_rx_flags { <nl> RX_FLAG_RADIOTAP_VENDOR_DATA = BIT ( 31 ), <nl> RX_FLAG_MIC_STRIPPED = BIT_ULL ( 32 ), <nl> RX_FLAG_ALLOW_SAME_PN = BIT_ULL ( 33 ), <nl> + RX_FLAG_ICV_STRIPPED = BIT_ULL ( 34 ), <nl> }; <nl>  <nl> # define RX_FLAG_STBC_SHIFT 26 <nl> ieee80211_crypto_tkip_decrypt ( struct ieee80211_rx_data * rx ) <nl> return RX_DROP_UNUSABLE ; <nl>  <nl> /* Trim ICV */ <nl> - skb_trim ( skb , skb -> len - IEEE80211_TKIP_ICV_LEN ); <nl> + if (!( status -> flag & RX_FLAG_ICV_STRIPPED )) <nl> + skb_trim ( skb , skb -> len - IEEE80211_TKIP_ICV_LEN ); <nl>  <nl> /* Remove IV */ <nl> memmove ( skb -> data + IEEE80211_TKIP_IV_LEN , skb -> data , hdrlen ); <nl> ieee80211_crypto_wep_decrypt ( struct ieee80211_rx_data * rx ) <nl> return RX_DROP_UNUSABLE ; <nl> ieee80211_wep_remove_iv ( rx -> local , rx -> skb , rx -> key ); <nl> /* remove ICV */ <nl> - if ( pskb_trim ( rx -> skb , rx -> skb -> len - IEEE80211_WEP_ICV_LEN )) <nl> + if (!( status -> flag & RX_FLAG_ICV_STRIPPED ) && <nl> + pskb_trim ( rx -> skb , rx -> skb -> len - IEEE80211_WEP_ICV_LEN )) <nl> return RX_DROP_UNUSABLE ; <nl> } <nl> 
mmm net / openvswitch / flow_netlink . c <nl> ppp net / openvswitch / flow_netlink . c <nl> static struct nlattr * reserve_sfa_size ( struct sw_flow_actions ** sfa , <nl> new_acts_size = max ( next_offset + req_size , ksize (* sfa ) * 2 ); <nl>  <nl> if ( new_acts_size > MAX_ACTIONS_BUFSIZE ) { <nl> - if (( MAX_ACTIONS_BUFSIZE - next_offset ) < req_size ) { <nl> + if (( next_offset + req_size ) > MAX_ACTIONS_BUFSIZE ) { <nl> OVS_NLERR ( log , " Flow action size exceeds max % u ", <nl> MAX_ACTIONS_BUFSIZE ); <nl> return ERR_PTR (- EMSGSIZE );
mmm drivers / firewire / core - cdev . c <nl> ppp drivers / firewire / core - cdev . c <nl> static int ioctl_send_response ( struct client * client , void * buffer ) <nl> if ( copy_from_user ( r -> data , u64_to_uptr ( request -> data ), <nl> r -> length )) { <nl> ret = - EFAULT ; <nl> + kfree ( r -> request ); <nl> goto out ; <nl> } <nl> fw_send_response ( client -> device -> card , r -> request ,
mmm sound / soc / codecs / ak4104 . c <nl> ppp sound / soc / codecs / ak4104 . c <nl> static struct snd_soc_codec_driver soc_codec_device_ak4104 = { <nl> . probe = ak4104_probe , <nl> . remove = ak4104_remove , <nl> . reg_cache_size = AK4104_NUM_REGS , <nl> - . reg_word_size = sizeof ( u16 ), <nl> + . reg_word_size = sizeof ( u8 ), <nl> }; <nl>  <nl> static int ak4104_spi_probe ( struct spi_device * spi )
mmm arch / powerpc / kernel / security . c <nl> ppp arch / powerpc / kernel / security . c <nl> unsigned long powerpc_security_features __read_mostly = SEC_FTR_DEFAULT ; <nl>  <nl> bool barrier_nospec_enabled ; <nl> + static bool no_nospec ; <nl>  <nl> static void enable_barrier_nospec ( bool enable ) <nl> { <nl> void setup_barrier_nospec ( void ) <nl> enable = security_ftr_enabled ( SEC_FTR_FAVOUR_SECURITY ) && <nl> security_ftr_enabled ( SEC_FTR_BNDS_CHK_SPEC_BAR ); <nl>  <nl> - enable_barrier_nospec ( enable ); <nl> + if (! no_nospec ) <nl> + enable_barrier_nospec ( enable ); <nl> } <nl>  <nl> + static int __init handle_nospectre_v1 ( char * p ) <nl> +{ <nl> + no_nospec = true ; <nl> + <nl> + return 0 ; <nl> +} <nl> + early_param (" nospectre_v1 ", handle_nospectre_v1 ); <nl> + <nl> # ifdef CONFIG_DEBUG_FS <nl> static int barrier_nospec_set ( void * data , u64 val ) <nl> {
mmm drivers / net / ethernet / intel / e1000 / e1000_ethtool . c <nl> ppp drivers / net / ethernet / intel / e1000 / e1000_ethtool . c <nl> static int e1000_set_ringparam ( struct net_device * netdev , <nl> err_alloc_rx : <nl> kfree ( txdr ); <nl> err_alloc_tx : <nl> - e1000_up ( adapter ); <nl> + if ( netif_running ( adapter -> netdev )) <nl> + e1000_up ( adapter ); <nl> err_setup : <nl> clear_bit ( __E1000_RESETTING , & adapter -> flags ); <nl> return err ;
mmm drivers / crypto / atmel - aes . c <nl> ppp drivers / crypto / atmel - aes . c <nl> static void atmel_aes_get_cap ( struct atmel_aes_dev * dd ) <nl>  <nl> /* keep only major version number */ <nl> switch ( dd -> hw_version & 0xff0 ) { <nl> + case 0x200 : <nl> + dd -> caps . has_dualbuff = 1 ; <nl> + dd -> caps . has_cfb64 = 1 ; <nl> + dd -> caps . max_burst_size = 4 ; <nl> + break ; <nl> case 0x130 : <nl> dd -> caps . has_dualbuff = 1 ; <nl> dd -> caps . has_cfb64 = 1 ;
mmm drivers / gpu / drm / i915 / intel_gvt . c <nl> ppp drivers / gpu / drm / i915 / intel_gvt . c <nl> int intel_gvt_init ( struct drm_i915_private * dev_priv ) <nl> goto bail ; <nl> } <nl>  <nl> + if (! i915 . enable_execlists ) { <nl> + DRM_INFO (" GPU guest virtualisation [ GVT - g ] disabled due to disabled execlist submission [ i915 . enable_execlists module parameter ]\ n "); <nl> + goto bail ; <nl> + } <nl> + <nl> /* <nl> * We ' re not in host or fail to find a MPT module , disable GVT - g <nl> */
mmm drivers / media / dvb - frontends / si21xx . c <nl> ppp drivers / media / dvb - frontends / si21xx . c <nl> static int si21_writeregs ( struct si21xx_state * state , u8 reg1 , <nl> . len = len + 1 <nl> }; <nl>  <nl> + if ( len > sizeof ( buf ) - 1 ) <nl> + return - EINVAL ; <nl> + <nl> msg . buf [ 0 ] = reg1 ; <nl> memcpy ( msg . buf + 1 , data , len ); <nl> 
mmm drivers / gpu / drm / amd / display / dc / core / dc_stream . c <nl> ppp drivers / gpu / drm / amd / display / dc / core / dc_stream . c <nl> bool dc_stream_set_cursor_position ( <nl> ! pipe_ctx -> ipp || ! pipe_ctx -> surface ) <nl> continue ; <nl>  <nl> + if ( pipe_ctx -> surface -> public . address . type <nl> + == PLN_ADDR_TYPE_VIDEO_PROGRESSIVE ) <nl> + pos_cpy . enable = false ; <nl> + <nl> if ( pipe_ctx -> top_pipe && pipe_ctx -> surface != pipe_ctx -> top_pipe -> surface ) <nl> pos_cpy . enable = false ; <nl> 
mmm sound / soc / codecs / wm8731 . c <nl> ppp sound / soc / codecs / wm8731 . c <nl> static int wm8731_hw_params ( struct snd_pcm_substream * substream , <nl> case 24 : <nl> iface |= 0x0008 ; <nl> break ; <nl> + case 32 : <nl> + iface |= 0x000c ; <nl> + break ; <nl> } <nl>  <nl> wm8731_set_deemph ( codec ); <nl> static int wm8731_startup ( struct snd_pcm_substream * substream , <nl> # define WM8731_RATES SNDRV_PCM_RATE_8000_96000 <nl>  <nl> # define WM8731_FORMATS ( SNDRV_PCM_FMTBIT_S16_LE | SNDRV_PCM_FMTBIT_S20_3LE |\ <nl> - SNDRV_PCM_FMTBIT_S24_LE ) <nl> + SNDRV_PCM_FMTBIT_S24_LE | SNDRV_PCM_FMTBIT_S32_LE ) <nl>  <nl> static const struct snd_soc_dai_ops wm8731_dai_ops = { <nl> . startup = wm8731_startup ,
mmm fs / ksmbd / smb2pdu . c <nl> ppp fs / ksmbd / smb2pdu . c <nl> int smb2_tree_disconnect ( struct ksmbd_work * work ) <nl>  <nl> ksmbd_close_tree_conn_fds ( work ); <nl> ksmbd_tree_conn_disconnect ( sess , tcon ); <nl> + work -> tcon = NULL ; <nl> return 0 ; <nl> } <nl> 
mmm drivers / ide / ide - disk . c <nl> ppp drivers / ide / ide - disk . c <nl> static ide_startstop_t __ide_do_rw_disk ( ide_drive_t * drive , struct request * rq , <nl> command = lba48 ? WIN_WRITE_EXT : WIN_WRITE ; <nl> } <nl>  <nl> - /* FIXME : -> OUTBSYNC ? */ <nl> - hwif -> OUTB ( command , IDE_COMMAND_REG ); <nl> + hwif -> OUTBSYNC ( drive , command , IDE_COMMAND_REG ); <nl>  <nl> return pre_task_out_intr ( drive , rq ); <nl> }
mmm fs / ext4 / file . c <nl> ppp fs / ext4 / file . c <nl> static int ext4_file_open ( struct inode * inode , struct file * filp ) <nl> path . dentry = mnt -> mnt_root ; <nl> cp = d_path (& path , buf , sizeof ( buf )); <nl> if (! IS_ERR ( cp )) { <nl> - memcpy ( sbi -> s_es -> s_last_mounted , cp , <nl> - sizeof ( sbi -> s_es -> s_last_mounted )); <nl> + strlcpy ( sbi -> s_es -> s_last_mounted , cp , <nl> + sizeof ( sbi -> s_es -> s_last_mounted )); <nl> ext4_mark_super_dirty ( sb ); <nl> } <nl> }
mmm drivers / scsi / ipr . c <nl> ppp drivers / scsi / ipr . c <nl> static void ipr_handle_log_data ( struct ipr_ioa_cfg * ioa_cfg , <nl>  <nl> if ( ioa_cfg -> log_level < IPR_DEFAULT_LOG_LEVEL ) <nl> return ; <nl> + if ( be32_to_cpu ( hostrcb -> hcam . length ) > sizeof ( hostrcb -> hcam . u . raw )) <nl> + hostrcb -> hcam . length = cpu_to_be32 ( sizeof ( hostrcb -> hcam . u . raw )); <nl>  <nl> switch ( hostrcb -> hcam . overlay_id ) { <nl> - case IPR_HOST_RCB_OVERLAY_ID_1 : <nl> - ipr_log_generic_error ( ioa_cfg , hostrcb ); <nl> - break ; <nl> case IPR_HOST_RCB_OVERLAY_ID_2 : <nl> ipr_log_cache_error ( ioa_cfg , hostrcb ); <nl> break ; <nl> static void ipr_handle_log_data ( struct ipr_ioa_cfg * ioa_cfg , <nl> case IPR_HOST_RCB_OVERLAY_ID_6 : <nl> ipr_log_array_error ( ioa_cfg , hostrcb ); <nl> break ; <nl> + case IPR_HOST_RCB_OVERLAY_ID_1 : <nl> case IPR_HOST_RCB_OVERLAY_ID_DEFAULT : <nl> default : <nl> ipr_log_generic_error ( ioa_cfg , hostrcb );
mmm crypto / arc4 . c <nl> ppp crypto / arc4 . c <nl> -/* <nl> +/* <nl> * Cryptographic API <nl> * <nl> * ARC4 Cipher Algorithm <nl> static int arc4_set_key ( struct crypto_tfm * tfm , const u8 * in_key , <nl> ctx -> x = 1 ; <nl> ctx -> y = 0 ; <nl>  <nl> - for ( i = 0 ; i < 256 ; i ++) <nl> + for ( i = 0 ; i < 256 ; i ++) <nl> ctx -> S [ i ] = i ; <nl>  <nl> - for ( i = 0 ; i < 256 ; i ++) <nl> - { <nl> + for ( i = 0 ; i < 256 ; i ++) { <nl> u8 a = ctx -> S [ i ]; <nl> j = ( j + in_key [ k ] + a ) & 0xff ; <nl> ctx -> S [ i ] = ctx -> S [ j ]; <nl> ctx -> S [ j ] = a ; <nl> - if (++ k >= key_len ) <nl> + if (++ k >= key_len ) <nl> k = 0 ; <nl> } <nl>  <nl> static struct crypto_alg arc4_alg = { <nl> . cra_u = { . cipher = { <nl> . cia_min_keysize = ARC4_MIN_KEY_SIZE , <nl> . cia_max_keysize = ARC4_MAX_KEY_SIZE , <nl> - . cia_setkey = arc4_set_key , <nl> - . cia_encrypt = arc4_crypt , <nl> - . cia_decrypt = arc4_crypt } } <nl> + . cia_setkey = arc4_set_key , <nl> + . cia_encrypt = arc4_crypt , <nl> + . cia_decrypt = arc4_crypt } } <nl> }; <nl>  <nl> static int __init arc4_init ( void )
mmm virt / kvm / kvm_main . c <nl> ppp virt / kvm / kvm_main . c <nl> static int kvm_ioctl_create_device ( struct kvm * kvm , <nl> if ( ops -> init ) <nl> ops -> init ( dev ); <nl>  <nl> + kvm_get_kvm ( kvm ); <nl> ret = anon_inode_getfd ( ops -> name , & kvm_device_fops , dev , O_RDWR | O_CLOEXEC ); <nl> if ( ret < 0 ) { <nl> + kvm_put_kvm ( kvm ); <nl> mutex_lock (& kvm -> lock ); <nl> list_del (& dev -> vm_node ); <nl> mutex_unlock (& kvm -> lock ); <nl> static int kvm_ioctl_create_device ( struct kvm * kvm , <nl> return ret ; <nl> } <nl>  <nl> - kvm_get_kvm ( kvm ); <nl> cd -> fd = ret ; <nl> return 0 ; <nl> }
mmm fs / nilfs2 / btree . c <nl> ppp fs / nilfs2 / btree . c <nl> static void nilfs_btree_add_dirty_buffer ( struct nilfs_btree * btree , <nl> node = ( struct nilfs_btree_node *) bh -> b_data ; <nl> key = nilfs_btree_node_get_key ( node , 0 ); <nl> level = nilfs_btree_node_get_level ( node ); <nl> + if ( level < NILFS_BTREE_LEVEL_NODE_MIN || <nl> + level >= NILFS_BTREE_LEVEL_MAX ) { <nl> + dump_stack (); <nl> + printk ( KERN_WARNING <nl> + "% s : invalid btree level : % d ( key =% llu , ino =% lu , " <nl> + " blocknr =% llu )\ n ", <nl> + __func__ , level , ( unsigned long long ) key , <nl> + NILFS_BMAP_I (& btree -> bt_bmap )-> vfs_inode . i_ino , <nl> + ( unsigned long long ) bh -> b_blocknr ); <nl> + return ; <nl> + } <nl> + <nl> list_for_each ( head , & lists [ level ]) { <nl> cbh = list_entry ( head , struct buffer_head , b_assoc_buffers ); <nl> cnode = ( struct nilfs_btree_node *) cbh -> b_data ;
mmm fs / xfs / xfs_qm . c <nl> ppp fs / xfs / xfs_qm . c <nl> xfs_qm_dquot_walk ( <nl> skipped = 0 ; <nl> break ; <nl> } <nl> + /* we ' re done if id overflows back to zero */ <nl> + if (! next_index ) <nl> + break ; <nl> } <nl>  <nl> if ( skipped ) {
mmm lib / rhashtable . c <nl> ppp lib / rhashtable . c <nl> static u32 obj_raw_hashfn ( struct rhashtable * ht , <nl> } <nl>  <nl> static u32 key_hashfn ( struct rhashtable * ht , const struct bucket_table * tbl , <nl> - const void * key , u32 len ) <nl> + const void * key ) <nl> { <nl> - return rht_bucket_index ( tbl , ht -> p . hashfn ( key , len , tbl -> hash_rnd ) >> <nl> + return rht_bucket_index ( tbl , ht -> p . hashfn ( key , ht -> p . key_len , <nl> + tbl -> hash_rnd ) >> <nl> HASH_RESERVED_SPACE ); <nl> } <nl>  <nl> void * rhashtable_lookup_compare ( struct rhashtable * ht , const void * key , <nl> rcu_read_lock (); <nl>  <nl> tbl = rht_dereference_rcu ( ht -> tbl , ht ); <nl> - hash = key_hashfn ( ht , tbl , key , ht -> p . key_len ); <nl> + hash = key_hashfn ( ht , tbl , key ); <nl> restart : <nl> rht_for_each_rcu ( he , tbl , hash ) { <nl> if (! compare ( rht_obj ( ht , he ), arg ))
mmm drivers / char / virtio_console . c <nl> ppp drivers / char / virtio_console . c <nl> static struct port_buffer * get_inbuf ( struct port * port ) <nl>  <nl> buf = virtqueue_get_buf ( port -> in_vq , & len ); <nl> if ( buf ) { <nl> - buf -> len = len ; <nl> + buf -> len = min_t ( size_t , len , buf -> size ); <nl> buf -> offset = 0 ; <nl> port -> stats . bytes_received += len ; <nl> } <nl> static void control_work_handler ( struct work_struct * work ) <nl> while (( buf = virtqueue_get_buf ( vq , & len ))) { <nl> spin_unlock (& portdev -> c_ivq_lock ); <nl>  <nl> - buf -> len = len ; <nl> + buf -> len = min_t ( size_t , len , buf -> size ); <nl> buf -> offset = 0 ; <nl>  <nl> handle_control_message ( vq -> vdev , portdev , buf );
mmm drivers / video / offb . c <nl> ppp drivers / video / offb . c <nl> static void __init offb_init_fb ( const char * name , const char * full_name , <nl> return ; <nl> } <nl>  <nl> - size = sizeof ( struct fb_info ) + sizeof ( u32 ) * 17 ; <nl> + size = sizeof ( struct fb_info ) + sizeof ( u32 ) * 16 ; <nl>  <nl> info = kmalloc ( size , GFP_ATOMIC ); <nl> 
mmm drivers / target / target_core_transport . c <nl> ppp drivers / target / target_core_transport . c <nl> static int transport_generic_cmd_sequencer ( <nl> cmd -> data_length = size ; <nl> } <nl>  <nl> + /* Let ' s limit control cdbs to a page , for simplicity ' s sake . */ <nl> + if (( cmd -> se_cmd_flags & SCF_SCSI_CONTROL_SG_IO_CDB ) && <nl> + size > PAGE_SIZE ) <nl> + goto out_invalid_cdb_field ; <nl> + <nl> transport_set_supported_SAM_opcode ( cmd ); <nl> return ret ; <nl> mmm include / target / target_core_base . h <nl> ppp include / target / target_core_base . h <nl> static int transport_generic_cmd_sequencer ( <nl> cmd -> data_length = size ; <nl> } <nl>  <nl> + /* Let ' s limit control cdbs to a page , for simplicity ' s sake . */ <nl> + if (( cmd -> se_cmd_flags & SCF_SCSI_CONTROL_SG_IO_CDB ) && <nl> + size > PAGE_SIZE ) <nl> + goto out_invalid_cdb_field ; <nl> + <nl> transport_set_supported_SAM_opcode ( cmd ); <nl> return ret ; <nl>  <nl> /* Used by transport_generic_allocate_iovecs () */ <nl> # define TRANSPORT_IOV_DATA_BUFFER 5 <nl> /* Maximum Number of LUNs per Target Portal Group */ <nl> +/* Don ' t raise above 511 or REPORT_LUNS needs to handle > 1 page */ <nl> # define TRANSPORT_MAX_LUNS_PER_TPG 256 <nl> /* <nl> * By default we use 32 - byte CDBs in TCM Core and subsystem plugin code .
mmm net / ipv4 / tcp_output . c <nl> ppp net / ipv4 / tcp_output . c <nl> int tcp_fragment ( struct sock * sk , struct sk_buff * skb , u32 len , unsigned int mss <nl> tp -> fackets_out -= diff ; <nl> if (( int ) tp -> fackets_out < 0 ) <nl> tp -> fackets_out = 0 ; <nl> + /* SACK fastpath might overwrite it unless dealt with */ <nl> + if ( tp -> fastpath_skb_hint != NULL && <nl> + after ( TCP_SKB_CB ( tp -> fastpath_skb_hint )-> seq , <nl> + TCP_SKB_CB ( skb )-> seq )) { <nl> + tp -> fastpath_cnt_hint -= diff ; <nl> + if (( int ) tp -> fastpath_cnt_hint < 0 ) <nl> + tp -> fastpath_cnt_hint = 0 ; <nl> + } <nl> } <nl> } <nl> 
mmm net / core / dev . c <nl> ppp net / core / dev . c <nl> struct net_device * alloc_netdev_mq ( int sizeof_priv , const char * name , <nl> alloc_size = ( sizeof (* dev ) + NETDEV_ALIGN_CONST + <nl> ( sizeof ( struct net_device_subqueue ) * ( queue_count - 1 ))) & <nl> ~ NETDEV_ALIGN_CONST ; <nl> - alloc_size += sizeof_priv + NETDEV_ALIGN_CONST ; <nl> + if ( sizeof_priv ) <nl> + alloc_size += sizeof_priv + NETDEV_ALIGN_CONST ; <nl>  <nl> p = kzalloc ( alloc_size , GFP_KERNEL ); <nl> if (! p ) {
mmm arch / arm / mach - omap2 / mux . c <nl> ppp arch / arm / mach - omap2 / mux . c <nl> int __init omap_mux_init ( u32 mux_pbase , u32 mux_size , <nl> } <nl>  <nl> # ifdef CONFIG_OMAP_MUX <nl> - omap_mux_package_fixup ( package_subset , superset ); <nl> - omap_mux_package_init_balls ( package_balls , superset ); <nl> + if ( package_subset ) <nl> + omap_mux_package_fixup ( package_subset , superset ); <nl> + if ( package_balls ) <nl> + omap_mux_package_init_balls ( package_balls , superset ); <nl> omap_mux_set_cmdline_signals (); <nl> omap_mux_set_board_signals ( board_mux ); <nl> # endif
mmm arch / s390 / tools / gen_opcode_table . c <nl> ppp arch / s390 / tools / gen_opcode_table . c <nl> static void add_to_group ( struct gen_opcode * desc , struct insn * insn , int offset ) <nl> if (! desc -> group ) <nl> exit ( EXIT_FAILURE ); <nl> group = & desc -> group [ desc -> nr_groups - 1 ]; <nl> - strncpy ( group -> opcode , insn -> opcode , 2 ); <nl> + memcpy ( group -> opcode , insn -> opcode , 2 ); <nl> group -> type = insn -> type ; <nl> group -> offset = offset ; <nl> group -> count = 1 ; <nl> static void print_opcode_table ( struct gen_opcode * desc ) <nl> continue ; <nl> add_to_group ( desc , insn , offset ); <nl> if ( strncmp ( opcode , insn -> opcode , 2 )) { <nl> - strncpy ( opcode , insn -> opcode , 2 ); <nl> + memcpy ( opcode , insn -> opcode , 2 ); <nl> printf ("\ t /* %. 2s */ \\\ n ", opcode ); <nl> } <nl> print_opcode ( insn , offset );
mmm drivers / staging / wilc1000 / wilc_wlan . c <nl> ppp drivers / staging / wilc1000 / wilc_wlan . c <nl> u32 total_acks = 0 , dropped_acks = 0 ; <nl> # ifdef TCP_ACK_FILTER <nl> struct ack_session_info ; <nl> struct ack_session_info { <nl> - u32 Ack_seq_num ; <nl> + u32 seq_num ; <nl> u32 Bigger_Ack_num ; <nl> u16 src_port ; <nl> u16 dst_port ; <nl> static inline int Init_TCP_tracking ( void ) <nl>  <nl> static inline int add_TCP_track_session ( u32 src_prt , u32 dst_prt , u32 seq ) <nl> { <nl> - Acks_keep_track_info [ Opened_TCP_session ]. Ack_seq_num = seq ; <nl> + Acks_keep_track_info [ Opened_TCP_session ]. seq_num = seq ; <nl> Acks_keep_track_info [ Opened_TCP_session ]. Bigger_Ack_num = 0 ; <nl> Acks_keep_track_info [ Opened_TCP_session ]. src_port = src_prt ; <nl> Acks_keep_track_info [ Opened_TCP_session ]. dst_port = dst_prt ; <nl> static inline int tcp_process ( struct net_device * dev , struct txq_entry_t * tqe ) <nl> Ack_no = ((( u32 ) tcp_hdr_ptr [ 8 ]) << 24 ) + ((( u32 ) tcp_hdr_ptr [ 9 ]) << 16 ) + ((( u32 ) tcp_hdr_ptr [ 10 ]) << 8 ) + (( u32 ) tcp_hdr_ptr [ 11 ]); <nl>  <nl> for ( i = 0 ; i < Opened_TCP_session ; i ++) { <nl> - if ( Acks_keep_track_info [ i ]. Ack_seq_num == seq_no ) { <nl> + if ( Acks_keep_track_info [ i ]. seq_num == seq_no ) { <nl> Update_TCP_track_session ( i , Ack_no ); <nl> break ; <nl> }
mmm net / ipv6 / xfrm6_tunnel . c <nl> ppp net / ipv6 / xfrm6_tunnel . c <nl> static int xfrm6_tunnel_rcv ( struct sk_buff * skb ) <nl> __be32 spi ; <nl>  <nl> spi = xfrm6_tunnel_spi_lookup (( xfrm_address_t *)& iph -> saddr ); <nl> - return xfrm6_rcv_spi ( skb , spi ); <nl> + return xfrm6_rcv_spi ( skb , spi ) > 0 ? : 0 ; <nl> } <nl>  <nl> static int xfrm6_tunnel_err ( struct sk_buff * skb , struct inet6_skb_parm * opt ,
mmm drivers / scsi / sg . c <nl> ppp drivers / scsi / sg . c <nl> sg_add ( struct class_device * cl_dev , struct class_interface * cl_intf ) <nl> MKDEV ( SCSI_GENERIC_MAJOR , sdp -> index ), <nl> cl_dev -> dev , "% s ", <nl> disk -> disk_name ); <nl> - if ( IS_ERR ( sg_class_member )) <nl> - printk ( KERN_WARNING " sg_add : " <nl> - " class_device_create failed \ n "); <nl> + if ( IS_ERR ( sg_class_member )) { <nl> + printk ( KERN_ERR " sg_add : " <nl> + " class_device_create failed \ n "); <nl> + error = PTR_ERR ( sg_class_member ); <nl> + goto cdev_add_err ; <nl> + } <nl> class_set_devdata ( sg_class_member , sdp ); <nl> - error = sysfs_create_link (& scsidp -> sdev_gendev . kobj , <nl> + error = sysfs_create_link (& scsidp -> sdev_gendev . kobj , <nl> & sg_class_member -> kobj , " generic "); <nl> if ( error ) <nl> printk ( KERN_ERR " sg_add : unable to make symlink "
mmm sound / soc / s3c24xx / s3c2412 - i2s . c <nl> ppp sound / soc / s3c24xx / s3c2412 - i2s . c <nl> # include < sound / soc . h > <nl> # include < mach / hardware . h > <nl>  <nl> -# include < plat / regs - s3c2412 - iis . h > <nl> - <nl> # include < mach / regs - gpio . h > <nl> # include < mach / dma . h > <nl>  <nl> # include " s3c - dma . h " <nl> +# include " regs - i2s - v2 . h " <nl> # include " s3c2412 - i2s . h " <nl>  <nl> # define S3C2412_I2S_DEBUG 0mmm sound / soc / s3c24xx / s3c64xx - i2s . c <nl> ppp sound / soc / s3c24xx / s3c64xx - i2s . c <nl> # include < sound / soc . h > <nl> # include < mach / hardware . h > <nl>  <nl> -# include < plat / regs - s3c2412 - iis . h > <nl> - <nl> # include < mach / regs - gpio . h > <nl> # include < mach / dma . h > <nl>  <nl> # include " s3c - dma . h " <nl> +# include " regs - i2s - v2 . h " <nl> # include " s3c2412 - i2s . h " <nl>  <nl> # define S3C2412_I2S_DEBUG 0 <nl>  <nl> # include < sound / soc . h > <nl>  <nl> -# include < plat / regs - s3c2412 - iis . h > <nl> # include < mach / gpio - bank - d . h > <nl> # include < mach / gpio - bank - e . h > <nl> # include < plat / gpio - cfg . h > <nl> # include < mach / dma . h > <nl>  <nl> # include " s3c - dma . h " <nl> +# include " regs - i2s - v2 . h " <nl> # include " s3c64xx - i2s . h " <nl>  <nl> /* The value should be set to maximum of the total numbermmm sound / soc / s3c24xx / s3c - i2s - v2 . c <nl> ppp sound / soc / s3c24xx / s3c - i2s - v2 . c <nl> # include < sound / soc . h > <nl> # include < mach / hardware . h > <nl>  <nl> -# include < plat / regs - s3c2412 - iis . h > <nl> - <nl> # include < mach / regs - gpio . h > <nl> # include < mach / dma . h > <nl>  <nl> # include " s3c - dma . h " <nl> +# include " regs - i2s - v2 . h " <nl> # include " s3c2412 - i2s . h " <nl>  <nl> # define S3C2412_I2S_DEBUG 0 <nl>  <nl> # include < sound / soc . h > <nl>  <nl> -# include < plat / regs - s3c2412 - iis . h > <nl> # include < mach / gpio - bank - d . h > <nl> # include < mach / gpio - bank - e . h > <nl> # include < plat / gpio - cfg . h > <nl> # include < mach / dma . h > <nl>  <nl> # include " s3c - dma . h " <nl> +# include " regs - i2s - v2 . h " <nl> # include " s3c64xx - i2s . h " <nl>  <nl> /* The value should be set to maximum of the total number <nl> # include < sound / pcm_params . h > <nl> # include < sound / soc . h > <nl>  <nl> -# include < plat / regs - s3c2412 - iis . h > <nl> - <nl> # include < mach / dma . h > <nl>  <nl> +# include " regs - i2s - v2 . h " <nl> # include " s3c - i2s - v2 . h " <nl> # include " s3c - dma . h " <nl> 
mmm fs / fuse / file . c <nl> ppp fs / fuse / file . c <nl> static ssize_t fuse_fill_write_pages ( struct fuse_req * req , <nl> if (! fc -> big_writes ) <nl> break ; <nl> } while ( iov_iter_count ( ii ) && count < fc -> max_write && <nl> - req -> num_pages < FUSE_MAX_PAGES_PER_REQ && offset == 0 ); <nl> + req -> num_pages < req -> max_pages && offset == 0 ); <nl>  <nl> return count > 0 ? count : err ; <nl> } <nl>  <nl> + static inline unsigned fuse_wr_pages ( loff_t pos , size_t len ) <nl> +{ <nl> + return min_t ( unsigned , <nl> + (( pos + len - 1 ) >> PAGE_CACHE_SHIFT ) - <nl> + ( pos >> PAGE_CACHE_SHIFT ) + 1 , <nl> + FUSE_MAX_PAGES_PER_REQ ); <nl> +} <nl> + <nl> static ssize_t fuse_perform_write ( struct file * file , <nl> struct address_space * mapping , <nl> struct iov_iter * ii , loff_t pos ) <nl> static ssize_t fuse_perform_write ( struct file * file , <nl> do { <nl> struct fuse_req * req ; <nl> ssize_t count ; <nl> + unsigned nr_pages = fuse_wr_pages ( pos , iov_iter_count ( ii )); <nl>  <nl> - req = fuse_get_req ( fc , FUSE_MAX_PAGES_PER_REQ ); <nl> + req = fuse_get_req ( fc , nr_pages ); <nl> if ( IS_ERR ( req )) { <nl> err = PTR_ERR ( req ); <nl> break ;
mmm arch / arm / mach - at91 / board - snapper9260 . c <nl> ppp arch / arm / mach - at91 / board - snapper9260 . c <nl> static struct i2c_board_info __initdata snapper9260_i2c_devices [] = { <nl> { <nl> /* RTC */ <nl> I2C_BOARD_INFO (" isl1208 ", 0x6f ), <nl> + . irq = gpio_to_irq ( AT91_PIN_PA31 ), <nl> }, <nl> }; <nl> 
mmm net / bluetooth / smp . c <nl> ppp net / bluetooth / smp . c <nl> static u8 smp_cmd_pairing_req ( struct l2cap_conn * conn , struct sk_buff * skb ) <nl>  <nl> if (! test_and_set_bit ( HCI_CONN_LE_SMP_PEND , & conn -> hcon -> flags )) <nl> smp = smp_chan_create ( conn ); <nl> + else <nl> + smp = conn -> smp_chan ; <nl>  <nl> - smp = conn -> smp_chan ; <nl> + if (! smp ) <nl> + return SMP_UNSPECIFIED ; <nl>  <nl> smp -> preq [ 0 ] = SMP_CMD_PAIRING_REQ ; <nl> memcpy (& smp -> preq [ 1 ], req , sizeof (* req ));
mmm drivers / staging / iio / industrialio - ring . c <nl> ppp drivers / staging / iio / industrialio - ring . c <nl> static ssize_t iio_ring_rip_outer ( struct file * filp , char __user * buf , <nl> return - EINVAL ; <nl> copied = rb -> access . rip_lots ( rb , count , & data , & dead_offset ); <nl>  <nl> - if ( copied < 0 ) { <nl> + if ( copied <= 0 ) { <nl> ret = copied ; <nl> goto error_ret ; <nl> }
mmm drivers / pinctrl / sh - pfc / pinctrl . c <nl> ppp drivers / pinctrl / sh - pfc / pinctrl . c <nl> static int sh_pfc_dt_node_to_map ( struct pinctrl_dev * pctldev , <nl> for_each_child_of_node ( np , child ) { <nl> ret = sh_pfc_dt_subnode_to_map ( pctldev , child , map , num_maps , <nl> & index ); <nl> - if ( ret < 0 ) <nl> + if ( ret < 0 ) { <nl> + of_node_put ( child ); <nl> goto done ; <nl> + } <nl> } <nl>  <nl> /* If no mapping has been found in child nodes try the config node . */
mmm drivers / media / video / ivtv / ivtv - fileops . c <nl> ppp drivers / media / video / ivtv / ivtv - fileops . c <nl> unsigned int ivtv_v4l2_dec_poll ( struct file * filp , poll_table * wait ) <nl> return res ; <nl> } <nl>  <nl> - unsigned int ivtv_v4l2_enc_poll ( struct file * filp , poll_table * wait ) <nl> + unsigned int ivtv_v4l2_enc_poll ( struct file * filp , poll_table * wait ) <nl> { <nl> + unsigned long req_events = poll_requested_events ( wait ); <nl> struct ivtv_open_id * id = fh2id ( filp -> private_data ); <nl> struct ivtv * itv = id -> itv ; <nl> struct ivtv_stream * s = & itv -> streams [ id -> type ]; <nl> unsigned int ivtv_v4l2_enc_poll ( struct file * filp , poll_table * wait ) <nl> unsigned res = 0 ; <nl>  <nl> /* Start a capture if there is none */ <nl> - if (! eof && ! test_bit ( IVTV_F_S_STREAMING , & s -> s_flags )) { <nl> + if (! eof && ! test_bit ( IVTV_F_S_STREAMING , & s -> s_flags ) && <nl> + ( req_events & ( POLLIN | POLLRDNORM ))) { <nl> int rc ; <nl>  <nl> rc = ivtv_start_capture ( id );
mmm drivers / iio / health / max30102 . c <nl> ppp drivers / iio / health / max30102 . c <nl> static int max30102_probe ( struct i2c_client * client , <nl> dev_err (& client -> dev , " regmap initialization failed \ n "); <nl> return PTR_ERR ( data -> regmap ); <nl> } <nl> - max30102_set_powermode ( data , false ); <nl> + <nl> + ret = max30102_set_powermode ( data , false ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> ret = max30102_chip_init ( data ); <nl> if ( ret )
mmm drivers / i2c / busses / i2c - designware - core . c <nl> ppp drivers / i2c / busses / i2c - designware - core . c <nl> i2c_dw_xfer ( struct i2c_adapter * adap , struct i2c_msg msgs [], int num ) <nl> i2c_dw_xfer_init ( dev ); <nl>  <nl> /* wait for tx to complete */ <nl> - if (! wait_for_completion_timeout (& dev -> cmd_complete , HZ )) { <nl> + if (! wait_for_completion_timeout (& dev -> cmd_complete , adap -> timeout )) { <nl> dev_err ( dev -> dev , " controller timed out \ n "); <nl> /* i2c_dw_init implicitly disables the adapter */ <nl> i2c_dw_init ( dev );
mmm drivers / usb / serial / upd78f0730 . c <nl> ppp drivers / usb / serial / upd78f0730 . c <nl> # define DRIVER_AUTHOR " Maksim Salau < maksim . salau @ gmail . com >" <nl>  <nl> static const struct usb_device_id id_table [] = { <nl> - { USB_DEVICE ( 0x045B , 0x0212 ) }, /* YRPBRL78G13 , YRPBRL78G14 */ <nl> { USB_DEVICE ( 0x0409 , 0x0063 ) }, /* V850ESJX3 - STICK */ <nl> + { USB_DEVICE ( 0x045B , 0x0212 ) }, /* YRPBRL78G13 , YRPBRL78G14 */ <nl> { USB_DEVICE ( 0x064B , 0x7825 ) }, /* Analog Devices EVAL - ADXL362Z - DB */ <nl> {} <nl> };
mmm drivers / edac / edac_mc_sysfs . c <nl> ppp drivers / edac / edac_mc_sysfs . c <nl> static ssize_t channel_dimm_label_store ( struct device * dev , <nl> if ( data [ count - 1 ] == '\ 0 ' || data [ count - 1 ] == '\ n ') <nl> copy_count -= 1 ; <nl>  <nl> - if ( copy_count >= sizeof ( rank -> dimm -> label )) <nl> + if ( copy_count == 0 || copy_count >= sizeof ( rank -> dimm -> label )) <nl> return - EINVAL ; <nl>  <nl> strncpy ( rank -> dimm -> label , data , copy_count ); <nl> static ssize_t dimmdev_label_store ( struct device * dev , <nl> if ( data [ count - 1 ] == '\ 0 ' || data [ count - 1 ] == '\ n ') <nl> copy_count -= 1 ; <nl>  <nl> - if ( copy_count >= sizeof ( dimm -> label )) <nl> + if ( copy_count == 0 || copy_count >= sizeof ( dimm -> label )) <nl> return - EINVAL ; <nl>  <nl> strncpy ( dimm -> label , data , copy_count );
mmm drivers / staging / lustre / lustre / llite / llite_lib . c <nl> ppp drivers / staging / lustre / lustre / llite / llite_lib . c <nl> struct md_op_data * ll_prep_md_op_data ( struct md_op_data * op_data , <nl> op_data -> op_default_stripe_offset = - 1 ; <nl> if ( S_ISDIR ( i1 -> i_mode )) { <nl> op_data -> op_mea1 = ll_i2info ( i1 )-> lli_lsm_md ; <nl> - op_data -> op_default_stripe_offset = <nl> - ll_i2info ( i1 )-> lli_def_stripe_offset ; <nl> + if ( opc == LUSTRE_OPC_MKDIR ) <nl> + op_data -> op_default_stripe_offset = <nl> + ll_i2info ( i1 )-> lli_def_stripe_offset ; <nl> } <nl>  <nl> if ( i2 ) {
mmm drivers / net / ethernet / xilinx / xilinx_emaclite . c <nl> ppp drivers / net / ethernet / xilinx / xilinx_emaclite . c <nl> static int xemaclite_of_probe ( struct platform_device * ofdev ) <nl> } <nl>  <nl> dev_info ( dev , <nl> - " Xilinx EmacLite at 0x % 08lX mapped to 0x % 08lX , irq =% d \ n ", <nl> - ( unsigned long __force ) ndev -> mem_start , <nl> - ( unsigned long __force ) lp -> base_addr , ndev -> irq ); <nl> + " Xilinx EmacLite at 0x % 08lX mapped to 0x % p , irq =% d \ n ", <nl> + ( unsigned long __force ) ndev -> mem_start , lp -> base_addr , ndev -> irq ); <nl> return 0 ; <nl>  <nl> error :
mmm include / sound / soc . h <nl> ppp include / sound / soc . h <nl> struct snd_soc_component_driver { <nl> int (* suspend )( struct snd_soc_component *); <nl> int (* resume )( struct snd_soc_component *); <nl>  <nl> + unsigned int (* read )( struct snd_soc_component *, unsigned int ); <nl> + int (* write )( struct snd_soc_component *, unsigned int , unsigned int ); <nl> + <nl> /* pcm creation and destruction */ <nl> int (* pcm_new )( struct snd_soc_pcm_runtime *); <nl> void (* pcm_free )( struct snd_pcm *);mmm sound / soc / soc - io . c <nl> ppp sound / soc / soc - io . c <nl> struct snd_soc_component_driver { <nl> int (* suspend )( struct snd_soc_component *); <nl> int (* resume )( struct snd_soc_component *); <nl>  <nl> + unsigned int (* read )( struct snd_soc_component *, unsigned int ); <nl> + int (* write )( struct snd_soc_component *, unsigned int , unsigned int ); <nl> + <nl> /* pcm creation and destruction */ <nl> int (* pcm_new )( struct snd_soc_pcm_runtime *); <nl> void (* pcm_free )( struct snd_pcm *); <nl> int snd_soc_component_read ( struct snd_soc_component * component , <nl> ret = regmap_read ( component -> regmap , reg , val ); <nl> else if ( component -> read ) <nl> ret = component -> read ( component , reg , val ); <nl> + else if ( component -> driver -> read ) { <nl> + * val = component -> driver -> read ( component , reg ); <nl> + ret = 0 ; <nl> + } <nl> else <nl> ret = - EIO ; <nl>  <nl> int snd_soc_component_write ( struct snd_soc_component * component , <nl> return regmap_write ( component -> regmap , reg , val ); <nl> else if ( component -> write ) <nl> return component -> write ( component , reg , val ); <nl> + else if ( component -> driver -> write ) <nl> + return component -> driver -> write ( component , reg , val ); <nl> else <nl> return - EIO ; <nl> }
mmm drivers / net / wireless / marvell / mwifiex / pcie . c <nl> ppp drivers / net / wireless / marvell / mwifiex / pcie . c <nl> static int mwifiex_pcie_init_evt_ring ( struct mwifiex_adapter * adapter ) <nl> skb_put ( skb , MAX_EVENT_SIZE ); <nl>  <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MAX_EVENT_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> + kfree ( card -> evtbd_ring_vbase ); <nl> return - 1 ; <nl> + } <nl>  <nl> buf_pa = MWIFIEX_SKB_DMA_ADDR ( skb ); <nl> 
mmm drivers / net / ethernet / tile / tilegx . c <nl> ppp drivers / net / ethernet / tile / tilegx . c <nl> static int tile_net_poll ( struct napi_struct * napi , int budget ) <nl> struct info_mpipe * info_mpipe = <nl> container_of ( napi , struct info_mpipe , napi ); <nl>  <nl> + if ( budget <= 0 ) <nl> + goto done ; <nl> + <nl> instance = info_mpipe -> instance ; <nl> while (( n = gxio_mpipe_iqueue_try_peek ( <nl> & info_mpipe -> iqueue ,
mmm fs / xfs / xfs_inode . c <nl> ppp fs / xfs / xfs_inode . c <nl> xfs_itruncate_start ( <nl> mp = ip -> i_mount ; <nl>  <nl> /* wait for the completion of any pending DIOs */ <nl> - if ( new_size < ip -> i_size ) <nl> + if ( new_size == 0 || new_size < ip -> i_size ) <nl> vn_iowait ( ip ); <nl>  <nl> /*
mmm drivers / staging / ozwpan / ozusbsvc1 . c <nl> ppp drivers / staging / ozwpan / ozusbsvc1 . c <nl> void oz_usb_rx ( struct oz_pd * pd , struct oz_elt * elt ) <nl> case OZ_GET_DESC_RSP : { <nl> struct oz_get_desc_rsp * body = <nl> ( struct oz_get_desc_rsp *) usb_hdr ; <nl> - int data_len = elt -> length - <nl> - sizeof ( struct oz_get_desc_rsp ) + 1 ; <nl> - u16 offs = le16_to_cpu ( get_unaligned (& body -> offset )); <nl> - u16 total_size = <nl> + u16 offs , total_size ; <nl> + u8 data_len ; <nl> + <nl> + if ( elt -> length < sizeof ( struct oz_get_desc_rsp ) - 1 ) <nl> + break ; <nl> + data_len = elt -> length - <nl> + ( sizeof ( struct oz_get_desc_rsp ) - 1 ); <nl> + offs = le16_to_cpu ( get_unaligned (& body -> offset )); <nl> + total_size = <nl> le16_to_cpu ( get_unaligned (& body -> total_size )); <nl> oz_dbg ( ON , " USB_REQ_GET_DESCRIPTOR - cnf \ n "); <nl> oz_hcd_get_desc_cnf ( usb_ctx -> hport , body -> req_id ,
mmm sound / core / timer . c <nl> ppp sound / core / timer . c <nl> static ssize_t snd_timer_user_read ( struct file * file , char __user * buffer , <nl>  <nl> tu = file -> private_data ; <nl> unit = tu -> tread ? sizeof ( struct snd_timer_tread ) : sizeof ( struct snd_timer_read ); <nl> + mutex_lock (& tu -> ioctl_lock ); <nl> spin_lock_irq (& tu -> qlock ); <nl> while (( long ) count - result >= unit ) { <nl> while (! tu -> qused ) { <nl> static ssize_t snd_timer_user_read ( struct file * file , char __user * buffer , <nl> add_wait_queue (& tu -> qchange_sleep , & wait ); <nl>  <nl> spin_unlock_irq (& tu -> qlock ); <nl> + mutex_unlock (& tu -> ioctl_lock ); <nl> schedule (); <nl> + mutex_lock (& tu -> ioctl_lock ); <nl> spin_lock_irq (& tu -> qlock ); <nl>  <nl> remove_wait_queue (& tu -> qchange_sleep , & wait ); <nl> static ssize_t snd_timer_user_read ( struct file * file , char __user * buffer , <nl> tu -> qused --; <nl> spin_unlock_irq (& tu -> qlock ); <nl>  <nl> - mutex_lock (& tu -> ioctl_lock ); <nl> if ( tu -> tread ) { <nl> if ( copy_to_user ( buffer , & tu -> tqueue [ qhead ], <nl> sizeof ( struct snd_timer_tread ))) <nl> static ssize_t snd_timer_user_read ( struct file * file , char __user * buffer , <nl> sizeof ( struct snd_timer_read ))) <nl> err = - EFAULT ; <nl> } <nl> - mutex_unlock (& tu -> ioctl_lock ); <nl>  <nl> spin_lock_irq (& tu -> qlock ); <nl> if ( err < 0 ) <nl> static ssize_t snd_timer_user_read ( struct file * file , char __user * buffer , <nl> } <nl> _error : <nl> spin_unlock_irq (& tu -> qlock ); <nl> + mutex_unlock (& tu -> ioctl_lock ); <nl> return result > 0 ? result : err ; <nl> } <nl> 
mmm net / ipv6 / netfilter / nf_conntrack_reasm . c <nl> ppp net / ipv6 / netfilter / nf_conntrack_reasm . c <nl> find_prev_fhdr ( struct sk_buff * skb , u8 * prevhdrp , int * prevhoff , int * fhoff ) <nl> if (! ipv6_ext_hdr ( nexthdr )) { <nl> return - 1 ; <nl> } <nl> - if ( len < ( int ) sizeof ( struct ipv6_opt_hdr )) { <nl> - pr_debug (" too short \ n "); <nl> - return - 1 ; <nl> - } <nl> if ( nexthdr == NEXTHDR_NONE ) { <nl> pr_debug (" next header is none \ n "); <nl> return - 1 ; <nl> } <nl> + if ( len < ( int ) sizeof ( struct ipv6_opt_hdr )) { <nl> + pr_debug (" too short \ n "); <nl> + return - 1 ; <nl> + } <nl> if ( skb_copy_bits ( skb , start , & hdr , sizeof ( hdr ))) <nl> BUG (); <nl> if ( nexthdr == NEXTHDR_AUTH )
mmm drivers / scsi / fcoe / fcoe . c <nl> ppp drivers / scsi / fcoe / fcoe . c <nl> static int fcoe_interface_setup ( struct fcoe_interface * fcoe , <nl> * use the first one for SPMA */ <nl> real_dev = ( netdev -> priv_flags & IFF_802_1Q_VLAN ) ? <nl> vlan_dev_real_dev ( netdev ) : netdev ; <nl> + fcoe -> realdev = real_dev ; <nl> rcu_read_lock (); <nl> for_each_dev_addr ( real_dev , ha ) { <nl> if (( ha -> type == NETDEV_HW_ADDR_T_SAN ) && <nl> int fcoe_xmit ( struct fc_lport * lport , struct fc_frame * fp ) <nl> skb_reset_network_header ( skb ); <nl> skb -> mac_len = elen ; <nl> skb -> protocol = htons ( ETH_P_FCOE ); <nl> - skb -> dev = fcoe -> netdev ; <nl> + if ( fcoe -> netdev -> priv_flags & IFF_802_1Q_VLAN && <nl> + fcoe -> realdev -> features & NETIF_F_HW_VLAN_TX ) { <nl> + skb -> vlan_tci = VLAN_TAG_PRESENT | <nl> + vlan_dev_vlan_id ( fcoe -> netdev ); <nl> + skb -> dev = fcoe -> realdev ; <nl> + } else <nl> + skb -> dev = fcoe -> netdev ; <nl>  <nl> /* fill up mac and fcoe headers */ <nl> eh = eth_hdr ( skb );mmm drivers / scsi / fcoe / fcoe . h <nl> ppp drivers / scsi / fcoe / fcoe . h <nl> static int fcoe_interface_setup ( struct fcoe_interface * fcoe , <nl> * use the first one for SPMA */ <nl> real_dev = ( netdev -> priv_flags & IFF_802_1Q_VLAN ) ? <nl> vlan_dev_real_dev ( netdev ) : netdev ; <nl> + fcoe -> realdev = real_dev ; <nl> rcu_read_lock (); <nl> for_each_dev_addr ( real_dev , ha ) { <nl> if (( ha -> type == NETDEV_HW_ADDR_T_SAN ) && <nl> int fcoe_xmit ( struct fc_lport * lport , struct fc_frame * fp ) <nl> skb_reset_network_header ( skb ); <nl> skb -> mac_len = elen ; <nl> skb -> protocol = htons ( ETH_P_FCOE ); <nl> - skb -> dev = fcoe -> netdev ; <nl> + if ( fcoe -> netdev -> priv_flags & IFF_802_1Q_VLAN && <nl> + fcoe -> realdev -> features & NETIF_F_HW_VLAN_TX ) { <nl> + skb -> vlan_tci = VLAN_TAG_PRESENT | <nl> + vlan_dev_vlan_id ( fcoe -> netdev ); <nl> + skb -> dev = fcoe -> realdev ; <nl> + } else <nl> + skb -> dev = fcoe -> netdev ; <nl>  <nl> /* fill up mac and fcoe headers */ <nl> eh = eth_hdr ( skb ); <nl> do { \ <nl> struct fcoe_interface { <nl> struct list_head list ; <nl> struct net_device * netdev ; <nl> + struct net_device * realdev ; <nl> struct packet_type fcoe_packet_type ; <nl> struct packet_type fip_packet_type ; <nl> struct fcoe_ctlr ctlr ;
mmm drivers / iommu / amd_iommu_v2 . c <nl> ppp drivers / iommu / amd_iommu_v2 . c <nl> static void do_fault ( struct work_struct * work ) <nl> goto out ; <nl> } <nl>  <nl> + if (!( vma -> vm_flags & ( VM_READ | VM_EXEC | VM_WRITE ))) { <nl> + /* handle_mm_fault would BUG_ON () */ <nl> + up_read (& mm -> mmap_sem ); <nl> + handle_fault_error ( fault ); <nl> + goto out ; <nl> + } <nl> + <nl> ret = handle_mm_fault ( mm , vma , address , write ); <nl> if ( ret & VM_FAULT_ERROR ) { <nl> /* failed to service fault */
mmm net / netfilter / x_tables . c <nl> ppp net / netfilter / x_tables . c <nl> struct xt_table_info * xt_alloc_table_info ( unsigned int size ) <nl> struct xt_table_info * info = NULL ; <nl> size_t sz = sizeof (* info ) + size ; <nl>  <nl> + if ( sz < sizeof (* info )) <nl> + return NULL ; <nl> + <nl> /* Pedantry : prevent them from hitting BUG () in vmalloc . c -- RR */ <nl> if (( SMP_ALIGN ( size ) >> PAGE_SHIFT ) + 2 > totalram_pages ) <nl> return NULL ;
mmm drivers / base / firmware_loader / main . c <nl> ppp drivers / base / firmware_loader / main . c <nl> static int fw_add_devm_name ( struct device * dev , const char * name ) <nl>  <nl> fwn = fw_find_devm_name ( dev , name ); <nl> if ( fwn ) <nl> - return 1 ; <nl> + return 0 ; <nl>  <nl> fwn = devres_alloc ( fw_name_devm_release , sizeof ( struct fw_name_devm ), <nl> GFP_KERNEL ); <nl> int assign_fw ( struct firmware * fw , struct device * device , <nl> unsigned int opt_flags ) <nl> { <nl> struct fw_priv * fw_priv = fw -> priv ; <nl> + int ret ; <nl>  <nl> mutex_lock (& fw_lock ); <nl> if (! fw_priv -> size || fw_state_is_aborted ( fw_priv )) { <nl> int assign_fw ( struct firmware * fw , struct device * device , <nl> */ <nl> /* don ' t cache firmware handled without uevent */ <nl> if ( device && ( opt_flags & FW_OPT_UEVENT ) && <nl> - !( opt_flags & FW_OPT_NOCACHE )) <nl> - fw_add_devm_name ( device , fw_priv -> fw_name ); <nl> + !( opt_flags & FW_OPT_NOCACHE )) { <nl> + ret = fw_add_devm_name ( device , fw_priv -> fw_name ); <nl> + if ( ret ) { <nl> + mutex_unlock (& fw_lock ); <nl> + return ret ; <nl> + } <nl> + } <nl>  <nl> /* <nl> * After caching firmware image is started , let it piggyback
mmm include / drm / drm_mipi_dsi . h <nl> ppp include / drm / drm_mipi_dsi . h <nl> ssize_t mipi_dsi_dcs_read ( struct mipi_dsi_device * dsi , unsigned int channel , <nl> * @ driver : device driver model driver <nl> * @ probe : callback for device binding <nl> * @ remove : callback for device unbinding <nl> + * @ shutdown : called at shutdown time to quiesce the device <nl> */ <nl> struct mipi_dsi_driver { <nl> struct device_driver driver ; <nl> int (* probe )( struct mipi_dsi_device * dsi ); <nl> int (* remove )( struct mipi_dsi_device * dsi ); <nl> + void (* shutdown )( struct mipi_dsi_device * dsi ); <nl> }; <nl>  <nl> # define to_mipi_dsi_driver ( d ) container_of ( d , struct mipi_dsi_driver , driver )mmm drivers / gpu / drm / drm_mipi_dsi . c <nl> ppp drivers / gpu / drm / drm_mipi_dsi . c <nl> ssize_t mipi_dsi_dcs_read ( struct mipi_dsi_device * dsi , unsigned int channel , <nl> * @ driver : device driver model driver <nl> * @ probe : callback for device binding <nl> * @ remove : callback for device unbinding <nl> + * @ shutdown : called at shutdown time to quiesce the device <nl> */ <nl> struct mipi_dsi_driver { <nl> struct device_driver driver ; <nl> int (* probe )( struct mipi_dsi_device * dsi ); <nl> int (* remove )( struct mipi_dsi_device * dsi ); <nl> + void (* shutdown )( struct mipi_dsi_device * dsi ); <nl> }; <nl>  <nl> # define to_mipi_dsi_driver ( d ) container_of ( d , struct mipi_dsi_driver , driver ) <nl> static int mipi_dsi_drv_remove ( struct device * dev ) <nl> return drv -> remove ( dsi ); <nl> } <nl>  <nl> + static void mipi_dsi_drv_shutdown ( struct device * dev ) <nl> +{ <nl> + struct mipi_dsi_driver * drv = to_mipi_dsi_driver ( dev -> driver ); <nl> + struct mipi_dsi_device * dsi = to_mipi_dsi_device ( dev ); <nl> + <nl> + drv -> shutdown ( dsi ); <nl> +} <nl> + <nl> /** <nl> * mipi_dsi_driver_register - register a driver for DSI devices <nl> * @ drv : DSI driver structure <nl> int mipi_dsi_driver_register ( struct mipi_dsi_driver * drv ) <nl> drv -> driver . probe = mipi_dsi_drv_probe ; <nl> if ( drv -> remove ) <nl> drv -> driver . remove = mipi_dsi_drv_remove ; <nl> + if ( drv -> shutdown ) <nl> + drv -> driver . shutdown = mipi_dsi_drv_shutdown ; <nl>  <nl> return driver_register (& drv -> driver ); <nl> }
mmm fs / btrfs / print - tree . c <nl> ppp fs / btrfs / print - tree . c <nl> void print_tree ( struct ctree_root * root , struct tree_buffer * t ) <nl> } <nl> printf (" node % Lu level % d total ptrs % d free spc % u \ n ", t -> blocknr , <nl> node_level ( c -> header . flags ), c -> header . nritems , <nl> - NODEPTRS_PER_BLOCK - c -> header . nritems ); <nl> + ( u32 ) NODEPTRS_PER_BLOCK - c -> header . nritems ); <nl> fflush ( stdout ); <nl> for ( i = 0 ; i < nr ; i ++) { <nl> printf ("\ tkey % d (% Lu % u % Lu ) block % Lu \ n ",
mmm drivers / media / video / pvrusb2 / pvrusb2 - v4l2 . c <nl> ppp drivers / media / video / pvrusb2 / pvrusb2 - v4l2 . c <nl> static struct v4l2_capability pvr_capability ={ <nl> . card = " Hauppauge WinTV pvr - usb2 ", <nl> . bus_info = " usb ", <nl> . version = KERNEL_VERSION ( 0 , 8 , 0 ), <nl> - . capabilities = ( V4L2_CAP_VIDEO_CAPTURE | V4L2_CAP_VBI_CAPTURE | <nl> + . capabilities = ( V4L2_CAP_VIDEO_CAPTURE | <nl> V4L2_CAP_TUNER | V4L2_CAP_AUDIO | V4L2_CAP_RADIO | <nl> V4L2_CAP_READWRITE ), <nl> . reserved = { 0 , 0 , 0 , 0 }
mmm arch / powerpc / mm / pgtable_64 . c <nl> ppp arch / powerpc / mm / pgtable_64 . c <nl> void __iomem * __ioremap ( unsigned long addr , unsigned long size , <nl> pa = addr & PAGE_MASK ; <nl> size = PAGE_ALIGN ( addr + size ) - pa ; <nl>  <nl> - if ( size == 0 ) <nl> + if (( size == 0 ) || ( pa == 0 )) <nl> return NULL ; <nl>  <nl> if ( mem_init_done ) {
mmm drivers / ata / libata - core . c <nl> ppp drivers / ata / libata - core . c <nl> static const struct ata_blacklist_entry ata_device_blacklist [] = { <nl> { " _NEC DV5800A ", NULL , ATA_HORKAGE_NODMA }, <nl> { " SAMSUNG CD - ROM SN - 124 ", " N001 ", ATA_HORKAGE_NODMA }, <nl> { " Seagate STT20000A ", NULL , ATA_HORKAGE_NODMA }, <nl> - { " 2GB ATA Flash Disk ", " ADMA428M ", ATA_HORKAGE_NODMA }, <nl> + { " 2GB ATA Flash Disk ", " ADMA428M ", ATA_HORKAGE_NODMA }, <nl> /* Odd clown on sil3726 / 4726 PMPs */ <nl> { " Config Disk ", NULL , ATA_HORKAGE_DISABLE }, <nl> 
mmm drivers / net / skge . c <nl> ppp drivers / net / skge . c <nl> static void skge_get_regs ( struct net_device * dev , struct ethtool_regs * regs , <nl> /* Wake on Lan only supported on Yukon chips with rev 1 or above */ <nl> static u32 wol_supported ( const struct skge_hw * hw ) <nl> { <nl> - if ( hw -> chip_id == CHIP_ID_YUKON && hw -> chip_rev != 0 ) <nl> - return WAKE_MAGIC | WAKE_PHY ; <nl> - else <nl> + if ( hw -> chip_id == CHIP_ID_GENESIS ) <nl> return 0 ; <nl> + <nl> + if ( hw -> chip_id == CHIP_ID_YUKON && hw -> chip_rev == 0 ) <nl> + return 0 ; <nl> + <nl> + return WAKE_MAGIC | WAKE_PHY ; <nl> } <nl>  <nl> static u32 pci_wake_enabled ( struct pci_dev * dev )
mmm Documentation / lguest / lguest . c <nl> ppp Documentation / lguest / lguest . c <nl> static bool service_io ( struct device * dev ) <nl> } <nl> } <nl>  <nl> + /* OK , so we noted that it was pretty poor to use an fdatasync as a <nl> + * barrier . But Christoph Hellwig points out that we need a sync <nl> + * * afterwards * as well : " Barriers specify no reordering to the front <nl> + * or the back ." And Jens Axboe confirmed it , so here we are : */ <nl> + if ( out -> type & VIRTIO_BLK_T_BARRIER ) <nl> + fdatasync ( vblk -> fd ); <nl> + <nl> /* We can ' t trigger an IRQ , because we ' re not the Launcher . It does <nl> * that when we tell it we ' re done . */ <nl> add_used ( dev -> vq , head , wlen );
mmm kernel / cgroup / cgroup . c <nl> ppp kernel / cgroup / cgroup . c <nl> static int cgroup_enable_threaded ( struct cgroup * cgrp ) <nl> if ( cgroup_is_threaded ( cgrp )) <nl> return 0 ; <nl>  <nl> + /* <nl> + * If @ cgroup is populated or has domain controllers enabled , it <nl> + * can ' t be switched . While the below cgroup_can_be_thread_root () <nl> + * test can catch the same conditions , that ' s only when @ parent is <nl> + * not mixable , so let ' s check it explicitly . <nl> + */ <nl> + if ( cgroup_is_populated ( cgrp ) || <nl> + cgrp -> subtree_control & ~ cgrp_dfl_threaded_ss_mask ) <nl> + return - EOPNOTSUPP ; <nl> + <nl> /* we ' re joining the parent ' s domain , ensure its validity */ <nl> if (! cgroup_is_valid_domain ( dom_cgrp ) || <nl> ! cgroup_can_be_thread_root ( dom_cgrp ))
mmm net / core / filter . c <nl> ppp net / core / filter . c <nl> unsigned int sk_run_filter ( struct sk_buff * skb , struct sock_filter * filter , int <nl> case SKF_AD_MARK : <nl> A = skb -> mark ; <nl> continue ; <nl> + case SKF_AD_QUEUE : <nl> + A = skb -> queue_mapping ; <nl> + continue ; <nl> case SKF_AD_NLATTR : { <nl> struct nlattr * nla ; <nl> mmm include / linux / filter . h <nl> ppp include / linux / filter . h <nl> unsigned int sk_run_filter ( struct sk_buff * skb , struct sock_filter * filter , int <nl> case SKF_AD_MARK : <nl> A = skb -> mark ; <nl> continue ; <nl> + case SKF_AD_QUEUE : <nl> + A = skb -> queue_mapping ; <nl> + continue ; <nl> case SKF_AD_NLATTR : { <nl> struct nlattr * nla ; <nl>  <nl> struct sock_fprog /* Required for SO_ATTACH_FILTER . */ <nl> # define SKF_AD_NLATTR 12 <nl> # define SKF_AD_NLATTR_NEST 16 <nl> # define SKF_AD_MARK 20 <nl> -# define SKF_AD_MAX 24 <nl> +# define SKF_AD_QUEUE 24 <nl> +# define SKF_AD_MAX 28 <nl> # define SKF_NET_OFF (- 0x100000 ) <nl> # define SKF_LL_OFF (- 0x200000 ) <nl> 
mmm drivers / staging / greybus / svc . c <nl> ppp drivers / staging / greybus / svc . c <nl> static void gb_svc_remove_modules ( struct gb_svc * svc ) <nl>  <nl> void gb_svc_del ( struct gb_svc * svc ) <nl> { <nl> - gb_connection_disable ( svc -> connection ); <nl> + gb_connection_disable_rx ( svc -> connection ); <nl>  <nl> /* <nl> * The SVC device and input device may have been registered <nl> void gb_svc_del ( struct gb_svc * svc ) <nl> flush_workqueue ( svc -> wq ); <nl>  <nl> gb_svc_remove_modules ( svc ); <nl> + <nl> + gb_connection_disable ( svc -> connection ); <nl> } <nl>  <nl> void gb_svc_put ( struct gb_svc * svc )
mmm drivers / mtd / nand / fsl_ifc_nand . c <nl> ppp drivers / mtd / nand / fsl_ifc_nand . c <nl> static int fsl_ifc_chip_init ( struct fsl_ifc_mtd * priv ) <nl> chip -> ecc . algo = NAND_ECC_HAMMING ; <nl> } <nl>  <nl> - if ( ctrl -> version == FSL_IFC_VERSION_1_1_0 ) <nl> + if ( ctrl -> version >= FSL_IFC_VERSION_1_1_0 ) <nl> fsl_ifc_sram_init ( priv ); <nl>  <nl> return 0 ;
mmm drivers / hid / i2c - hid / i2c - hid . c <nl> ppp drivers / hid / i2c - hid / i2c - hid . c <nl> static int i2c_hid_hwreset ( struct i2c_client * client ) <nl> static void i2c_hid_get_input ( struct i2c_hid * ihid ) <nl> { <nl> int ret , ret_size ; <nl> - int size = le16_to_cpu ( ihid -> hdesc . wMaxInputLength ); <nl> + int size = ihid -> bufsize ; <nl>  <nl> ret = i2c_master_recv ( ihid -> client , ihid -> inbuf , size ); <nl> if ( ret != size ) {
mmm drivers / rtc / rtc - rp5c01 . c <nl> ppp drivers / rtc / rtc - rp5c01 . c <nl> static ssize_t rp5c01_nvram_read ( struct file * filp , struct kobject * kobj , <nl>  <nl> spin_lock_irq (& priv -> lock ); <nl>  <nl> - for ( count = 0 ; size > 0 && pos < RP5C01_MODE ; count ++, size --) { <nl> + for ( count = 0 ; count < size ; count ++) { <nl> u8 data ; <nl>  <nl> rp5c01_write ( priv , <nl> static ssize_t rp5c01_nvram_write ( struct file * filp , struct kobject * kobj , <nl>  <nl> spin_lock_irq (& priv -> lock ); <nl>  <nl> - for ( count = 0 ; size > 0 && pos < RP5C01_MODE ; count ++, size --) { <nl> + for ( count = 0 ; count < size ; count ++) { <nl> u8 data = * buf ++; <nl>  <nl> rp5c01_write ( priv ,
mmm arch / x86 / mm / highmem_32 . c <nl> ppp arch / x86 / mm / highmem_32 . c <nl> void * kmap_atomic_pfn ( unsigned long pfn , enum km_type type ) <nl>  <nl> return ( void *) vaddr ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( kmap_atomic_pfn ); /* temporarily in use by i915 GEM until vmap */ <nl>  <nl> struct page * kmap_atomic_to_page ( void * ptr ) <nl> {
mmm arch / arm / common / icst525 . c <nl> ppp arch / arm / common / icst525 . c <nl> icst525_khz_to_vco ( const struct icst525_params * p , unsigned long freq ) <nl> break ; <nl> } while ( i < ARRAY_SIZE ( idx2s )); <nl>  <nl> - if ( i > ARRAY_SIZE ( idx2s )) <nl> + if ( i >= ARRAY_SIZE ( idx2s )) <nl> return vco ; <nl>  <nl> vco . s = idx2s [ i ]; <nl> icst525_ps_to_vco ( const struct icst525_params * p , unsigned long period ) <nl> break ; <nl> } while ( i < ARRAY_SIZE ( idx2s )); <nl>  <nl> - if ( i > ARRAY_SIZE ( idx2s )) <nl> + if ( i >= ARRAY_SIZE ( idx2s )) <nl> return vco ; <nl>  <nl> vco . s = idx2s [ i ];mmm arch / arm / common / icst307 . c <nl> ppp arch / arm / common / icst307 . c <nl> icst525_khz_to_vco ( const struct icst525_params * p , unsigned long freq ) <nl> break ; <nl> } while ( i < ARRAY_SIZE ( idx2s )); <nl>  <nl> - if ( i > ARRAY_SIZE ( idx2s )) <nl> + if ( i >= ARRAY_SIZE ( idx2s )) <nl> return vco ; <nl>  <nl> vco . s = idx2s [ i ]; <nl> icst525_ps_to_vco ( const struct icst525_params * p , unsigned long period ) <nl> break ; <nl> } while ( i < ARRAY_SIZE ( idx2s )); <nl>  <nl> - if ( i > ARRAY_SIZE ( idx2s )) <nl> + if ( i >= ARRAY_SIZE ( idx2s )) <nl> return vco ; <nl>  <nl> vco . s = idx2s [ i ]; <nl> icst307_khz_to_vco ( const struct icst307_params * p , unsigned long freq ) <nl> break ; <nl> } while ( i < ARRAY_SIZE ( idx2s )); <nl>  <nl> - if ( i > ARRAY_SIZE ( idx2s )) <nl> + if ( i >= ARRAY_SIZE ( idx2s )) <nl> return vco ; <nl>  <nl> vco . s = idx2s [ i ]; <nl> icst307_ps_to_vco ( const struct icst307_params * p , unsigned long period ) <nl> break ; <nl> } while ( i < ARRAY_SIZE ( idx2s )); <nl>  <nl> - if ( i > ARRAY_SIZE ( idx2s )) <nl> + if ( i >= ARRAY_SIZE ( idx2s )) <nl> return vco ; <nl>  <nl> vco . s = idx2s [ i ];
mmm drivers / watchdog / watchdog_dev . c <nl> ppp drivers / watchdog / watchdog_dev . c <nl> static int watchdog_release ( struct inode * inode , struct file * file ) <nl> struct watchdog_core_data * wd_data = file -> private_data ; <nl> struct watchdog_device * wdd ; <nl> int err = - EBUSY ; <nl> + bool running ; <nl>  <nl> mutex_lock (& wd_data -> lock ); <nl>  <nl> static int watchdog_release ( struct inode * inode , struct file * file ) <nl> clear_bit ( _WDOG_DEV_OPEN , & wd_data -> status ); <nl>  <nl> done : <nl> + running = wdd && watchdog_hw_running ( wdd ); <nl> mutex_unlock (& wd_data -> lock ); <nl> /* <nl> * Allow the owner module to be unloaded again unless the watchdog <nl> * is still running . If the watchdog is still running , it can not <nl> * be stopped , and its driver must not be unloaded . <nl> */ <nl> - if (! watchdog_hw_running ( wdd )) { <nl> - module_put ( wdd -> ops -> owner ); <nl> + if (! running ) { <nl> + module_put ( wd_data -> cdev . owner ); <nl> kref_put (& wd_data -> kref , watchdog_core_data_release ); <nl> } <nl> return 0 ;
mmm drivers / infiniband / hw / mlx4 / qp . c <nl> ppp drivers / infiniband / hw / mlx4 / qp . c <nl> static int __mlx4_ib_modify_qp ( struct ib_qp * ibqp , <nl> attr -> path_mtu ); <nl> goto out ; <nl> } <nl> - context -> mtu_msgmax = ( attr -> path_mtu << 5 ) | 31 ; <nl> + context -> mtu_msgmax = ( attr -> path_mtu << 5 ) | <nl> + ilog2 ( dev -> dev -> caps . max_msg_sz ); <nl> } <nl>  <nl> if ( qp -> rq . wqe_cnt )
mmm drivers / hid / hidraw . c <nl> ppp drivers / hid / hidraw . c <nl> static long hidraw_ioctl ( struct file * file , unsigned int cmd , <nl>  <nl> mutex_lock (& minors_lock ); <nl> dev = hidraw_table [ minor ]; <nl> + if (! dev ) { <nl> + ret = - ENODEV ; <nl> + goto out ; <nl> + } <nl>  <nl> switch ( cmd ) { <nl> case HIDIOCGRDESCSIZE : <nl> static long hidraw_ioctl ( struct file * file , unsigned int cmd , <nl>  <nl> ret = - ENOTTY ; <nl> } <nl> + out : <nl> mutex_unlock (& minors_lock ); <nl> return ret ; <nl> }
mmm drivers / tty / serial / serial_core . c <nl> ppp drivers / tty / serial / serial_core . c <nl> uart_get_console ( struct uart_port * ports , int nr , struct console * co ) <nl> * @ options : ptr for < options > field ; NULL if not present ( out ) <nl> * <nl> * Decodes earlycon kernel command line parameters of the form <nl> - * earlycon =< name >, io | mmio | mmio32 | mmio32be ,< addr >,< options > <nl> - * console =< name >, io | mmio | mmio32 | mmio32be ,< addr >,< options > <nl> + * earlycon =< name >, io | mmio | mmio32 | mmio32be | mmio32native ,< addr >,< options > <nl> + * console =< name >, io | mmio | mmio32 | mmio32be | mmio32native ,< addr >,< options > <nl> * <nl> * The optional form <nl> * earlycon =< name >, 0x < addr >,< options > <nl> int uart_parse_earlycon ( char * p , unsigned char * iotype , unsigned long * addr , <nl> } else if ( strncmp ( p , " mmio32be ,", 9 ) == 0 ) { <nl> * iotype = UPIO_MEM32BE ; <nl> p += 9 ; <nl> + } else if ( strncmp ( p , " mmio32native ,", 13 ) == 0 ) { <nl> + * iotype = IS_ENABLED ( CONFIG_CPU_BIG_ENDIAN ) ? <nl> + UPIO_MEM32BE : UPIO_MEM32 ; <nl> + p += 13 ; <nl> } else if ( strncmp ( p , " io ,", 3 ) == 0 ) { <nl> * iotype = UPIO_PORT ; <nl> p += 3 ;
mmm drivers / scsi / fcoe / fcoe_ctlr . c <nl> ppp drivers / scsi / fcoe / fcoe_ctlr . c <nl> static void fcoe_ctlr_vn_send ( struct fcoe_ctlr * fip , <nl> frame = ( struct fip_frame *) skb -> data ; <nl> memset ( frame , 0 , len ); <nl> memcpy ( frame -> eth . h_dest , dest , ETH_ALEN ); <nl> - memcpy ( frame -> eth . h_source , fip -> ctl_src_addr , ETH_ALEN ); <nl> + <nl> + if ( sub == FIP_SC_VN_BEACON ) { <nl> + hton24 ( frame -> eth . h_source , FIP_VN_FC_MAP ); <nl> + hton24 ( frame -> eth . h_source + 3 , fip -> port_id ); <nl> + } else { <nl> + memcpy ( frame -> eth . h_source , fip -> ctl_src_addr , ETH_ALEN ); <nl> + } <nl> frame -> eth . h_proto = htons ( ETH_P_FIP ); <nl>  <nl> frame -> fip . fip_ver = FIP_VER_ENCAPS ( FIP_VER );
mmm drivers / staging / ozwpan / ozhcd . c <nl> ppp drivers / staging / ozwpan / ozhcd . c <nl> static int oz_build_endpoints_for_interface ( struct usb_hcd * hcd , <nl> int request_heartbeat = 0 ; <nl>  <nl> oz_dbg ( ON , " interface [% d ] = % p \ n ", if_ix , intf ); <nl> + if ( if_ix >= port -> num_iface || port -> iface == NULL ) <nl> + return - ENOMEM ; <nl> for ( i = 0 ; i < intf -> desc . bNumEndpoints ; i ++) { <nl> struct usb_host_endpoint * hep = & intf -> endpoint [ i ]; <nl> u8 ep_addr = hep -> desc . bEndpointAddress ;
mmm kernel / trace / trace_uprobe . c <nl> ppp kernel / trace / trace_uprobe . c <nl> static int create_trace_uprobe ( int argc , char ** argv ) <nl> goto fail_address_parse ; <nl>  <nl> inode = igrab ( path . dentry -> d_inode ); <nl> + if (! S_ISREG ( inode -> i_mode )) { <nl> + ret = - EINVAL ; <nl> + goto fail_address_parse ; <nl> + } <nl>  <nl> argc -= 2 ; <nl> argv += 2 ; <nl> static int create_trace_uprobe ( int argc , char ** argv ) <nl> if ( inode ) <nl> iput ( inode ); <nl>  <nl> - pr_info (" Failed to parse address .\ n "); <nl> + pr_info (" Failed to parse address or file .\ n "); <nl>  <nl> return ret ; <nl> }
mmm drivers / bluetooth / bpa10x . c <nl> ppp drivers / bluetooth / bpa10x . c <nl> static int bpa10x_send_frame ( struct sk_buff * skb ) <nl> return 0 ; <nl> } <nl>  <nl> - static void bpa10x_destruct ( struct hci_dev * hdev ) <nl> -{ <nl> - struct bpa10x_data * data = hdev -> driver_data ; <nl> - <nl> - BT_DBG ("% s ", hdev -> name ); <nl> - <nl> - kfree_skb ( data -> rx_skb [ 0 ]); <nl> - kfree_skb ( data -> rx_skb [ 1 ]); <nl> - kfree ( data ); <nl> -} <nl> - <nl> static int bpa10x_probe ( struct usb_interface * intf , const struct usb_device_id * id ) <nl> { <nl> struct bpa10x_data * data ; <nl> static int bpa10x_probe ( struct usb_interface * intf , const struct usb_device_id * <nl> hdev -> close = bpa10x_close ; <nl> hdev -> flush = bpa10x_flush ; <nl> hdev -> send = bpa10x_send_frame ; <nl> - hdev -> destruct = bpa10x_destruct ; <nl>  <nl> hdev -> owner = THIS_MODULE ; <nl>  <nl> static void bpa10x_disconnect ( struct usb_interface * intf ) <nl> hci_unregister_dev ( data -> hdev ); <nl>  <nl> hci_free_dev ( data -> hdev ); <nl> + kfree_skb ( data -> rx_skb [ 0 ]); <nl> + kfree_skb ( data -> rx_skb [ 1 ]); <nl> + kfree ( data ); <nl> } <nl>  <nl> static struct usb_driver bpa10x_driver = {
mmm drivers / net / can / c_can / c_can_platform . c <nl> ppp drivers / net / can / c_can / c_can_platform . c <nl> # include < linux / clk . h > <nl> # include < linux / of . h > <nl> # include < linux / of_device . h > <nl> -# include < linux / pinctrl / consumer . h > <nl>  <nl> # include < linux / can / dev . h > <nl>  <nl> static int c_can_plat_probe ( struct platform_device * pdev ) <nl> struct c_can_priv * priv ; <nl> const struct of_device_id * match ; <nl> const struct platform_device_id * id ; <nl> - struct pinctrl * pinctrl ; <nl> struct resource * mem , * res ; <nl> int irq ; <nl> struct clk * clk ; <nl> static int c_can_plat_probe ( struct platform_device * pdev ) <nl> id = platform_get_device_id ( pdev ); <nl> } <nl>  <nl> - pinctrl = devm_pinctrl_get_select_default (& pdev -> dev ); <nl> - if ( IS_ERR ( pinctrl )) <nl> - dev_warn (& pdev -> dev , <nl> - " failed to configure pins from driver \ n "); <nl> - <nl> /* get the appropriate clk */ <nl> clk = clk_get (& pdev -> dev , NULL ); <nl> if ( IS_ERR ( clk )) {
mmm net / nfc / llcp / sock . c <nl> ppp net / nfc / llcp / sock . c <nl> static int llcp_sock_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl>  <nl> pr_debug ("% p % zu \ n ", sk , len ); <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( sk -> sk_state == LLCP_CLOSED && <nl> static int llcp_sock_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl>  <nl> pr_debug (" Datagram socket % d % d \ n ", ui_cb -> dsap , ui_cb -> ssap ); <nl>  <nl> + memset ( sockaddr , 0 , sizeof (* sockaddr )); <nl> sockaddr -> sa_family = AF_NFC ; <nl> sockaddr -> nfc_protocol = NFC_PROTO_NFC_DEP ; <nl> sockaddr -> dsap = ui_cb -> dsap ;
mmm drivers / nfc / nfcmrvl / main . c <nl> ppp drivers / nfc / nfcmrvl / main . c <nl> void nfcmrvl_nci_unregister_dev ( struct nfcmrvl_private * priv ) <nl> { <nl> struct nci_dev * ndev = priv -> ndev ; <nl>  <nl> + nci_unregister_device ( ndev ); <nl> if ( priv -> ndev -> nfc_dev -> fw_download_in_progress ) <nl> nfcmrvl_fw_dnld_abort ( priv ); <nl>  <nl> void nfcmrvl_nci_unregister_dev ( struct nfcmrvl_private * priv ) <nl> if ( gpio_is_valid ( priv -> config . reset_n_io )) <nl> gpio_free ( priv -> config . reset_n_io ); <nl>  <nl> - nci_unregister_device ( ndev ); <nl> nci_free_device ( ndev ); <nl> kfree ( priv ); <nl> }
mmm drivers / gpu / drm / sun4i / sun4i_tcon . h <nl> ppp drivers / gpu / drm / sun4i / sun4i_tcon . h <nl> struct sun4i_tcon { <nl> /* Associated crtc */ <nl> struct sun4i_crtc * crtc ; <nl>  <nl> + int id ; <nl> + <nl> /* TCON list management */ <nl> struct list_head list ; <nl> };mmm drivers / gpu / drm / sun4i / sun4i_tcon . c <nl> ppp drivers / gpu / drm / sun4i / sun4i_tcon . c <nl> struct sun4i_tcon { <nl> /* Associated crtc */ <nl> struct sun4i_crtc * crtc ; <nl>  <nl> + int id ; <nl> + <nl> /* TCON list management */ <nl> struct list_head list ; <nl> }; <nl> static int sun4i_tcon_bind ( struct device * dev , struct device * master , <nl> dev_set_drvdata ( dev , tcon ); <nl> tcon -> drm = drm ; <nl> tcon -> dev = dev ; <nl> + tcon -> id = backend -> id ; <nl> tcon -> quirks = of_device_get_match_data ( dev ); <nl>  <nl> tcon -> lcd_rst = devm_reset_control_get ( dev , " lcd ");
mmm drivers / media / video / ivtv / ivtv - ioctl . c <nl> ppp drivers / media / video / ivtv / ivtv - ioctl . c <nl> int ivtv_v4l2_ioctls ( struct ivtv * itv , struct file * filp , unsigned int cmd , void <nl>  <nl> memset ( vcap , 0 , sizeof (* vcap )); <nl> strcpy ( vcap -> driver , IVTV_DRIVER_NAME ); /* driver name */ <nl> - strcpy ( vcap -> card , itv -> card_name ); /* card type */ <nl> + strncpy ( vcap -> card , itv -> card_name , <nl> + sizeof ( vcap -> card )- 1 ); /* card type */ <nl> strcpy ( vcap -> bus_info , pci_name ( itv -> dev )); /* bus info ... */ <nl> vcap -> version = IVTV_DRIVER_VERSION ; /* version */ <nl> vcap -> capabilities = itv -> v4l2_cap ; /* capabilities */
mmm drivers / platform / x86 / thinkpad_acpi . c <nl> ppp drivers / platform / x86 / thinkpad_acpi . c <nl> static void thinkpad_acpi_module_exit ( void ) <nl> kfree ( thinkpad_id . bios_version_str ); <nl> kfree ( thinkpad_id . ec_version_str ); <nl> kfree ( thinkpad_id . model_str ); <nl> + kfree ( thinkpad_id . nummodel_str ); <nl> } <nl>  <nl> 
mmm drivers / staging / comedi / drivers / jr3_pci . c <nl> ppp drivers / staging / comedi / drivers / jr3_pci . c <nl> static struct pci_driver jr3_pci_pci_driver = { <nl> module_comedi_pci_driver ( jr3_pci_driver , jr3_pci_pci_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for JR3 / PCI force sensor board "); <nl> MODULE_LICENSE (" GPL "); <nl> MODULE_FIRMWARE (" comedi / jr3pci . idm ");
mmm drivers / media / pci / saa7146 / mxb . c <nl> ppp drivers / media / pci / saa7146 / mxb . c <nl> static int vidioc_g_register ( struct file * file , void * fh , struct v4l2_dbg_regist <nl> { <nl> struct saa7146_dev * dev = (( struct saa7146_fh *) fh )-> dev ; <nl>  <nl> + if ( reg -> reg > pci_resource_len ( dev -> pci , 0 ) - 4 ) <nl> + return - EINVAL ; <nl> reg -> val = saa7146_read ( dev , reg -> reg ); <nl> reg -> size = 4 ; <nl> return 0 ; <nl> static int vidioc_s_register ( struct file * file , void * fh , const struct v4l2_dbg_ <nl> { <nl> struct saa7146_dev * dev = (( struct saa7146_fh *) fh )-> dev ; <nl>  <nl> + if ( reg -> reg > pci_resource_len ( dev -> pci , 0 ) - 4 ) <nl> + return - EINVAL ; <nl> saa7146_write ( dev , reg -> reg , reg -> val ); <nl> return 0 ; <nl> }
mmm drivers / serial / uartlite . c <nl> ppp drivers / serial / uartlite . c <nl> static int ulite_transmit ( struct uart_port * port , int stat ) <nl> static irqreturn_t ulite_isr ( int irq , void * dev_id ) <nl> { <nl> struct uart_port * port = dev_id ; <nl> - int busy ; <nl> + int busy , n = 0 ; <nl>  <nl> do { <nl> int stat = readb ( port -> membase + ULITE_STATUS ); <nl> busy = ulite_receive ( port , stat ); <nl> busy |= ulite_transmit ( port , stat ); <nl> + n ++; <nl> } while ( busy ); <nl>  <nl> - tty_flip_buffer_push ( port -> state -> port . tty ); <nl> - <nl> - return IRQ_HANDLED ; <nl> + /* work done ? */ <nl> + if ( n > 1 ) { <nl> + tty_flip_buffer_push ( port -> state -> port . tty ); <nl> + return IRQ_HANDLED ; <nl> + } else { <nl> + return IRQ_NONE ; <nl> + } <nl> } <nl>  <nl> static unsigned int ulite_tx_empty ( struct uart_port * port ) <nl> static int ulite_startup ( struct uart_port * port ) <nl> int ret ; <nl>  <nl> ret = request_irq ( port -> irq , ulite_isr , <nl> - IRQF_DISABLED | IRQF_SAMPLE_RANDOM , " uartlite ", port ); <nl> + IRQF_SHARED | IRQF_SAMPLE_RANDOM , " uartlite ", port ); <nl> if ( ret ) <nl> return ret ; <nl> 
mmm drivers / mfd / t7l66xb . c <nl> ppp drivers / mfd / t7l66xb . c <nl> static int t7l66xb_probe ( struct platform_device * dev ) <nl> t7l66xb -> clk48m = clk_get (& dev -> dev , " CLK_CK48M "); <nl> if ( IS_ERR ( t7l66xb -> clk48m )) { <nl> ret = PTR_ERR ( t7l66xb -> clk48m ); <nl> - clk_put ( t7l66xb -> clk32k ); <nl> goto err_clk48m_get ; <nl> } <nl>  <nl> static int t7l66xb_remove ( struct platform_device * dev ) <nl> ret = pdata -> disable ( dev ); <nl> clk_disable ( t7l66xb -> clk48m ); <nl> clk_put ( t7l66xb -> clk48m ); <nl> + clk_disable ( t7l66xb -> clk32k ); <nl> + clk_put ( t7l66xb -> clk32k ); <nl> t7l66xb_detach_irq ( dev ); <nl> iounmap ( t7l66xb -> scr ); <nl> release_resource (& t7l66xb -> rscr );
mmm kernel / user_namespace . c <nl> ppp kernel / user_namespace . c <nl> static ssize_t map_write ( struct file * file , const char __user * buf , <nl> if (! new_idmap_permitted ( file , ns , cap_setid , & new_map )) <nl> goto out ; <nl>  <nl> - ret = sort_idmaps (& new_map ); <nl> - if ( ret < 0 ) <nl> - goto out ; <nl> - <nl> ret = - EPERM ; <nl> /* Map the lower ids from the parent user namespace to the <nl> * kernel global id space . <nl> static ssize_t map_write ( struct file * file , const char __user * buf , <nl> e -> lower_first = lower_first ; <nl> } <nl>  <nl> + /* <nl> + * If we want to use binary search for lookup , this clones the extent <nl> + * array and sorts both copies . <nl> + */ <nl> + ret = sort_idmaps (& new_map ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> + <nl> /* Install the map */ <nl> if ( new_map . nr_extents <= UID_GID_MAP_MAX_BASE_EXTENTS ) { <nl> memcpy ( map -> extent , new_map . extent ,
mmm net / sctp / sm_statefuns . c <nl> ppp net / sctp / sm_statefuns . c <nl> sctp_disposition_t sctp_sf_eat_auth ( const struct sctp_endpoint * ep , <nl> struct sctp_chunk * err_chunk ; <nl> sctp_ierror_t error ; <nl>  <nl> + /* Make sure that the peer has AUTH capable */ <nl> + if (! asoc -> peer . auth_capable ) <nl> + return sctp_sf_unk_chunk ( ep , asoc , type , arg , commands ); <nl> + <nl> if (! sctp_vtag_verify ( chunk , asoc )) { <nl> sctp_add_cmd_sf ( commands , SCTP_CMD_REPORT_BAD_TAG , <nl> SCTP_NULL ());
mmm drivers / acpi / scan . c <nl> ppp drivers / acpi / scan . c <nl> int acpi_bus_start ( struct acpi_device * device ) <nl> { <nl> struct acpi_bus_ops ops ; <nl>  <nl> + if (! device ) <nl> + return - EINVAL ; <nl> + <nl> memset (& ops , 0 , sizeof ( ops )); <nl> ops . acpi_op_start = 1 ; <nl> 
mmm fs / btrfs / volumes . c <nl> ppp fs / btrfs / volumes . c <nl> void btrfs_destroy_dev_replace_tgtdev ( struct btrfs_fs_info * fs_info , <nl> mutex_lock (& uuid_mutex ); <nl> WARN_ON (! tgtdev ); <nl> mutex_lock (& fs_info -> fs_devices -> device_list_mutex ); <nl> + <nl> + btrfs_kobj_rm_device ( fs_info -> fs_devices , tgtdev ); <nl> + <nl> if ( tgtdev -> bdev ) { <nl> btrfs_scratch_superblock ( tgtdev ); <nl> fs_info -> fs_devices -> open_devices --;mmm fs / btrfs / dev - replace . c <nl> ppp fs / btrfs / dev - replace . c <nl> void btrfs_destroy_dev_replace_tgtdev ( struct btrfs_fs_info * fs_info , <nl> mutex_lock (& uuid_mutex ); <nl> WARN_ON (! tgtdev ); <nl> mutex_lock (& fs_info -> fs_devices -> device_list_mutex ); <nl> + <nl> + btrfs_kobj_rm_device ( fs_info -> fs_devices , tgtdev ); <nl> + <nl> if ( tgtdev -> bdev ) { <nl> btrfs_scratch_superblock ( tgtdev ); <nl> fs_info -> fs_devices -> open_devices --; <nl> int btrfs_dev_replace_start ( struct btrfs_root * root , <nl> WARN_ON (! tgt_device ); <nl> dev_replace -> tgtdev = tgt_device ; <nl>  <nl> + ret = btrfs_kobj_add_device ( tgt_device -> fs_devices , tgt_device ); <nl> + if ( ret ) <nl> + btrfs_error ( root -> fs_info , ret , " kobj add dev failed "); <nl> + <nl> printk_in_rcu ( KERN_INFO <nl> " BTRFS : dev_replace from % s ( devid % llu ) to % s started \ n ", <nl> src_device -> missing ? "< missing disk >" : <nl> static int btrfs_dev_replace_finishing ( struct btrfs_fs_info * fs_info , <nl>  <nl> /* replace the sysfs entry */ <nl> btrfs_kobj_rm_device ( fs_info -> fs_devices , src_device ); <nl> - btrfs_kobj_add_device ( fs_info -> fs_devices , tgt_device ); <nl> btrfs_rm_dev_replace_free_srcdev ( fs_info , src_device ); <nl>  <nl> /* write back the superblocks */
mmm net / sched / cls_flower . c <nl> ppp net / sched / cls_flower . c <nl> static int fl_dump ( struct net * net , struct tcf_proto * tp , void * fh , <nl> if ( fl_dump_key_vlan ( skb , & key -> vlan , & mask -> vlan )) <nl> goto nla_put_failure ; <nl>  <nl> + if ( mask -> vlan . vlan_tpid && <nl> + nla_put_be16 ( skb , TCA_FLOWER_KEY_VLAN_ETH_TYPE , key -> basic . n_proto )) <nl> + goto nla_put_failure ; <nl> + <nl> if (( key -> basic . n_proto == htons ( ETH_P_IP ) || <nl> key -> basic . n_proto == htons ( ETH_P_IPV6 )) && <nl> ( fl_dump_key_val ( skb , & key -> basic . ip_proto , TCA_FLOWER_KEY_IP_PROTO ,
mmm drivers / net / ethernet / netronome / nfp / flower / action . c <nl> ppp drivers / net / ethernet / netronome / nfp / flower / action . c <nl> nfp_fl_output ( struct nfp_fl_output * output , const struct tc_action * action , <nl> */ <nl> if (! switchdev_port_same_parent_id ( in_dev , out_dev )) <nl> return - EOPNOTSUPP ; <nl> + if (! nfp_netdev_is_nfp_repr ( out_dev )) <nl> + return - EOPNOTSUPP ; <nl>  <nl> output -> port = cpu_to_be32 ( nfp_repr_get_port_id ( out_dev )); <nl> if (! output -> port )
mmm drivers / usb / core / devio . c <nl> ppp drivers / usb / core / devio . c <nl> static int proc_do_submiturb ( struct usb_dev_state * ps , struct usbdevfs_urb * uurb <nl> u = ( is_in ? URB_DIR_IN : URB_DIR_OUT ); <nl> if ( uurb -> flags & USBDEVFS_URB_ISO_ASAP ) <nl> u |= URB_ISO_ASAP ; <nl> - if ( uurb -> flags & USBDEVFS_URB_SHORT_NOT_OK ) <nl> + if ( uurb -> flags & USBDEVFS_URB_SHORT_NOT_OK && is_in ) <nl> u |= URB_SHORT_NOT_OK ; <nl> if ( uurb -> flags & USBDEVFS_URB_NO_FSBR ) <nl> u |= URB_NO_FSBR ;
mmm net / ipv6 / addrconf . c <nl> ppp net / ipv6 / addrconf . c <nl> static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> break ; <nl> } <nl>  <nl> + if (! idev && dev -> mtu >= IPV6_MIN_MTU ) <nl> + idev = ipv6_add_dev ( dev ); <nl> + <nl> if ( idev ) <nl> idev -> if_flags |= IF_READY ; <nl> } else { <nl> static int addrconf_notify ( struct notifier_block * this , unsigned long event , <nl> break ; <nl>  <nl> case NETDEV_CHANGEMTU : <nl> - if ( idev && dev -> mtu >= IPV6_MIN_MTU ) { <nl> + if ( idev && dev -> mtu >= IPV6_MIN_MTU ) { <nl> rt6_mtu_change ( dev , dev -> mtu ); <nl> idev -> cnf . mtu6 = dev -> mtu ; <nl> break ; <nl> } <nl>  <nl> + if (! idev && dev -> mtu >= IPV6_MIN_MTU ) { <nl> + idev = ipv6_add_dev ( dev ); <nl> + if ( idev ) <nl> + break ; <nl> + } <nl> + <nl> /* MTU falled under IPV6_MIN_MTU . Stop IPv6 on this interface . */ <nl>  <nl> case NETDEV_DOWN :
mmm drivers / char / serial167 . c <nl> ppp drivers / char / serial167 . c <nl> cy_put_char ( struct tty_struct * tty , unsigned char ch ) <nl> if ( serial_paranoia_check ( info , tty -> name , " cy_put_char ")) <nl> return ; <nl>  <nl> - if (! tty || ! info -> xmit_buf ) <nl> + if (! info -> xmit_buf ) <nl> return ; <nl>  <nl> local_irq_save ( flags ); <nl> cy_write ( struct tty_struct * tty , <nl> return 0 ; <nl> } <nl>  <nl> - if (! tty || ! info -> xmit_buf ){ <nl> + if (! info -> xmit_buf ){ <nl> return 0 ; <nl> } <nl> 
mmm net / sctp / associola . c <nl> ppp net / sctp / associola . c <nl> void sctp_association_free ( struct sctp_association * asoc ) <nl> /* Only real associations count against the endpoint , so <nl> * don ' t bother for if this is a temporary association . <nl> */ <nl> - if (! asoc -> temp ) { <nl> + if (! list_empty (& asoc -> asocs )) { <nl> list_del (& asoc -> asocs ); <nl>  <nl> /* Decrement the backlog value for a TCP - style listening
mmm drivers / gpu / drm / msm / hdmi / hdmi_audio . c <nl> ppp drivers / gpu / drm / msm / hdmi / hdmi_audio . c <nl> # include < linux / hdmi . h > <nl> # include " hdmi . h " <nl>  <nl> - <nl> -/* Supported HDMI Audio channels */ <nl> -# define MSM_HDMI_AUDIO_CHANNEL_2 0 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_4 1 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_6 2 <nl> -# define MSM_HDMI_AUDIO_CHANNEL_8 3 <nl> - <nl> /* maps MSM_HDMI_AUDIO_CHANNEL_n consts used by audio driver to # of channels : */ <nl> static int nchannels [] = { 2 , 4 , 6 , 8 }; <nl> 
mmm drivers / dma / dma - jz4780 . c <nl> ppp drivers / dma / dma - jz4780 . c <nl> static struct dma_chan * jz4780_of_dma_xlate ( struct of_phandle_args * dma_spec , <nl> data . channel ); <nl> return NULL ; <nl> } <nl> - } <nl>  <nl> - return dma_request_channel ( mask , jz4780_dma_filter_fn , & data ); <nl> + jzdma -> chan [ data . channel ]. transfer_type = data . transfer_type ; <nl> + <nl> + return dma_get_slave_channel ( <nl> + & jzdma -> chan [ data . channel ]. vchan . chan ); <nl> + } else { <nl> + return dma_request_channel ( mask , jz4780_dma_filter_fn , & data ); <nl> + } <nl> } <nl>  <nl> static int jz4780_dma_probe ( struct platform_device * pdev )
mmm drivers / misc / cxl / irq . c <nl> ppp drivers / misc / cxl / irq . c <nl> int afu_register_irqs ( struct cxl_context * ctx , u32 count ) <nl> */ <nl> INIT_LIST_HEAD (& ctx -> irq_names ); <nl> for ( r = 1 ; r < CXL_IRQ_RANGES ; r ++) { <nl> - for ( i = 0 ; i < ctx -> irqs . range [ r ]; hwirq ++, i ++) { <nl> + for ( i = 0 ; i < ctx -> irqs . range [ r ]; i ++) { <nl> irq_name = kmalloc ( sizeof ( struct cxl_irq_name ), <nl> GFP_KERNEL ); <nl> if (! irq_name )
mmm drivers / gpu / drm / amd / amdgpu / gfx_v9_0 . c <nl> ppp drivers / gpu / drm / amd / amdgpu / gfx_v9_0 . c <nl> static int gfx_v9_0_ngg_init ( struct amdgpu_device * adev ) <nl> adev -> gfx . ngg . gds_reserve_size = ALIGN ( 5 * 4 , 0x40 ); <nl> adev -> gds . mem . total_size -= adev -> gfx . ngg . gds_reserve_size ; <nl> adev -> gds . mem . gfx_partition_size -= adev -> gfx . ngg . gds_reserve_size ; <nl> - adev -> gfx . ngg . gds_reserve_addr = SOC15_REG_OFFSET ( GC , 0 , mmGDS_VMID0_BASE ); <nl> - adev -> gfx . ngg . gds_reserve_addr += adev -> gds . mem . gfx_partition_size ; <nl> + adev -> gfx . ngg . gds_reserve_addr = RREG32_SOC15 ( GC , 0 , mmGDS_VMID0_BASE ); <nl> + adev -> gfx . ngg . gds_reserve_addr += RREG32_SOC15 ( GC , 0 , mmGDS_VMID0_SIZE ); <nl>  <nl> /* Primitive Buffer */ <nl> r = gfx_v9_0_ngg_create_buf ( adev , & adev -> gfx . ngg . buf [ NGG_PRIM ], <nl> static int gfx_v9_0_ngg_en ( struct amdgpu_device * adev ) <nl>  <nl> amdgpu_ring_write ( ring , PACKET3 ( PACKET3_DMA_DATA , 5 )); <nl> amdgpu_ring_write ( ring , ( PACKET3_DMA_DATA_CP_SYNC | <nl> + PACKET3_DMA_DATA_DST_SEL ( 1 ) | <nl> PACKET3_DMA_DATA_SRC_SEL ( 2 ))); <nl> amdgpu_ring_write ( ring , 0 ); <nl> amdgpu_ring_write ( ring , 0 ); <nl> amdgpu_ring_write ( ring , adev -> gfx . ngg . gds_reserve_addr ); <nl> amdgpu_ring_write ( ring , 0 ); <nl> - amdgpu_ring_write ( ring , adev -> gfx . ngg . gds_reserve_size ); <nl> - <nl> + amdgpu_ring_write ( ring , PACKET3_DMA_DATA_CMD_RAW_WAIT | <nl> + adev -> gfx . ngg . gds_reserve_size ); <nl>  <nl> gfx_v9_0_write_data_to_reg ( ring , 0 , false , <nl> SOC15_REG_OFFSET ( GC , 0 , mmGDS_VMID0_SIZE ), 0 );
mmm drivers / gpu / drm / radeon / r600_cs . c <nl> ppp drivers / gpu / drm / radeon / r600_cs . c <nl> static inline int r600_cs_track_validate_cb ( struct radeon_cs_parser * p , int i ) <nl> if ( array_mode == V_0280A0_ARRAY_LINEAR_GENERAL ) { <nl> /* the initial DDX does bad things with the CB size occasionally */ <nl> /* it rounds up height too far for slice tile max but the BO is smaller */ <nl> - tmp = ( height - 7 ) * 8 * bpe ; <nl> + tmp = ( height - 7 ) * pitch * bpe ; <nl> if (( tmp + track -> cb_color_bo_offset [ i ]) > radeon_bo_size ( track -> cb_color_bo [ i ])) { <nl> dev_warn ( p -> dev , "% s offset [% d ] % d % d % lu too big \ n ", __func__ , i , track -> cb_color_bo_offset [ i ], tmp , radeon_bo_size ( track -> cb_color_bo [ i ])); <nl> return - EINVAL ;
mmm drivers / phy / phy - rockchip - usb . c <nl> ppp drivers / phy / phy - rockchip - usb . c <nl> static int rockchip_usb_phy_init ( struct rockchip_usb_phy_base * base , <nl> goto err_clk_prov ; <nl> } <nl>  <nl> - err = devm_add_action ( base -> dev , rockchip_usb_phy_action , rk_phy ); <nl> + err = devm_add_action_or_reset ( base -> dev , rockchip_usb_phy_action , <nl> + rk_phy ); <nl> if ( err ) <nl> - goto err_devm_action ; <nl> + return err ; <nl>  <nl> rk_phy -> phy = devm_phy_create ( base -> dev , child , & ops ); <nl> if ( IS_ERR ( rk_phy -> phy )) { <nl> static int rockchip_usb_phy_init ( struct rockchip_usb_phy_base * base , <nl> else <nl> return rockchip_usb_phy_power ( rk_phy , 1 ); <nl>  <nl> - err_devm_action : <nl> - if (! rk_phy -> uart_enabled ) <nl> - of_clk_del_provider ( child ); <nl> err_clk_prov : <nl> if (! rk_phy -> uart_enabled ) <nl> clk_unregister ( rk_phy -> clk480m );
mmm fs / ext4 / xattr . c <nl> ppp fs / ext4 / xattr . c <nl> int ext4_expand_extra_isize_ea ( struct inode * inode , int new_extra_isize , <nl> goto cleanup ; <nl> kfree ( b_entry_name ); <nl> kfree ( buffer ); <nl> + b_entry_name = NULL ; <nl> + buffer = NULL ; <nl> brelse ( is -> iloc . bh ); <nl> kfree ( is ); <nl> kfree ( bs );
mmm drivers / media / rc / imon . c <nl> ppp drivers / media / rc / imon . c <nl> static void imon_incoming_packet ( struct imon_context * ictx , <nl> if ( press_type == 0 ) <nl> rc_keyup ( ictx -> rdev ); <nl> else { <nl> - if ( ictx -> rc_type == RC_BIT_RC6_MCE ) <nl> + if ( ictx -> rc_type == RC_BIT_RC6_MCE || <nl> + ictx -> rc_type == RC_BIT_OTHER ) <nl> rc_keydown ( ictx -> rdev , <nl> ictx -> rc_type == RC_BIT_RC6_MCE ? RC_TYPE_RC6_MCE : RC_TYPE_OTHER , <nl> ictx -> rc_scancode , ictx -> rc_toggle );
mmm arch / x86 / kvm / mmu . c <nl> ppp arch / x86 / kvm / mmu . c <nl> void kvm_mmu_zap_all ( struct kvm * kvm ) <nl> kvm_flush_remote_tlbs ( kvm ); <nl> } <nl>  <nl> - static void kvm_mmu_remove_one_alloc_mmu_page ( struct kvm * kvm ) <nl> + static int kvm_mmu_remove_some_alloc_mmu_pages ( struct kvm * kvm ) <nl> { <nl> struct kvm_mmu_page * page ; <nl>  <nl> page = container_of ( kvm -> arch . active_mmu_pages . prev , <nl> struct kvm_mmu_page , link ); <nl> - kvm_mmu_zap_page ( kvm , page ); <nl> + return kvm_mmu_zap_page ( kvm , page ) + 1 ; <nl> } <nl>  <nl> static int mmu_shrink ( int nr_to_scan , gfp_t gfp_mask ) <nl> static int mmu_shrink ( int nr_to_scan , gfp_t gfp_mask ) <nl> spin_lock (& kvm_lock ); <nl>  <nl> list_for_each_entry ( kvm , & vm_list , vm_list ) { <nl> - int npages , idx ; <nl> + int npages , idx , freed_pages ; <nl>  <nl> idx = srcu_read_lock (& kvm -> srcu ); <nl> spin_lock (& kvm -> mmu_lock ); <nl> static int mmu_shrink ( int nr_to_scan , gfp_t gfp_mask ) <nl> kvm -> arch . n_free_mmu_pages ; <nl> cache_count += npages ; <nl> if (! kvm_freed && nr_to_scan > 0 && npages > 0 ) { <nl> - kvm_mmu_remove_one_alloc_mmu_page ( kvm ); <nl> - cache_count --; <nl> + freed_pages = kvm_mmu_remove_some_alloc_mmu_pages ( kvm ); <nl> + cache_count -= freed_pages ; <nl> kvm_freed = kvm ; <nl> } <nl> nr_to_scan --;
mmm drivers / dma / pxa_dma . c <nl> ppp drivers / dma / pxa_dma . c <nl> static int pxad_probe ( struct platform_device * op ) <nl> pdev -> slave . dst_addr_widths = widths ; <nl> pdev -> slave . directions = BIT ( DMA_MEM_TO_DEV ) | BIT ( DMA_DEV_TO_MEM ); <nl> pdev -> slave . residue_granularity = DMA_RESIDUE_GRANULARITY_DESCRIPTOR ; <nl> + pdev -> slave . descriptor_reuse = true ; <nl>  <nl> pdev -> slave . dev = & op -> dev ; <nl> ret = pxad_init_dmadev ( op , pdev , dma_channels );
mmm net / irda / iriap . c <nl> ppp net / irda / iriap . c <nl> static void iriap_getvaluebyclass_indication ( struct iriap_cb * self , <nl> n = 1 ; <nl>  <nl> name_len = fp [ n ++]; <nl> + <nl> + IRDA_ASSERT ( name_len < IAS_MAX_CLASSNAME + 1 , return ;); <nl> + <nl> memcpy ( name , fp + n , name_len ); n += name_len ; <nl> name [ name_len ] = '\ 0 '; <nl>  <nl> attr_len = fp [ n ++]; <nl> + <nl> + IRDA_ASSERT ( attr_len < IAS_MAX_ATTRIBNAME + 1 , return ;); <nl> + <nl> memcpy ( attr , fp + n , attr_len ); n += attr_len ; <nl> attr [ attr_len ] = '\ 0 '; <nl> 
mmm arch / ia64 / kernel / efi . c <nl> ppp arch / ia64 / kernel / efi . c <nl> efi_initialize_iomem_resources ( struct resource * code_resource , <nl> if ( md -> attribute & EFI_MEMORY_WP ) { <nl> name = " System ROM "; <nl> flags |= IORESOURCE_READONLY ; <nl> - } else { <nl> + } else if ( md -> attribute == EFI_MEMORY_UC ) <nl> + name = " Uncached RAM "; <nl> + else <nl> name = " System RAM "; <nl> - } <nl> break ; <nl>  <nl> case EFI_ACPI_MEMORY_NVS :
mmm sound / soc / fsl / imx - pcm - dma . c <nl> ppp sound / soc / fsl / imx - pcm - dma . c <nl> static int snd_imx_open ( struct snd_pcm_substream * substream ) <nl> dma_params = snd_soc_dai_get_dma_data ( rtd -> cpu_dai , substream ); <nl>  <nl> dma_data = kzalloc ( sizeof (* dma_data ), GFP_KERNEL ); <nl> + if (! dma_data ) <nl> + return - ENOMEM ; <nl> + <nl> dma_data -> peripheral_type = dma_params -> shared_peripheral ? <nl> IMX_DMATYPE_SSI_SP : IMX_DMATYPE_SSI ; <nl> dma_data -> priority = DMA_PRIO_HIGH ;
mmm drivers / gpu / drm / i915 / i915_debugfs . c <nl> ppp drivers / gpu / drm / i915 / i915_debugfs . c <nl> static int i915_sink_crc ( struct seq_file * m , void * data ) <nl> static int i915_energy_uJ ( struct seq_file * m , void * data ) <nl> { <nl> struct drm_i915_private * dev_priv = node_to_i915 ( m -> private ); <nl> - u64 power ; <nl> + unsigned long long power ; <nl> u32 units ; <nl>  <nl> if ( INTEL_GEN ( dev_priv ) < 6 ) <nl> static int i915_energy_uJ ( struct seq_file * m , void * data ) <nl>  <nl> intel_runtime_pm_get ( dev_priv ); <nl>  <nl> - rdmsrl ( MSR_RAPL_POWER_UNIT , power ); <nl> - power = ( power & 0x1f00 ) >> 8 ; <nl> - units = 1000000 / ( 1 << power ); /* convert to uJ */ <nl> + if ( rdmsrl_safe ( MSR_RAPL_POWER_UNIT , & power )) { <nl> + intel_runtime_pm_put ( dev_priv ); <nl> + return - ENODEV ; <nl> + } <nl> + <nl> + units = ( power & 0x1f00 ) >> 8 ; <nl> power = I915_READ ( MCH_SECP_NRG_STTS ); <nl> - power *= units ; <nl> + power = ( 1000000 * power ) >> units ; /* convert to uJ */ <nl>  <nl> intel_runtime_pm_put ( dev_priv ); <nl>  <nl> - seq_printf ( m , "% llu ", ( long long unsigned ) power ); <nl> + seq_printf ( m , "% llu ", power ); <nl>  <nl> return 0 ; <nl> }
mmm drivers / nfc / port100 . c <nl> ppp drivers / nfc / port100 . c <nl> static const struct port100_in_rf_setting in_rf_settings [] = { <nl> . in_recv_set_number = 15 , <nl> . in_recv_comm_type = PORT100_COMM_TYPE_IN_106A , <nl> }, <nl> + /* Ensures the array has NFC_DIGITAL_RF_TECH_LAST elements */ <nl> + [ NFC_DIGITAL_RF_TECH_LAST ] = { 0 }, <nl> }; <nl>  <nl> /** <nl> static const struct port100_tg_rf_setting tg_rf_settings [] = { <nl> . tg_set_number = 8 , <nl> . tg_comm_type = PORT100_COMM_TYPE_TG_424F , <nl> }, <nl> + /* Ensures the array has NFC_DIGITAL_RF_TECH_LAST elements */ <nl> + [ NFC_DIGITAL_RF_TECH_LAST ] = { 0 }, <nl> + <nl> }; <nl>  <nl> # define PORT100_IN_PROT_INITIAL_GUARD_TIME 0x00 <nl> in_protocols [][ PORT100_IN_MAX_NUM_PROTOCOLS + 1 ] = { <nl> [ NFC_DIGITAL_FRAMING_NFC_DEP_ACTIVATED ] = { <nl> { PORT100_IN_PROT_END , 0 }, <nl> }, <nl> + /* Ensures the array has NFC_DIGITAL_FRAMING_LAST elements */ <nl> + [ NFC_DIGITAL_FRAMING_LAST ] = { <nl> + { PORT100_IN_PROT_END , 0 }, <nl> + }, <nl> }; <nl>  <nl> static struct port100_protocol <nl> tg_protocols [][ PORT100_TG_MAX_NUM_PROTOCOLS + 1 ] = { <nl> { PORT100_TG_PROT_RF_OFF , 1 }, <nl> { PORT100_TG_PROT_END , 0 }, <nl> }, <nl> + /* Ensures the array has NFC_DIGITAL_FRAMING_LAST elements */ <nl> + [ NFC_DIGITAL_FRAMING_LAST ] = { <nl> + { PORT100_TG_PROT_END , 0 }, <nl> + }, <nl> }; <nl>  <nl> struct port100 {
mmm drivers / net / ethernet / netronome / nfp / nfp_net_common . c <nl> ppp drivers / net / ethernet / netronome / nfp / nfp_net_common . c <nl> static void nfp_net_tx_complete ( struct nfp_net_tx_ring * tx_ring ) <nl> int fidx ; <nl> int idx ; <nl>  <nl> + if ( tx_ring -> wr_p == tx_ring -> rd_p ) <nl> + return ; <nl> + <nl> /* Work out how many descriptors have been transmitted */ <nl> qcp_rd_p = nfp_qcp_rd_ptr_read ( tx_ring -> qcp_q ); <nl>  <nl> static void nfp_net_xdp_complete ( struct nfp_net_tx_ring * tx_ring ) <nl> int idx , todo ; <nl> u32 qcp_rd_p ; <nl>  <nl> + if ( tx_ring -> wr_p == tx_ring -> rd_p ) <nl> + return ; <nl> + <nl> /* Work out how many descriptors have been transmitted */ <nl> qcp_rd_p = nfp_qcp_rd_ptr_read ( tx_ring -> qcp_q ); <nl> 
mmm drivers / cpufreq / intel_pstate . c <nl> ppp drivers / cpufreq / intel_pstate . c <nl> static int intel_pstate_set_policy ( struct cpufreq_policy * policy ) <nl>  <nl> cpu = all_cpu_data [ policy -> cpu ]; <nl>  <nl> + if (! policy -> cpuinfo . max_freq ) <nl> + return - ENODEV ; <nl> + <nl> intel_pstate_get_min_max ( cpu , & min , & max ); <nl>  <nl> limits . min_perf_pct = ( policy -> min * 100 ) / policy -> cpuinfo . max_freq ;
mmm drivers / spi / spi - gpio . c <nl> ppp drivers / spi / spi - gpio . c <nl> static int spi_gpio_probe ( struct platform_device * pdev ) <nl> return - ENOMEM ; <nl>  <nl> status = devm_add_action_or_reset (& pdev -> dev , spi_gpio_put , master ); <nl> - if ( status ) <nl> + if ( status ) { <nl> + spi_master_put ( master ); <nl> return status ; <nl> + } <nl>  <nl> if ( of_id ) <nl> status = spi_gpio_probe_dt ( pdev , master );
mmm lib / seq_buf . c <nl> ppp lib / seq_buf . c <nl> int seq_buf_putmem_hex ( struct seq_buf * s , const void * mem , <nl>  <nl> WARN_ON ( s -> size == 0 ); <nl>  <nl> + BUILD_BUG_ON ( MAX_MEMHEX_BYTES * 2 >= HEX_CHARS ); <nl> + <nl> while ( len ) { <nl> - start_len = min ( len , HEX_CHARS - 1 ); <nl> + start_len = min ( len , MAX_MEMHEX_BYTES ); <nl> # ifdef __BIG_ENDIAN <nl> for ( i = 0 , j = 0 ; i < start_len ; i ++) { <nl> # else
mmm drivers / gpu / drm / i915 / intel_crt . c <nl> ppp drivers / gpu / drm / i915 / intel_crt . c <nl> static bool intel_crt_detect_ddc ( struct drm_connector * connector ) <nl> * This may be a DVI - I connector with a shared DDC <nl> * link between analog and digital outputs , so we <nl> * have to check the EDID input spec of the attached device . <nl> + * <nl> + * On the other hand , what should we do if it is a broken EDID ? <nl> */ <nl> if ( edid != NULL ) { <nl> is_digital = edid -> input & DRM_EDID_INPUT_DIGITAL ; <nl> static bool intel_crt_detect_ddc ( struct drm_connector * connector ) <nl> if (! is_digital ) { <nl> DRM_DEBUG_KMS (" CRT detected via DDC : 0x50 [ EDID ]\ n "); <nl> return true ; <nl> + } else { <nl> + DRM_DEBUG_KMS (" CRT not detected via DDC : 0x50 [ EDID reports a digital panel ]\ n "); <nl> } <nl> } <nl> 
mmm tools / iio / generic_buffer . c <nl> ppp tools / iio / generic_buffer . c <nl> int main ( int argc , char ** argv ) <nl> "% s - dev % d ", device_name , dev_num ); <nl> if ( ret < 0 ) { <nl> ret = - ENOMEM ; <nl> - goto error_ret ; <nl> + goto error_free_dev_dir_name ; <nl> } <nl> } <nl>  <nl> int main ( int argc , char ** argv ) <nl> error_free_triggername : <nl> if ( datardytrigger ) <nl> free ( trigger_name ); <nl> + error_free_dev_dir_name : <nl> + free ( dev_dir_name ); <nl> error_ret : <nl> return ret ; <nl> }
mmm drivers / edac / edac_mc . c <nl> ppp drivers / edac / edac_mc . c <nl> struct mem_ctl_info * edac_mc_alloc ( unsigned mc_num , <nl> /* <nl> * Alocate and fill the csrow / channels structs <nl> */ <nl> - mci -> csrows = kcalloc ( sizeof (* mci -> csrows ), tot_csrows , GFP_KERNEL ); <nl> + mci -> csrows = kcalloc ( tot_csrows , sizeof (* mci -> csrows ), GFP_KERNEL ); <nl> if (! mci -> csrows ) <nl> goto error ; <nl> for ( row = 0 ; row < tot_csrows ; row ++) { <nl> struct mem_ctl_info * edac_mc_alloc ( unsigned mc_num , <nl> csr -> csrow_idx = row ; <nl> csr -> mci = mci ; <nl> csr -> nr_channels = tot_channels ; <nl> - csr -> channels = kcalloc ( sizeof (* csr -> channels ), tot_channels , <nl> + csr -> channels = kcalloc ( tot_channels , sizeof (* csr -> channels ), <nl> GFP_KERNEL ); <nl> if (! csr -> channels ) <nl> goto error ; <nl> struct mem_ctl_info * edac_mc_alloc ( unsigned mc_num , <nl> /* <nl> * Allocate and fill the dimm structs <nl> */ <nl> - mci -> dimms = kcalloc ( sizeof (* mci -> dimms ), tot_dimms , GFP_KERNEL ); <nl> + mci -> dimms = kcalloc ( tot_dimms , sizeof (* mci -> dimms ), GFP_KERNEL ); <nl> if (! mci -> dimms ) <nl> goto error ; <nl> 
mmm drivers / block / drbd / drbd_worker . c <nl> ppp drivers / block / drbd / drbd_worker . c <nl> void drbd_start_resync ( struct drbd_device * device , enum drbd_conns side ) <nl> return ; <nl> } <nl>  <nl> + if (! connection ) { <nl> + drbd_err ( device , " No connection to peer , aborting !\ n "); <nl> + return ; <nl> + } <nl> + <nl> if (! test_bit ( B_RS_H_DONE , & device -> flags )) { <nl> if ( side == C_SYNC_TARGET ) { <nl> /* Since application IO was locked out during C_WF_BITMAP_T and
mmm drivers / acpi / processor_idle . c <nl> ppp drivers / acpi / processor_idle . c <nl> static unsigned int max_cstate __read_mostly = ACPI_PROCESSOR_MAX_POWER ; <nl> module_param ( max_cstate , uint , 0000 ); <nl> static unsigned int nocst __read_mostly ; <nl> module_param ( nocst , uint , 0000 ); <nl> + static int bm_check_disable __read_mostly ; <nl> + module_param ( bm_check_disable , uint , 0000 ); <nl>  <nl> static unsigned int latency_factor __read_mostly = 2 ; <nl> module_param ( latency_factor , uint , 0644 ); <nl> static int acpi_idle_bm_check ( void ) <nl> { <nl> u32 bm_status = 0 ; <nl>  <nl> + if ( bm_check_disable ) <nl> + return 0 ; <nl> + <nl> acpi_read_bit_register ( ACPI_BITREG_BUS_MASTER_STATUS , & bm_status ); <nl> if ( bm_status ) <nl> acpi_write_bit_register ( ACPI_BITREG_BUS_MASTER_STATUS , 1 );
mmm drivers / net / ethernet / intel / i40e / i40e_virtchnl_pf . c <nl> ppp drivers / net / ethernet / intel / i40e / i40e_virtchnl_pf . c <nl> int i40e_ndo_get_vf_config ( struct net_device * netdev , <nl> else <nl> ivi -> linkstate = IFLA_VF_LINK_STATE_DISABLE ; <nl> ivi -> spoofchk = vf -> spoofchk ; <nl> + ivi -> trusted = vf -> trusted ; <nl> ret = 0 ; <nl>  <nl> error_param :
mmm drivers / accessibility / speakup / spk_ttyio . c <nl> ppp drivers / accessibility / speakup / spk_ttyio . c <nl> static int spk_ttyio_ldisc_open ( struct tty_struct * tty ) <nl>  <nl> if (! tty -> ops -> write ) <nl> return - EOPNOTSUPP ; <nl> + <nl> + mutex_lock (& speakup_tty_mutex ); <nl> + if ( speakup_tty ) { <nl> + mutex_unlock (& speakup_tty_mutex ); <nl> + return - EBUSY ; <nl> + } <nl> speakup_tty = tty ; <nl>  <nl> ldisc_data = kmalloc ( sizeof (* ldisc_data ), GFP_KERNEL ); <nl> - if (! ldisc_data ) <nl> + if (! ldisc_data ) { <nl> + speakup_tty = NULL ; <nl> + mutex_unlock (& speakup_tty_mutex ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> init_completion (& ldisc_data -> completion ); <nl> ldisc_data -> buf_free = true ; <nl> speakup_tty -> disc_data = ldisc_data ; <nl> + mutex_unlock (& speakup_tty_mutex ); <nl>  <nl> return 0 ; <nl> }
mmm drivers / infiniband / hw / i40iw / i40iw_verbs . c <nl> ppp drivers / infiniband / hw / i40iw / i40iw_verbs . c <nl> static struct ib_qp * i40iw_create_qp ( struct ib_pd * ibpd , <nl> return & iwqp -> ibqp ; <nl> error : <nl> i40iw_free_qp_resources ( iwdev , iwqp , qp_num ); <nl> - kfree ( mem ); <nl> return ERR_PTR ( err_code ); <nl> } <nl> 
mmm include / crypto / hash . h <nl> ppp include / crypto / hash . h <nl> struct ahash_request { <nl> void * __ctx [] CRYPTO_MINALIGN_ATTR ; <nl> }; <nl>  <nl> +# define AHASH_REQUEST_ON_STACK ( name , ahash ) \ <nl> + char __ ## name ## _desc [ sizeof ( struct ahash_request ) + \ <nl> + crypto_ahash_reqsize ( ahash )] CRYPTO_MINALIGN_ATTR ; \ <nl> + struct ahash_request * name = ( void *) __ ## name ## _desc <nl> + <nl> /** <nl> * struct ahash_alg - asynchronous message digest definition <nl> * @ init : Initialize the transformation context . Intended only to initialize the
mmm drivers / media / platform / s5p - mfc / s5p_mfc_enc . c <nl> ppp drivers / media / platform / s5p - mfc / s5p_mfc_enc . c <nl> static int s5p_mfc_queue_setup ( struct vb2_queue * vq , <nl> struct s5p_mfc_ctx * ctx = fh_to_ctx ( vq -> drv_priv ); <nl> struct s5p_mfc_dev * dev = ctx -> dev ; <nl>  <nl> - if ( ctx -> state != MFCINST_GOT_INST ) { <nl> - mfc_err (" inavlid state : % d \ n ", ctx -> state ); <nl> - return - EINVAL ; <nl> - } <nl> if ( vq -> type == V4L2_BUF_TYPE_VIDEO_CAPTURE_MPLANE ) { <nl> + if ( ctx -> state != MFCINST_GOT_INST ) { <nl> + mfc_err (" inavlid state : % d \ n ", ctx -> state ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> if ( ctx -> dst_fmt ) <nl> * plane_count = ctx -> dst_fmt -> num_planes ; <nl> else
mmm drivers / infiniband / hw / mlx5 / main . c <nl> ppp drivers / infiniband / hw / mlx5 / main . c <nl> static void * mlx5_ib_add ( struct mlx5_core_dev * mdev ) <nl> ( 1ull << IB_USER_VERBS_CMD_CREATE_XSRQ ) | <nl> ( 1ull << IB_USER_VERBS_CMD_OPEN_QP ); <nl> dev -> ib_dev . uverbs_ex_cmd_mask = <nl> - ( 1ull << IB_USER_VERBS_EX_CMD_QUERY_DEVICE ); <nl> + ( 1ull << IB_USER_VERBS_EX_CMD_QUERY_DEVICE ) | <nl> + ( 1ull << IB_USER_VERBS_EX_CMD_CREATE_CQ ) | <nl> + ( 1ull << IB_USER_VERBS_EX_CMD_CREATE_QP ); <nl>  <nl> dev -> ib_dev . query_device = mlx5_ib_query_device ; <nl> dev -> ib_dev . query_port = mlx5_ib_query_port ;
mmm drivers / media / rc / mceusb . c <nl> ppp drivers / media / rc / mceusb . c <nl> static void mceusb_process_ir_data ( struct mceusb_dev * ir , int buf_len ) <nl> init_ir_raw_event (& rawir ); <nl> rawir . pulse = (( ir -> buf_in [ i ] & MCE_PULSE_BIT ) != 0 ); <nl> rawir . duration = ( ir -> buf_in [ i ] & MCE_PULSE_MASK ); <nl> + if ( unlikely (! rawir . duration )) { <nl> + dev_warn ( ir -> dev , " nonsensical irdata % 02x with duration 0 ", <nl> + ir -> buf_in [ i ]); <nl> + break ; <nl> + } <nl> if ( rawir . pulse ) { <nl> ir -> pulse_tunit += rawir . duration ; <nl> ir -> pulse_count ++;
mmm drivers / net / tg3 . c <nl> ppp drivers / net / tg3 . c <nl> static int tg3_bmcr_reset ( struct tg3 * tp ) <nl> } <nl> udelay ( 10 ); <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ; <nl> static int tg3_wait_macro_done ( struct tg3 * tp ) <nl> break ; <nl> } <nl> } <nl> - if ( limit <= 0 ) <nl> + if ( limit < 0 ) <nl> return - EBUSY ; <nl>  <nl> return 0 ;
mmm arch / x86 / kvm / emulate . c <nl> ppp arch / x86 / kvm / emulate . c <nl> static int decode_modrm ( struct x86_emulate_ctxt * ctxt , <nl> c -> modrm_reg |= ( c -> modrm & 0x38 ) >> 3 ; <nl> c -> modrm_rm |= ( c -> modrm & 0x07 ); <nl> c -> modrm_ea = 0 ; <nl> - c -> use_modrm_ea = 1 ; <nl> c -> modrm_seg = VCPU_SREG_DS ; <nl>  <nl> if ( c -> modrm_mod == 3 ) {mmm arch / x86 / include / asm / kvm_emulate . h <nl> ppp arch / x86 / include / asm / kvm_emulate . h <nl> static int decode_modrm ( struct x86_emulate_ctxt * ctxt , <nl> c -> modrm_reg |= ( c -> modrm & 0x38 ) >> 3 ; <nl> c -> modrm_rm |= ( c -> modrm & 0x07 ); <nl> c -> modrm_ea = 0 ; <nl> - c -> use_modrm_ea = 1 ; <nl> c -> modrm_seg = VCPU_SREG_DS ; <nl>  <nl> if ( c -> modrm_mod == 3 ) { <nl> struct decode_cache { <nl> u8 modrm_reg ; <nl> u8 modrm_rm ; <nl> u8 modrm_seg ; <nl> - u8 use_modrm_ea ; <nl> bool rip_relative ; <nl> unsigned long modrm_ea ; <nl> void * modrm_ptr ;
mmm drivers / base / regmap / regmap . c <nl> ppp drivers / base / regmap / regmap . c <nl> static int _regmap_read ( struct regmap * map , unsigned int reg , <nl> if ( map -> cache_only ) <nl> return - EBUSY ; <nl>  <nl> + if (! regmap_readable ( map , reg )) <nl> + return - EIO ; <nl> + <nl> ret = map -> reg_read ( context , reg , val ); <nl> if ( ret == 0 ) { <nl> # ifdef LOG_DEVICE
mmm drivers / usb / storage / uas . c <nl> ppp drivers / usb / storage / uas . c <nl> static int uas_find_endpoints ( struct usb_host_interface * alt , <nl> for ( i = 0 ; i < n_endpoints ; i ++) { <nl> unsigned char * extra = endpoint [ i ]. extra ; <nl> int len = endpoint [ i ]. extralen ; <nl> - while ( len > 1 ) { <nl> + while ( len >= 3 ) { <nl> if ( extra [ 1 ] == USB_DT_PIPE_USAGE ) { <nl> unsigned pipe_id = extra [ 2 ]; <nl> if ( pipe_id > 0 && pipe_id < 5 )
mmm arch / x86 / kernel / unwind_frame . c <nl> ppp arch / x86 / kernel / unwind_frame . c <nl> bool unwind_next_frame ( struct unwind_state * state ) <nl> state -> regs -> sp < ( unsigned long ) task_pt_regs ( state -> task )) <nl> goto the_end ; <nl>  <nl> + /* <nl> + * There are some known frame pointer issues on 32 - bit . Disable <nl> + * unwinder warnings on 32 - bit until it gets objtool support . <nl> + */ <nl> + if ( IS_ENABLED ( CONFIG_X86_32 )) <nl> + goto the_end ; <nl> + <nl> if ( state -> regs ) { <nl> printk_deferred_once ( KERN_WARNING <nl> " WARNING : kernel stack regs at % p in % s :% d has bad ' bp ' value % p \ n ",
mmm mm / slab . c <nl> ppp mm / slab . c <nl> static int __init_refok setup_cpu_cache ( struct kmem_cache * cachep , gfp_t gfp ) <nl> int <nl> __kmem_cache_create ( struct kmem_cache * cachep , unsigned long flags ) <nl> { <nl> - size_t left_over , freelist_size , ralign ; <nl> + size_t left_over , freelist_size ; <nl> + size_t ralign = BYTES_PER_WORD ; <nl> gfp_t gfp ; <nl> int err ; <nl> size_t size = cachep -> size ; <nl> __kmem_cache_create ( struct kmem_cache * cachep , unsigned long flags ) <nl> size &= ~( BYTES_PER_WORD - 1 ); <nl> } <nl>  <nl> - /* <nl> - * Redzoning and user store require word alignment or possibly larger . <nl> - * Note this will be overridden by architecture or caller mandated <nl> - * alignment if either is greater than BYTES_PER_WORD . <nl> - */ <nl> - if ( flags & SLAB_STORE_USER ) <nl> - ralign = BYTES_PER_WORD ; <nl> - <nl> if ( flags & SLAB_RED_ZONE ) { <nl> ralign = REDZONE_ALIGN ; <nl> /* If redzoning , ensure that the second redzone is suitably
mmm drivers / staging / iio / adc / mxs - lradc . c <nl> ppp drivers / staging / iio / adc / mxs - lradc . c <nl> static int mxs_lradc_probe ( struct platform_device * pdev ) <nl> * of the array . <nl> */ <nl> scale_uv = (( u64 ) lradc -> vref_mv [ i ] * 100000000 ) >> <nl> - ( iio -> channels [ i ]. scan_type . realbits - s ); <nl> + ( LRADC_RESOLUTION - s ); <nl> lradc -> scale_avail [ i ][ s ]. nano = <nl> do_div ( scale_uv , 100000000 ) * 10 ; <nl> lradc -> scale_avail [ i ][ s ]. integer = scale_uv ;
mmm drivers / edac / i7core_edac . c <nl> ppp drivers / edac / i7core_edac . c <nl> static int i7core_register_mci ( struct i7core_dev * i7core_dev , <nl> } <nl>  <nl> fail : <nl> - edac_mc_free ( mci ); <nl> + if ( rc < 0 ) <nl> + edac_mc_free ( mci ); <nl> return rc ; <nl> } <nl> 
mmm drivers / gpu / drm / i915 / i915_gem_execbuffer . c <nl> ppp drivers / gpu / drm / i915 / i915_gem_execbuffer . c <nl> i915_gem_execbuffer_relocate_entry ( struct drm_i915_gem_object * obj , <nl> else <nl> ret = relocate_entry_gtt ( obj , reloc ); <nl>  <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> /* and update the user ' s relocation entry */ <nl> reloc -> presumed_offset = target_offset ; <nl> 
mmm fs / f2fs / segment . c <nl> ppp fs / f2fs / segment . c <nl> int create_flush_cmd_control ( struct f2fs_sb_info * sbi ) <nl> init_waitqueue_head (& fcc -> flush_wait_queue ); <nl> init_llist_head (& fcc -> issue_list ); <nl> SM_I ( sbi )-> fcc_info = fcc ; <nl> + if (! test_opt ( sbi , FLUSH_MERGE )) <nl> + return err ; <nl> + <nl> init_thread : <nl> fcc -> f2fs_issue_flush = kthread_run ( issue_flush_thread , sbi , <nl> " f2fs_flush -% u :% u ", MAJOR ( dev ), MINOR ( dev )); <nl> int build_segment_manager ( struct f2fs_sb_info * sbi ) <nl>  <nl> INIT_LIST_HEAD (& sm_info -> sit_entry_set ); <nl>  <nl> - if ( test_opt ( sbi , FLUSH_MERGE ) && ! f2fs_readonly ( sbi -> sb )) { <nl> + if (! f2fs_readonly ( sbi -> sb )) { <nl> err = create_flush_cmd_control ( sbi ); <nl> if ( err ) <nl> return err ;
mmm include / linux / input . h <nl> ppp include / linux / input . h <nl> struct input_keymap_entry { <nl> # define BTN_TOOL_FINGER 0x145 <nl> # define BTN_TOOL_MOUSE 0x146 <nl> # define BTN_TOOL_LENS 0x147 <nl> +# define BTN_TOOL_QUINTTAP 0x148 /* Five fingers on trackpad */ <nl> # define BTN_TOUCH 0x14a <nl> # define BTN_STYLUS 0x14b <nl> # define BTN_STYLUS2 0x14cmmm drivers / input / input - mt . c <nl> ppp drivers / input / input - mt . c <nl> struct input_keymap_entry { <nl> # define BTN_TOOL_FINGER 0x145 <nl> # define BTN_TOOL_MOUSE 0x146 <nl> # define BTN_TOOL_LENS 0x147 <nl> +# define BTN_TOOL_QUINTTAP 0x148 /* Five fingers on trackpad */ <nl> # define BTN_TOUCH 0x14a <nl> # define BTN_STYLUS 0x14b <nl> # define BTN_STYLUS2 0x14c <nl> void input_mt_report_finger_count ( struct input_dev * dev , int count ) <nl> input_event ( dev , EV_KEY , BTN_TOOL_DOUBLETAP , count == 2 ); <nl> input_event ( dev , EV_KEY , BTN_TOOL_TRIPLETAP , count == 3 ); <nl> input_event ( dev , EV_KEY , BTN_TOOL_QUADTAP , count == 4 ); <nl> + input_event ( dev , EV_KEY , BTN_TOOL_QUINTTAP , count == 5 ); <nl> } <nl> EXPORT_SYMBOL ( input_mt_report_finger_count ); <nl> 
mmm fs / ext4 / page - io . c <nl> ppp fs / ext4 / page - io . c <nl> static void ext4_end_bio ( struct bio * bio , int error ) <nl> struct inode * inode ; <nl> unsigned long flags ; <nl> int i ; <nl> + sector_t bi_sector = bio -> bi_sector ; <nl>  <nl> BUG_ON (! io_end ); <nl> bio -> bi_private = NULL ; <nl> static void ext4_end_bio ( struct bio * bio , int error ) <nl> if ( error ) <nl> SetPageError ( page ); <nl> BUG_ON (! head ); <nl> - if ( head -> b_size == PAGE_CACHE_SIZE ) <nl> - clear_buffer_dirty ( head ); <nl> - else { <nl> + if ( head -> b_size != PAGE_CACHE_SIZE ) { <nl> loff_t offset ; <nl> loff_t io_end_offset = io_end -> offset + io_end -> size ; <nl>  <nl> static void ext4_end_bio ( struct bio * bio , int error ) <nl> if ( error ) <nl> buffer_io_error ( bh ); <nl>  <nl> - clear_buffer_dirty ( bh ); <nl> } <nl> if ( buffer_delay ( bh )) <nl> partial_write = 1 ; <nl> static void ext4_end_bio ( struct bio * bio , int error ) <nl> ( unsigned long long ) io_end -> offset , <nl> ( long ) io_end -> size , <nl> ( unsigned long long ) <nl> - bio -> bi_sector >> ( inode -> i_blkbits - 9 )); <nl> + bi_sector >> ( inode -> i_blkbits - 9 )); <nl> } <nl>  <nl> /* Add the io_end to per - inode completed io list */ <nl> int ext4_bio_write_page ( struct ext4_io_submit * io , <nl>  <nl> blocksize = 1 << inode -> i_blkbits ; <nl>  <nl> + BUG_ON (! PageLocked ( page )); <nl> BUG_ON ( PageWriteback ( page )); <nl> set_page_writeback ( page ); <nl> ClearPageError ( page ); <nl> int ext4_bio_write_page ( struct ext4_io_submit * io , <nl> for ( bh = head = page_buffers ( page ), block_start = 0 ; <nl> bh != head || ! block_start ; <nl> block_start = block_end , bh = bh -> b_this_page ) { <nl> + <nl> block_end = block_start + blocksize ; <nl> if ( block_start >= len ) { <nl> clear_buffer_dirty ( bh ); <nl> set_buffer_uptodate ( bh ); <nl> continue ; <nl> } <nl> + clear_buffer_dirty ( bh ); <nl> ret = io_submit_add_bh ( io , io_page , inode , wbc , bh ); <nl> if ( ret ) { <nl> /*
mmm fs / ext4 / super . c <nl> ppp fs / ext4 / super . c <nl> static int ext4_fill_flex_info ( struct super_block * sb ) <nl> struct ext4_group_desc * gdp = NULL ; <nl> ext4_group_t flex_group_count ; <nl> ext4_group_t flex_group ; <nl> - int groups_per_flex = 0 ; <nl> + unsigned int groups_per_flex = 0 ; <nl> size_t size ; <nl> int i ; <nl>  <nl> sbi -> s_log_groups_per_flex = sbi -> s_es -> s_log_groups_per_flex ; <nl> - groups_per_flex = 1 << sbi -> s_log_groups_per_flex ; <nl> - <nl> - if ( groups_per_flex < 2 ) { <nl> + if ( sbi -> s_log_groups_per_flex < 1 || sbi -> s_log_groups_per_flex > 31 ) { <nl> sbi -> s_log_groups_per_flex = 0 ; <nl> return 1 ; <nl> } <nl> + groups_per_flex = 1 << sbi -> s_log_groups_per_flex ; <nl>  <nl> /* We allocate both existing and potentially added groups */ <nl> flex_group_count = (( sbi -> s_groups_count + groups_per_flex - 1 ) +
mmm security / commoncap . c <nl> ppp security / commoncap . c <nl> int cap_bprm_set_creds ( struct linux_binprm * bprm ) <nl> } <nl> skip : <nl>  <nl> + /* if we have fs caps , clear dangerous personality flags */ <nl> + if (! cap_issubset ( new -> cap_permitted , old -> cap_permitted )) <nl> + bprm -> per_clear |= PER_CLEAR_ON_SETID ; <nl> + <nl> + <nl> /* Don ' t let someone trace a set [ ug ] id / setpcap binary with the revised <nl> * credentials unless they have the appropriate permit <nl> */
mmm drivers / scsi / iscsi_tcp . c <nl> ppp drivers / scsi / iscsi_tcp . c <nl> static void <nl> iscsi_tcp_conn_stop ( struct iscsi_cls_conn * cls_conn , int flag ) <nl> { <nl> struct iscsi_conn * conn = cls_conn -> dd_data ; <nl> + struct iscsi_tcp_conn * tcp_conn = conn -> dd_data ; <nl>  <nl> iscsi_conn_stop ( cls_conn , flag ); <nl> iscsi_tcp_release_conn ( conn ); <nl> + tcp_conn -> hdr_size = sizeof ( struct iscsi_hdr ); <nl> } <nl>  <nl> static int
mmm drivers / media / platform / soc_camera / soc_camera . c <nl> ppp drivers / media / platform / soc_camera / soc_camera . c <nl> static int soc_camera_try_fmt_vid_cap ( struct file * file , void * priv , <nl> static int soc_camera_enum_input ( struct file * file , void * priv , <nl> struct v4l2_input * inp ) <nl> { <nl> + struct soc_camera_device * icd = file -> private_data ; <nl> + <nl> if ( inp -> index != 0 ) <nl> return - EINVAL ; <nl>  <nl> /* default is camera */ <nl> inp -> type = V4L2_INPUT_TYPE_CAMERA ; <nl> + inp -> std = icd -> vdev -> tvnorms ; <nl> strcpy ( inp -> name , " Camera "); <nl>  <nl> return 0 ;
mmm drivers / media / rc / rc - main . c <nl> ppp drivers / media / rc / rc - main . c <nl> static struct rc_map_list empty_map = { <nl> static int ir_create_table ( struct rc_map * rc_map , <nl> const char * name , u64 rc_type , size_t size ) <nl> { <nl> - rc_map -> name = name ; <nl> + rc_map -> name = kstrdup ( name , GFP_KERNEL ); <nl> + if (! rc_map -> name ) <nl> + return - ENOMEM ; <nl> rc_map -> rc_type = rc_type ; <nl> rc_map -> alloc = roundup_pow_of_two ( size * sizeof ( struct rc_map_table )); <nl> rc_map -> size = rc_map -> alloc / sizeof ( struct rc_map_table ); <nl> rc_map -> scan = kmalloc ( rc_map -> alloc , GFP_KERNEL ); <nl> - if (! rc_map -> scan ) <nl> + if (! rc_map -> scan ) { <nl> + kfree ( rc_map -> name ); <nl> + rc_map -> name = NULL ; <nl> return - ENOMEM ; <nl> + } <nl>  <nl> IR_dprintk ( 1 , " Allocated space for % u keycode entries (% u bytes )\ n ", <nl> rc_map -> size , rc_map -> alloc ); <nl> static int ir_create_table ( struct rc_map * rc_map , <nl> static void ir_free_table ( struct rc_map * rc_map ) <nl> { <nl> rc_map -> size = 0 ; <nl> + kfree ( rc_map -> name ); <nl> kfree ( rc_map -> scan ); <nl> rc_map -> scan = NULL ; <nl> }
mmm fs / aio . c <nl> ppp fs / aio . c <nl> static struct kioctx * ioctx_alloc ( unsigned nr_events ) <nl> err_cleanup : <nl> aio_nr_sub ( ctx -> max_reqs ); <nl> err : <nl> - aio_free_ring ( ctx ); <nl> free_percpu ( ctx -> cpu ); <nl> free_percpu ( ctx -> reqs . pcpu_count ); <nl> free_percpu ( ctx -> users . pcpu_count );
mmm drivers / net / wireless / rsi / rsi_91x_mgmt . c <nl> ppp drivers / net / wireless / rsi / rsi_91x_mgmt . c <nl> static int rsi_send_beacon ( struct rsi_common * common ) <nl> skb_pull ( skb , ( 64 - dword_align_bytes )); <nl> if ( rsi_prepare_beacon ( common , skb )) { <nl> rsi_dbg ( ERR_ZONE , " Failed to prepare beacon \ n "); <nl> + dev_kfree_skb ( skb ); <nl> return - EINVAL ; <nl> } <nl> skb_queue_tail (& common -> tx_queue [ MGMT_BEACON_Q ], skb );
mmm arch / arm / mach - omap2 / omap_hwmod . c <nl> ppp arch / arm / mach - omap2 / omap_hwmod . c <nl> struct powerdomain * omap_hwmod_get_pwrdm ( struct omap_hwmod * oh ) <nl> c = oh -> slaves [ oh -> _mpu_port_index ]-> _clk ; <nl> } <nl>  <nl> + if (! c -> clkdm ) <nl> + return NULL ; <nl> + <nl> return c -> clkdm -> pwrdm . ptr ; <nl>  <nl> }
mmm net / xfrm / xfrm_policy . c <nl> ppp net / xfrm / xfrm_policy . c <nl> int xfrm_policy_flush ( u8 type , struct xfrm_audit * audit_info ) <nl> continue ; <nl> hlist_del (& pol -> bydst ); <nl> hlist_del (& pol -> byidx ); <nl> + list_del (& pol -> walk . all ); <nl> write_unlock_bh (& xfrm_policy_lock ); <nl>  <nl> xfrm_audit_policy_delete ( pol , 1 , audit_info -> loginuid ,
mmm net / core / flow_dissector . c <nl> ppp net / core / flow_dissector . c <nl> bool __skb_flow_dissect ( const struct sk_buff * skb , <nl>  <nl> /* Only look inside GRE without routing */ <nl> if ( hdr -> flags & GRE_ROUTING ) <nl> - break ; <nl> + goto out_good ; <nl>  <nl> /* Only look inside GRE for version 0 and 1 */ <nl> gre_ver = ntohs ( hdr -> flags & GRE_VERSION ); <nl> if ( gre_ver > 1 ) <nl> - break ; <nl> + goto out_good ; <nl>  <nl> proto = hdr -> protocol ; <nl> if ( gre_ver ) { <nl> /* Version1 must be PPTP , and check the flags */ <nl> if (!( proto == GRE_PROTO_PPP && ( hdr -> flags & GRE_KEY ))) <nl> - break ; <nl> + goto out_good ; <nl> } <nl>  <nl> offset += sizeof ( struct gre_base_hdr );
mmm drivers / mtd / nand / lpc32xx_slc . c <nl> ppp drivers / mtd / nand / lpc32xx_slc . c <nl> static int __devinit lpc32xx_nand_probe ( struct platform_device * pdev ) <nl> dev_err (& pdev -> dev , " Missing platform data \ n "); <nl> return - ENOENT ; <nl> } <nl> + if ( host -> ncfg -> wp_gpio == - EPROBE_DEFER ) <nl> + return - EPROBE_DEFER ; <nl> if ( gpio_is_valid ( host -> ncfg -> wp_gpio ) && <nl> gpio_request ( host -> ncfg -> wp_gpio , " NAND WP ")) { <nl> dev_err (& pdev -> dev , " GPIO not available \ n ");
mmm drivers / staging / comedi / drivers / ni_labpc . c <nl> ppp drivers / staging / comedi / drivers / ni_labpc . c <nl> static int labpc_ai_cmd ( struct comedi_device * dev , struct comedi_subdevice * s ) <nl> devpriv -> write_byte ( INTERVAL_LOAD_BITS , <nl> dev -> iobase + INTERVAL_LOAD_REG ); <nl>  <nl> - if ( cmd -> convert_src == TRIG_TIMER || cmd -> scan_begin_src == TRIG_TIMER ) { <nl> + if ( cmd -> convert_src == TRIG_TIMER || <nl> + cmd -> scan_begin_src == TRIG_TIMER ) { <nl> /* set up pacing */ <nl> labpc_adc_timing ( dev , cmd , mode ); <nl> /* load counter b0 in mode 3 */
mmm drivers / gpu / drm / amd / display / amdgpu_dm / amdgpu_dm . c <nl> ppp drivers / gpu / drm / amd / display / amdgpu_dm / amdgpu_dm . c <nl> static int amdgpu_dm_atomic_check ( struct drm_device * dev , <nl> } <nl> } else { <nl> for_each_oldnew_crtc_in_state ( state , crtc , old_crtc_state , new_crtc_state , i ) { <nl> - if (! drm_atomic_crtc_needs_modeset ( new_crtc_state )) <nl> + if (! drm_atomic_crtc_needs_modeset ( new_crtc_state ) && <nl> + ! new_crtc_state -> color_mgmt_changed ) <nl> continue ; <nl>  <nl> if (! new_crtc_state -> enable )
mmm drivers / net / bonding / bond_main . c <nl> ppp drivers / net / bonding / bond_main . c <nl> static int bond_validate ( struct nlattr * tb [], struct nlattr * data []) <nl> return 0 ; <nl> } <nl>  <nl> + static int bond_get_tx_queues ( struct net * net , struct nlattr * tb [], <nl> + unsigned int * num_queues , <nl> + unsigned int * real_num_queues ) <nl> +{ <nl> + * num_queues = tx_queues ; <nl> + return 0 ; <nl> +} <nl> + <nl> static struct rtnl_link_ops bond_link_ops __read_mostly = { <nl> . kind = " bond ", <nl> . priv_size = sizeof ( struct bonding ), <nl> . setup = bond_setup , <nl> . validate = bond_validate , <nl> + . get_tx_queues = bond_get_tx_queues , <nl> }; <nl>  <nl> /* Create a new bond based on the specified name and bonding parameters .
mmm net / vmw_vsock / af_vsock . c <nl> ppp net / vmw_vsock / af_vsock . c <nl> vsock_stream_recvmsg ( struct kiocb * kiocb , <nl> vsk = vsock_sk ( sk ); <nl> err = 0 ; <nl>  <nl> + msg -> msg_namelen = 0 ; <nl> + <nl> lock_sock ( sk ); <nl>  <nl> if ( sk -> sk_state != SS_CONNECTED ) {
mmm drivers / pinctrl / bcm / pinctrl - nsp - gpio . c <nl> ppp drivers / pinctrl / bcm / pinctrl - nsp - gpio . c <nl> static int nsp_gpio_get_strength ( struct nsp_gpio * chip , unsigned gpio , <nl> return 0 ; <nl> } <nl>  <nl> - int nsp_pin_config_group_get ( struct pinctrl_dev * pctldev , unsigned selector , <nl> + static int nsp_pin_config_group_get ( struct pinctrl_dev * pctldev , <nl> + unsigned selector , <nl> unsigned long * config ) <nl> { <nl> return 0 ; <nl> } <nl>  <nl> - int nsp_pin_config_group_set ( struct pinctrl_dev * pctldev , unsigned selector , <nl> + static int nsp_pin_config_group_set ( struct pinctrl_dev * pctldev , <nl> + unsigned selector , <nl> unsigned long * configs , unsigned num_configs ) <nl> { <nl> return 0 ;
mmm drivers / net / wireless / ipw2200 . c <nl> ppp drivers / net / wireless / ipw2200 . c <nl> static int ipw_wx_set_retry ( struct net_device * dev , <nl> if (!( wrqu -> retry . flags & IW_RETRY_LIMIT )) <nl> return 0 ; <nl>  <nl> - if ( wrqu -> retry . value < 0 || wrqu -> retry . value > 255 ) <nl> + if ( wrqu -> retry . value < 0 || wrqu -> retry . value >= 255 ) <nl> return - EINVAL ; <nl>  <nl> mutex_lock (& priv -> mutex );
mmm net / can / bcm . c <nl> ppp net / can / bcm . c <nl> static int bcm_delete_rx_op ( struct list_head * ops , struct bcm_msg_head * mh , <nl> bcm_rx_handler , op ); <nl>  <nl> list_del (& op -> list ); <nl> + synchronize_rcu (); <nl> bcm_remove_op ( op ); <nl> return 1 ; /* done */ <nl> } <nl> static int bcm_release ( struct socket * sock ) <nl> REGMASK ( op -> can_id ), <nl> bcm_rx_handler , op ); <nl>  <nl> - bcm_remove_op ( op ); <nl> } <nl>  <nl> + synchronize_rcu (); <nl> + <nl> + list_for_each_entry_safe ( op , next , & bo -> rx_ops , list ) <nl> + bcm_remove_op ( op ); <nl> + <nl> # if IS_ENABLED ( CONFIG_PROC_FS ) <nl> /* remove procfs entry */ <nl> if ( net -> can . bcmproc_dir && bo -> bcm_proc_read )
mmm tools / perf / util / cs - etm . c <nl> ppp tools / perf / util / cs - etm . c <nl> static int cs_etm__sample ( struct cs_etm_queue * etmq ) <nl> static int cs_etm__flush ( struct cs_etm_queue * etmq ) <nl> { <nl> int err = 0 ; <nl> + struct cs_etm_auxtrace * etm = etmq -> etm ; <nl> struct cs_etm_packet * tmp ; <nl>  <nl> if (! etmq -> prev_packet ) <nl> static int cs_etm__flush ( struct cs_etm_queue * etmq ) <nl>  <nl> } <nl>  <nl> + if ( etm -> sample_branches && <nl> + etmq -> prev_packet -> sample_type == CS_ETM_RANGE ) { <nl> + err = cs_etm__synth_branch_sample ( etmq ); <nl> + if ( err ) <nl> + return err ; <nl> + } <nl> + <nl> swap_packet : <nl> if ( etmq -> etm -> synth_opts . last_branch ) { <nl> /*
mmm sound / soc / codecs / arizona . c <nl> ppp sound / soc / codecs / arizona . c <nl> int arizona_out_ev ( struct snd_soc_dapm_widget * w , <nl> case ARIZONA_OUT3R_ENA_SHIFT : <nl> priv -> out_up_pending --; <nl> if (! priv -> out_up_pending ) { <nl> + dev_dbg ( codec -> dev , " Power up delay : % d \ n ", <nl> + priv -> out_up_delay ); <nl> msleep ( priv -> out_up_delay ); <nl> priv -> out_up_delay = 0 ; <nl> } <nl> int arizona_out_ev ( struct snd_soc_dapm_widget * w , <nl> case ARIZONA_OUT3R_ENA_SHIFT : <nl> priv -> out_down_pending --; <nl> if (! priv -> out_down_pending ) { <nl> + dev_dbg ( codec -> dev , " Power down delay : % d \ n ", <nl> + priv -> out_down_delay ); <nl> msleep ( priv -> out_down_delay ); <nl> priv -> out_down_delay = 0 ; <nl> }
mmm arch / powerpc / platforms / maple / pci . c <nl> ppp arch / powerpc / platforms / maple / pci . c <nl> static int u3_ht_read_config ( struct pci_bus * bus , unsigned int devfn , <nl> if ( hose == NULL ) <nl> return PCIBIOS_DEVICE_NOT_FOUND ; <nl>  <nl> + if ( offset > 0xff ) <nl> + return PCIBIOS_BAD_REGISTER_NUMBER ; <nl> + <nl> addr = u3_ht_cfg_access ( hose , bus -> number , devfn , offset ); <nl> if (! addr ) <nl> return PCIBIOS_DEVICE_NOT_FOUND ; <nl> static int u3_ht_write_config ( struct pci_bus * bus , unsigned int devfn , <nl> if ( hose == NULL ) <nl> return PCIBIOS_DEVICE_NOT_FOUND ; <nl>  <nl> + if ( offset > 0xff ) <nl> + return PCIBIOS_BAD_REGISTER_NUMBER ; <nl> + <nl> addr = u3_ht_cfg_access ( hose , bus -> number , devfn , offset ); <nl> if (! addr ) <nl> return PCIBIOS_DEVICE_NOT_FOUND ;
mmm fs / crypto / keyinfo . c <nl> ppp fs / crypto / keyinfo . c <nl> static int validate_user_key ( struct fscrypt_info * crypt_info , <nl> goto out ; <nl> } <nl> ukp = user_key_payload_locked ( keyring_key ); <nl> + if (! ukp ) { <nl> + /* key was revoked before we acquired its semaphore */ <nl> + res = - EKEYREVOKED ; <nl> + goto out ; <nl> + } <nl> if ( ukp -> datalen != sizeof ( struct fscrypt_key )) { <nl> res = - EINVAL ; <nl> goto out ;
mmm include / linux / of_irq . h <nl> ppp include / linux / of_irq . h <nl> extern int of_irq_parse_one ( struct device_node * device , int index , <nl> extern unsigned int irq_create_of_mapping ( struct of_phandle_args * irq_data ); <nl> extern int of_irq_to_resource ( struct device_node * dev , int index , <nl> struct resource * r ); <nl> - extern int of_irq_to_resource_table ( struct device_node * dev , <nl> - struct resource * res , int nr_irqs ); <nl>  <nl> extern void of_irq_init ( const struct of_device_id * matches ); <nl>  <nl> extern void of_irq_init ( const struct of_device_id * matches ); <nl> extern int of_irq_count ( struct device_node * dev ); <nl> extern int of_irq_get ( struct device_node * dev , int index ); <nl> extern int of_irq_get_byname ( struct device_node * dev , const char * name ); <nl> + extern int of_irq_to_resource_table ( struct device_node * dev , <nl> + struct resource * res , int nr_irqs ); <nl> # else <nl> static inline int of_irq_count ( struct device_node * dev ) <nl> { <nl> static inline int of_irq_get_byname ( struct device_node * dev , const char * name ) <nl> { <nl> return 0 ; <nl> } <nl> + static inline int of_irq_to_resource_table ( struct device_node * dev , <nl> + struct resource * res , int nr_irqs ) <nl> +{ <nl> + return 0 ; <nl> +} <nl> # endif <nl>  <nl> # if defined ( CONFIG_OF )
mmm drivers / tty / vt / vt_ioctl . c <nl> ppp drivers / tty / vt / vt_ioctl . c <nl> int vt_ioctl ( struct tty_struct * tty , struct file * file , <nl> if ( ret ) <nl> break ; <nl> /* Commence switch and lock */ <nl> - set_console ( arg ); <nl> + set_console ( vsa . console ); <nl> } <nl> + break ; <nl> } <nl>  <nl> /*
mmm drivers / tty / serial / sh - sci . c <nl> ppp drivers / tty / serial / sh - sci . c <nl> static int sci_init_clocks ( struct sci_port * sci_port , struct device * dev ) <nl> dev_dbg ( dev , " failed to get % s (% ld )\ n ", clk_names [ i ], <nl> PTR_ERR ( clk )); <nl> else <nl> - dev_dbg ( dev , " clk % s is % pC rate % pCr \ n ", clk_names [ i ], <nl> - clk , clk ); <nl> + dev_dbg ( dev , " clk % s is % pC rate % lu \ n ", clk_names [ i ], <nl> + clk , clk_get_rate ( clk )); <nl> sci_port -> clks [ i ] = IS_ERR ( clk ) ? NULL : clk ; <nl> } <nl> return 0 ;
mmm drivers / usb / host / ehci - sched . c <nl> ppp drivers / usb / host / ehci - sched . c <nl> static int disable_periodic ( struct ehci_hcd * ehci ) <nl> ehci_writel ( ehci , cmd , & ehci -> regs -> command ); <nl> /* posted write ... */ <nl>  <nl> + free_cached_itd_list ( ehci ); <nl> + <nl> ehci -> next_uframe = - 1 ; <nl> return 0 ; <nl> }
mmm kernel / module . c <nl> ppp kernel / module . c <nl> static const struct kernel_symbol * resolve_symbol ( struct module * mod , <nl> const unsigned long * crc ; <nl> int err ; <nl>  <nl> + /* <nl> + * The module_mutex should not be a heavily contended lock ; <nl> + * if we get the occasional sleep here , we ' ll go an extra iteration <nl> + * in the wait_event_interruptible (), which is harmless . <nl> + */ <nl> + sched_annotate_sleep (); <nl> mutex_lock (& module_mutex ); <nl> sym = find_symbol ( name , & owner , & crc , <nl> !( mod -> taints & ( 1 << TAINT_PROPRIETARY_MODULE )), true );
mmm drivers / block / drbd / drbd_req . c <nl> ppp drivers / block / drbd / drbd_req . c <nl> int __req_mod ( struct drbd_request * req , enum drbd_req_event what , <nl> _req_may_be_done ( req , m ); /* Allowed while state . susp */ <nl> break ; <nl>  <nl> - case write_acked_by_peer_and_sis : <nl> - req -> rq_state |= RQ_NET_SIS ; <nl> case conflict_discarded_by_peer : <nl> /* for discarded conflicting writes of multiple primaries , <nl> * there is no need to keep anything in the tl , potential <nl> int __req_mod ( struct drbd_request * req , enum drbd_req_event what , <nl> ( unsigned long long ) req -> sector , req -> size ); <nl> req -> rq_state |= RQ_NET_DONE ; <nl> /* fall through */ <nl> + case write_acked_by_peer_and_sis : <nl> case write_acked_by_peer : <nl> + if ( what == write_acked_by_peer_and_sis ) <nl> + req -> rq_state |= RQ_NET_SIS ; <nl> /* protocol C ; successfully written on peer . <nl> - * Nothing to do here . <nl> + * Nothing more to do here . <nl> * We want to keep the tl in place for all protocols , to cater <nl> - * for volatile write - back caches on lower level devices . <nl> - * <nl> - * A barrier request is expected to have forced all prior <nl> - * requests onto stable storage , so completion of a barrier <nl> - * request could set NET_DONE right here , and not wait for the <nl> - * P_BARRIER_ACK , but that is an unnecessary optimization . */ <nl> + * for volatile write - back caches on lower level devices . */ <nl>  <nl> - /* this makes it effectively the same as for : */ <nl> case recv_acked_by_peer : <nl> /* protocol B ; pretends to be successfully written on peer . <nl> * see also notes above in handed_over_to_network about
mmm drivers / staging / vt6655 / device_main . c <nl> ppp drivers / staging / vt6655 / device_main . c <nl> static int vnt_tx_packet ( struct vnt_private * priv , struct sk_buff * skb ) <nl> if ( dma_idx == TYPE_AC0DMA ) <nl> head_td -> pTDInfo -> byFlags = TD_FLAGS_NETIF_SKB ; <nl>  <nl> - priv -> iTDUsed [ dma_idx ]++; <nl> - <nl> - /* Take ownership */ <nl> - wmb (); <nl> - head_td -> m_td0TD0 . f1Owner = OWNED_BY_NIC ; <nl> - <nl> - /* get Next */ <nl> - wmb (); <nl> priv -> apCurrTD [ dma_idx ] = head_td -> next ; <nl>  <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl> static int vnt_tx_packet ( struct vnt_private * priv , struct sk_buff * skb ) <nl>  <nl> head_td -> buff_addr = cpu_to_le32 ( head_td -> pTDInfo -> skb_dma ); <nl>  <nl> + /* Poll Transmit the adapter */ <nl> + wmb (); <nl> + head_td -> m_td0TD0 . f1Owner = OWNED_BY_NIC ; <nl> + wmb (); /* second memory barrier */ <nl> + <nl> if ( head_td -> pTDInfo -> byFlags & TD_FLAGS_NETIF_SKB ) <nl> MACvTransmitAC0 ( priv -> PortOffset ); <nl> else <nl> MACvTransmit0 ( priv -> PortOffset ); <nl>  <nl> + priv -> iTDUsed [ dma_idx ]++; <nl> + <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl>  <nl> return 0 ;
mmm net / core / scm . c <nl> ppp net / core / scm . c <nl> static __inline__ int scm_check_creds ( struct ucred * creds ) <nl> return - EINVAL ; <nl>  <nl> if (( creds -> pid == task_tgid_vnr ( current ) || <nl> - ns_capable ( current -> nsproxy -> pid_ns -> user_ns , CAP_SYS_ADMIN )) && <nl> + ns_capable ( task_active_pid_ns ( current )-> user_ns , CAP_SYS_ADMIN )) && <nl> (( uid_eq ( uid , cred -> uid ) || uid_eq ( uid , cred -> euid ) || <nl> uid_eq ( uid , cred -> suid )) || nsown_capable ( CAP_SETUID )) && <nl> (( gid_eq ( gid , cred -> gid ) || gid_eq ( gid , cred -> egid ) ||
mmm fs / autofs4 / waitq . c <nl> ppp fs / autofs4 / waitq . c <nl> void autofs4_catatonic_mode ( struct autofs_sb_info * sbi ) <nl> mutex_unlock (& sbi -> wq_mutex ); <nl> } <nl>  <nl> - static int autofs4_write ( struct file * file , const void * addr , int bytes ) <nl> + static int autofs4_write ( struct autofs_sb_info * sbi , <nl> + struct file * file , const void * addr , int bytes ) <nl> { <nl> unsigned long sigpipe , flags ; <nl> mm_segment_t fs ; <nl> const char * data = ( const char *) addr ; <nl> ssize_t wr = 0 ; <nl>  <nl> - /** WARNING : this is not safe for writing more than PIPE_BUF bytes ! **/ <nl> - <nl> sigpipe = sigismember (& current -> pending . signal , SIGPIPE ); <nl>  <nl> /* Save pointer to user space and point back to kernel space */ <nl> fs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl>  <nl> + mutex_lock (& sbi -> pipe_mutex ); <nl> while ( bytes && <nl> ( wr = file -> f_op -> write ( file , data , bytes ,& file -> f_pos )) > 0 ) { <nl> data += wr ; <nl> bytes -= wr ; <nl> } <nl> + mutex_lock (& sbi -> pipe_mutex ); <nl>  <nl> set_fs ( fs ); <nl>  <nl> static void autofs4_notify_daemon ( struct autofs_sb_info * sbi , <nl>  <nl> mutex_unlock (& sbi -> wq_mutex ); <nl>  <nl> - if ( autofs4_write ( pipe , & pkt , pktsz )) <nl> + if ( autofs4_write ( sbi , pipe , & pkt , pktsz )) <nl> autofs4_catatonic_mode ( sbi ); <nl> fput ( pipe ); <nl> }mmm fs / autofs4 / inode . c <nl> ppp fs / autofs4 / inode . c <nl> void autofs4_catatonic_mode ( struct autofs_sb_info * sbi ) <nl> mutex_unlock (& sbi -> wq_mutex ); <nl> } <nl>  <nl> - static int autofs4_write ( struct file * file , const void * addr , int bytes ) <nl> + static int autofs4_write ( struct autofs_sb_info * sbi , <nl> + struct file * file , const void * addr , int bytes ) <nl> { <nl> unsigned long sigpipe , flags ; <nl> mm_segment_t fs ; <nl> const char * data = ( const char *) addr ; <nl> ssize_t wr = 0 ; <nl>  <nl> - /** WARNING : this is not safe for writing more than PIPE_BUF bytes ! **/ <nl> - <nl> sigpipe = sigismember (& current -> pending . signal , SIGPIPE ); <nl>  <nl> /* Save pointer to user space and point back to kernel space */ <nl> fs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl>  <nl> + mutex_lock (& sbi -> pipe_mutex ); <nl> while ( bytes && <nl> ( wr = file -> f_op -> write ( file , data , bytes ,& file -> f_pos )) > 0 ) { <nl> data += wr ; <nl> bytes -= wr ; <nl> } <nl> + mutex_lock (& sbi -> pipe_mutex ); <nl>  <nl> set_fs ( fs ); <nl>  <nl> static void autofs4_notify_daemon ( struct autofs_sb_info * sbi , <nl>  <nl> mutex_unlock (& sbi -> wq_mutex ); <nl>  <nl> - if ( autofs4_write ( pipe , & pkt , pktsz )) <nl> + if ( autofs4_write ( sbi , pipe , & pkt , pktsz )) <nl> autofs4_catatonic_mode ( sbi ); <nl> fput ( pipe ); <nl> } <nl> int autofs4_fill_super ( struct super_block * s , void * data , int silent ) <nl> sbi -> min_proto = 0 ; <nl> sbi -> max_proto = 0 ; <nl> mutex_init (& sbi -> wq_mutex ); <nl> + mutex_init (& sbi -> pipe_mutex ); <nl> spin_lock_init (& sbi -> fs_lock ); <nl> sbi -> queues = NULL ; <nl> spin_lock_init (& sbi -> lookup_lock );mmm fs / autofs4 / autofs_i . h <nl> ppp fs / autofs4 / autofs_i . h <nl> void autofs4_catatonic_mode ( struct autofs_sb_info * sbi ) <nl> mutex_unlock (& sbi -> wq_mutex ); <nl> } <nl>  <nl> - static int autofs4_write ( struct file * file , const void * addr , int bytes ) <nl> + static int autofs4_write ( struct autofs_sb_info * sbi , <nl> + struct file * file , const void * addr , int bytes ) <nl> { <nl> unsigned long sigpipe , flags ; <nl> mm_segment_t fs ; <nl> const char * data = ( const char *) addr ; <nl> ssize_t wr = 0 ; <nl>  <nl> - /** WARNING : this is not safe for writing more than PIPE_BUF bytes ! **/ <nl> - <nl> sigpipe = sigismember (& current -> pending . signal , SIGPIPE ); <nl>  <nl> /* Save pointer to user space and point back to kernel space */ <nl> fs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl>  <nl> + mutex_lock (& sbi -> pipe_mutex ); <nl> while ( bytes && <nl> ( wr = file -> f_op -> write ( file , data , bytes ,& file -> f_pos )) > 0 ) { <nl> data += wr ; <nl> bytes -= wr ; <nl> } <nl> + mutex_lock (& sbi -> pipe_mutex ); <nl>  <nl> set_fs ( fs ); <nl>  <nl> static void autofs4_notify_daemon ( struct autofs_sb_info * sbi , <nl>  <nl> mutex_unlock (& sbi -> wq_mutex ); <nl>  <nl> - if ( autofs4_write ( pipe , & pkt , pktsz )) <nl> + if ( autofs4_write ( sbi , pipe , & pkt , pktsz )) <nl> autofs4_catatonic_mode ( sbi ); <nl> fput ( pipe ); <nl> } <nl> int autofs4_fill_super ( struct super_block * s , void * data , int silent ) <nl> sbi -> min_proto = 0 ; <nl> sbi -> max_proto = 0 ; <nl> mutex_init (& sbi -> wq_mutex ); <nl> + mutex_init (& sbi -> pipe_mutex ); <nl> spin_lock_init (& sbi -> fs_lock ); <nl> sbi -> queues = NULL ; <nl> spin_lock_init (& sbi -> lookup_lock ); <nl> struct autofs_sb_info { <nl> int needs_reghost ; <nl> struct super_block * sb ; <nl> struct mutex wq_mutex ; <nl> + struct mutex pipe_mutex ; <nl> spinlock_t fs_lock ; <nl> struct autofs_wait_queue * queues ; /* Wait queue pointer */ <nl> spinlock_t lookup_lock ;
mmm net / core / dev . c <nl> ppp net / core / dev . c <nl> static int __netdev_upper_dev_link ( struct net_device * dev , <nl> if ( __netdev_find_adj ( upper_dev , dev , & upper_dev -> all_adj_list . upper )) <nl> return - EBUSY ; <nl>  <nl> - if ( __netdev_find_adj ( dev , upper_dev , & dev -> all_adj_list . upper )) <nl> + if ( __netdev_find_adj ( dev , upper_dev , & dev -> adj_list . upper )) <nl> return - EEXIST ; <nl>  <nl> if ( master && netdev_master_upper_dev_get ( dev ))
mmm security / apparmor / apparmorfs . c <nl> ppp security / apparmor / apparmorfs . c <nl> void __aa_fs_profile_migrate_dents ( struct aa_profile * old , <nl>  <nl> for ( i = 0 ; i < AAFS_PROF_SIZEOF ; i ++) { <nl> new -> dents [ i ] = old -> dents [ i ]; <nl> + if ( new -> dents [ i ]) <nl> + new -> dents [ i ]-> d_inode -> i_mtime = CURRENT_TIME ; <nl> old -> dents [ i ] = NULL ; <nl> } <nl> }
mmm drivers / scsi / sata_mv . c <nl> ppp drivers / scsi / sata_mv . c <nl> static void mv_host_intr ( struct ata_host_set * host_set , u32 relevant , <nl> handled ++; <nl> } <nl>  <nl> - if ( ap && <nl> - ( ap -> flags & ( ATA_FLAG_PORT_DISABLED | ATA_FLAG_NOINTR ))) <nl> + if ( ap && ( ap -> flags & ATA_FLAG_PORT_DISABLED )) <nl> continue ; <nl>  <nl> err_mask = ac_err_mask ( ata_status ); <nl> static void mv_host_intr ( struct ata_host_set * host_set , u32 relevant , <nl> VPRINTK (" port % u IRQ found for qc , " <nl> " ata_status 0x % x \ n ", port , ata_status ); <nl> /* mark qc status appropriately */ <nl> - if (!( qc -> tf . ctl & ATA_NIEN )) <nl> + if (!( qc -> tf . flags & ATA_TFLAG_POLLING )) <nl> ata_qc_complete ( qc , err_mask ); <nl> } <nl> }
mmm fs / nfs / nfs3xdr . c <nl> ppp fs / nfs / nfs3xdr . c <nl> static void nfs3_xdr_enc_setacl3args ( struct rpc_rqst * req , <nl> if ( args -> npages != 0 ) <nl> xdr_write_pages ( xdr , args -> pages , 0 , args -> len ); <nl> else <nl> - xdr_reserve_space ( xdr , NFS_ACL_INLINE_BUFSIZE ); <nl> + xdr_reserve_space ( xdr , args -> len ); <nl>  <nl> error = nfsacl_encode ( xdr -> buf , base , args -> inode , <nl> ( args -> mask & NFS_ACL ) ?
mmm arch / arm64 / kernel / cpufeature . c <nl> ppp arch / arm64 / kernel / cpufeature . c <nl> void __init init_cpu_features ( struct cpuinfo_arm64 * info ) <nl> * Run the errata work around checks on the boot CPU , once we have <nl> * initialised the cpu feature infrastructure . <nl> */ <nl> - update_cpu_capabilities ( arm64_errata , SCOPE_ALL , <nl> + update_cpu_capabilities ( arm64_errata , SCOPE_LOCAL_CPU , <nl> " enabling workaround for "); <nl> } <nl>  <nl> void check_local_cpu_capabilities ( void ) <nl> * advertised capabilities . <nl> */ <nl> if (! sys_caps_initialised ) <nl> - update_cpu_capabilities ( arm64_errata , SCOPE_ALL , <nl> + update_cpu_capabilities ( arm64_errata , SCOPE_LOCAL_CPU , <nl> " enabling workaround for "); <nl> else <nl> verify_local_cpu_capabilities (); <nl> void __init setup_cpu_features ( void ) <nl>  <nl> /* Set the CPU feature capabilies */ <nl> update_cpu_capabilities ( arm64_features , SCOPE_ALL , " detected :"); <nl> + update_cpu_capabilities ( arm64_errata , SCOPE_SYSTEM , <nl> + " enabling workaround for "); <nl> enable_cpu_capabilities ( arm64_features , SCOPE_ALL ); <nl> enable_cpu_capabilities ( arm64_errata , SCOPE_ALL ); <nl> mark_const_caps_ready ();
mmm init / main . c <nl> ppp init / main . c <nl> static void run_init_process ( const char * init_filename ) <nl> kernel_execve ( init_filename , argv_init , envp_init ); <nl> } <nl>  <nl> -/* This is a non __init function . Force it to be noinline otherwise gcc <nl> - * makes it inline to init () and it becomes part of init . text section <nl> - */ <nl> - static noinline int init_post ( void ) <nl> + static void __init kernel_init_freeable ( void ); <nl> + <nl> + static int __ref kernel_init ( void * unused ) <nl> { <nl> + kernel_init_freeable (); <nl> /* need to finish all async __init code before freeing the memory */ <nl> async_synchronize_full (); <nl> free_initmem (); <nl> static noinline int init_post ( void ) <nl> " See Linux Documentation / init . txt for guidance ."); <nl> } <nl>  <nl> - static int __init kernel_init ( void * unused ) <nl> + static void __init kernel_init_freeable ( void ) <nl> { <nl> /* <nl> * Wait until kthreadd is all set - up . <nl> static int __init kernel_init ( void * unused ) <nl> * we ' re essentially up and running . Get rid of the <nl> * initmem segments and start the user - mode stuff .. <nl> */ <nl> - <nl> - init_post (); <nl> - return 0 ; <nl> }
mmm drivers / net / wireless / iwlwifi / mvm / rs . c <nl> ppp drivers / net / wireless / iwlwifi / mvm / rs . c <nl> void iwl_mvm_rs_tx_status ( struct iwl_mvm * mvm , struct ieee80211_sta * sta , <nl> * first index into rate scale table . <nl> */ <nl> if ( info -> flags & IEEE80211_TX_STAT_AMPDU ) { <nl> + /* ampdu_ack_len = 0 marks no BA was received . In this case <nl> + * treat it as a single frame loss as we don ' t want the success <nl> + * ratio to dip too quickly because a BA wasn ' t received <nl> + */ <nl> + if ( info -> status . ampdu_ack_len == 0 ) <nl> + info -> status . ampdu_len = 1 ; <nl> + <nl> ucode_rate = le32_to_cpu ( table -> rs_table [ 0 ]); <nl> rs_rate_from_ucode_rate ( ucode_rate , info -> band , & rate ); <nl> rs_collect_tx_data ( lq_sta , curr_tbl , rate . index ,
mmm net / tipc / socket . c <nl> ppp net / tipc / socket . c <nl> static int __tipc_sendmsg ( struct socket * sock , struct msghdr * m , size_t dlen ) <nl> msg_set_syn ( hdr , 1 ); <nl> } <nl>  <nl> + memset (& skaddr , 0 , sizeof ( skaddr )); <nl> + <nl> /* Determine destination */ <nl> if ( atype == TIPC_SERVICE_RANGE ) { <nl> return tipc_sendmcast ( sock , ua , m , dlen , timeout );
mmm drivers / gpio / gpio - davinci . c <nl> ppp drivers / gpio / gpio - davinci . c <nl> static int davinci_gpio_probe ( struct platform_device * pdev ) <nl> spin_lock_init (& chips [ i ]. lock ); <nl>  <nl> regs = gpio2regs ( base ); <nl> + if (! regs ) <nl> + return - ENXIO ; <nl> chips [ i ]. regs = regs ; <nl> chips [ i ]. set_data = & regs -> set_data ; <nl> chips [ i ]. clr_data = & regs -> clr_data ;
mmm drivers / firewire / ohci . c <nl> ppp drivers / firewire / ohci . c <nl> static void bus_reset_work ( struct work_struct * work ) <nl> { <nl> struct fw_ohci * ohci = <nl> container_of ( work , struct fw_ohci , bus_reset_work ); <nl> - int self_id_count , i , j , reg ; <nl> - int generation , new_generation ; <nl> + int self_id_count , generation , new_generation , i , j ; <nl> + u32 reg ; <nl> unsigned long flags ; <nl> void * free_rom = NULL ; <nl> dma_addr_t free_rom_bus = 0 ;
mmm drivers / pwm / core . c <nl> ppp drivers / pwm / core . c <nl> struct pwm_device * pwm_get ( struct device * dev , const char * con_id ) <nl> unsigned int best = 0 ; <nl> struct pwm_lookup * p ; <nl> unsigned int match ; <nl> + unsigned int period ; <nl> + enum pwm_polarity polarity ; <nl>  <nl> /* look up via DT first */ <nl> if ( IS_ENABLED ( CONFIG_OF ) && dev && dev -> of_node ) <nl> struct pwm_device * pwm_get ( struct device * dev , const char * con_id ) <nl> if ( match > best ) { <nl> chip = pwmchip_find_by_name ( p -> provider ); <nl> index = p -> index ; <nl> + period = p -> period ; <nl> + polarity = p -> polarity ; <nl>  <nl> if ( match != 3 ) <nl> best = match ; <nl> struct pwm_device * pwm_get ( struct device * dev , const char * con_id ) <nl> if ( IS_ERR ( pwm )) <nl> return pwm ; <nl>  <nl> - pwm_set_period ( pwm , p -> period ); <nl> - pwm_set_polarity ( pwm , p -> polarity ); <nl> + pwm_set_period ( pwm , period ); <nl> + pwm_set_polarity ( pwm , polarity ); <nl>  <nl>  <nl> return pwm ;
mmm arch / x86 / events / intel / uncore_nhmex . c <nl> ppp arch / x86 / events / intel / uncore_nhmex . c <nl> static void nhmex_uncore_msr_enable_event ( struct intel_uncore_box * box , struct p <nl> { <nl> struct hw_perf_event * hwc = & event -> hw ; <nl>  <nl> - if ( hwc -> idx >= UNCORE_PMC_IDX_FIXED ) <nl> + if ( hwc -> idx == UNCORE_PMC_IDX_FIXED ) <nl> wrmsrl ( hwc -> config_base , NHMEX_PMON_CTL_EN_BIT0 ); <nl> else if ( box -> pmu -> type -> event_mask & NHMEX_PMON_CTL_EN_BIT0 ) <nl> wrmsrl ( hwc -> config_base , hwc -> config | NHMEX_PMON_CTL_EN_BIT22 );
mmm tools / lib / bpf / libbpf . c <nl> ppp tools / lib / bpf / libbpf . c <nl> BPF_PROG_TYPE_FNS ( tracepoint , BPF_PROG_TYPE_TRACEPOINT ); <nl> BPF_PROG_TYPE_FNS ( xdp , BPF_PROG_TYPE_XDP ); <nl> BPF_PROG_TYPE_FNS ( perf_event , BPF_PROG_TYPE_PERF_EVENT ); <nl>  <nl> -# define BPF_PROG_SEC ( string , type ) { string , sizeof ( string ), type } <nl> +# define BPF_PROG_SEC ( string , type ) { string , sizeof ( string ) - 1 , type } <nl> static const struct { <nl> const char * sec ; <nl> size_t len ;
mmm fs / io_uring . c <nl> ppp fs / io_uring . c <nl> static int io_files_update_with_index_alloc ( struct io_kiocb * req , <nl> struct file * file ; <nl> int ret , fd ; <nl>  <nl> + if (! req -> ctx -> file_data ) <nl> + return - ENXIO ; <nl> + <nl> for ( done = 0 ; done < req -> rsrc_update . nr_args ; done ++) { <nl> if ( copy_from_user (& fd , & fds [ done ], sizeof ( fd ))) { <nl> ret = - EFAULT ;
mmm net / mac80211 / sta_info . c <nl> ppp net / mac80211 / sta_info . c <nl> struct sta_info * sta_info_alloc ( struct ieee80211_sub_if_data * sdata , <nl> if ( sta -> sta . txq [ 0 ]) <nl> kfree ( to_txq_info ( sta -> sta . txq [ 0 ])); <nl> free : <nl> + free_percpu ( sta -> pcpu_rx_stats ); <nl> # ifdef CONFIG_MAC80211_MESH <nl> kfree ( sta -> mesh ); <nl> # endif
mmm fs / btrfs / send . c <nl> ppp fs / btrfs / send . c <nl> verbose_printk (" btrfs : send_create_inode % llu \ n ", ino ); <nl> TLV_PUT_PATH ( sctx , BTRFS_SEND_A_PATH_LINK , p ); <nl> } else if ( S_ISCHR ( mode ) || S_ISBLK ( mode ) || <nl> S_ISFIFO ( mode ) || S_ISSOCK ( mode )) { <nl> - TLV_PUT_U64 ( sctx , BTRFS_SEND_A_RDEV , rdev ); <nl> + TLV_PUT_U64 ( sctx , BTRFS_SEND_A_RDEV , new_encode_dev ( rdev )); <nl> + TLV_PUT_U64 ( sctx , BTRFS_SEND_A_MODE , mode ); <nl> } <nl>  <nl> ret = send_cmd ( sctx );
mmm net / bridge / br_netfilter . c <nl> ppp net / bridge / br_netfilter . c <nl> struct brnf_frag_data { <nl> char mac [ NF_BRIDGE_MAX_MAC_HEADER_LENGTH ]; <nl> u8 encap_size ; <nl> u8 size ; <nl> + u16 vlan_tci ; <nl> + __be16 vlan_proto ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct brnf_frag_data , brnf_frag_data_storage ); <nl> static int br_nf_push_frag_xmit ( struct sock * sk , struct sk_buff * skb ) <nl> return 0 ; <nl> } <nl>  <nl> + if ( data -> vlan_tci ) { <nl> + skb -> vlan_tci = data -> vlan_tci ; <nl> + skb -> vlan_proto = data -> vlan_proto ; <nl> + } <nl> + <nl> skb_copy_to_linear_data_offset ( skb , - data -> size , data -> mac , data -> size ); <nl> __skb_push ( skb , data -> encap_size ); <nl>  <nl> static int br_nf_dev_queue_xmit ( struct sock * sk , struct sk_buff * skb ) <nl> nf_bridge_update_protocol ( skb ); <nl>  <nl> data = this_cpu_ptr (& brnf_frag_data_storage ); <nl> + <nl> + data -> vlan_tci = skb -> vlan_tci ; <nl> + data -> vlan_proto = skb -> vlan_proto ; <nl> data -> encap_size = nf_bridge_encap_header_len ( skb ); <nl> data -> size = ETH_HLEN + data -> encap_size ; <nl> 
mmm drivers / video / omap2 / dss / apply . c <nl> ppp drivers / video / omap2 / dss / apply . c <nl> int dss_mgr_enable ( struct omap_overlay_manager * mgr ) <nl> if (! mgr_manual_update ( mgr )) <nl> mp -> updating = true ; <nl>  <nl> + if (! dss_data . irq_enabled && need_isr ()) <nl> + dss_register_vsync_isr (); <nl> + <nl> spin_unlock_irqrestore (& data_lock , flags ); <nl>  <nl> if (! mgr_manual_update ( mgr ))
mmm net / batman - adv / bat_iv_ogm . c <nl> ppp net / batman - adv / bat_iv_ogm . c <nl> static void bat_iv_ogm_iface_enable ( struct hard_iface * hard_iface ) <nl> { <nl> struct batman_ogm_packet * batman_ogm_packet ; <nl> + uint32_t random_seqno ; <nl> + <nl> + /* randomize initial seqno to avoid collision */ <nl> + get_random_bytes (& random_seqno , sizeof ( random_seqno )); <nl> + atomic_set (& hard_iface -> seqno , random_seqno ); <nl>  <nl> hard_iface -> packet_len = BATMAN_OGM_LEN ; <nl> hard_iface -> packet_buff = kmalloc ( hard_iface -> packet_len , GFP_ATOMIC );
mmm drivers / media / dvb / ttpci / av7110_v4l . c <nl> ppp drivers / media / dvb / ttpci / av7110_v4l . c <nl> static struct saa7146_ext_vv av7110_vv_data_st = { <nl> static struct saa7146_ext_vv av7110_vv_data_c = { <nl> . inputs = 1 , <nl> . audios = 1 , <nl> - . capabilities = V4L2_CAP_TUNER | V4L2_CAP_VBI_CAPTURE | V4L2_CAP_SLICED_VBI_OUTPUT , <nl> + . capabilities = V4L2_CAP_TUNER | V4L2_CAP_SLICED_VBI_OUTPUT , <nl> . flags = SAA7146_USE_PORT_B_FOR_VBI , <nl>  <nl> . stds = & standard [ 0 ],
mmm drivers / crypto / chelsio / chcr_algo . c <nl> ppp drivers / crypto / chelsio / chcr_algo . c <nl> static inline void chcr_handle_ahash_resp ( struct ahash_request * req , <nl>  <nl> if ( input == NULL ) <nl> goto out ; <nl> - reqctx = ahash_request_ctx ( req ); <nl> digestsize = crypto_ahash_digestsize ( crypto_ahash_reqtfm ( req )); <nl> if ( reqctx -> is_sg_map ) <nl> chcr_hash_dma_unmap (& u_ctx -> lldi . pdev -> dev , req ); <nl> static int chcr_aead_common_init ( struct aead_request * req , <nl> struct chcr_aead_ctx * aeadctx = AEAD_CTX ( a_ctx ( tfm )); <nl> struct chcr_aead_reqctx * reqctx = aead_request_ctx ( req ); <nl> int error = - EINVAL ; <nl> - unsigned int dst_size ; <nl> unsigned int authsize = crypto_aead_authsize ( tfm ); <nl>  <nl> - dst_size = req -> assoclen + req -> cryptlen + ( op_type ? <nl> - - authsize : authsize ); <nl> /* validate key size */ <nl> if ( aeadctx -> enckey_len == 0 ) <nl> goto err ;
mmm include / linux / if_team . h <nl> ppp include / linux / if_team . h <nl> struct team_port { <nl> s32 priority ; /* lower number ~ higher priority */ <nl> u16 queue_id ; <nl> struct list_head qom_list ; /* node in queue override mapping list */ <nl> + struct rcu_head rcu ; <nl> long mode_priv [ 0 ]; <nl> }; <nl> mmm drivers / net / team / team . c <nl> ppp drivers / net / team / team . c <nl> struct team_port { <nl> s32 priority ; /* lower number ~ higher priority */ <nl> u16 queue_id ; <nl> struct list_head qom_list ; /* node in queue override mapping list */ <nl> + struct rcu_head rcu ; <nl> long mode_priv [ 0 ]; <nl> }; <nl>  <nl> static int team_port_del ( struct team * team , struct net_device * port_dev ) <nl>  <nl> team_port_set_orig_dev_addr ( port ); <nl> dev_set_mtu ( port_dev , port -> orig . mtu ); <nl> - synchronize_rcu (); <nl> - kfree ( port ); <nl> + kfree_rcu ( port , rcu ); <nl> netdev_info ( dev , " Port device % s removed \ n ", portname ); <nl> __team_compute_features ( team ); <nl> 
mmm drivers / gpu / drm / amd / amdkfd / kfd_device_queue_manager . c <nl> ppp drivers / gpu / drm / amd / amdkfd / kfd_device_queue_manager . c <nl> static int create_queue_cpsch ( struct device_queue_manager * dqm , struct queue * q , <nl> return retval ; <nl> } <nl>  <nl> - int fence_wait_timeout ( unsigned int * fence_addr , unsigned int fence_value , <nl> - unsigned long timeout ) <nl> + static int fence_wait_timeout ( unsigned int * fence_addr , <nl> + unsigned int fence_value , <nl> + unsigned long timeout ) <nl> { <nl> BUG_ON (! fence_addr ); <nl> timeout += jiffies ;
mmm sound / oss / soundcard . c <nl> ppp sound / oss / soundcard . c <nl> int * load_mixer_volumes ( char * name , int * levels , int present ) <nl> int i , n ; <nl>  <nl> for ( i = 0 ; i < num_mixer_volumes ; i ++) { <nl> - if ( strcmp ( name , mixer_vols [ i ]. name ) == 0 ) { <nl> + if ( strncmp ( name , mixer_vols [ i ]. name , 32 ) == 0 ) { <nl> if ( present ) <nl> mixer_vols [ i ]. num = i ; <nl> return mixer_vols [ i ]. levels ; <nl> int * load_mixer_volumes ( char * name , int * levels , int present ) <nl> } <nl> n = num_mixer_volumes ++; <nl>  <nl> - strcpy ( mixer_vols [ n ]. name , name ); <nl> + strncpy ( mixer_vols [ n ]. name , name , 32 ); <nl>  <nl> if ( present ) <nl> mixer_vols [ n ]. num = n ;
mmm sound / pci / ice1712 / ice1724 . c <nl> ppp sound / pci / ice1712 / ice1724 . c <nl> static int __devinit snd_vt1724_read_eeprom ( struct snd_ice1712 * ice , <nl> static void __devinit snd_vt1724_chip_reset ( struct snd_ice1712 * ice ) <nl> { <nl> outb ( VT1724_RESET , ICEREG1724 ( ice , CONTROL )); <nl> + inb ( ICEREG1724 ( ice , CONTROL )); /* pci posting flush */ <nl> msleep ( 10 ); <nl> outb ( 0 , ICEREG1724 ( ice , CONTROL )); <nl> + inb ( ICEREG1724 ( ice , CONTROL )); /* pci posting flush */ <nl> msleep ( 10 ); <nl> } <nl> 
mmm drivers / vhost / net . c <nl> ppp drivers / vhost / net . c <nl> static int get_rx_bufs ( struct vhost_virtqueue * vq , <nl> * iovcount = seg ; <nl> if ( unlikely ( log )) <nl> * log_num = nlogs ; <nl> + <nl> + /* Detect overrun */ <nl> + if ( unlikely ( datalen > 0 )) { <nl> + r = UIO_MAXIOV + 1 ; <nl> + goto err ; <nl> + } <nl> return headcount ; <nl> err : <nl> vhost_discard_vq_desc ( vq , headcount ); <nl> static void handle_rx ( struct vhost_net * net ) <nl> /* On error , stop handling until the next kick . */ <nl> if ( unlikely ( headcount < 0 )) <nl> break ; <nl> + /* On overrun , truncate and discard */ <nl> + if ( unlikely ( headcount > UIO_MAXIOV )) { <nl> + msg . msg_iovlen = 1 ; <nl> + err = sock -> ops -> recvmsg ( NULL , sock , & msg , <nl> + 1 , MSG_DONTWAIT | MSG_TRUNC ); <nl> + pr_debug (" Discarded rx packet : len % zd \ n ", sock_len ); <nl> + continue ; <nl> + } <nl> /* OK , now we need to know about added descriptors . */ <nl> if (! headcount ) { <nl> if ( unlikely ( vhost_enable_notify (& net -> dev , vq ))) {
mmm drivers / staging / gdm724x / gdm_usb . c <nl> ppp drivers / staging / gdm724x / gdm_usb . c <nl> static void gdm_usb_disconnect ( struct usb_interface * intf ) <nl> { <nl> struct phy_dev * phy_dev ; <nl> struct lte_udev * udev ; <nl> - u16 idVendor , idProduct ; <nl> struct usb_device * usbdev ; <nl>  <nl> usbdev = interface_to_usbdev ( intf ); <nl> - <nl> - idVendor = __le16_to_cpu ( usbdev -> descriptor . idVendor ); <nl> - idProduct = __le16_to_cpu ( usbdev -> descriptor . idProduct ); <nl> - <nl> phy_dev = usb_get_intfdata ( intf ); <nl>  <nl> udev = phy_dev -> priv_dev ;
mmm drivers / media / video / gspca / jeilinj . c <nl> ppp drivers / media / video / gspca / jeilinj . c <nl> static int sd_start ( struct gspca_dev * gspca_dev ) <nl>  <nl> /* create the JPEG header */ <nl> dev -> jpeg_hdr = kmalloc ( JPEG_HDR_SZ , GFP_KERNEL ); <nl> + if ( dev -> jpeg_hdr == NULL ) <nl> + return - ENOMEM ; <nl> jpeg_define ( dev -> jpeg_hdr , gspca_dev -> height , gspca_dev -> width , <nl> 0x21 ); /* JPEG 422 */ <nl> jpeg_set_qual ( dev -> jpeg_hdr , dev -> quality );
mmm sound / core / seq / seq_ports . c <nl> ppp sound / core / seq / seq_ports . c <nl> int snd_seq_port_connect ( struct snd_seq_client * connector , <nl> atomic_set (& subs -> ref_count , 2 ); <nl>  <nl> down_write (& src -> list_mutex ); <nl> - down_write (& dest -> list_mutex ); <nl> + down_write_nested (& dest -> list_mutex , SINGLE_DEPTH_NESTING ); <nl>  <nl> exclusive = info -> flags & SNDRV_SEQ_PORT_SUBS_EXCLUSIVE ? 1 : 0 ; <nl> err = - EBUSY ; <nl> int snd_seq_port_disconnect ( struct snd_seq_client * connector , <nl> unsigned long flags ; <nl>  <nl> down_write (& src -> list_mutex ); <nl> - down_write (& dest -> list_mutex ); <nl> + down_write_nested (& dest -> list_mutex , SINGLE_DEPTH_NESTING ); <nl>  <nl> /* look for the connection */ <nl> list_for_each ( p , & src -> list_head ) {
mmm drivers / spi / atmel_spi . c <nl> ppp drivers / spi / atmel_spi . c <nl> static void atmel_spi_next_xfer ( struct spi_master * master , <nl> xfer , xfer -> len , xfer -> tx_buf , xfer -> tx_dma , <nl> xfer -> rx_buf , xfer -> rx_dma , spi_readl ( as , IMR )); <nl>  <nl> - spi_writel ( as , TCR , len ); <nl> spi_writel ( as , RCR , len ); <nl> + spi_writel ( as , TCR , len ); <nl> spi_writel ( as , PTCR , SPI_BIT ( TXTEN ) | SPI_BIT ( RXTEN )); <nl> } <nl> 
mmm net / bridge / netfilter / ebtables . c <nl> ppp net / bridge / netfilter / ebtables . c <nl> static int do_replace ( struct net * net , const void __user * user , <nl> if ( tmp . num_counters >= INT_MAX / sizeof ( struct ebt_counter )) <nl> return - ENOMEM ; <nl>  <nl> + tmp . name [ sizeof ( tmp . name ) - 1 ] = 0 ; <nl> + <nl> countersize = COUNTER_OFFSET ( tmp . nentries ) * nr_cpu_ids ; <nl> newinfo = vmalloc ( sizeof (* newinfo ) + countersize ); <nl> if (! newinfo )
mmm fs / btrfs / super . c <nl> ppp fs / btrfs / super . c <nl> static struct file_system_type btrfs_fs_type = { <nl> }; <nl> MODULE_ALIAS_FS (" btrfs "); <nl>  <nl> + static int btrfs_control_open ( struct inode * inode , struct file * file ) <nl> +{ <nl> + /* <nl> + * The control file ' s private_data is used to hold the <nl> + * transaction when it is started and is used to keep <nl> + * track of whether a transaction is already in progress . <nl> + */ <nl> + file -> private_data = NULL ; <nl> + return 0 ; <nl> +} <nl> + <nl> /* <nl> * used by btrfsctl to scan devices when no FS is mounted <nl> */ <nl> static const struct super_operations btrfs_super_ops = { <nl> }; <nl>  <nl> static const struct file_operations btrfs_ctl_fops = { <nl> + . open = btrfs_control_open , <nl> . unlocked_ioctl = btrfs_control_ioctl , <nl> . compat_ioctl = btrfs_control_ioctl , <nl> . owner = THIS_MODULE ,
mmm drivers / pci / pcie / aer / aerdrv_acpi . c <nl> ppp drivers / pci / pcie / aer / aerdrv_acpi . c <nl> int aer_osc_setup ( struct pcie_device * pciedev ) <nl> } <nl>  <nl> if ( handle ) { <nl> - pci_osc_support_set ( OSC_EXT_PCI_CONFIG_SUPPORT ); <nl> + pcie_osc_support_set ( OSC_EXT_PCI_CONFIG_SUPPORT ); <nl> status = pci_osc_control_set ( handle , <nl> OSC_PCI_EXPRESS_AER_CONTROL | <nl> OSC_PCI_EXPRESS_CAP_STRUCTURE_CONTROL );
mmm fs / btrfs / volumes . c <nl> ppp fs / btrfs / volumes . c <nl> int btrfs_read_sys_array ( struct btrfs_root * root ) <nl> sb_array_offset += len ; <nl> cur_offset += len ; <nl> } <nl> + clear_extent_buffer_uptodate ( sb ); <nl> free_extent_buffer_stale ( sb ); <nl> return ret ; <nl>  <nl> out_short_read : <nl> printk ( KERN_ERR " BTRFS : sys_array too short to read % u bytes at offset % u \ n ", <nl> len , cur_offset ); <nl> + clear_extent_buffer_uptodate ( sb ); <nl> free_extent_buffer_stale ( sb ); <nl> return - EIO ; <nl> }
mmm arch / x86 / mm / init_64 . c <nl> ppp arch / x86 / mm / init_64 . c <nl> void __init init_extra_mapping_uc ( unsigned long phys , unsigned long size ) <nl> void __init cleanup_highmap ( void ) <nl> { <nl> unsigned long vaddr = __START_KERNEL_map ; <nl> - unsigned long end = round_up (( unsigned long ) _end , PMD_SIZE ) - 1 ; <nl> + unsigned long end = roundup (( unsigned long ) _end , PMD_SIZE ) - 1 ; <nl> pmd_t * pmd = level2_kernel_pgt ; <nl> pmd_t * last_pmd = pmd + PTRS_PER_PMD ; <nl>  <nl> static void __init find_early_table_space ( unsigned long end ) <nl> unsigned long puds , pmds , ptes , tables , start ; <nl>  <nl> puds = ( end + PUD_SIZE - 1 ) >> PUD_SHIFT ; <nl> - tables = round_up ( puds * sizeof ( pud_t ), PAGE_SIZE ); <nl> + tables = roundup ( puds * sizeof ( pud_t ), PAGE_SIZE ); <nl> if ( direct_gbpages ) { <nl> unsigned long extra ; <nl> extra = end - (( end >> PUD_SHIFT ) << PUD_SHIFT ); <nl> pmds = ( extra + PMD_SIZE - 1 ) >> PMD_SHIFT ; <nl> } else <nl> pmds = ( end + PMD_SIZE - 1 ) >> PMD_SHIFT ; <nl> - tables += round_up ( pmds * sizeof ( pmd_t ), PAGE_SIZE ); <nl> + tables += roundup ( pmds * sizeof ( pmd_t ), PAGE_SIZE ); <nl>  <nl> if ( cpu_has_pse ) { <nl> unsigned long extra ; <nl> static void __init find_early_table_space ( unsigned long end ) <nl> ptes = ( extra + PAGE_SIZE - 1 ) >> PAGE_SHIFT ; <nl> } else <nl> ptes = ( end + PAGE_SIZE - 1 ) >> PAGE_SHIFT ; <nl> - tables += round_up ( ptes * sizeof ( pte_t ), PAGE_SIZE ); <nl> + tables += roundup ( ptes * sizeof ( pte_t ), PAGE_SIZE ); <nl>  <nl> /* <nl> * RED - PEN putting page tables only on node 0 could
mmm drivers / block / aoe / aoenet . c <nl> ppp drivers / block / aoe / aoenet . c <nl> aoenet_xmit ( struct sk_buff_head * queue ) <nl> { <nl> struct sk_buff * skb , * tmp ; <nl>  <nl> - skb_queue_walk_safe ( queue , skb , tmp ) <nl> + skb_queue_walk_safe ( queue , skb , tmp ) { <nl> + __skb_unlink ( skb , queue ); <nl> dev_queue_xmit ( skb ); <nl> + } <nl> } <nl>  <nl> /*
mmm drivers / net / ethernet / natsemi / xtsonic . c <nl> ppp drivers / net / ethernet / natsemi / xtsonic . c <nl> static int xtsonic_open ( struct net_device * dev ) <nl> { <nl> int retval ; <nl>  <nl> - retval = request_irq ( dev -> irq , sonic_interrupt , IRQF_DISABLED , <nl> - " sonic ", dev ); <nl> + retval = request_irq ( dev -> irq , sonic_interrupt , 0 , " sonic ", dev ); <nl> if ( retval ) { <nl> printk ( KERN_ERR "% s : unable to get IRQ % d .\ n ", <nl> dev -> name , dev -> irq );mmm drivers / net / ethernet / natsemi / jazzsonic . c <nl> ppp drivers / net / ethernet / natsemi / jazzsonic . c <nl> static int xtsonic_open ( struct net_device * dev ) <nl> { <nl> int retval ; <nl>  <nl> - retval = request_irq ( dev -> irq , sonic_interrupt , IRQF_DISABLED , <nl> - " sonic ", dev ); <nl> + retval = request_irq ( dev -> irq , sonic_interrupt , 0 , " sonic ", dev ); <nl> if ( retval ) { <nl> printk ( KERN_ERR "% s : unable to get IRQ % d .\ n ", <nl> dev -> name , dev -> irq ); <nl> static int jazzsonic_open ( struct net_device * dev ) <nl> { <nl> int retval ; <nl>  <nl> - retval = request_irq ( dev -> irq , sonic_interrupt , IRQF_DISABLED , <nl> - " sonic ", dev ); <nl> + retval = request_irq ( dev -> irq , sonic_interrupt , 0 , " sonic ", dev ); <nl> if ( retval ) { <nl> printk ( KERN_ERR "% s : unable to get IRQ % d .\ n ", <nl> dev -> name , dev -> irq );
mmm include / linux / mmc / host . h <nl> ppp include / linux / mmc / host . h <nl> struct mmc_host_ops { <nl>  <nl> int (* start_signal_voltage_switch )( struct mmc_host * host , struct mmc_ios * ios ); <nl>  <nl> + /* Check if the card is pulling dat [ 0 : 3 ] low */ <nl> + int (* card_busy )( struct mmc_host * host ); <nl> + <nl> /* The tuning command opcode value is different for SD and eMMC cards */ <nl> int (* execute_tuning )( struct mmc_host * host , u32 opcode ); <nl> void (* enable_preset_value )( struct mmc_host * host , bool enable );
mmm drivers / media / common / siano / smscoreapi . c <nl> ppp drivers / media / common / siano / smscoreapi . c <nl> void smscore_onresponse ( struct smscore_device_t * coredev , <nl> - sizeof ( struct SmsMsgHdr_ST )); <nl> break ; <nl>  <nl> + case MSG_SMS_DVBT_BDA_DATA : <nl> + /* <nl> + * It can be received here , if the frontend is <nl> + * tuned into a valid channel and the proper firmware <nl> + * is loaded . That happens when the module got removed <nl> + * and re - inserted , without powering the device off <nl> + */ <nl> + break ; <nl> + <nl> default : <nl> sms_debug (" message % s (% d ) not handled .", <nl> smscore_translate_msg ( phdr -> msgType ),
mmm drivers / gpu / drm / i915 / gvt / handlers . c <nl> ppp drivers / gpu / drm / i915 / gvt / handlers . c <nl> static int mailbox_write ( struct intel_vgpu * vgpu , unsigned int offset , <nl> else <nl> * data0 = 0x61514b3d ; <nl> break ; <nl> + case SKL_PCODE_CDCLK_CONTROL : <nl> + * data0 = SKL_CDCLK_READY_FOR_CHANGE ; <nl> + break ; <nl> case 0x5 : <nl> * data0 |= 0x1 ; <nl> break ; <nl> static int mailbox_write ( struct intel_vgpu * vgpu , unsigned int offset , <nl>  <nl> gvt_dbg_core (" VM (% d ) write % x to mailbox , return data0 % x \ n ", <nl> vgpu -> id , value , * data0 ); <nl> - <nl> - value &= ~( 1 << 31 ); <nl> + /** <nl> + * PCODE_READY clear means ready for pcode read / write , <nl> + * PCODE_ERROR_MASK clear means no error happened . In GVT - g we <nl> + * always emulate as pcode read / write success and ready for access <nl> + * anytime , since we don ' t touch real physical registers here . <nl> + */ <nl> + value &= ~( GEN6_PCODE_READY | GEN6_PCODE_ERROR_MASK ); <nl> return intel_vgpu_default_mmio_write ( vgpu , offset , & value , bytes ); <nl> } <nl> 
mmm fs / f2fs / f2fs . h <nl> ppp fs / f2fs / f2fs . h <nl> static inline bool f2fs_has_xattr_block ( unsigned int ofs ) <nl> return ofs == XATTR_NODE_OFFSET ; <nl> } <nl>  <nl> - static inline bool __allow_reserved_blocks ( struct f2fs_sb_info * sbi ) <nl> + static inline bool __allow_reserved_blocks ( struct f2fs_sb_info * sbi , <nl> + struct inode * inode ) <nl> { <nl> + if (! inode ) <nl> + return true ; <nl> if (! test_opt ( sbi , RESERVE_ROOT )) <nl> return false ; <nl> + if ( IS_NOQUOTA ( inode )) <nl> + return true ; <nl> if ( capable ( CAP_SYS_RESOURCE )) <nl> return true ; <nl> if ( uid_eq ( sbi -> s_resuid , current_fsuid ())) <nl> static inline int inc_valid_block_count ( struct f2fs_sb_info * sbi , <nl> avail_user_block_count = sbi -> user_block_count - <nl> sbi -> current_reserved_blocks ; <nl>  <nl> - if (! __allow_reserved_blocks ( sbi )) <nl> + if (! __allow_reserved_blocks ( sbi , inode )) <nl> avail_user_block_count -= sbi -> root_reserved_blocks ; <nl>  <nl> if ( unlikely ( sbi -> total_valid_block_count > avail_user_block_count )) { <nl> static inline int inc_valid_node_count ( struct f2fs_sb_info * sbi , <nl> valid_block_count = sbi -> total_valid_block_count + <nl> sbi -> current_reserved_blocks + 1 ; <nl>  <nl> - if (! __allow_reserved_blocks ( sbi )) <nl> + if (! __allow_reserved_blocks ( sbi , inode )) <nl> valid_block_count += sbi -> root_reserved_blocks ; <nl>  <nl> if ( unlikely ( valid_block_count > sbi -> user_block_count )) {
mmm drivers / bus / imx - weim . c <nl> ppp drivers / bus / imx - weim . c <nl> static const struct imx_weim_devtype imx51_weim_devtype = { <nl> . cs_stride = 0x18 , <nl> }; <nl>  <nl> +# define MAX_CS_REGS_COUNT 6 <nl> + <nl> static const struct of_device_id weim_id_table [] = { <nl> /* i . MX1 / 21 */ <nl> { . compatible = " fsl , imx1 - weim ", . data = & imx1_weim_devtype , }, <nl> static int __init imx_weim_gpr_setup ( struct platform_device * pdev ) <nl> static int __init weim_timing_setup ( struct device_node * np , void __iomem * base , <nl> const struct imx_weim_devtype * devtype ) <nl> { <nl> - u32 cs_idx , value [ devtype -> cs_regs_count ]; <nl> + u32 cs_idx , value [ MAX_CS_REGS_COUNT ]; <nl> int i , ret ; <nl>  <nl> + if ( WARN_ON ( devtype -> cs_regs_count > MAX_CS_REGS_COUNT )) <nl> + return - EINVAL ; <nl> + <nl> /* get the CS index from this child node ' s " reg " property . */ <nl> ret = of_property_read_u32 ( np , " reg ", & cs_idx ); <nl> if ( ret )
mmm drivers / platform / x86 / asus - laptop . c <nl> ppp drivers / platform / x86 / asus - laptop . c <nl> static int asus_laptop_get_info ( struct asus_laptop * asus ) <nl> } <nl> } <nl> asus -> name = kstrdup ( string , GFP_KERNEL ); <nl> - if (! asus -> name ) <nl> + if (! asus -> name ) { <nl> + kfree ( buffer . pointer ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> if (* string ) <nl> pr_notice (" % s model detected \ n ", string );
mmm drivers / media / rc / mceusb . c <nl> ppp drivers / media / rc / mceusb . c <nl> enum mceusb_model_type { <nl> POLARIS_EVK , <nl> CX_HYBRID_TV , <nl> MULTIFUNCTION , <nl> + TIVO_KIT , <nl> }; <nl>  <nl> struct mceusb_model { <nl> static const struct mceusb_model mceusb_model [] = { <nl> . mce_gen2 = 1 , <nl> . ir_intfnum = 2 , <nl> }, <nl> + [ TIVO_KIT ] = { <nl> + . mce_gen2 = 1 , <nl> + . rc_map = RC_MAP_TIVO , <nl> + }, <nl> }; <nl>  <nl> static struct usb_device_id mceusb_dev_table [] = { <nl> static struct usb_device_id mceusb_dev_table [] = { <nl> /* Northstar Systems , Inc . eHome Infrared Transceiver */ <nl> { USB_DEVICE ( VENDOR_NORTHSTAR , 0xe004 ) }, <nl> /* TiVo PC IR Receiver */ <nl> - { USB_DEVICE ( VENDOR_TIVO , 0x2000 ) }, <nl> + { USB_DEVICE ( VENDOR_TIVO , 0x2000 ), <nl> + . driver_info = TIVO_KIT }, <nl> /* Conexant Hybrid TV " Shelby " Polaris SDK */ <nl> { USB_DEVICE ( VENDOR_CONEXANT , 0x58a1 ), <nl> . driver_info = POLARIS_EVK },
mmm arch / cris / arch - v10 / kernel / irq . c <nl> ppp arch / cris / arch - v10 / kernel / irq . c <nl> static void end_crisv10_irq ( unsigned int irq ) <nl> } <nl>  <nl> static struct irq_chip crisv10_irq_type = { <nl> - . typename = " CRISv10 ", <nl> + . name = " CRISv10 ", <nl> . startup = startup_crisv10_irq , <nl> . shutdown = shutdown_crisv10_irq , <nl> . enable = enable_crisv10_irq ,mmm arch / cris / arch - v32 / kernel / irq . c <nl> ppp arch / cris / arch - v32 / kernel / irq . c <nl> static void end_crisv10_irq ( unsigned int irq ) <nl> } <nl>  <nl> static struct irq_chip crisv10_irq_type = { <nl> - . typename = " CRISv10 ", <nl> + . name = " CRISv10 ", <nl> . startup = startup_crisv10_irq , <nl> . shutdown = shutdown_crisv10_irq , <nl> . enable = enable_crisv10_irq , <nl> int set_affinity_crisv32_irq ( unsigned int irq , const struct cpumask * dest ) <nl> } <nl>  <nl> static struct irq_chip crisv32_irq_type = { <nl> - . typename = " CRISv32 ", <nl> + . name = " CRISv32 ", <nl> . startup = startup_crisv32_irq , <nl> . shutdown = shutdown_crisv32_irq , <nl> . enable = enable_crisv32_irq ,mmm arch / cris / kernel / irq . c <nl> ppp arch / cris / kernel / irq . c <nl> static void end_crisv10_irq ( unsigned int irq ) <nl> } <nl>  <nl> static struct irq_chip crisv10_irq_type = { <nl> - . typename = " CRISv10 ", <nl> + . name = " CRISv10 ", <nl> . startup = startup_crisv10_irq , <nl> . shutdown = shutdown_crisv10_irq , <nl> . enable = enable_crisv10_irq , <nl> int set_affinity_crisv32_irq ( unsigned int irq , const struct cpumask * dest ) <nl> } <nl>  <nl> static struct irq_chip crisv32_irq_type = { <nl> - . typename = " CRISv32 ", <nl> + . name = " CRISv32 ", <nl> . startup = startup_crisv32_irq , <nl> . shutdown = shutdown_crisv32_irq , <nl> . enable = enable_crisv32_irq , <nl> int show_interrupts ( struct seq_file * p , void * v ) <nl> for_each_online_cpu ( j ) <nl> seq_printf ( p , "% 10u ", kstat_irqs_cpu ( i , j )); <nl> # endif <nl> - seq_printf ( p , " % 14s ", irq_desc [ i ]. chip -> typename ); <nl> + seq_printf ( p , " % 14s ", irq_desc [ i ]. chip -> name ); <nl> seq_printf ( p , " % s ", action -> name ); <nl>  <nl> for ( action = action -> next ; action ; action = action -> next )
mmm arch / x86 / kvm / emulate . c <nl> ppp arch / x86 / kvm / emulate . c <nl> int x86_decode_insn ( struct x86_emulate_ctxt * ctxt , void * insn , int insn_len ) <nl> /* Decode and fetch the destination operand : register or memory . */ <nl> rc = decode_operand ( ctxt , & ctxt -> dst , ( ctxt -> d >> DstShift ) & OpMask ); <nl>  <nl> - if ( ctxt -> rip_relative ) <nl> + if ( ctxt -> rip_relative && likely ( ctxt -> memopp )) <nl> ctxt -> memopp -> addr . mem . ea = address_mask ( ctxt , <nl> ctxt -> memopp -> addr . mem . ea + ctxt -> _eip ); <nl> 
mmm kernel / trace / trace . c <nl> ppp kernel / trace / trace . c <nl> int trace_vbprintk ( unsigned long ip , const char * fmt , va_list args ) <nl> entry -> fmt = fmt ; <nl>  <nl> memcpy ( entry -> buf , trace_buf , sizeof ( u32 ) * len ); <nl> - if (! filter_check_discard ( call , entry , buffer , event )) <nl> + if (! filter_check_discard ( call , entry , buffer , event )) { <nl> ring_buffer_unlock_commit ( buffer , event ); <nl> + ftrace_trace_stack ( buffer , flags , 6 , pc ); <nl> + } <nl>  <nl> out_unlock : <nl> arch_spin_unlock (& trace_buf_lock ); <nl> int trace_array_vprintk ( struct trace_array * tr , <nl>  <nl> memcpy (& entry -> buf , trace_buf , len ); <nl> entry -> buf [ len ] = '\ 0 '; <nl> - if (! filter_check_discard ( call , entry , buffer , event )) <nl> + if (! filter_check_discard ( call , entry , buffer , event )) { <nl> ring_buffer_unlock_commit ( buffer , event ); <nl> + ftrace_trace_stack ( buffer , irq_flags , 6 , pc ); <nl> + } <nl>  <nl> out_unlock : <nl> arch_spin_unlock (& trace_buf_lock );
mmm fs / f2fs / file . c <nl> ppp fs / f2fs / file . c <nl> static int f2fs_move_file_range ( struct file * file_in , loff_t pos_in , <nl> if ( f2fs_encrypted_inode ( src ) || f2fs_encrypted_inode ( dst )) <nl> return - EOPNOTSUPP ; <nl>  <nl> + if ( src == dst ) { <nl> + if ( pos_in == pos_out ) <nl> + return 0 ; <nl> + if ( pos_out > pos_in && pos_out < pos_in + len ) <nl> + return - EINVAL ; <nl> + } <nl> + <nl> inode_lock ( src ); <nl> if ( src != dst ) { <nl> if (! inode_trylock ( dst )) {
mmm net / ipv6 / udp . c <nl> ppp net / ipv6 / udp . c <nl> static int __udp6_lib_mcast_deliver ( struct net * net , struct sk_buff * skb , <nl> uh -> source , saddr , dif ))) { <nl> struct sk_buff * buff = skb_clone ( skb , GFP_ATOMIC ); <nl> if ( buff ) { <nl> - bh_lock_sock_nested ( sk2 ); <nl> + bh_lock_sock ( sk2 ); <nl> if (! sock_owned_by_user ( sk2 )) <nl> udpv6_queue_rcv_skb ( sk2 , buff ); <nl> else <nl> static int __udp6_lib_mcast_deliver ( struct net * net , struct sk_buff * skb , <nl> bh_unlock_sock ( sk2 ); <nl> } <nl> } <nl> - bh_lock_sock_nested ( sk ); <nl> + bh_lock_sock ( sk ); <nl> if (! sock_owned_by_user ( sk )) <nl> udpv6_queue_rcv_skb ( sk , skb ); <nl> else <nl> int __udp6_lib_rcv ( struct sk_buff * skb , struct hlist_head udptable [], <nl>  <nl> /* deliver */ <nl>  <nl> - bh_lock_sock_nested ( sk ); <nl> + bh_lock_sock ( sk ); <nl> if (! sock_owned_by_user ( sk )) <nl> udpv6_queue_rcv_skb ( sk , skb ); <nl> elsemmm net / ipv4 / udp . c <nl> ppp net / ipv4 / udp . c <nl> static int __udp6_lib_mcast_deliver ( struct net * net , struct sk_buff * skb , <nl> uh -> source , saddr , dif ))) { <nl> struct sk_buff * buff = skb_clone ( skb , GFP_ATOMIC ); <nl> if ( buff ) { <nl> - bh_lock_sock_nested ( sk2 ); <nl> + bh_lock_sock ( sk2 ); <nl> if (! sock_owned_by_user ( sk2 )) <nl> udpv6_queue_rcv_skb ( sk2 , buff ); <nl> else <nl> static int __udp6_lib_mcast_deliver ( struct net * net , struct sk_buff * skb , <nl> bh_unlock_sock ( sk2 ); <nl> } <nl> } <nl> - bh_lock_sock_nested ( sk ); <nl> + bh_lock_sock ( sk ); <nl> if (! sock_owned_by_user ( sk )) <nl> udpv6_queue_rcv_skb ( sk , skb ); <nl> else <nl> int __udp6_lib_rcv ( struct sk_buff * skb , struct hlist_head udptable [], <nl>  <nl> /* deliver */ <nl>  <nl> - bh_lock_sock_nested ( sk ); <nl> + bh_lock_sock ( sk ); <nl> if (! sock_owned_by_user ( sk )) <nl> udpv6_queue_rcv_skb ( sk , skb ); <nl> else <nl> int udp_queue_rcv_skb ( struct sock * sk , struct sk_buff * skb ) <nl> up -> encap_rcv != NULL ) { <nl> int ret ; <nl>  <nl> + bh_unlock_sock ( sk ); <nl> ret = (* up -> encap_rcv )( sk , skb ); <nl> + bh_lock_sock ( sk ); <nl> if ( ret <= 0 ) { <nl> UDP_INC_STATS_BH ( sock_net ( sk ), <nl> UDP_MIB_INDATAGRAMS , <nl> static int __udp4_lib_mcast_deliver ( struct net * net , struct sk_buff * skb , <nl> if ( skb1 ) { <nl> int ret = 0 ; <nl>  <nl> - bh_lock_sock_nested ( sk ); <nl> + bh_lock_sock ( sk ); <nl> if (! sock_owned_by_user ( sk )) <nl> ret = udp_queue_rcv_skb ( sk , skb1 ); <nl> else <nl> int __udp4_lib_rcv ( struct sk_buff * skb , struct hlist_head udptable [], <nl>  <nl> if ( sk != NULL ) { <nl> int ret = 0 ; <nl> - bh_lock_sock_nested ( sk ); <nl> + bh_lock_sock ( sk ); <nl> if (! sock_owned_by_user ( sk )) <nl> ret = udp_queue_rcv_skb ( sk , skb ); <nl> else
mmm drivers / gpu / drm / i915 / intel_display . c <nl> ppp drivers / gpu / drm / i915 / intel_display . c <nl> i9xx_get_initial_plane_config ( struct intel_crtc * crtc , <nl> struct drm_framebuffer * fb ; <nl> struct intel_framebuffer * intel_fb ; <nl>  <nl> - intel_fb = kzalloc ( sizeof ( struct intel_framebuffer ), GFP_KERNEL ); <nl> + intel_fb = kzalloc ( sizeof (* intel_fb ), GFP_KERNEL ); <nl> if (! intel_fb ) { <nl> DRM_DEBUG_KMS (" failed to alloc fb \ n "); <nl> return ; <nl> skylake_get_initial_plane_config ( struct intel_crtc * crtc , <nl> struct drm_framebuffer * fb ; <nl> struct intel_framebuffer * intel_fb ; <nl>  <nl> - intel_fb = kzalloc ( sizeof ( struct intel_framebuffer ), GFP_KERNEL ); <nl> + intel_fb = kzalloc ( sizeof (* intel_fb ), GFP_KERNEL ); <nl> if (! intel_fb ) { <nl> DRM_DEBUG_KMS (" failed to alloc fb \ n "); <nl> return ; <nl> ironlake_get_initial_plane_config ( struct intel_crtc * crtc , <nl> struct drm_framebuffer * fb ; <nl> struct intel_framebuffer * intel_fb ; <nl>  <nl> - intel_fb = kzalloc ( sizeof ( struct intel_framebuffer ), GFP_KERNEL ); <nl> + intel_fb = kzalloc ( sizeof (* intel_fb ), GFP_KERNEL ); <nl> if (! intel_fb ) { <nl> DRM_DEBUG_KMS (" failed to alloc fb \ n "); <nl> return ;
mmm drivers / video / fbdev / sm501fb . c <nl> ppp drivers / video / fbdev / sm501fb . c <nl> static void sm501_free_init_fb ( struct sm501fb_info * info , <nl> { <nl> struct fb_info * fbi = info -> fb [ head ]; <nl>  <nl> + if (! fbi ) <nl> + return ; <nl> + <nl> fb_dealloc_cmap (& fbi -> cmap ); <nl> } <nl> 
mmm arch / parisc / mm / init . c <nl> ppp arch / parisc / mm / init . c <nl> static void __init setup_bootmem ( void ) <nl> } <nl> memset ( pfnnid_map , 0xff , sizeof ( pfnnid_map )); <nl>  <nl> - for ( i = 0 ; i < npmem_ranges ; i ++) <nl> + for ( i = 0 ; i < npmem_ranges ; i ++) { <nl> + node_set_state ( i , N_NORMAL_MEMORY ); <nl> node_set_online ( i ); <nl> + } <nl> # endif <nl>  <nl> /*
mmm drivers / gpu / drm / i915 / gvt / mmio_context . c <nl> ppp drivers / gpu / drm / i915 / gvt / mmio_context . c <nl> static struct engine_mmio gen8_engine_mmio_list [] __cacheline_aligned = { <nl> { BCS , RING_INSTPM ( BLT_RING_BASE ), 0xffff , false }, /* 0x220c0 */ <nl> { BCS , RING_HWSTAM ( BLT_RING_BASE ), 0x0 , false }, /* 0x22098 */ <nl> { BCS , RING_EXCC ( BLT_RING_BASE ), 0x0 , false }, /* 0x22028 */ <nl> - { /* Terminated */ } <nl> + { RCS , INVALID_MMIO_REG , 0 , false } /* Terminated */ <nl> }; <nl>  <nl> static struct engine_mmio gen9_engine_mmio_list [] __cacheline_aligned = { <nl> static struct engine_mmio gen9_engine_mmio_list [] __cacheline_aligned = { <nl> { RCS , GEN8_GARBCNTL , 0x0 , false }, /* 0xb004 */ <nl> { RCS , GEN7_FF_THREAD_MODE , 0x0 , false }, /* 0x20a0 */ <nl> { RCS , FF_SLICE_CS_CHICKEN2 , 0xffff , false }, /* 0x20e4 */ <nl> - { /* Terminated */ } <nl> + { RCS , INVALID_MMIO_REG , 0 , false } /* Terminated */ <nl> }; <nl>  <nl> static struct { <nl> static void switch_mmio ( struct intel_vgpu * pre , <nl> if ( IS_SKYLAKE ( dev_priv ) || IS_KABYLAKE ( dev_priv )) <nl> switch_mocs ( pre , next , ring_id ); <nl>  <nl> - mmio = dev_priv -> gvt -> engine_mmio_list ; <nl> - while ( i915_mmio_reg_offset (( mmio ++)-> reg )) { <nl> + for ( mmio = dev_priv -> gvt -> engine_mmio_list ; <nl> + i915_mmio_reg_valid ( mmio -> reg ); mmio ++) { <nl> if ( mmio -> ring_id != ring_id ) <nl> continue ; <nl> // save
mmm net / openvswitch / actions . c <nl> ppp net / openvswitch / actions . c <nl> static int sample ( struct datapath * dp , struct sk_buff * skb , <nl> skb_get ( skb ); <nl> } else { <nl> sample_skb = skb_clone ( skb , GFP_ATOMIC ); <nl> + if (! sample_skb ) /* Skip sample action when out of memory . */ <nl> + return 0 ; <nl> } <nl>  <nl> /* Note that do_execute_actions () never consumes skb .
mmm drivers / tty / serial / amba - pl011 . c <nl> ppp drivers / tty / serial / amba - pl011 . c <nl> static void pl011_dma_probe ( struct uart_amba_port * uap ) <nl> /* Optionally make use of an RX channel as well */ <nl> chan = dma_request_slave_channel ( dev , " rx "); <nl>  <nl> - if (! chan && plat -> dma_rx_param ) { <nl> + if (! chan && plat && plat -> dma_rx_param ) { <nl> chan = dma_request_channel ( mask , plat -> dma_filter , plat -> dma_rx_param ); <nl>  <nl> if (! chan ) {
mmm drivers / atm / eni . c <nl> ppp drivers / atm / eni . c <nl> static int eni_start ( struct atm_dev * dev ) <nl> /* initialize memory management */ <nl> buffer_mem = eni_dev -> mem - ( buf - eni_dev -> ram ); <nl> eni_dev -> free_list_size = buffer_mem / MID_MIN_BUF_SIZE / 2 ; <nl> - eni_dev -> free_list = kmalloc ( <nl> - sizeof ( struct eni_free )*( eni_dev -> free_list_size + 1 ), GFP_KERNEL ); <nl> + eni_dev -> free_list = kmalloc_array ( eni_dev -> free_list_size + 1 , <nl> + sizeof (* eni_dev -> free_list ), <nl> + GFP_KERNEL ); <nl> if (! eni_dev -> free_list ) { <nl> printk ( KERN_ERR DEV_LABEL "( itf % d ): couldn ' t get free page \ n ", <nl> dev -> number );
mmm kernel / cgroup . c <nl> ppp kernel / cgroup . c <nl> static long cgroup_create ( struct cgroup * parent , struct dentry * dentry , <nl> } <nl>  <nl> err = percpu_ref_init (& css -> refcnt , css_release ); <nl> - if ( err ) <nl> + if ( err ) { <nl> + ss -> css_free ( cgrp ); <nl> goto err_free_all ; <nl> + } <nl>  <nl> init_cgroup_css ( css , ss , cgrp ); <nl> 
mmm drivers / target / target_core_fabric_configfs . c <nl> ppp drivers / target / target_core_fabric_configfs . c <nl> static struct config_group * target_fabric_make_mappedlun ( <nl> struct se_node_acl , acl_group ); <nl> struct se_portal_group * se_tpg = se_nacl -> se_tpg ; <nl> struct target_fabric_configfs * tf = se_tpg -> se_tpg_wwn -> wwn_tf ; <nl> - struct se_lun_acl * lacl ; <nl> + struct se_lun_acl * lacl = NULL ; <nl> struct config_item * acl_ci ; <nl> struct config_group * lacl_cg = NULL , * ml_stat_grp = NULL ; <nl> char * buf ; <nl> static struct config_group * target_fabric_make_mappedlun ( <nl> out : <nl> if ( lacl_cg ) <nl> kfree ( lacl_cg -> default_groups ); <nl> + kfree ( lacl ); <nl> kfree ( buf ); <nl> return ERR_PTR ( ret ); <nl> }
mmm drivers / net / wireless / iwlwifi / dvm / mac80211 . c <nl> ppp drivers / net / wireless / iwlwifi / dvm / mac80211 . c <nl> int iwlagn_mac_setup_register ( struct iwl_priv * priv , <nl> ARRAY_SIZE ( iwlagn_iface_combinations_dualmode ); <nl> } <nl>  <nl> - hw -> wiphy -> max_remain_on_channel_duration = 1000 ; <nl> + hw -> wiphy -> max_remain_on_channel_duration = 500 ; <nl>  <nl> hw -> wiphy -> flags |= WIPHY_FLAG_CUSTOM_REGULATORY | <nl> WIPHY_FLAG_DISABLE_BEACON_HINTS |
mmm net / dsa / port . c <nl> ppp net / dsa / port . c <nl> int dsa_port_vlan_add ( struct dsa_port * dp , <nl> . vlan = vlan , <nl> }; <nl>  <nl> + if ( netif_is_bridge_master ( vlan -> obj . orig_dev )) <nl> + return - EOPNOTSUPP ; <nl> + <nl> if ( br_vlan_enabled ( dp -> bridge_dev )) <nl> return dsa_port_notify ( dp , DSA_NOTIFIER_VLAN_ADD , & info ); <nl>  <nl> int dsa_port_vlan_del ( struct dsa_port * dp , <nl> . vlan = vlan , <nl> }; <nl>  <nl> + if ( netif_is_bridge_master ( vlan -> obj . orig_dev )) <nl> + return - EOPNOTSUPP ; <nl> + <nl> if ( br_vlan_enabled ( dp -> bridge_dev )) <nl> return dsa_port_notify ( dp , DSA_NOTIFIER_VLAN_DEL , & info ); <nl> 
mmm drivers / net / ethernet / freescale / fec_main . c <nl> ppp drivers / net / ethernet / freescale / fec_main . c <nl> static void fec_enet_work ( struct work_struct * work ) <nl>  <nl> if ( fep -> delay_work . timeout ) { <nl> fep -> delay_work . timeout = false ; <nl> + rtnl_lock (); <nl> fec_restart ( fep -> netdev , fep -> full_duplex ); <nl> netif_wake_queue ( fep -> netdev ); <nl> + rtnl_unlock (); <nl> } <nl>  <nl> if ( fep -> delay_work . trig_tx ) { <nl> fec_suspend ( struct device * dev ) <nl> struct net_device * ndev = dev_get_drvdata ( dev ); <nl> struct fec_enet_private * fep = netdev_priv ( ndev ); <nl>  <nl> + rtnl_lock (); <nl> if ( netif_running ( ndev )) { <nl> phy_stop ( fep -> phy_dev ); <nl> fec_stop ( ndev ); <nl> netif_device_detach ( ndev ); <nl> } <nl> + rtnl_unlock (); <nl> + <nl> fec_enet_clk_enable ( ndev , false ); <nl> pinctrl_pm_select_sleep_state (& fep -> pdev -> dev ); <nl>  <nl> fec_resume ( struct device * dev ) <nl> if ( ret ) <nl> goto failed_clk ; <nl>  <nl> + rtnl_lock (); <nl> if ( netif_running ( ndev )) { <nl> fec_restart ( ndev , fep -> full_duplex ); <nl> netif_device_attach ( ndev ); <nl> phy_start ( fep -> phy_dev ); <nl> } <nl> + rtnl_unlock (); <nl>  <nl> return 0 ; <nl> 
mmm net / netfilter / x_tables . c <nl> ppp net / netfilter / x_tables . c <nl> xt_request_find_match ( uint8_t nfproto , const char * name , uint8_t revision ) <nl> { <nl> struct xt_match * match ; <nl>  <nl> + if ( strnlen ( name , XT_EXTENSION_MAXNAMELEN ) == XT_EXTENSION_MAXNAMELEN ) <nl> + return ERR_PTR (- EINVAL ); <nl> + <nl> match = xt_find_match ( nfproto , name , revision ); <nl> if ( IS_ERR ( match )) { <nl> request_module ("% st_ % s ", xt_prefix [ nfproto ], name ); <nl> struct xt_target * xt_request_find_target ( u8 af , const char * name , u8 revision ) <nl> { <nl> struct xt_target * target ; <nl>  <nl> + if ( strnlen ( name , XT_EXTENSION_MAXNAMELEN ) == XT_EXTENSION_MAXNAMELEN ) <nl> + return ERR_PTR (- EINVAL ); <nl> + <nl> target = xt_find_target ( af , name , revision ); <nl> if ( IS_ERR ( target )) { <nl> request_module ("% st_ % s ", xt_prefix [ af ], name );
mmm drivers / char / sx . c <nl> ppp drivers / char / sx . c <nl> static void __devexit sx_remove_card ( struct sx_board * board , <nl> del_timer (& board -> timer ); <nl> if ( pdev ) { <nl> # ifdef CONFIG_PCI <nl> - pci_iounmap ( pdev , board -> base2 ); <nl> + iounmap ( board -> base2 ); <nl> pci_release_region ( pdev , IS_CF_BOARD ( board ) ? 3 : 2 ); <nl> # endif <nl> } else { <nl> static int __devinit sx_pci_probe ( struct pci_dev * pdev , <nl> } <nl> board -> hw_base = pci_resource_start ( pdev , reg ); <nl> board -> base2 = <nl> - board -> base = pci_iomap ( pdev , reg , WINDOW_LEN ( board )); <nl> + board -> base = ioremap_nocache ( board -> hw_base , WINDOW_LEN ( board )); <nl> if (! board -> base ) { <nl> dev_err (& pdev -> dev , " ioremap failed \ n "); <nl> goto err_reg ; <nl> static int __devinit sx_pci_probe ( struct pci_dev * pdev , <nl>  <nl> return 0 ; <nl> err_unmap : <nl> - pci_iounmap ( pdev , board -> base2 ); <nl> + iounmap ( board -> base2 ); <nl> err_reg : <nl> pci_release_region ( pdev , reg ); <nl> err_flag :
mmm drivers / net / can / usb / kvaser_usb / kvaser_usb_leaf . c <nl> ppp drivers / net / can / usb / kvaser_usb / kvaser_usb_leaf . c <nl> static int kvaser_usb_leaf_simple_cmd_async ( struct kvaser_usb_net_priv * priv , <nl> struct kvaser_cmd * cmd ; <nl> int err ; <nl>  <nl> - cmd = kmalloc ( sizeof (* cmd ), GFP_ATOMIC ); <nl> + cmd = kzalloc ( sizeof (* cmd ), GFP_ATOMIC ); <nl> if (! cmd ) <nl> return - ENOMEM ; <nl>  <nl> static int kvaser_usb_leaf_set_opt_mode ( const struct kvaser_usb_net_priv * priv ) <nl> struct kvaser_cmd * cmd ; <nl> int rc ; <nl>  <nl> - cmd = kmalloc ( sizeof (* cmd ), GFP_KERNEL ); <nl> + cmd = kzalloc ( sizeof (* cmd ), GFP_KERNEL ); <nl> if (! cmd ) <nl> return - ENOMEM ; <nl>  <nl> static int kvaser_usb_leaf_flush_queue ( struct kvaser_usb_net_priv * priv ) <nl> struct kvaser_cmd * cmd ; <nl> int rc ; <nl>  <nl> - cmd = kmalloc ( sizeof (* cmd ), GFP_KERNEL ); <nl> + cmd = kzalloc ( sizeof (* cmd ), GFP_KERNEL ); <nl> if (! cmd ) <nl> return - ENOMEM ; <nl> 
mmm fs / proc / proc_misc . c <nl> ppp fs / proc / proc_misc . c <nl> static int show_stat ( struct seq_file * p , void * v ) <nl> struct timespec boottime ; <nl> unsigned int * per_irq_sum ; <nl>  <nl> - per_irq_sum = kzalloc ( sizeof ( unsigned int )* NR_IRQS , GFP_KERNEL ); <nl> + per_irq_sum = kzalloc ( sizeof ( unsigned int )* nr_irqs , GFP_KERNEL ); <nl> if (! per_irq_sum ) <nl> return - ENOMEM ; <nl>  <nl> static int show_stat ( struct seq_file * p , void * v ) <nl> softirq = cputime64_add ( softirq , kstat_cpu ( i ). cpustat . softirq ); <nl> steal = cputime64_add ( steal , kstat_cpu ( i ). cpustat . steal ); <nl> guest = cputime64_add ( guest , kstat_cpu ( i ). cpustat . guest ); <nl> - for ( j = 0 ; j < NR_IRQS ; j ++) { <nl> + for ( j = 0 ; j < nr_irqs ; j ++) { <nl> unsigned int temp = kstat_cpu ( i ). irqs [ j ]; <nl> sum += temp ; <nl> per_irq_sum [ j ] += temp ; <nl> static int show_stat ( struct seq_file * p , void * v ) <nl> } <nl> seq_printf ( p , " intr % llu ", ( unsigned long long ) sum ); <nl>  <nl> - for ( i = 0 ; i < NR_IRQS ; i ++) <nl> + for ( i = 0 ; i < nr_irqs ; i ++) <nl> seq_printf ( p , " % u ", per_irq_sum [ i ]); <nl>  <nl> seq_printf ( p , <nl> static const struct file_operations proc_stat_operations = { <nl> */ <nl> static void * int_seq_start ( struct seq_file * f , loff_t * pos ) <nl> { <nl> - return (* pos <= NR_IRQS ) ? pos : NULL ; <nl> + return (* pos <= nr_irqs ) ? pos : NULL ; <nl> } <nl>  <nl> static void * int_seq_next ( struct seq_file * f , void * v , loff_t * pos ) <nl> { <nl> (* pos )++; <nl> - if (* pos > NR_IRQS ) <nl> + if (* pos > nr_irqs ) <nl> return NULL ; <nl> return pos ; <nl> }
mmm sound / soc / intel / skylake / skl - topology . h <nl> ppp sound / soc / intel / skylake / skl - topology . h <nl> struct skl_up_down_mixer_cfg { <nl> u32 coeff_sel ; <nl> /* Pass the user coeff in this array */ <nl> s32 coeff [ UP_DOWN_MIXER_MAX_COEFF ]; <nl> + u32 ch_map ; <nl> } __packed ; <nl>  <nl> struct skl_algo_cfg {mmm sound / soc / intel / skylake / skl - messages . c <nl> ppp sound / soc / intel / skylake / skl - messages . c <nl> struct skl_up_down_mixer_cfg { <nl> u32 coeff_sel ; <nl> /* Pass the user coeff in this array */ <nl> s32 coeff [ UP_DOWN_MIXER_MAX_COEFF ]; <nl> + u32 ch_map ; <nl> } __packed ; <nl>  <nl> struct skl_algo_cfg { <nl> static void skl_set_updown_mixer_format ( struct skl_sst * ctx , <nl> skl_set_base_module_format ( ctx , mconfig , <nl> ( struct skl_base_cfg *) mixer_mconfig ); <nl> mixer_mconfig -> out_ch_cfg = fmt -> ch_cfg ; <nl> + mixer_mconfig -> ch_map = fmt -> ch_map ; <nl> } <nl>  <nl> /*
mmm include / linux / tracepoint . h <nl> ppp include / linux / tracepoint . h <nl> struct tracepoint { <nl> do { \ <nl> void ** it_func ; \ <nl> \ <nl> - rcu_read_lock_sched (); \ <nl> + rcu_read_lock_sched_notrace (); \ <nl> it_func = rcu_dereference (( tp )-> funcs ); \ <nl> if ( it_func ) { \ <nl> do { \ <nl> (( void (*)( proto ))(* it_func ))( args ); \ <nl> } while (*(++ it_func )); \ <nl> } \ <nl> - rcu_read_unlock_sched (); \ <nl> + rcu_read_unlock_sched_notrace (); \ <nl> } while ( 0 ) <nl>  <nl> /*
mmm drivers / block / floppy . c <nl> ppp drivers / block / floppy . c <nl> static int set_geometry ( unsigned int cmd , struct floppy_struct * g , <nl> int cnt ; <nl>  <nl> /* sanity checking for parameters . */ <nl> - if ( g -> sect <= 0 || <nl> - g -> head <= 0 || <nl> + if (( int ) g -> sect <= 0 || <nl> + ( int ) g -> head <= 0 || <nl> + /* check for overflow in max_sector */ <nl> + ( int )( g -> sect * g -> head ) <= 0 || <nl> /* check for zero in F_SECT_PER_TRACK */ <nl> ( unsigned char )(( g -> sect << 2 ) >> FD_SIZECODE ( g )) == 0 || <nl> g -> track <= 0 || g -> track > UDP -> tracks >> STRETCH ( g ) ||
mmm arch / cris / kernel / time . c <nl> ppp arch / cris / kernel / time . c <nl> -/* $ Id : time . c , v 1 . 18 2005 / 03 / 04 08 : 16 : 17 starvik Exp $ <nl> - * <nl> +/* <nl> * linux / arch / cris / kernel / time . c <nl> * <nl> * Copyright ( C ) 1991 , 1992 , 1995 Linus Torvalds <nl> * Linux / CRIS specific code : <nl> * <nl> * Authors : Bjorn Wesen <nl> - * Johan Adolfsson <nl> + * Johan Adolfsson <nl> * <nl> */ <nl>  <nl> cris_do_profile ( struct pt_regs * regs ) <nl> # endif <nl>  <nl> # ifdef CONFIG_PROFILING <nl> - profile_tick ( CPU_PROFILING ); <nl> + profile_tick ( CPU_PROFILING , regs ); <nl> # endif <nl> } <nl>  <nl> + unsigned long long sched_clock ( void ) <nl> +{ <nl> + return ( unsigned long long ) jiffies * ( 1000000000 / HZ ) + <nl> + get_ns_in_jiffie (); <nl> +} <nl> + <nl> static int <nl> __init init_udelay ( void ) <nl> {
mmm arch / s390 / kernel / ptrace . c <nl> ppp arch / s390 / kernel / ptrace . c <nl> static int __poke_user ( struct task_struct * child , addr_t addr , addr_t data ) <nl> unsigned long mask = PSW_MASK_USER ; <nl>  <nl> mask |= is_ri_task ( child ) ? PSW_MASK_RI : 0 ; <nl> - if (( data & ~ mask ) != PSW_USER_BITS ) <nl> + if (( data ^ PSW_USER_BITS ) & ~ mask ) <nl> + /* Invalid psw mask . */ <nl> + return - EINVAL ; <nl> + if (( data & PSW_MASK_ASC ) == PSW_ASC_HOME ) <nl> + /* Invalid address - space - control bits */ <nl> return - EINVAL ; <nl> if (( data & PSW_MASK_EA ) && !( data & PSW_MASK_BA )) <nl> + /* Invalid addressing mode bits */ <nl> return - EINVAL ; <nl> } <nl> *( addr_t *)(( addr_t ) & task_pt_regs ( child )-> psw + addr ) = data ; <nl> static int __poke_user_compat ( struct task_struct * child , <nl>  <nl> mask |= is_ri_task ( child ) ? PSW32_MASK_RI : 0 ; <nl> /* Build a 64 bit psw mask from 31 bit mask . */ <nl> - if (( tmp & ~ mask ) != PSW32_USER_BITS ) <nl> + if (( tmp ^ PSW32_USER_BITS ) & ~ mask ) <nl> /* Invalid psw mask . */ <nl> return - EINVAL ; <nl> + if (( data & PSW32_MASK_ASC ) == PSW32_ASC_HOME ) <nl> + /* Invalid address - space - control bits */ <nl> + return - EINVAL ; <nl> regs -> psw . mask = ( regs -> psw . mask & ~ PSW_MASK_USER ) | <nl> ( regs -> psw . mask & PSW_MASK_BA ) | <nl> ( __u64 )( tmp & mask ) << 32 ;
mmm sound / usb / quirks . c <nl> ppp sound / usb / quirks . c <nl> static int snd_usb_cm106_boot_quirk ( struct usb_device * dev ) <nl> */ <nl> static int snd_usb_cm6206_boot_quirk ( struct usb_device * dev ) <nl> { <nl> - int err , reg ; <nl> + int err = 0 , reg ; <nl> int val [] = { 0x2004 , 0x3000 , 0xf800 , 0x143f , 0x0000 , 0x3000 }; <nl>  <nl> for ( reg = 0 ; reg < ARRAY_SIZE ( val ); reg ++) {
mmm fs / f2fs / extent_cache . c <nl> ppp fs / f2fs / extent_cache . c <nl> static void __drop_largest_extent ( struct inode * inode , <nl> } <nl>  <nl> /* return true , if inode page is changed */ <nl> - bool f2fs_init_extent_tree ( struct inode * inode , struct f2fs_extent * i_ext ) <nl> + static bool __f2fs_init_extent_tree ( struct inode * inode , struct f2fs_extent * i_ext ) <nl> { <nl> struct f2fs_sb_info * sbi = F2FS_I_SB ( inode ); <nl> struct extent_tree * et ; <nl> bool f2fs_init_extent_tree ( struct inode * inode , struct f2fs_extent * i_ext ) <nl> return false ; <nl> } <nl>  <nl> + bool f2fs_init_extent_tree ( struct inode * inode , struct f2fs_extent * i_ext ) <nl> +{ <nl> + bool ret = __f2fs_init_extent_tree ( inode , i_ext ); <nl> + <nl> + if (! F2FS_I ( inode )-> extent_tree ) <nl> + set_inode_flag ( inode , FI_NO_EXTENT ); <nl> + <nl> + return ret ; <nl> +} <nl> + <nl> static bool f2fs_lookup_extent_tree ( struct inode * inode , pgoff_t pgofs , <nl> struct extent_info * ei ) <nl> {
mmm drivers / net / sky2 . c <nl> ppp drivers / net / sky2 . c <nl> static const char * sky2_name ( u8 chipid , char * buf , int sz ) <nl> " Optima ", /* 0xbc */ <nl> }; <nl>  <nl> - if ( chipid >= CHIP_ID_YUKON_XL && chipid < CHIP_ID_YUKON_OPT ) <nl> + if ( chipid >= CHIP_ID_YUKON_XL && chipid <= CHIP_ID_YUKON_OPT ) <nl> strncpy ( buf , name [ chipid - CHIP_ID_YUKON_XL ], sz ); <nl> else <nl> snprintf ( buf , sz , "( chip %# x )", chipid );
mmm drivers / ata / libata - core . c <nl> ppp drivers / ata / libata - core . c <nl> struct ata_host * ata_host_alloc ( struct device * dev , int max_ports ) <nl> return NULL ; <nl>  <nl> if (! devres_open_group ( dev , NULL , GFP_KERNEL )) <nl> - return NULL ; <nl> + goto err_free ; <nl>  <nl> dr = devres_alloc ( ata_devres_release , 0 , GFP_KERNEL ); <nl> if (! dr ) <nl> struct ata_host * ata_host_alloc ( struct device * dev , int max_ports ) <nl>  <nl> err_out : <nl> devres_release_group ( dev , NULL ); <nl> + err_free : <nl> + kfree ( host ); <nl> return NULL ; <nl> } <nl> 
mmm net / decnet / dn_fib . c <nl> ppp net / decnet / dn_fib . c <nl> int dn_fib_dump ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> if ( t < s_t ) <nl> continue ; <nl> if ( t > s_t ) <nl> - memset (& cb -> args [ 1 ], 0 , sizeof ( cb -> args )- sizeof ( int )); <nl> + memset (& cb -> args [ 1 ], 0 , <nl> + sizeof ( cb -> args ) - sizeof ( cb -> args [ 0 ])); <nl> tb = dn_fib_get_table ( t , 0 ); <nl> if ( tb == NULL ) <nl> continue ;
mmm net / rds / sysctl . c <nl> ppp net / rds / sysctl . c <nl> static struct ctl_table rds_sysctl_rds_table [] = { <nl> { <nl> . procname = " max_unacked_packets ", <nl> . data = & rds_sysctl_max_unacked_packets , <nl> - . maxlen = sizeof ( unsigned long ), <nl> + . maxlen = sizeof ( int ), <nl> . mode = 0644 , <nl> . proc_handler = proc_dointvec , <nl> }, <nl> { <nl> . procname = " max_unacked_bytes ", <nl> . data = & rds_sysctl_max_unacked_bytes , <nl> - . maxlen = sizeof ( unsigned long ), <nl> + . maxlen = sizeof ( int ), <nl> . mode = 0644 , <nl> . proc_handler = proc_dointvec , <nl> },
mmm drivers / net / e1000e / ich8lan . c <nl> ppp drivers / net / e1000e / ich8lan . c <nl> static s32 e1000_phy_hw_reset_ich8lan ( struct e1000_hw * hw ) <nl> u32 i ; <nl> u32 data , cnf_size , cnf_base_addr , sw_cfg_mask ; <nl> s32 ret_val ; <nl> - u16 word_addr , reg_data , reg_addr , phy_page = 0 ; <nl> + u16 reg , word_addr , reg_data , reg_addr , phy_page = 0 ; <nl>  <nl> ret_val = e1000e_phy_hw_reset_generic ( hw ); <nl> if ( ret_val ) <nl> static s32 e1000_phy_hw_reset_ich8lan ( struct e1000_hw * hw ) <nl> return ret_val ; <nl> } <nl>  <nl> + /* Dummy read to clear the phy wakeup bit after lcd reset */ <nl> + if ( hw -> mac . type == e1000_pchlan ) <nl> + e1e_rphy ( hw , BM_WUC , & reg ); <nl> + <nl> /* <nl> * Initialize the PHY from the NVM on ICH platforms . This <nl> * is needed due to an issue where the NVM configuration is <nl> static s32 e1000_get_bus_info_ich8lan ( struct e1000_hw * hw ) <nl> **/ <nl> static s32 e1000_reset_hw_ich8lan ( struct e1000_hw * hw ) <nl> { <nl> + u16 reg ; <nl> u32 ctrl , icr , kab ; <nl> s32 ret_val ; <nl>  <nl> static s32 e1000_reset_hw_ich8lan ( struct e1000_hw * hw ) <nl> hw_dbg ( hw , " Auto Read Done did not complete \ n "); <nl> } <nl> } <nl> + /* Dummy read to clear the phy wakeup bit after lcd reset */ <nl> + if ( hw -> mac . type == e1000_pchlan ) <nl> + e1e_rphy ( hw , BM_WUC , & reg ); <nl>  <nl> /* <nl> * For PCH , this write will make sure that any noise
mmm drivers / net / smc911x . c <nl> ppp drivers / net / smc911x . c <nl> static void smc911x_reset ( struct net_device * dev ) <nl> do { <nl> udelay ( 10 ); <nl> reg = SMC_GET_PMT_CTRL () & PMT_CTRL_READY_ ; <nl> - } while ( timeout -- && ! reg ); <nl> + } while (-- timeout && ! reg ); <nl> if ( timeout == 0 ) { <nl> PRINTK ("% s : smc911x_reset timeout waiting for PM restore \ n ", dev -> name ); <nl> return ; <nl> static void smc911x_reset ( struct net_device * dev ) <nl> resets ++; <nl> break ; <nl> } <nl> - } while ( timeout -- && ( reg & HW_CFG_SRST_ )); <nl> + } while (-- timeout && ( reg & HW_CFG_SRST_ )); <nl> } <nl> if ( timeout == 0 ) { <nl> PRINTK ("% s : smc911x_reset timeout waiting for reset \ n ", dev -> name ); <nl> static inline void smc911x_drop_pkt ( struct net_device * dev ) <nl> do { <nl> udelay ( 10 ); <nl> reg = SMC_GET_RX_DP_CTRL () & RX_DP_CTRL_FFWD_BUSY_ ; <nl> - } while ( timeout -- && reg ); <nl> + } while (-- timeout && reg ); <nl> if ( timeout == 0 ) { <nl> PRINTK ("% s : timeout waiting for RX fast forward \ n ", dev -> name ); <nl> }
mmm drivers / hwmon / asb100 . c <nl> ppp drivers / hwmon / asb100 . c <nl> static ssize_t set_vrm ( struct device * dev , struct device_attribute * attr , <nl> err = kstrtoul ( buf , 10 , & val ); <nl> if ( err ) <nl> return err ; <nl> + <nl> + if ( val > 255 ) <nl> + return - EINVAL ; <nl> + <nl> data -> vrm = val ; <nl> return count ; <nl> }
mmm drivers / net / wireless / mwifiex / scan . c <nl> ppp drivers / net / wireless / mwifiex / scan . c <nl> mwifiex_update_curr_bss_params ( struct mwifiex_private * priv , u8 * bssid , <nl> s32 rssi , const u8 * ie_buf , size_t ie_len , <nl> u16 beacon_period , u16 cap_info_bitmap , u8 band ) <nl> { <nl> - struct mwifiex_bssdescriptor * bss_desc = NULL ; <nl> + struct mwifiex_bssdescriptor * bss_desc ; <nl> int ret ; <nl> unsigned long flags ; <nl> u8 * beacon_ie ; <nl> mwifiex_update_curr_bss_params ( struct mwifiex_private * priv , u8 * bssid , <nl>  <nl> beacon_ie = kmemdup ( ie_buf , ie_len , GFP_KERNEL ); <nl> if (! beacon_ie ) { <nl> + kfree ( bss_desc ); <nl> dev_err ( priv -> adapter -> dev , " failed to alloc beacon_ie \ n "); <nl> return - ENOMEM ; <nl> }
mmm sound / pci / ali5451 / ali5451 . c <nl> ppp sound / pci / ali5451 / ali5451 . c <nl> snd_ali_playback_pointer ( struct snd_pcm_substream * substream ) <nl> spin_unlock (& codec -> reg_lock ); <nl> dev_dbg ( codec -> card -> dev , " playback pointer returned cso =% xh .\ n ", cso ); <nl>  <nl> + cso %= runtime -> buffer_size ; <nl> return cso ; <nl> } <nl>  <nl> static snd_pcm_uframes_t snd_ali_pointer ( struct snd_pcm_substream * substream ) <nl> cso = inw ( ALI_REG ( codec , ALI_CSO_ALPHA_FMS + 2 )); <nl> spin_unlock (& codec -> reg_lock ); <nl>  <nl> + cso %= runtime -> buffer_size ; <nl> return cso ; <nl> } <nl> 
mmm drivers / input / touchscreen / wm97xx - core . c <nl> ppp drivers / input / touchscreen / wm97xx - core . c <nl> static int wm97xx_init_pen_irq ( struct wm97xx * wm ) <nl> * provided . */ <nl> BUG_ON (! wm -> mach_ops -> irq_enable ); <nl>  <nl> - if ( request_irq ( wm -> pen_irq , wm97xx_pen_interrupt , IRQF_SHARED , <nl> + if ( request_irq ( wm -> pen_irq , wm97xx_pen_interrupt , <nl> + IRQF_SHARED | IRQF_SAMPLE_RANDOM , <nl> " wm97xx - pen ", wm )) { <nl> dev_err ( wm -> dev , <nl> " Failed to register pen down interrupt , polling ");
mmm drivers / net / wireless / marvell / mwifiex / pcie . c <nl> ppp drivers / net / wireless / marvell / mwifiex / pcie . c <nl> static int mwifiex_pcie_alloc_cmdrsp_buf ( struct mwifiex_adapter * adapter ) <nl> } <nl> skb_put ( skb , MWIFIEX_UPLD_SIZE ); <nl> if ( mwifiex_map_pci_memory ( adapter , skb , MWIFIEX_UPLD_SIZE , <nl> - PCI_DMA_FROMDEVICE )) <nl> + PCI_DMA_FROMDEVICE )) { <nl> + kfree_skb ( skb ); <nl> return - 1 ; <nl> + } <nl>  <nl> card -> cmdrsp_buf = skb ; <nl> 
mmm net / bluetooth / hci_event . c <nl> ppp net / bluetooth / hci_event . c <nl> static void hci_cc_read_local_name ( struct hci_dev * hdev , struct sk_buff * skb ) <nl> if ( rp -> status ) <nl> return ; <nl>  <nl> - memcpy ( hdev -> dev_name , rp -> name , HCI_MAX_NAME_LENGTH ); <nl> + if ( test_bit ( HCI_SETUP , & hdev -> dev_flags )) <nl> + memcpy ( hdev -> dev_name , rp -> name , HCI_MAX_NAME_LENGTH ); <nl> } <nl>  <nl> static void hci_cc_write_auth_enable ( struct hci_dev * hdev , struct sk_buff * skb )
mmm drivers / staging / pi433 / pi433_if . c <nl> ppp drivers / staging / pi433 / pi433_if . c <nl> pi433_tx_thread ( void * data ) <nl> while (( repetitions > 0 ) && ( size > position )) { <nl> if (( size - position ) > device -> free_in_fifo ) { <nl> /* msg to big for fifo - take a part */ <nl> - int temp = device -> free_in_fifo ; <nl> + int write_size = device -> free_in_fifo ; <nl> + <nl> device -> free_in_fifo = 0 ; <nl> rf69_write_fifo ( spi , <nl> & device -> buffer [ position ], <nl> - temp ); <nl> - position += temp ; <nl> + write_size ); <nl> + position += write_size ; <nl> } else { <nl> /* msg fits into fifo - take all */ <nl> device -> free_in_fifo -= size ;
mmm drivers / media / platform / s3c - camif / camif - core . c <nl> ppp drivers / media / platform / s3c - camif / camif - core . c <nl> static int camif_media_dev_init ( struct camif_dev * camif ) <nl> ip_rev == S3C6410_CAMIF_IP_REV ? " 6410 " : " 244X "); <nl> strlcpy ( md -> bus_info , " platform ", sizeof ( md -> bus_info )); <nl> md -> hw_revision = ip_rev ; <nl> - md -> driver_version = KERNEL_VERSION ( 1 , 0 , 0 ); <nl> + md -> driver_version = LINUX_VERSION_CODE ; <nl>  <nl> md -> dev = camif -> dev ; <nl> 
mmm drivers / net / wireless / ath / ath9k / ar9003_phy . c <nl> ppp drivers / net / wireless / ath / ath9k / ar9003_phy . c <nl> static int ar9003_hw_set_channel ( struct ath_hw * ah , struct ath9k_channel * chan ) <nl> u32 chan_frac ; <nl>  <nl> channelSel = ( freq * 2 ) / 75 ; <nl> - chan_frac = (( freq % 75 ) * 0x20000 ) / 75 ; <nl> + chan_frac = ((( freq * 2 ) % 75 ) * 0x20000 ) / 75 ; <nl> channelSel = ( channelSel << 17 ) | chan_frac ; <nl> } else { <nl> channelSel = CHANSEL_5G ( freq );
mmm net / socket . c <nl> ppp net / socket . c <nl> static int copy_msghdr_from_user ( struct msghdr * kmsg , <nl> { <nl> if ( copy_from_user ( kmsg , umsg , sizeof ( struct msghdr ))) <nl> return - EFAULT ; <nl> + <nl> + if ( kmsg -> msg_namelen < 0 ) <nl> + return - EINVAL ; <nl> + <nl> if ( kmsg -> msg_namelen > sizeof ( struct sockaddr_storage )) <nl> kmsg -> msg_namelen = sizeof ( struct sockaddr_storage ); <nl> return 0 ;
mmm net / packet / af_packet . c <nl> ppp net / packet / af_packet . c <nl> static int tpacket_snd ( struct packet_sock * po , struct msghdr * msg ) <nl> } <nl> tp_len = tpacket_fill_skb ( po , skb , ph , dev , size_max , proto , <nl> addr , hlen ); <nl> - if ( tp_len > dev -> mtu + dev -> hard_header_len ) { <nl> + if ( likely ( tp_len >= 0 ) && <nl> + tp_len > dev -> mtu + dev -> hard_header_len ) { <nl> struct ethhdr * ehdr ; <nl> /* Earlier code assumed this would be a VLAN pkt , <nl> * double - check this now that we have the actual
mmm drivers / pcmcia / cs_internal . h <nl> ppp drivers / pcmcia / cs_internal . h <nl> struct pccard_resource_ops { <nl> /* Flags in socket state */ <nl> # define SOCKET_PRESENT 0x0008 <nl> # define SOCKET_INUSE 0x0010 <nl> +# define SOCKET_IN_RESUME 0x0040 <nl> # define SOCKET_SUSPEND 0x0080 <nl> # define SOCKET_WIN_REQ ( i ) ( 0x0100 <<( i )) <nl> # define SOCKET_CARDBUS 0x8000mmm drivers / pcmcia / cs . c <nl> ppp drivers / pcmcia / cs . c <nl> struct pccard_resource_ops { <nl> /* Flags in socket state */ <nl> # define SOCKET_PRESENT 0x0008 <nl> # define SOCKET_INUSE 0x0010 <nl> +# define SOCKET_IN_RESUME 0x0040 <nl> # define SOCKET_SUSPEND 0x0080 <nl> # define SOCKET_WIN_REQ ( i ) ( 0x0100 <<( i )) <nl> # define SOCKET_CARDBUS 0x8000 <nl> static int socket_insert ( struct pcmcia_socket * skt ) <nl>  <nl> static int socket_suspend ( struct pcmcia_socket * skt ) <nl> { <nl> - if ( skt -> state & SOCKET_SUSPEND ) <nl> + if (( skt -> state & SOCKET_SUSPEND ) && !( skt -> state & SOCKET_IN_RESUME )) <nl> return - EBUSY ; <nl>  <nl> mutex_lock (& skt -> ops_mutex ); <nl> - skt -> suspended_state = skt -> state ; <nl> + /* store state on first suspend , but not after spurious wakeups */ <nl> + if (!( skt -> state & SOCKET_IN_RESUME )) <nl> + skt -> suspended_state = skt -> state ; <nl>  <nl> skt -> socket = dead_socket ; <nl> skt -> ops -> set_socket ( skt , & skt -> socket ); <nl> if ( skt -> ops -> suspend ) <nl> skt -> ops -> suspend ( skt ); <nl> skt -> state |= SOCKET_SUSPEND ; <nl> + skt -> state &= ~ SOCKET_IN_RESUME ; <nl> mutex_unlock (& skt -> ops_mutex ); <nl> return 0 ; <nl> } <nl> static int socket_early_resume ( struct pcmcia_socket * skt ) <nl> skt -> ops -> set_socket ( skt , & skt -> socket ); <nl> if ( skt -> state & SOCKET_PRESENT ) <nl> skt -> resume_status = socket_setup ( skt , resume_delay ); <nl> + skt -> state |= SOCKET_IN_RESUME ; <nl> mutex_unlock (& skt -> ops_mutex ); <nl> return 0 ; <nl> } <nl> static int socket_late_resume ( struct pcmcia_socket * skt ) <nl> int ret = 0 ; <nl>  <nl> mutex_lock (& skt -> ops_mutex ); <nl> - skt -> state &= ~ SOCKET_SUSPEND ; <nl> + skt -> state &= ~( SOCKET_SUSPEND | SOCKET_IN_RESUME ); <nl> mutex_unlock (& skt -> ops_mutex ); <nl>  <nl> if (!( skt -> state & SOCKET_PRESENT )) {
mmm arch / mips / include / asm / mach - tx49xx / mangle - port . h <nl> ppp arch / mips / include / asm / mach - tx49xx / mangle - port . h <nl> # define ioswabb ( a , x ) ( x ) <nl> # define __mem_ioswabb ( a , x ) ( x ) <nl> # if defined ( CONFIG_TOSHIBA_RBTX4939 ) && \ <nl> - ( defined ( CONFIG_SMC91X ) || defined ( CONFIG_SMC91X_MODULE )) && \ <nl> + IS_ENABLED ( CONFIG_SMC91X ) && \ <nl> defined ( __BIG_ENDIAN ) <nl> # define NEEDS_TXX9_IOSWABW <nl> extern u16 (* ioswabw )( volatile u16 * a , u16 x );
mmm drivers / s390 / cio / ccwreq . c <nl> ppp drivers / s390 / cio / ccwreq . c <nl> static enum io_status ccwreq_status ( struct ccw_device * cdev , struct irb * lcirb ) <nl> /* Ask the driver what to do */ <nl> if ( cdev -> drv && cdev -> drv -> uc_handler ) { <nl> todo = cdev -> drv -> uc_handler ( cdev , lcirb ); <nl> + CIO_TRACE_EVENT ( 2 , " uc_response "); <nl> + CIO_HEX_EVENT ( 2 , & todo , sizeof ( todo )); <nl> switch ( todo ) { <nl> case UC_TODO_RETRY : <nl> return IO_STATUS_ERROR ;
mmm drivers / perf / arm_pmu . c <nl> ppp drivers / perf / arm_pmu . c <nl> int arm_pmu_device_probe ( struct platform_device * pdev , <nl> ret = of_pmu_irq_cfg ( pmu ); <nl> if (! ret ) <nl> ret = init_fn ( pmu ); <nl> - } else { <nl> + } else if ( probe_table ) { <nl> cpumask_setall (& pmu -> supported_cpus ); <nl> ret = probe_current_pmu ( pmu , probe_table ); <nl> }mmm arch / arm64 / kernel / perf_event . c <nl> ppp arch / arm64 / kernel / perf_event . c <nl> int arm_pmu_device_probe ( struct platform_device * pdev , <nl> ret = of_pmu_irq_cfg ( pmu ); <nl> if (! ret ) <nl> ret = init_fn ( pmu ); <nl> - } else { <nl> + } else if ( probe_table ) { <nl> cpumask_setall (& pmu -> supported_cpus ); <nl> ret = probe_current_pmu ( pmu , probe_table ); <nl> } <nl> # include < asm / sysreg . h > <nl> # include < asm / virt . h > <nl>  <nl> +# include < linux / acpi . h > <nl> # include < linux / of . h > <nl> # include < linux / perf / arm_pmu . h > <nl> # include < linux / platform_device . h > <nl> static const struct of_device_id armv8_pmu_of_device_ids [] = { <nl> {}, <nl> }; <nl>  <nl> + static const struct pmu_probe_info armv8_pmu_probe_table [] = { <nl> + PMU_PROBE ( 0 , 0 , armv8_pmuv3_init ), /* if all else fails ... */ <nl> + { /* sentinel value */ } <nl> +}; <nl> + <nl> static int armv8_pmu_device_probe ( struct platform_device * pdev ) <nl> { <nl> - return arm_pmu_device_probe ( pdev , armv8_pmu_of_device_ids , NULL ); <nl> + if ( acpi_disabled ) <nl> + return arm_pmu_device_probe ( pdev , armv8_pmu_of_device_ids , <nl> + NULL ); <nl> + <nl> + return arm_pmu_device_probe ( pdev , armv8_pmu_of_device_ids , <nl> + armv8_pmu_probe_table ); <nl> } <nl>  <nl> static struct platform_driver armv8_pmu_driver = {
mmm drivers / spi / spi_txx9 . c <nl> ppp drivers / spi / spi_txx9 . c <nl>  <nl>  <nl> # define SPI_FIFO_SIZE 4 <nl> +# define SPI_MAX_DIVIDER 0xff /* Max . value for SPCR1 . SER */ <nl> +# define SPI_MIN_DIVIDER 1 /* Min . value for SPCR1 . SER */ <nl>  <nl> # define TXx9_SPMCR 0x00 <nl> # define TXx9_SPCR0 0x04 <nl> static void txx9spi_work_one ( struct txx9spi * c , struct spi_message * m ) <nl>  <nl> if ( prev_speed_hz != speed_hz <nl> || prev_bits_per_word != bits_per_word ) { <nl> - u32 n = ( c -> baseclk + speed_hz - 1 ) / speed_hz ; <nl> - if ( n < 1 ) <nl> - n = 1 ; <nl> - else if ( n > 0xff ) <nl> - n = 0xff ; <nl> + int n = DIV_ROUND_UP ( c -> baseclk , speed_hz ) - 1 ; <nl> + n = clamp ( n , SPI_MIN_DIVIDER , SPI_MAX_DIVIDER ); <nl> /* enter config mode */ <nl> txx9spi_wr ( c , mcr | TXx9_SPMCR_CONFIG | TXx9_SPMCR_BCLR , <nl> TXx9_SPMCR ); <nl> static int __init txx9spi_probe ( struct platform_device * dev ) <nl> goto exit ; <nl> } <nl> c -> baseclk = clk_get_rate ( c -> clk ); <nl> - c -> min_speed_hz = ( c -> baseclk + 0xff - 1 ) / 0xff ; <nl> - c -> max_speed_hz = c -> baseclk ; <nl> + c -> min_speed_hz = DIV_ROUND_UP ( c -> baseclk , SPI_MAX_DIVIDER + 1 ); <nl> + c -> max_speed_hz = c -> baseclk / ( SPI_MIN_DIVIDER + 1 ); <nl>  <nl> res = platform_get_resource ( dev , IORESOURCE_MEM , 0 ); <nl> if (! res )
mmm fs / jbd2 / journal . c <nl> ppp fs / jbd2 / journal . c <nl> static int kjournald2 ( void * arg ) <nl> goto loop ; <nl>  <nl> end_loop : <nl> - write_unlock (& journal -> j_state_lock ); <nl> del_timer_sync (& journal -> j_commit_timer ); <nl> journal -> j_task = NULL ; <nl> wake_up (& journal -> j_wait_done_commit ); <nl> jbd_debug ( 1 , " Journal thread exiting .\ n "); <nl> + write_unlock (& journal -> j_state_lock ); <nl> return 0 ; <nl> } <nl> 
mmm drivers / staging / unisys / visorbus / visorchipset . c <nl> ppp drivers / staging / unisys / visorbus / visorchipset . c <nl> static ssize_t toolaction_store ( struct device * dev , <nl> const char * buf , size_t count ) <nl> { <nl> u8 tool_action ; <nl> - int ret ; <nl> + int err ; <nl>  <nl> if ( kstrtou8 ( buf , 10 , & tool_action )) <nl> return - EINVAL ; <nl>  <nl> - ret = visorchannel_write <nl> + err = visorchannel_write <nl> ( chipset_dev -> controlvm_channel , <nl> offsetof ( struct spar_controlvm_channel_protocol , <nl> tool_action ), <nl> & tool_action , sizeof ( u8 )); <nl>  <nl> - if ( ret ) <nl> - return ret ; <nl> + if ( err ) <nl> + return err ; <nl> return count ; <nl> } <nl> static DEVICE_ATTR_RW ( toolaction );
mmm drivers / media / usb / cx231xx / cx231xx - core . c <nl> ppp drivers / media / usb / cx231xx / cx231xx - core . c <nl> int cx231xx_set_mode ( struct cx231xx * dev , enum cx231xx_mode set_mode ) <nl> } <nl> } <nl>  <nl> - return errCode ? - EINVAL : 0 ; <nl> + if ( errCode < 0 ) { <nl> + dev_err ( dev -> dev , " Failed to set devmode to % s : error : % i ", <nl> + dev -> mode == CX231XX_DIGITAL_MODE ? " digital " : " analog ", <nl> + errCode ); <nl> + return errCode ; <nl> + } <nl> + <nl> + return 0 ; <nl> } <nl> EXPORT_SYMBOL_GPL ( cx231xx_set_mode ); <nl> 
mmm sound / soc / davinci / davinci - evm . c <nl> ppp sound / soc / davinci / davinci - evm . c <nl> static struct snd_soc_dai_link da8xx_evm_dai = { <nl> . stream_name = " AIC3X ", <nl> . cpu_dai_name = " davinci - mcasp . 0 ", <nl> . codec_dai_name = " tlv320aic3x - hifi ", <nl> - . codec_name = " tlv320aic3x - codec . 0 - 001a ", <nl> + . codec_name = " tlv320aic3x - codec . 1 - 0018 ", <nl> . platform_name = " davinci - pcm - audio ", <nl> . init = evm_aic3x_init , <nl> . ops = & evm_ops ,
mmm drivers / usb / host / whci / qset . c <nl> ppp drivers / usb / host / whci / qset . c <nl> static int qset_add_urb_sg ( struct whc * whc , struct whc_qset * qset , struct urb * u <nl> || ( prev_end & ( WHCI_PAGE_SIZE - 1 )) <nl> || ( dma_addr & ( WHCI_PAGE_SIZE - 1 )) <nl> || std -> len + WHCI_PAGE_SIZE > QTD_MAX_XFER_SIZE ) { <nl> - if ( std -> len % qset -> max_packet != 0 ) <nl> + if ( std && std -> len % qset -> max_packet != 0 ) <nl> return - EINVAL ; <nl> std = qset_new_std ( whc , qset , urb , mem_flags ); <nl> if ( std == NULL ) {
mmm tools / perf / builtin - top . c <nl> ppp tools / perf / builtin - top . c <nl> static void handle_keypress ( int c ) <nl> switch ( c ) { <nl> case ' d ': <nl> prompt_integer (& delay_secs , " Enter display delay "); <nl> + if ( delay_secs < 1 ) <nl> + delay_secs = 1 ; <nl> break ; <nl> case ' e ': <nl> prompt_integer (& print_entries , " Enter display entries ( lines )");
mmm fs / f2fs / file . c <nl> ppp fs / f2fs / file . c <nl> static ssize_t f2fs_file_write_iter ( struct kiocb * iocb , struct iov_iter * from ) <nl>  <nl> ret = generic_write_checks ( iocb , from ); <nl> if ( ret > 0 ) { <nl> + bool preallocated = false ; <nl> + size_t target_size = 0 ; <nl> int err ; <nl>  <nl> if ( iov_iter_fault_in_readable ( from , iov_iter_count ( from ))) <nl> static ssize_t f2fs_file_write_iter ( struct kiocb * iocb , struct iov_iter * from ) <nl> } <nl>  <nl> } else { <nl> + preallocated = true ; <nl> + target_size = iocb -> ki_pos + iov_iter_count ( from ); <nl> + <nl> err = f2fs_preallocate_blocks ( iocb , from ); <nl> if ( err ) { <nl> clear_inode_flag ( inode , FI_NO_PREALLOC ); <nl> static ssize_t f2fs_file_write_iter ( struct kiocb * iocb , struct iov_iter * from ) <nl> blk_finish_plug (& plug ); <nl> clear_inode_flag ( inode , FI_NO_PREALLOC ); <nl>  <nl> + /* if we couldn ' t write data , we should deallocate blocks . */ <nl> + if ( preallocated && i_size_read ( inode ) < target_size ) <nl> + f2fs_truncate ( inode ); <nl> + <nl> if ( ret > 0 ) <nl> f2fs_update_iostat ( F2FS_I_SB ( inode ), APP_WRITE_IO , ret ); <nl> }
mmm kernel / srcu . c <nl> ppp kernel / srcu . c <nl> static bool srcu_readers_active_idx_check ( struct srcu_struct * sp , int idx ) <nl> */ <nl> static int srcu_readers_active ( struct srcu_struct * sp ) <nl> { <nl> - return srcu_readers_active_idx ( sp , 0 ) + srcu_readers_active_idx ( sp , 1 ); <nl> + int cpu ; <nl> + unsigned long sum = 0 ; <nl> + <nl> + for_each_possible_cpu ( cpu ) { <nl> + sum += ACCESS_ONCE ( per_cpu_ptr ( sp -> per_cpu_ref , cpu )-> c [ 0 ]); <nl> + sum += ACCESS_ONCE ( per_cpu_ptr ( sp -> per_cpu_ref , cpu )-> c [ 1 ]); <nl> + } <nl> + return sum ; <nl> } <nl>  <nl> /**
mmm tools / iio / iio_utils . c <nl> ppp tools / iio / iio_utils . c <nl> int iioutils_get_type ( unsigned * is_signed , <nl> ret = - errno ; <nl> printf (" failed to pass scan type description \ n "); <nl> goto error_close_sysfsfp ; <nl> + } else if ( ret != 5 ) { <nl> + ret = - EIO ; <nl> + printf (" scan type description didn ' t match \ n "); <nl> + goto error_close_sysfsfp ; <nl> } <nl> * be = ( endianchar == ' b '); <nl> * bytes = padint / 8 ;
mmm drivers / leds / leds - mlxcpld . c <nl> ppp drivers / leds / leds - mlxcpld . c <nl> static int __init mlxcpld_led_init ( void ) <nl> struct platform_device * pdev ; <nl> int err ; <nl>  <nl> + if (! dmi_match ( DMI_CHASSIS_VENDOR , " Mellanox Technologies Ltd .")) <nl> + return - ENODEV ; <nl> + <nl> pdev = platform_device_register_simple ( KBUILD_MODNAME , - 1 , NULL , 0 ); <nl> if ( IS_ERR ( pdev )) { <nl> pr_err (" Device allocation failed \ n "); <nl> module_exit ( mlxcpld_led_exit ); <nl>  <nl> MODULE_AUTHOR (" Vadim Pasternak < vadimp @ mellanox . com >"); <nl> MODULE_DESCRIPTION (" Mellanox board LED driver "); <nl> - MODULE_LICENSE (" GPL v2 "); <nl> + MODULE_LICENSE (" Dual BSD / GPL "); <nl> MODULE_ALIAS (" platform : leds_mlxcpld ");
mmm drivers / bluetooth / hci_intel . c <nl> ppp drivers / bluetooth / hci_intel . c <nl> static int intel_set_power ( struct hci_uart * hu , bool powered ) <nl> struct list_head * p ; <nl> int err = - ENODEV ; <nl>  <nl> + if (! hu -> tty -> dev ) <nl> + return err ; <nl> + <nl> mutex_lock (& intel_device_list_lock ); <nl>  <nl> list_for_each ( p , & intel_device_list ) { <nl> static void intel_busy_work ( struct work_struct * work ) <nl> struct intel_data * intel = container_of ( work , struct intel_data , <nl> busy_work ); <nl>  <nl> + if (! intel -> hu -> tty -> dev ) <nl> + return ; <nl> + <nl> /* Link is busy , delay the suspend */ <nl> mutex_lock (& intel_device_list_lock ); <nl> list_for_each ( p , & intel_device_list ) { <nl> static int intel_setup ( struct hci_uart * hu ) <nl> list_for_each ( p , & intel_device_list ) { <nl> struct intel_device * dev = list_entry ( p , struct intel_device , <nl> list ); <nl> + if (! hu -> tty -> dev ) <nl> + break ; <nl> if ( hu -> tty -> dev -> parent == dev -> pdev -> dev . parent ) { <nl> if ( device_may_wakeup (& dev -> pdev -> dev )) { <nl> set_bit ( STATE_LPM_ENABLED , & intel -> flags ); <nl> static int intel_enqueue ( struct hci_uart * hu , struct sk_buff * skb ) <nl>  <nl> BT_DBG (" hu % p skb % p ", hu , skb ); <nl>  <nl> + if (! hu -> tty -> dev ) <nl> + goto out_enqueue ; <nl> + <nl> /* Be sure our controller is resumed and potential LPM transaction <nl> * completed before enqueuing any packet . <nl> */ <nl> static int intel_enqueue ( struct hci_uart * hu , struct sk_buff * skb ) <nl> } <nl> } <nl> mutex_unlock (& intel_device_list_lock ); <nl> - <nl> + out_enqueue : <nl> skb_queue_tail (& intel -> txq , skb ); <nl>  <nl> return 0 ;
mmm include / linux / compiler . h <nl> ppp include / linux / compiler . h <nl> # define __releases ( x ) __attribute__ (( context ( 1 , 0 ))) <nl> # define __acquire ( x ) __context__ ( 1 ) <nl> # define __release ( x ) __context__ (- 1 ) <nl> -# define __cond_lock ( x ) (( x ) ? ({ __context__ ( 1 ); 1 ; }) : 0 ) <nl> +# define __cond_lock ( x , c ) (( c ) ? ({ __acquire ( x ); 1 ; }) : 0 ) <nl> extern void __chk_user_ptr ( void __user *); <nl> extern void __chk_io_ptr ( void __iomem *); <nl> # else <nl> extern void __chk_io_ptr ( void __iomem *); <nl> # define __releases ( x ) <nl> # define __acquire ( x ) ( void ) 0 <nl> # define __release ( x ) ( void ) 0 <nl> -# define __cond_lock ( x ) ( x ) <nl> +# define __cond_lock ( x , c ) ( c ) <nl> # endif <nl>  <nl> # ifdef __KERNEL__mmm include / linux / spinlock . h <nl> ppp include / linux / spinlock . h <nl> # define __releases ( x ) __attribute__ (( context ( 1 , 0 ))) <nl> # define __acquire ( x ) __context__ ( 1 ) <nl> # define __release ( x ) __context__ (- 1 ) <nl> -# define __cond_lock ( x ) (( x ) ? ({ __context__ ( 1 ); 1 ; }) : 0 ) <nl> +# define __cond_lock ( x , c ) (( c ) ? ({ __acquire ( x ); 1 ; }) : 0 ) <nl> extern void __chk_user_ptr ( void __user *); <nl> extern void __chk_io_ptr ( void __iomem *); <nl> # else <nl> extern void __chk_io_ptr ( void __iomem *); <nl> # define __releases ( x ) <nl> # define __acquire ( x ) ( void ) 0 <nl> # define __release ( x ) ( void ) 0 <nl> -# define __cond_lock ( x ) ( x ) <nl> +# define __cond_lock ( x , c ) ( c ) <nl> # endif <nl>  <nl> # ifdef __KERNEL__ <nl> do { \ <nl> * regardless of whether CONFIG_SMP or CONFIG_PREEMPT are set . The various <nl> * methods are defined as nops in the case they are not required . <nl> */ <nl> -# define spin_trylock ( lock ) __cond_lock ( _spin_trylock ( lock )) <nl> -# define read_trylock ( lock ) __cond_lock ( _read_trylock ( lock )) <nl> -# define write_trylock ( lock ) __cond_lock ( _write_trylock ( lock )) <nl> +# define spin_trylock ( lock ) __cond_lock ( lock , _spin_trylock ( lock )) <nl> +# define read_trylock ( lock ) __cond_lock ( lock , _read_trylock ( lock )) <nl> +# define write_trylock ( lock ) __cond_lock ( lock , _write_trylock ( lock )) <nl>  <nl> # define spin_lock ( lock ) _spin_lock ( lock ) <nl>  <nl> do { \ <nl> _write_unlock_irqrestore ( lock , flags ) <nl> # define write_unlock_bh ( lock ) _write_unlock_bh ( lock ) <nl>  <nl> -# define spin_trylock_bh ( lock ) __cond_lock ( _spin_trylock_bh ( lock )) <nl> +# define spin_trylock_bh ( lock ) __cond_lock ( lock , _spin_trylock_bh ( lock )) <nl>  <nl> # define spin_trylock_irq ( lock ) \ <nl> ({ \ <nl> do { \ <nl> */ <nl> extern int _atomic_dec_and_lock ( atomic_t * atomic , spinlock_t * lock ); <nl> # define atomic_dec_and_lock ( atomic , lock ) \ <nl> - __cond_lock ( _atomic_dec_and_lock ( atomic , lock )) <nl> + __cond_lock ( lock , _atomic_dec_and_lock ( atomic , lock )) <nl>  <nl> /** <nl> * spin_can_lock - would spin_trylock () succeed ?
mmm net / ipv4 / fib_trie . c <nl> ppp net / ipv4 / fib_trie . c <nl> static int check_leaf ( struct fib_table * tb , struct trie * t , struct leaf * l , <nl>  <nl> if ( fa -> fa_tos && fa -> fa_tos != flp -> flowi4_tos ) <nl> continue ; <nl> + if ( fi -> fib_dead ) <nl> + continue ; <nl> if ( fa -> fa_info -> fib_scope < flp -> flowi4_scope ) <nl> continue ; <nl> fib_alias_accessed ( fa );
mmm drivers / gpu / drm / i915 / intel_runtime_pm . c <nl> ppp drivers / gpu / drm / i915 / intel_runtime_pm . c <nl> static void intel_power_well_enable ( struct drm_i915_private * dev_priv , <nl> power_well -> hw_enabled = true ; <nl> } <nl>  <nl> + static void intel_power_well_disable ( struct drm_i915_private * dev_priv , <nl> + struct i915_power_well * power_well ) <nl> +{ <nl> + DRM_DEBUG_KMS (" disabling % s \ n ", power_well -> name ); <nl> + power_well -> hw_enabled = false ; <nl> + power_well -> ops -> disable ( dev_priv , power_well ); <nl> +} <nl> + <nl> /* <nl> * We should only use the power well if we explicitly asked the hardware to <nl> * enable it , so check if it ' s enabled and also check if we ' ve requested it to <nl> void intel_display_power_put ( struct drm_i915_private * dev_priv , <nl> for_each_power_well_rev ( i , power_well , BIT ( domain ), power_domains ) { <nl> WARN_ON (! power_well -> count ); <nl>  <nl> - if (!-- power_well -> count && i915 . disable_power_well ) { <nl> - DRM_DEBUG_KMS (" disabling % s \ n ", power_well -> name ); <nl> - power_well -> hw_enabled = false ; <nl> - power_well -> ops -> disable ( dev_priv , power_well ); <nl> - } <nl> + if (!-- power_well -> count && i915 . disable_power_well ) <nl> + intel_power_well_disable ( dev_priv , power_well ); <nl> } <nl>  <nl> mutex_unlock (& power_domains -> lock );
mmm drivers / staging / rtl8188eu / core / rtw_security . c <nl> ppp drivers / staging / rtl8188eu / core / rtw_security . c <nl> static int aes_cipher ( u8 * key , uint hdrlen , u8 * pframe , uint plen ) <nl> num_blocks = plen / 16 ; <nl>  <nl> /* Find start of payload */ <nl> - payload_index = ( hdrlen + 8 ); <nl> + payload_index = hdrlen + 8 ; <nl>  <nl> /* Calculate MIC */ <nl> aes128k128d ( key , mic_iv , aes_out ); <nl> static int aes_decipher ( u8 * key , uint hdrlen , <nl> num_blocks = ( plen - 8 ) / 16 ; <nl>  <nl> /* Find start of payload */ <nl> - payload_index = ( hdrlen + 8 ); <nl> + payload_index = hdrlen + 8 ; <nl>  <nl> /* Calculate MIC */ <nl> aes128k128d ( key , mic_iv , aes_out );
mmm kernel / early_res . c <nl> ppp kernel / early_res . c <nl> static void __init drop_range_partial ( int i , u64 start , u64 end ) <nl> /* make head segment */ <nl> early_res [ i ]. end = common_start ; <nl> if ( old_end > common_end ) { <nl> + char name [ 15 ]; <nl> + <nl> + /* <nl> + * Save a local copy of the name , since the <nl> + * early_res array could get resized inside <nl> + * reserve_early_without_check () -> <nl> + * __check_and_double_early_res (), which would <nl> + * make the current name pointer invalid . <nl> + */ <nl> + strncpy ( name , early_res [ i ]. name , <nl> + sizeof ( early_res [ i ]. name ) - 1 ); <nl> /* add another for left over on tail */ <nl> - reserve_early_without_check ( common_end , old_end , <nl> - early_res [ i ]. name ); <nl> + reserve_early_without_check ( common_end , old_end , name ); <nl> } <nl> return ; <nl> } else {
mmm drivers / watchdog / bcm7038_wdt . c <nl> ppp drivers / watchdog / bcm7038_wdt . c <nl> static int bcm7038_wdt_probe ( struct platform_device * pdev ) <nl> wdt -> clk = devm_clk_get ( dev , NULL ); <nl> /* If unable to get clock , use default frequency */ <nl> if (! IS_ERR ( wdt -> clk )) { <nl> - clk_prepare_enable ( wdt -> clk ); <nl> + err = clk_prepare_enable ( wdt -> clk ); <nl> + if ( err ) <nl> + return err ; <nl> wdt -> rate = clk_get_rate ( wdt -> clk ); <nl> /* Prevent divide - by - zero exception */ <nl> if (! wdt -> rate )
mmm drivers / net / wireless / broadcom / brcm80211 / brcmfmac / fweh . c <nl> ppp drivers / net / wireless / broadcom / brcm80211 / brcmfmac / fweh . c <nl> void brcmf_fweh_process_event ( struct brcmf_pub * drvr , <nl> if ( code != BRCMF_E_IF && ! fweh -> evt_handler [ code ]) <nl> return ; <nl>  <nl> - if ( datalen > BRCMF_DCMD_MAXLEN ) <nl> + if ( datalen > BRCMF_DCMD_MAXLEN || <nl> + datalen + sizeof (* event_packet ) > packet_len ) <nl> return ; <nl>  <nl> if ( in_interrupt ())
mmm drivers / tty / tty_ldisc . c <nl> ppp drivers / tty / tty_ldisc . c <nl> EXPORT_SYMBOL_GPL ( tty_ldisc_flush ); <nl> * they are not on hot paths so a little discipline won ' t do <nl> * any harm . <nl> * <nl> + * The line discipline - related tty_struct fields are reset to <nl> + * prevent the ldisc driver from re - using stale information for <nl> + * the new ldisc instance . <nl> + * <nl> * Locking : takes termios_rwsem <nl> */ <nl>  <nl> static void tty_set_termios_ldisc ( struct tty_struct * tty , int num ) <nl> down_write (& tty -> termios_rwsem ); <nl> tty -> termios . c_line = num ; <nl> up_write (& tty -> termios_rwsem ); <nl> + <nl> + tty -> disc_data = NULL ; <nl> + tty -> receive_room = 0 ; <nl> } <nl>  <nl> /**
mmm drivers / net / bonding / bond_main . c <nl> ppp drivers / net / bonding / bond_main . c <nl> static void bond_info_show_slave ( struct seq_file * seq , <nl> seq_printf ( seq , "\ nSlave Interface : % s \ n ", slave -> dev -> name ); <nl> seq_printf ( seq , " MII Status : % s \ n ", <nl> ( slave -> link == BOND_LINK_UP ) ? " up " : " down "); <nl> + seq_printf ( seq , " Speed : % d Mbps \ n ", slave -> speed ); <nl> + seq_printf ( seq , " Duplex : % s \ n ", slave -> duplex ? " full " : " half "); <nl> seq_printf ( seq , " Link Failure Count : % u \ n ", <nl> slave -> link_failure_count ); <nl> 
mmm drivers / dma / dmaengine . c <nl> ppp drivers / dma / dmaengine . c <nl> void dma_run_dependencies ( struct dma_async_tx_descriptor * tx ) <nl> if (! dep ) <nl> return ; <nl>  <nl> + /* we ' ll submit tx -> next now , so clear the link */ <nl> + tx -> next = NULL ; <nl> chan = dep -> chan ; <nl>  <nl> /* keep submitting up until a channel switch is detected
mmm arch / arm64 / mm / dma - mapping . c <nl> ppp arch / arm64 / mm / dma - mapping . c <nl> static void * __iommu_alloc_attrs ( struct device * dev , size_t size , <nl> size >> PAGE_SHIFT ); <nl> return NULL ; <nl> } <nl> - if (! coherent ) <nl> - __dma_flush_area ( page_to_virt ( page ), iosize ); <nl> - <nl> addr = dma_common_contiguous_remap ( page , size , VM_USERMAP , <nl> prot , <nl> __builtin_return_address ( 0 )); <nl> - if (! addr ) { <nl> + if ( addr ) { <nl> + memset ( addr , 0 , size ); <nl> + if (! coherent ) <nl> + __dma_flush_area ( page_to_virt ( page ), iosize ); <nl> + } else { <nl> iommu_dma_unmap_page ( dev , * handle , iosize , 0 , attrs ); <nl> dma_release_from_contiguous ( dev , page , <nl> size >> PAGE_SHIFT );
mmm drivers / vhost / net . c <nl> ppp drivers / vhost / net . c <nl> static void vhost_net_ubuf_put_and_wait ( struct vhost_net_ubuf_ref * ubufs ) <nl> { <nl> kref_put (& ubufs -> kref , vhost_net_zerocopy_done_signal ); <nl> wait_event ( ubufs -> wait , ! atomic_read (& ubufs -> kref . refcount )); <nl> +} <nl> + <nl> + static void vhost_net_ubuf_put_wait_and_free ( struct vhost_net_ubuf_ref * ubufs ) <nl> +{ <nl> + vhost_net_ubuf_put_and_wait ( ubufs ); <nl> kfree ( ubufs ); <nl> } <nl>  <nl> static long vhost_net_set_backend ( struct vhost_net * n , unsigned index , int fd ) <nl> mutex_unlock (& vq -> mutex ); <nl>  <nl> if ( oldubufs ) { <nl> - vhost_net_ubuf_put_and_wait ( oldubufs ); <nl> + vhost_net_ubuf_put_wait_and_free ( oldubufs ); <nl> mutex_lock (& vq -> mutex ); <nl> vhost_zerocopy_signal_used ( n , vq ); <nl> mutex_unlock (& vq -> mutex ); <nl> static long vhost_net_set_backend ( struct vhost_net * n , unsigned index , int fd ) <nl> rcu_assign_pointer ( vq -> private_data , oldsock ); <nl> vhost_net_enable_vq ( n , vq ); <nl> if ( ubufs ) <nl> - vhost_net_ubuf_put_and_wait ( ubufs ); <nl> + vhost_net_ubuf_put_wait_and_free ( ubufs ); <nl> err_ubufs : <nl> fput ( sock -> file ); <nl> err_vq :
mmm net / bluetooth / mgmt . c <nl> ppp net / bluetooth / mgmt . c <nl> static int send_pin_code_neg_reply ( struct sock * sk , struct hci_dev * hdev , <nl> if (! cmd ) <nl> return - ENOMEM ; <nl>  <nl> + cmd -> cmd_complete = addr_cmd_complete ; <nl> + <nl> err = hci_send_cmd ( hdev , HCI_OP_PIN_CODE_NEG_REPLY , <nl> sizeof ( cp -> addr . bdaddr ), & cp -> addr . bdaddr ); <nl> if ( err < 0 )
mmm fs / btrfs / extent - tree . c <nl> ppp fs / btrfs / extent - tree . c <nl> static int alloc_reserved_tree_block ( struct btrfs_trans_handle * trans , <nl> ret = btrfs_insert_empty_item ( trans , fs_info -> extent_root , path , <nl> ins , size ); <nl> if ( ret ) { <nl> + btrfs_free_path ( path ); <nl> btrfs_free_and_pin_reserved_extent ( root , ins -> objectid , <nl> root -> nodesize ); <nl> - btrfs_free_path ( path ); <nl> return ret ; <nl> } <nl> 
mmm fs / nfsd / nfs4state . c <nl> ppp fs / nfsd / nfs4state . c <nl> void <nl> free_session ( struct kref * kref ) <nl> { <nl> struct nfsd4_session * ses ; <nl> + int mem ; <nl>  <nl> ses = container_of ( kref , struct nfsd4_session , se_ref ); <nl> spin_lock (& nfsd_drc_lock ); <nl> - nfsd_drc_mem_used -= ses -> se_fchannel . maxreqs * NFSD_SLOT_CACHE_SIZE ; <nl> + mem = ses -> se_fchannel . maxreqs <nl> + * ( ses -> se_fchannel . maxresp_cached - NFSD_MIN_HDR_SEQ_SZ ); <nl> + nfsd_drc_mem_used -= mem ; <nl> spin_unlock (& nfsd_drc_lock ); <nl> free_session_slots ( ses ); <nl> kfree ( ses );
mmm kernel / exit . c <nl> ppp kernel / exit . c <nl> long kernel_wait4 ( pid_t upid , int __user * stat_addr , int options , <nl> __WNOTHREAD | __WCLONE | __WALL )) <nl> return - EINVAL ; <nl>  <nl> + /* - INT_MIN is not defined */ <nl> + if ( upid == INT_MIN ) <nl> + return - ESRCH ; <nl> + <nl> if ( upid == - 1 ) <nl> type = PIDTYPE_MAX ; <nl> else if ( upid < 0 ) {
mmm arch / mips / kernel / smp - cps . c <nl> ppp arch / mips / kernel / smp - cps . c <nl> static void boot_core ( unsigned core ) <nl> write_gcr_access ( access ); <nl>  <nl> if ( mips_cpc_present ()) { <nl> - /* Select the appropriate core */ <nl> - write_cpc_cl_other ( core << CPC_Cx_OTHER_CORENUM_SHF ); <nl> - <nl> /* Reset the core */ <nl> + mips_cpc_lock_other ( core ); <nl> write_cpc_co_cmd ( CPC_Cx_CMD_RESET ); <nl> + mips_cpc_unlock_other (); <nl> } else { <nl> /* Take the core out of reset */ <nl> write_gcr_co_reset_release ( 0 );
mmm net / ipv6 / inet6_connection_sock . c <nl> ppp net / ipv6 / inet6_connection_sock . c <nl> int inet6_csk_bind_conflict ( const struct sock * sk , <nl> if ( ipv6_rcv_saddr_equal ( sk , sk2 )) <nl> break ; <nl> } <nl> + if (! relax && reuse && sk2 -> sk_reuse && <nl> + sk2 -> sk_state != TCP_LISTEN && <nl> + ipv6_rcv_saddr_equal ( sk , sk2 )) <nl> + break ; <nl> } <nl> } <nl> 
mmm drivers / net / ethernet / intel / i40e / i40e_debugfs . c <nl> ppp drivers / net / ethernet / intel / i40e / i40e_debugfs . c <nl> static ssize_t i40e_dbg_command_write ( struct file * filp , <nl> if (! cmd_buf ) <nl> return count ; <nl> bytes_not_copied = copy_from_user ( cmd_buf , buffer , count ); <nl> - if ( bytes_not_copied < 0 ) <nl> + if ( bytes_not_copied < 0 ) { <nl> + kfree ( cmd_buf ); <nl> return bytes_not_copied ; <nl> + } <nl> if ( bytes_not_copied > 0 ) <nl> count -= bytes_not_copied ; <nl> cmd_buf [ count ] = '\ 0 ';
mmm include / asm - mips / unistd . h <nl> ppp include / asm - mips / unistd . h <nl> # define __NR_mknodat ( __NR_Linux + 290 ) <nl> # define __NR_fchownat ( __NR_Linux + 291 ) <nl> # define __NR_futimesat ( __NR_Linux + 292 ) <nl> -# define __NR_fstatat ( __NR_Linux + 293 ) <nl> +# define __NR_fstatat64 ( __NR_Linux + 293 ) <nl> # define __NR_unlinkat ( __NR_Linux + 294 ) <nl> # define __NR_renameat ( __NR_Linux + 295 ) <nl> # define __NR_linkat ( __NR_Linux + 296 ) <nl> # define __NR_mknodat ( __NR_Linux + 249 ) <nl> # define __NR_fchownat ( __NR_Linux + 250 ) <nl> # define __NR_futimesat ( __NR_Linux + 251 ) <nl> -# define __NR_fstatat ( __NR_Linux + 252 ) <nl> +# define __NR_newfstatat ( __NR_Linux + 252 ) <nl> # define __NR_unlinkat ( __NR_Linux + 253 ) <nl> # define __NR_renameat ( __NR_Linux + 254 ) <nl> # define __NR_linkat ( __NR_Linux + 255 ) <nl> # define __NR_mknodat ( __NR_Linux + 253 ) <nl> # define __NR_fchownat ( __NR_Linux + 254 ) <nl> # define __NR_futimesat ( __NR_Linux + 255 ) <nl> -# define __NR_fstatat ( __NR_Linux + 256 ) <nl> +# define __NR_newfstatat ( __NR_Linux + 256 ) <nl> # define __NR_unlinkat ( __NR_Linux + 257 ) <nl> # define __NR_renameat ( __NR_Linux + 258 ) <nl> # define __NR_linkat ( __NR_Linux + 259 )
mmm drivers / media / video / videobuf2 - vmalloc . c <nl> ppp drivers / media / video / videobuf2 - vmalloc . c <nl> # include < linux / vmalloc . h > <nl>  <nl> # include < media / videobuf2 - core . h > <nl> +# include < media / videobuf2 - vmalloc . h > <nl> # include < media / videobuf2 - memops . h > <nl>  <nl> struct vb2_vmalloc_buf {
mmm drivers / staging / omapdrm / omap_fb . c <nl> ppp drivers / staging / omapdrm / omap_fb . c <nl> struct drm_connector * omap_framebuffer_get_next_connector ( <nl> struct list_head * connector_list = & dev -> mode_config . connector_list ; <nl> struct drm_connector * connector = from ; <nl>  <nl> - if (! from ) { <nl> + if (! from ) <nl> return list_first_entry ( connector_list , typeof (* from ), head ); <nl> - } <nl>  <nl> list_for_each_entry_from ( connector , connector_list , head ) { <nl> if ( connector != from ) { <nl> struct drm_encoder * encoder = connector -> encoder ; <nl> struct drm_crtc * crtc = encoder ? encoder -> crtc : NULL ; <nl> - if ( crtc && crtc -> fb == fb ) { <nl> + if ( crtc && crtc -> fb == fb ) <nl> return connector ; <nl> - } <nl> + <nl> } <nl> } <nl>  <nl> struct drm_framebuffer * omap_framebuffer_init ( struct drm_device * dev , <nl> return fb ; <nl>  <nl> fail : <nl> - if ( fb ) { <nl> + if ( fb ) <nl> omap_framebuffer_destroy ( fb ); <nl> - } <nl> + <nl> return ERR_PTR ( ret ); <nl> }
mmm drivers / gpu / drm / i915 / i915_gem_stolen . c <nl> ppp drivers / gpu / drm / i915 / i915_gem_stolen . c <nl> int i915_gem_init_stolen ( struct drm_i915_private * dev_priv ) <nl>  <nl> mutex_init (& dev_priv -> mm . stolen_lock ); <nl>  <nl> + if ( intel_vgpu_active ( dev_priv )) { <nl> + DRM_INFO (" iGVT - g active , disabling use of stolen memory \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> # ifdef CONFIG_INTEL_IOMMU <nl> if ( intel_iommu_gfx_mapped && INTEL_GEN ( dev_priv ) < 8 ) { <nl> DRM_INFO (" DMAR active , disabling use of stolen memory \ n ");
mmm mm / swapfile . c <nl> ppp mm / swapfile . c <nl> int swap_type_of ( dev_t device , sector_t offset , struct block_device ** bdev_p ) <nl>  <nl> if (! bdev ) { <nl> if ( bdev_p ) <nl> - * bdev_p = bdget ( sis -> bdev -> bd_dev ); <nl> + * bdev_p = bdgrab ( sis -> bdev ); <nl>  <nl> spin_unlock (& swap_lock ); <nl> return i ; <nl> int swap_type_of ( dev_t device , sector_t offset , struct block_device ** bdev_p ) <nl> struct swap_extent , list ); <nl> if ( se -> start_block == offset ) { <nl> if ( bdev_p ) <nl> - * bdev_p = bdget ( sis -> bdev -> bd_dev ); <nl> + * bdev_p = bdgrab ( sis -> bdev ); <nl>  <nl> spin_unlock (& swap_lock ); <nl> bdput ( bdev );mmm include / linux / fs . h <nl> ppp include / linux / fs . h <nl> int swap_type_of ( dev_t device , sector_t offset , struct block_device ** bdev_p ) <nl>  <nl> if (! bdev ) { <nl> if ( bdev_p ) <nl> - * bdev_p = bdget ( sis -> bdev -> bd_dev ); <nl> + * bdev_p = bdgrab ( sis -> bdev ); <nl>  <nl> spin_unlock (& swap_lock ); <nl> return i ; <nl> int swap_type_of ( dev_t device , sector_t offset , struct block_device ** bdev_p ) <nl> struct swap_extent , list ); <nl> if ( se -> start_block == offset ) { <nl> if ( bdev_p ) <nl> - * bdev_p = bdget ( sis -> bdev -> bd_dev ); <nl> + * bdev_p = bdgrab ( sis -> bdev ); <nl>  <nl> spin_unlock (& swap_lock ); <nl> bdput ( bdev ); <nl> extern void putname ( const char * name ); <nl> extern int register_blkdev ( unsigned int , const char *); <nl> extern void unregister_blkdev ( unsigned int , const char *); <nl> extern struct block_device * bdget ( dev_t ); <nl> + extern struct block_device * bdgrab ( struct block_device * bdev ); <nl> extern void bd_set_size ( struct block_device *, loff_t size ); <nl> extern void bd_forget ( struct inode * inode ); <nl> extern void bdput ( struct block_device *);mmm fs / block_dev . c <nl> ppp fs / block_dev . c <nl> int swap_type_of ( dev_t device , sector_t offset , struct block_device ** bdev_p ) <nl>  <nl> if (! bdev ) { <nl> if ( bdev_p ) <nl> - * bdev_p = bdget ( sis -> bdev -> bd_dev ); <nl> + * bdev_p = bdgrab ( sis -> bdev ); <nl>  <nl> spin_unlock (& swap_lock ); <nl> return i ; <nl> int swap_type_of ( dev_t device , sector_t offset , struct block_device ** bdev_p ) <nl> struct swap_extent , list ); <nl> if ( se -> start_block == offset ) { <nl> if ( bdev_p ) <nl> - * bdev_p = bdget ( sis -> bdev -> bd_dev ); <nl> + * bdev_p = bdgrab ( sis -> bdev ); <nl>  <nl> spin_unlock (& swap_lock ); <nl> bdput ( bdev ); <nl> extern void putname ( const char * name ); <nl> extern int register_blkdev ( unsigned int , const char *); <nl> extern void unregister_blkdev ( unsigned int , const char *); <nl> extern struct block_device * bdget ( dev_t ); <nl> + extern struct block_device * bdgrab ( struct block_device * bdev ); <nl> extern void bd_set_size ( struct block_device *, loff_t size ); <nl> extern void bd_forget ( struct inode * inode ); <nl> extern void bdput ( struct block_device *); <nl> struct block_device * bdget ( dev_t dev ) <nl>  <nl> EXPORT_SYMBOL ( bdget ); <nl>  <nl> +/** <nl> + * bdgrab -- Grab a reference to an already referenced block device <nl> + * @ bdev : Block device to grab a reference to . <nl> + */ <nl> + struct block_device * bdgrab ( struct block_device * bdev ) <nl> +{ <nl> + atomic_inc (& bdev -> bd_inode -> i_count ); <nl> + return bdev ; <nl> +} <nl> + <nl> long nr_blockdev_pages ( void ) <nl> { <nl> struct block_device * bdev ;
mmm drivers / net / wireless / ath / ath10k / htt_tx . c <nl> ppp drivers / net / wireless / ath / ath10k / htt_tx . c <nl> void ath10k_htt_tx_free ( struct ath10k_htt * htt ) <nl> { <nl> int size ; <nl>  <nl> + tasklet_kill (& htt -> txrx_compl_task ); <nl> + <nl> idr_for_each (& htt -> pending_tx , ath10k_htt_tx_clean_up_pending , htt -> ar ); <nl> idr_destroy (& htt -> pending_tx ); <nl> 
mmm drivers / dma / amba - pl08x . c <nl> ppp drivers / dma / amba - pl08x . c <nl> static int pl08x_probe ( struct amba_device * adev , const struct amba_id * id ) <nl> if ( ret ) <nl> return ret ; <nl>  <nl> + /* Ensure that we can do DMA */ <nl> + ret = dma_set_mask_and_coherent (& adev -> dev , DMA_BIT_MASK ( 32 )); <nl> + if ( ret ) <nl> + goto out_no_pl08x ; <nl> + <nl> /* Create the driver state holder */ <nl> pl08x = kzalloc ( sizeof (* pl08x ), GFP_KERNEL ); <nl> if (! pl08x ) {
mmm net / bridge / br_fdb . c <nl> ppp net / bridge / br_fdb . c <nl> static int fdb_insert ( struct net_bridge * br , struct net_bridge_port * source , <nl> */ <nl> if ( fdb -> is_local ) <nl> return 0 ; <nl> - br_warn ( br , " adding interface % s with same address " <nl> - " as a received packet \ n ", <nl> - source ? source -> dev -> name : br -> dev -> name ); <nl> + br_warn ( br , " adding interface % s with same address as a received packet ( addr :% pM , vlan :% u )\ n ", <nl> + source ? source -> dev -> name : br -> dev -> name , addr , vid ); <nl> fdb_delete ( br , fdb ); <nl> } <nl>  <nl> void br_fdb_update ( struct net_bridge * br , struct net_bridge_port * source , <nl> /* attempt to update an entry for a local interface */ <nl> if ( unlikely ( fdb -> is_local )) { <nl> if ( net_ratelimit ()) <nl> - br_warn ( br , " received packet on % s with " <nl> - " own address as source address \ n ", <nl> - source -> dev -> name ); <nl> + br_warn ( br , " received packet on % s with own address as source address ( addr :% pM , vlan :% u )\ n ", <nl> + source -> dev -> name , addr , vid ); <nl> } else { <nl> /* fastpath : update of existing entry */ <nl> if ( unlikely ( source != fdb -> dst )) {
mmm include / linux / skbuff . h <nl> ppp include / linux / skbuff . h <nl> struct sk_buff { <nl> __u32 reserved_tailroom ; <nl> }; <nl>  <nl> + kmemcheck_bitfield_begin ( flags3 ); <nl> + /* 16 bit hole */ <nl> + kmemcheck_bitfield_end ( flags3 ); <nl> + <nl> __be16 inner_protocol ; <nl> __u16 inner_transport_header ; <nl> __u16 inner_network_header ;
mmm tools / perf / util / evsel . h <nl> ppp tools / perf / util / evsel . h <nl> int perf_evsel__fprintf_callchain ( struct perf_evsel * evsel , <nl>  <nl> int perf_evsel__fprintf_sym ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> struct addr_location * al , int left_alignment , <nl> - unsigned int print_opts , unsigned int stack_depth , <nl> - FILE * fp ); <nl> + unsigned int print_opts , bool print_callchain , <nl> + unsigned int stack_depth , FILE * fp ); <nl>  <nl> bool perf_evsel__fallback ( struct perf_evsel * evsel , int err , <nl> char * msg , size_t msgsize );mmm tools / perf / util / evsel . c <nl> ppp tools / perf / util / evsel . c <nl> int perf_evsel__fprintf_callchain ( struct perf_evsel * evsel , <nl>  <nl> int perf_evsel__fprintf_sym ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> struct addr_location * al , int left_alignment , <nl> - unsigned int print_opts , unsigned int stack_depth , <nl> - FILE * fp ); <nl> + unsigned int print_opts , bool print_callchain , <nl> + unsigned int stack_depth , FILE * fp ); <nl>  <nl> bool perf_evsel__fallback ( struct perf_evsel * evsel , int err , <nl> char * msg , size_t msgsize ); <nl> int perf_evsel__fprintf_callchain ( struct perf_evsel * evsel , struct perf_sample * <nl>  <nl> int perf_evsel__fprintf_sym ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> struct addr_location * al , int left_alignment , <nl> - unsigned int print_opts , unsigned int stack_depth , <nl> - FILE * fp ) <nl> + unsigned int print_opts , bool print_callchain , <nl> + unsigned int stack_depth , FILE * fp ) <nl> { <nl> int printed = 0 ; <nl> int print_ip = print_opts & EVSEL__PRINT_IP ; <nl> int perf_evsel__fprintf_sym ( struct perf_evsel * evsel , struct perf_sample * sample <nl> int print_srcline = print_opts & EVSEL__PRINT_SRCLINE ; <nl> int print_unknown_as_addr = print_opts & EVSEL__PRINT_UNKNOWN_AS_ADDR ; <nl>  <nl> - if ( symbol_conf . use_callchain && sample -> callchain ) { <nl> + if ( print_callchain && sample -> callchain ) { <nl> printed += perf_evsel__fprintf_callchain ( evsel , sample , al , left_alignment , <nl> print_opts , stack_depth , fp ); <nl> } else if (!( al -> sym && al -> sym -> ignore )) {mmm tools / perf / builtin - script . c <nl> ppp tools / perf / builtin - script . c <nl> int perf_evsel__fprintf_callchain ( struct perf_evsel * evsel , <nl>  <nl> int perf_evsel__fprintf_sym ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> struct addr_location * al , int left_alignment , <nl> - unsigned int print_opts , unsigned int stack_depth , <nl> - FILE * fp ); <nl> + unsigned int print_opts , bool print_callchain , <nl> + unsigned int stack_depth , FILE * fp ); <nl>  <nl> bool perf_evsel__fallback ( struct perf_evsel * evsel , int err , <nl> char * msg , size_t msgsize ); <nl> int perf_evsel__fprintf_callchain ( struct perf_evsel * evsel , struct perf_sample * <nl>  <nl> int perf_evsel__fprintf_sym ( struct perf_evsel * evsel , struct perf_sample * sample , <nl> struct addr_location * al , int left_alignment , <nl> - unsigned int print_opts , unsigned int stack_depth , <nl> - FILE * fp ) <nl> + unsigned int print_opts , bool print_callchain , <nl> + unsigned int stack_depth , FILE * fp ) <nl> { <nl> int printed = 0 ; <nl> int print_ip = print_opts & EVSEL__PRINT_IP ; <nl> int perf_evsel__fprintf_sym ( struct perf_evsel * evsel , struct perf_sample * sample <nl> int print_srcline = print_opts & EVSEL__PRINT_SRCLINE ; <nl> int print_unknown_as_addr = print_opts & EVSEL__PRINT_UNKNOWN_AS_ADDR ; <nl>  <nl> - if ( symbol_conf . use_callchain && sample -> callchain ) { <nl> + if ( print_callchain && sample -> callchain ) { <nl> printed += perf_evsel__fprintf_callchain ( evsel , sample , al , left_alignment , <nl> print_opts , stack_depth , fp ); <nl> } else if (!( al -> sym && al -> sym -> ignore )) { <nl> static void print_sample_bts ( struct perf_sample * sample , <nl> } <nl> } <nl> perf_evsel__fprintf_sym ( evsel , sample , al , 0 , print_opts , <nl> + symbol_conf . use_callchain , <nl> scripting_max_stack , stdout ); <nl> } <nl>  <nl> static void process_event ( struct perf_script * script , <nl>  <nl> perf_evsel__fprintf_sym ( evsel , sample , al , 0 , <nl> output [ attr -> type ]. print_ip_opts , <nl> + symbol_conf . use_callchain , <nl> scripting_max_stack , stdout ); <nl> } <nl> 
mmm drivers / net / gianfar . c <nl> ppp drivers / net / gianfar . c <nl> static int gfar_of_init ( struct of_device * ofdev , struct net_device ** pdev ) <nl> priv -> rx_queue [ i ] = NULL ; <nl>  <nl> for ( i = 0 ; i < priv -> num_tx_queues ; i ++) { <nl> - priv -> tx_queue [ i ] = ( struct gfar_priv_tx_q *) kzalloc ( <nl> - sizeof ( struct gfar_priv_tx_q ), GFP_KERNEL ); <nl> + priv -> tx_queue [ i ] = kzalloc ( sizeof ( struct gfar_priv_tx_q ), <nl> + GFP_KERNEL ); <nl> if (! priv -> tx_queue [ i ]) { <nl> err = - ENOMEM ; <nl> goto tx_alloc_failed ; <nl> static int gfar_of_init ( struct of_device * ofdev , struct net_device ** pdev ) <nl> } <nl>  <nl> for ( i = 0 ; i < priv -> num_rx_queues ; i ++) { <nl> - priv -> rx_queue [ i ] = ( struct gfar_priv_rx_q *) kzalloc ( <nl> - sizeof ( struct gfar_priv_rx_q ), GFP_KERNEL ); <nl> + priv -> rx_queue [ i ] = kzalloc ( sizeof ( struct gfar_priv_rx_q ), <nl> + GFP_KERNEL ); <nl> if (! priv -> rx_queue [ i ]) { <nl> err = - ENOMEM ; <nl> goto rx_alloc_failed ;
mmm lib / dma - direct . c <nl> ppp lib / dma - direct . c <nl> void * dma_direct_alloc ( struct device * dev , size_t size , dma_addr_t * dma_handle , <nl> __free_pages ( page , page_order ); <nl> page = NULL ; <nl>  <nl> + if ( IS_ENABLED ( CONFIG_ZONE_DMA32 ) && <nl> + dev -> coherent_dma_mask < DMA_BIT_MASK ( 64 ) && <nl> + !( gfp & ( GFP_DMA32 | GFP_DMA ))) { <nl> + gfp |= GFP_DMA32 ; <nl> + goto again ; <nl> + } <nl> + <nl> if ( IS_ENABLED ( CONFIG_ZONE_DMA ) && <nl> dev -> coherent_dma_mask < DMA_BIT_MASK ( 32 ) && <nl> !( gfp & GFP_DMA )) {
mmm drivers / i2c / i2c - core . c <nl> ppp drivers / i2c / i2c - core . c <nl> int i2c_attach_client ( struct i2c_client * client ) <nl> if ( client -> driver ) <nl> client -> dev . driver = & client -> driver -> driver ; <nl>  <nl> - if ( client -> driver && ! is_newstyle_driver ( client -> driver )) <nl> + if ( client -> driver && ! is_newstyle_driver ( client -> driver )) { <nl> client -> dev . release = i2c_client_release ; <nl> - else <nl> + client -> dev . uevent_suppress = 1 ; <nl> + } else <nl> client -> dev . release = i2c_client_dev_release ; <nl>  <nl> snprintf (& client -> dev . bus_id [ 0 ], sizeof ( client -> dev . bus_id ),
mmm kernel / exit . c <nl> ppp kernel / exit . c <nl> void mm_update_next_owner ( struct mm_struct * mm ) <nl> /* <nl> * Search in the siblings <nl> */ <nl> - list_for_each_entry ( c , & p -> parent -> children , sibling ) { <nl> + list_for_each_entry ( c , & p -> real_parent -> children , sibling ) { <nl> if ( c -> mm == mm ) <nl> goto assign_new_owner ; <nl> }
mmm drivers / target / target_core_spc . c <nl> ppp drivers / target / target_core_spc . c <nl> static int spc_emulate_inquiry ( struct se_cmd * cmd ) <nl> unsigned char buf [ SE_INQUIRY_BUF ]; <nl> int p , ret ; <nl>  <nl> + memset ( buf , 0 , SE_INQUIRY_BUF ); <nl> + <nl> if ( dev == tpg -> tpg_virt_lun0 . lun_se_dev ) <nl> buf [ 0 ] = 0x3f ; /* Not connected */ <nl> else
mmm arch / arm / mm / dma - mapping . c <nl> ppp arch / arm / mm / dma - mapping . c <nl> static void __dma_page_dev_to_cpu ( struct page * page , unsigned long off , <nl> unsigned long paddr = page_to_phys ( page ) + off ; <nl>  <nl> /* FIXME : non - speculating : not required */ <nl> - /* don ' t bother invalidating if DMA to device */ <nl> - if ( dir != DMA_TO_DEVICE ) <nl> + /* in any case , don ' t bother invalidating if DMA to device */ <nl> + if ( dir != DMA_TO_DEVICE ) { <nl> outer_inv_range ( paddr , paddr + size ); <nl>  <nl> - dma_cache_maint_page ( page , off , size , dir , dmac_unmap_area ); <nl> + dma_cache_maint_page ( page , off , size , dir , dmac_unmap_area ); <nl> + } <nl>  <nl> /* <nl> * Mark the D - cache clean for these pages to avoid extra flushing .
mmm sound / core / pcm_lib . c <nl> ppp sound / core / pcm_lib . c <nl> static int snd_pcm_update_hw_ptr_interrupt ( struct snd_pcm_substream * substream ) <nl> new_hw_ptr = hw_base + pos ; <nl> hw_ptr_interrupt = runtime -> hw_ptr_interrupt + runtime -> period_size ; <nl> delta = new_hw_ptr - hw_ptr_interrupt ; <nl> - if ( hw_ptr_interrupt == runtime -> boundary ) <nl> - hw_ptr_interrupt = 0 ; <nl> + if ( hw_ptr_interrupt >= runtime -> boundary ) { <nl> + hw_ptr_interrupt %= runtime -> boundary ; <nl> + if (! hw_base ) /* hw_base was already lapped ; recalc delta */ <nl> + delta = new_hw_ptr - hw_ptr_interrupt ; <nl> + } <nl> if ( delta < 0 ) { <nl> delta += runtime -> buffer_size ; <nl> if ( delta < 0 ) { <nl> static int snd_pcm_update_hw_ptr_interrupt ( struct snd_pcm_substream * substream ) <nl> ( long ) hw_ptr_interrupt ); <nl> /* rebase to interrupt position */ <nl> hw_base = new_hw_ptr = hw_ptr_interrupt ; <nl> + /* align hw_base to buffer_size */ <nl> + hw_base -= hw_base % runtime -> buffer_size ; <nl> delta = 0 ; <nl> } else { <nl> hw_base += runtime -> buffer_size ;
mmm drivers / net / wireless / broadcom / brcm80211 / brcmfmac / cfg80211 . c <nl> ppp drivers / net / wireless / broadcom / brcm80211 / brcmfmac / cfg80211 . c <nl> brcmf_cfg80211_start_ap ( struct wiphy * wiphy , struct net_device * ndev , <nl> ( u8 *)& settings -> beacon . head [ ie_offset ], <nl> settings -> beacon . head_len - ie_offset , <nl> WLAN_EID_SSID ); <nl> - if (! ssid_ie ) <nl> + if (! ssid_ie || ssid_ie -> len > IEEE80211_MAX_SSID_LEN ) <nl> return - EINVAL ; <nl>  <nl> memcpy ( ssid_le . SSID , ssid_ie -> data , ssid_ie -> len );
mmm drivers / ata / ata_piix . c <nl> ppp drivers / ata / ata_piix . c <nl> static void do_pata_set_dmamode ( struct ata_port * ap , struct ata_device * adev , i <nl> u16 master_data ; <nl> u8 speed = adev -> dma_mode ; <nl> int devid = adev -> devno + 2 * ap -> port_no ; <nl> - u8 udma_enable ; <nl> + u8 udma_enable = 0 ; <nl>  <nl> static const /* ISP RTC */ <nl> u8 timings [][ 2 ] = { { 0 , 0 },
mmm drivers / md / md . c <nl> ppp drivers / md / md . c <nl> static void md_clean ( mddev_t * mddev ) <nl> mddev -> plug = NULL ; <nl> } <nl>  <nl> - void md_stop_writes ( mddev_t * mddev ) <nl> + static void __md_stop_writes ( mddev_t * mddev ) <nl> { <nl> if ( mddev -> sync_thread ) { <nl> set_bit ( MD_RECOVERY_FROZEN , & mddev -> recovery ); <nl> void md_stop_writes ( mddev_t * mddev ) <nl> md_update_sb ( mddev , 1 ); <nl> } <nl> } <nl> + <nl> + void md_stop_writes ( mddev_t * mddev ) <nl> +{ <nl> + mddev_lock ( mddev ); <nl> + __md_stop_writes ( mddev ); <nl> + mddev_unlock ( mddev ); <nl> +} <nl> EXPORT_SYMBOL_GPL ( md_stop_writes ); <nl>  <nl> void md_stop ( mddev_t * mddev ) <nl> static int md_set_readonly ( mddev_t * mddev , int is_open ) <nl> goto out ; <nl> } <nl> if ( mddev -> pers ) { <nl> - md_stop_writes ( mddev ); <nl> + __md_stop_writes ( mddev ); <nl>  <nl> err = - ENXIO ; <nl> if ( mddev -> ro == 1 ) <nl> static int do_md_stop ( mddev_t * mddev , int mode , int is_open ) <nl> if ( mddev -> ro ) <nl> set_disk_ro ( disk , 0 ); <nl>  <nl> - md_stop_writes ( mddev ); <nl> + __md_stop_writes ( mddev ); <nl> md_stop ( mddev ); <nl> mddev -> queue -> merge_bvec_fn = NULL ; <nl> mddev -> queue -> unplug_fn = NULL ;
mmm drivers / ieee1394 / ieee1394_transactions . c <nl> ppp drivers / ieee1394 / ieee1394_transactions . c <nl> # ifndef HPSB_DEBUG_TLABELS <nl> static <nl> # endif <nl> - spinlock_t hpsb_tlabel_lock = SPIN_LOCK_UNLOCKED ; <nl> + DEFINE_SPINLOCK ( hpsb_tlabel_lock ); <nl>  <nl> static DECLARE_WAIT_QUEUE_HEAD ( tlabel_wq ); <nl> 
mmm fs / gfs2 / super . c <nl> ppp fs / gfs2 / super . c <nl> static int gfs2_lock_fs_check_clean ( struct gfs2_sbd * sdp , <nl> struct gfs2_holder * t_gh ) <nl> { <nl> struct gfs2_inode * ip ; <nl> - struct gfs2_holder ji_gh ; <nl> struct gfs2_jdesc * jd ; <nl> struct lfcc * lfcc ; <nl> LIST_HEAD ( list ); <nl> static int gfs2_lock_fs_check_clean ( struct gfs2_sbd * sdp , <nl> gfs2_glock_dq_uninit (& lfcc -> gh ); <nl> kfree ( lfcc ); <nl> } <nl> - gfs2_glock_dq_uninit (& ji_gh ); <nl> return error ; <nl> } <nl> 
mmm drivers / net / bnx2x_main . c <nl> ppp drivers / net / bnx2x_main . c <nl> static int __devinit bnx2x_init_one ( struct pci_dev * pdev , <nl> bp = netdev_priv ( dev ); <nl> bp -> msglevel = debug ; <nl>  <nl> + pci_set_drvdata ( pdev , dev ); <nl> + <nl> rc = bnx2x_init_dev ( pdev , dev ); <nl> if ( rc < 0 ) { <nl> free_netdev ( dev ); <nl> return rc ; <nl> } <nl>  <nl> - pci_set_drvdata ( pdev , dev ); <nl> - <nl> rc = bnx2x_init_bp ( bp ); <nl> if ( rc ) <nl> goto init_one_exit ;
mmm drivers / usb / storage / shuttle_usbat . c <nl> ppp drivers / usb / storage / shuttle_usbat . c <nl> static int usbat_probe ( struct usb_interface * intf , <nl> us -> transport_name = " Shuttle USBAT "; <nl> us -> transport = usbat_flash_transport ; <nl> us -> transport_reset = usb_stor_CB_reset ; <nl> - us -> max_lun = 1 ; <nl> + us -> max_lun = 0 ; <nl>  <nl> result = usb_stor_probe2 ( us ); <nl> return result ;
mmm drivers / infiniband / hw / hns / hns_roce_main . c <nl> ppp drivers / infiniband / hw / hns / hns_roce_main . c <nl> static struct ib_ucontext * hns_roce_alloc_ucontext ( struct ib_device * ib_dev , <nl> { <nl> int ret = 0 ; <nl> struct hns_roce_ucontext * context ; <nl> - struct hns_roce_ib_alloc_ucontext_resp resp ; <nl> + struct hns_roce_ib_alloc_ucontext_resp resp = {}; <nl> struct hns_roce_dev * hr_dev = to_hr_dev ( ib_dev ); <nl>  <nl> resp . qp_tab_size = hr_dev -> caps . num_qps ;
mmm net / sctp / socket . c <nl> ppp net / sctp / socket . c <nl> int sctp_do_peeloff ( struct sock * sk , sctp_assoc_t id , struct socket ** sockp ) <nl> struct socket * sock ; <nl> int err = 0 ; <nl>  <nl> + /* Do not peel off from one netns to another one . */ <nl> + if (! net_eq ( current -> nsproxy -> net_ns , sock_net ( sk ))) <nl> + return - EINVAL ; <nl> + <nl> if (! asoc ) <nl> return - EINVAL ; <nl> 
mmm arch / s390 / include / asm / mmu . h <nl> ppp arch / s390 / include / asm / mmu . h <nl> typedef struct { <nl> unsigned long asce ; <nl> unsigned long asce_limit ; <nl> unsigned long vdso_base ; <nl> - /* The mmu context allocates 4K page tables . */ <nl> + /* <nl> + * The following bitfields need a down_write on the mm <nl> + * semaphore when they are written to . As they are only <nl> + * written once , they can be read without a lock . <nl> + * <nl> + * The mmu context allocates 4K page tables . <nl> + */ <nl> unsigned int alloc_pgste : 1 ; <nl> /* The mmu context uses extended page tables . */ <nl> unsigned int has_pgste : 1 ;mmm arch / s390 / kvm / kvm - s390 . c <nl> ppp arch / s390 / kvm / kvm - s390 . c <nl> typedef struct { <nl> unsigned long asce ; <nl> unsigned long asce_limit ; <nl> unsigned long vdso_base ; <nl> - /* The mmu context allocates 4K page tables . */ <nl> + /* <nl> + * The following bitfields need a down_write on the mm <nl> + * semaphore when they are written to . As they are only <nl> + * written once , they can be read without a lock . <nl> + * <nl> + * The mmu context allocates 4K page tables . <nl> + */ <nl> unsigned int alloc_pgste : 1 ; <nl> /* The mmu context uses extended page tables . */ <nl> unsigned int has_pgste : 1 ; <nl> static int kvm_vm_ioctl_enable_cap ( struct kvm * kvm , struct kvm_enable_cap * cap ) <nl> r = - EINVAL ; <nl> else { <nl> r = 0 ; <nl> + down_write (& kvm -> mm -> mmap_sem ); <nl> kvm -> mm -> context . allow_gmap_hpage_1m = 1 ; <nl> + up_write (& kvm -> mm -> mmap_sem ); <nl> /* <nl> * We might have to create fake 4k page <nl> * tables . To avoid that the hardware works on
mmm drivers / usb / gadget / function / u_audio . c <nl> ppp drivers / usb / gadget / function / u_audio . c <nl> int g_audio_setup ( struct g_audio * g_audio , const char * pcm_name , <nl> if ( err < 0 ) <nl> goto snd_fail ; <nl>  <nl> - strcpy ( pcm -> name , pcm_name ); <nl> + strlcpy ( pcm -> name , pcm_name , sizeof ( pcm -> name )); <nl> pcm -> private_data = uac ; <nl> uac -> pcm = pcm ; <nl>  <nl> snd_pcm_set_ops ( pcm , SNDRV_PCM_STREAM_PLAYBACK , & uac_pcm_ops ); <nl> snd_pcm_set_ops ( pcm , SNDRV_PCM_STREAM_CAPTURE , & uac_pcm_ops ); <nl>  <nl> - strcpy ( card -> driver , card_name ); <nl> - strcpy ( card -> shortname , card_name ); <nl> + strlcpy ( card -> driver , card_name , sizeof ( card -> driver )); <nl> + strlcpy ( card -> shortname , card_name , sizeof ( card -> shortname )); <nl> sprintf ( card -> longname , "% s % i ", card_name , card -> dev -> id ); <nl>  <nl> snd_pcm_lib_preallocate_pages_for_all ( pcm , SNDRV_DMA_TYPE_CONTINUOUS ,
mmm drivers / dma / dw_dmac . c <nl> ppp drivers / dma / dw_dmac . c <nl> static int __init dw_probe ( struct platform_device * pdev ) <nl> dma_writel ( dw , CFG , DW_CFG_DMA_EN ); <nl>  <nl> printk ( KERN_INFO "% s : DesignWare DMA Controller , % d channels \ n ", <nl> - pdev -> dev . bus_id , dw -> dma . chancnt ); <nl> + dev_name (& pdev -> dev ), dw -> dma . chancnt ); <nl>  <nl> dma_async_device_register (& dw -> dma ); <nl> 
mmm net / sctp / socket . c <nl> ppp net / sctp / socket . c <nl> int sctp_do_peeloff ( struct sock * sk , sctp_assoc_t id , struct socket ** sockp ) <nl> if (! asoc ) <nl> return - EINVAL ; <nl>  <nl> + /* If there is a thread waiting on more sndbuf space for <nl> + * sending on this asoc , it cannot be peeled . <nl> + */ <nl> + if ( waitqueue_active (& asoc -> wait )) <nl> + return - EBUSY ; <nl> + <nl> /* An association cannot be branched off from an already peeled - off <nl> * socket , nor is this supported for tcp style sockets . <nl> */ <nl> static int sctp_wait_for_sndbuf ( struct sctp_association * asoc , long * timeo_p , <nl> */ <nl> release_sock ( sk ); <nl> current_timeo = schedule_timeout ( current_timeo ); <nl> - if ( sk != asoc -> base . sk ) <nl> - goto do_error ; <nl> lock_sock ( sk ); <nl>  <nl> * timeo_p = current_timeo ;
mmm drivers / net / ethernet / renesas / sh_eth . c <nl> ppp drivers / net / ethernet / renesas / sh_eth . c <nl> static int sh_mdio_init ( struct net_device * ndev , int id , <nl> /* bitbang init */ <nl> bitbang -> addr = mdp -> addr + mdp -> reg_offset [ PIR ]; <nl> bitbang -> set_gate = pd -> set_mdio_gate ; <nl> - bitbang -> mdi_msk = 0x08 ; <nl> - bitbang -> mdo_msk = 0x04 ; <nl> - bitbang -> mmd_msk = 0x02 ;/* MMD */ <nl> - bitbang -> mdc_msk = 0x01 ; <nl> + bitbang -> mdi_msk = PIR_MDI ; <nl> + bitbang -> mdo_msk = PIR_MDO ; <nl> + bitbang -> mmd_msk = PIR_MMD ; <nl> + bitbang -> mdc_msk = PIR_MDC ; <nl> bitbang -> ctrl . ops = & bb_ops ; <nl>  <nl> /* MII controller setting */
mmm drivers / pinctrl / pinctrl - at91 . c <nl> ppp drivers / pinctrl / pinctrl - at91 . c <nl> static int __devinit at91_pinctrl_probe_dt ( struct platform_device * pdev , <nl> return - ENODEV ; <nl>  <nl> info -> dev = & pdev -> dev ; <nl> - info -> ops = <nl> + info -> ops = ( struct at91_pinctrl_mux_ops *) <nl> of_match_device ( at91_pinctrl_of_match , & pdev -> dev )-> data ; <nl> at91_pinctrl_child_count ( info , np ); <nl>  <nl> static int __devinit at91_gpio_probe ( struct platform_device * pdev ) <nl> goto err ; <nl> } <nl>  <nl> - at91_chip -> ops = <nl> + at91_chip -> ops = ( struct at91_pinctrl_mux_ops *) <nl> of_match_device ( at91_gpio_of_match , & pdev -> dev )-> data ; <nl> at91_chip -> pioc_virq = irq ; <nl> at91_chip -> pioc_idx = alias_idx ;
mmm drivers / platform / x86 / samsung - laptop . c <nl> ppp drivers / platform / x86 / samsung - laptop . c <nl> # include < linux / seq_file . h > <nl> # include < linux / debugfs . h > <nl> # include < linux / ctype . h > <nl> +# include < linux / efi . h > <nl> # include < acpi / video . h > <nl>  <nl> /* <nl> static int __init samsung_init ( void ) <nl> struct samsung_laptop * samsung ; <nl> int ret ; <nl>  <nl> + if ( efi_enabled ( EFI_BOOT )) <nl> + return - ENODEV ; <nl> + <nl> quirks = & samsung_unknown ; <nl> if (! force && ! dmi_check_system ( samsung_dmi_table )) <nl> return - ENODEV ;
mmm drivers / net / phy / dp83640 . c <nl> ppp drivers / net / phy / dp83640 . c <nl> static void recalibrate ( struct dp83640_clock * clock ) <nl> u16 cal_gpio , cfg0 , evnt , ptp_trig , trigger , val ; <nl>  <nl> trigger = CAL_TRIGGER ; <nl> - cal_gpio = gpio_tab [ CALIBRATE_GPIO ]; <nl> + cal_gpio = 1 + ptp_find_pin ( clock -> ptp_clock , PTP_PF_PHYSYNC , 0 ); <nl> + if ( cal_gpio < 1 ) { <nl> + pr_err (" PHY calibration pin not avaible - PHY is not calibrated ."); <nl> + return ; <nl> + } <nl>  <nl> mutex_lock (& clock -> extreg_lock ); <nl> 
mmm drivers / scsi / lpfc / lpfc_els . c <nl> ppp drivers / scsi / lpfc / lpfc_els . c <nl> lpfc_issue_els_plogi ( struct lpfc_vport * vport , uint32_t did , uint8_t retry ) <nl> if ( sp -> cmn . fcphHigh < FC_PH3 ) <nl> sp -> cmn . fcphHigh = FC_PH3 ; <nl>  <nl> + sp -> cmn . valid_vendor_ver_level = 0 ; <nl> + memset ( sp -> vendorVersion , 0 , sizeof ( sp -> vendorVersion )); <nl> + <nl> lpfc_debugfs_disc_trc ( vport , LPFC_DISC_TRC_ELS_CMD , <nl> " Issue PLOGI : did : x % x ", <nl> did , 0 , 0 ); <nl> lpfc_els_rsp_acc ( struct lpfc_vport * vport , uint32_t flag , <nl> } else { <nl> memcpy ( pcmd , & vport -> fc_sparam , <nl> sizeof ( struct serv_parm )); <nl> + <nl> + sp -> cmn . valid_vendor_ver_level = 0 ; <nl> + memset ( sp -> vendorVersion , 0 , sizeof ( sp -> vendorVersion )); <nl> } <nl>  <nl> lpfc_debugfs_disc_trc ( vport , LPFC_DISC_TRC_ELS_RSP ,mmm drivers / scsi / lpfc / lpfc_hw . h <nl> ppp drivers / scsi / lpfc / lpfc_hw . h <nl> lpfc_issue_els_plogi ( struct lpfc_vport * vport , uint32_t did , uint8_t retry ) <nl> if ( sp -> cmn . fcphHigh < FC_PH3 ) <nl> sp -> cmn . fcphHigh = FC_PH3 ; <nl>  <nl> + sp -> cmn . valid_vendor_ver_level = 0 ; <nl> + memset ( sp -> vendorVersion , 0 , sizeof ( sp -> vendorVersion )); <nl> + <nl> lpfc_debugfs_disc_trc ( vport , LPFC_DISC_TRC_ELS_CMD , <nl> " Issue PLOGI : did : x % x ", <nl> did , 0 , 0 ); <nl> lpfc_els_rsp_acc ( struct lpfc_vport * vport , uint32_t flag , <nl> } else { <nl> memcpy ( pcmd , & vport -> fc_sparam , <nl> sizeof ( struct serv_parm )); <nl> + <nl> + sp -> cmn . valid_vendor_ver_level = 0 ; <nl> + memset ( sp -> vendorVersion , 0 , sizeof ( sp -> vendorVersion )); <nl> } <nl>  <nl> lpfc_debugfs_disc_trc ( vport , LPFC_DISC_TRC_ELS_RSP , <nl> struct csp { <nl> * Word 1 Bit 30 in PLOGI request is random offset <nl> */ <nl> # define virtual_fabric_support randomOffset /* Word 1 , bit 30 */ <nl> +/* <nl> + * Word 1 Bit 29 in common service parameter is overloaded . <nl> + * Word 1 Bit 29 in FLOGI response is multiple NPort assignment <nl> + * Word 1 Bit 29 in FLOGI / PLOGI request is Valid Vendor Version Level <nl> + */ <nl> +# define valid_vendor_ver_level response_multiple_NPort /* Word 1 , bit 29 */ <nl> # ifdef __BIG_ENDIAN_BITFIELD <nl> uint16_t request_multiple_Nport : 1 ; /* FC Word 1 , bit 31 */ <nl> uint16_t randomOffset : 1 ; /* FC Word 1 , bit 30 */
mmm net / netfilter / nf_tables_api . c <nl> ppp net / netfilter / nf_tables_api . c <nl> static int nft_verdict_init ( const struct nft_ctx * ctx , struct nft_data * data , <nl> return PTR_ERR ( chain ); <nl> if ( nft_is_base_chain ( chain )) <nl> return - EOPNOTSUPP ; <nl> + if ( nft_chain_is_bound ( chain )) <nl> + return - EINVAL ; <nl> if ( desc -> flags & NFT_DATA_DESC_SETELEM && <nl> chain -> flags & NFT_CHAIN_BINDING ) <nl> return - EINVAL ;
mmm drivers / video / fbdev / fsl - diu - fb . c <nl> ppp drivers / video / fbdev / fsl - diu - fb . c <nl> static int fsl_diu_suspend ( struct platform_device * ofdev , pm_message_t state ) <nl> static int fsl_diu_resume ( struct platform_device * ofdev ) <nl> { <nl> struct fsl_diu_data * data ; <nl> + unsigned int i ; <nl>  <nl> data = dev_get_drvdata (& ofdev -> dev ); <nl> - enable_lcdc ( data -> fsl_diu_info ); <nl> + <nl> + fsl_diu_enable_interrupts ( data ); <nl> + update_lcdc ( data -> fsl_diu_info ); <nl> + for ( i = 0 ; i < NUM_AOIS ; i ++) { <nl> + if ( data -> mfb [ i ]. count ) <nl> + fsl_diu_enable_panel (& data -> fsl_diu_info [ i ]); <nl> + } <nl>  <nl> return 0 ; <nl> }
mmm drivers / of / of_mdio . c <nl> ppp drivers / of / of_mdio . c <nl> void of_mdiobus_link_phydev ( struct mii_bus * mdio , <nl> } <nl> } <nl> } <nl> + EXPORT_SYMBOL ( of_mdiobus_link_phydev ); <nl>  <nl> /* Helper function for of_phy_find_device */ <nl> static int of_phy_match ( struct device * dev , void * phy_np )
mmm drivers / net / wireless / iwlwifi / mvm / fw . c <nl> ppp drivers / net / wireless / iwlwifi / mvm / fw . c <nl> int iwl_run_init_mvm_ucode ( struct iwl_mvm * mvm , bool read_nvm ) <nl> ret = iwl_nvm_check_version ( mvm -> nvm_data , mvm -> trans ); <nl> WARN_ON ( ret ); <nl>  <nl> + /* Send TX valid antennas before triggering calibrations */ <nl> + ret = iwl_send_tx_ant_cfg ( mvm , mvm -> nvm_data -> valid_tx_ant ); <nl> + if ( ret ) <nl> + goto error ; <nl> + <nl> /* Override the calibrations from TLV and the const of fw */ <nl> iwl_set_default_calib_trigger ( mvm ); <nl> 
mmm drivers / iio / industrialio - buffer . c <nl> ppp drivers / iio / industrialio - buffer . c <nl> void iio_disable_all_buffers ( struct iio_dev * indio_dev ) <nl> indio_dev -> currentmode = INDIO_DIRECT_MODE ; <nl> if ( indio_dev -> setup_ops -> postdisable ) <nl> indio_dev -> setup_ops -> postdisable ( indio_dev ); <nl> + <nl> + if ( indio_dev -> available_scan_masks == NULL ) <nl> + kfree ( indio_dev -> active_scan_mask ); <nl> } <nl>  <nl> int iio_update_buffers ( struct iio_dev * indio_dev ,
mmm net / bridge / br_netlink . c <nl> ppp net / bridge / br_netlink . c <nl> static inline size_t br_port_info_size ( void ) <nl> + nla_total_size ( sizeof ( u16 )) /* IFLA_BRPORT_DESIGNATED_COST */ <nl> + nla_total_size ( sizeof ( u16 )) /* IFLA_BRPORT_ID */ <nl> + nla_total_size ( sizeof ( u16 )) /* IFLA_BRPORT_NO */ <nl> + + nla_total_size ( sizeof ( u8 )) /* IFLA_BRPORT_TOPOLOGY_CHANGE_ACK */ <nl> + + nla_total_size ( sizeof ( u8 )) /* IFLA_BRPORT_CONFIG_PENDING */ <nl> + 0 ; <nl> } <nl>  <nl> static int br_port_fill_attrs ( struct sk_buff * skb , <nl> nla_put_u16 ( skb , IFLA_BRPORT_DESIGNATED_PORT , p -> designated_port ) || <nl> nla_put_u16 ( skb , IFLA_BRPORT_DESIGNATED_COST , p -> designated_cost ) || <nl> nla_put_u16 ( skb , IFLA_BRPORT_ID , p -> port_id ) || <nl> - nla_put_u16 ( skb , IFLA_BRPORT_NO , p -> port_no )) <nl> + nla_put_u16 ( skb , IFLA_BRPORT_NO , p -> port_no ) || <nl> + nla_put_u8 ( skb , IFLA_BRPORT_TOPOLOGY_CHANGE_ACK , <nl> + p -> topology_change_ack ) || <nl> + nla_put_u8 ( skb , IFLA_BRPORT_CONFIG_PENDING , p -> config_pending )) <nl> return - EMSGSIZE ; <nl>  <nl> return 0 ;mmm include / uapi / linux / if_link . h <nl> ppp include / uapi / linux / if_link . h <nl> static inline size_t br_port_info_size ( void ) <nl> + nla_total_size ( sizeof ( u16 )) /* IFLA_BRPORT_DESIGNATED_COST */ <nl> + nla_total_size ( sizeof ( u16 )) /* IFLA_BRPORT_ID */ <nl> + nla_total_size ( sizeof ( u16 )) /* IFLA_BRPORT_NO */ <nl> + + nla_total_size ( sizeof ( u8 )) /* IFLA_BRPORT_TOPOLOGY_CHANGE_ACK */ <nl> + + nla_total_size ( sizeof ( u8 )) /* IFLA_BRPORT_CONFIG_PENDING */ <nl> + 0 ; <nl> } <nl>  <nl> static int br_port_fill_attrs ( struct sk_buff * skb , <nl> nla_put_u16 ( skb , IFLA_BRPORT_DESIGNATED_PORT , p -> designated_port ) || <nl> nla_put_u16 ( skb , IFLA_BRPORT_DESIGNATED_COST , p -> designated_cost ) || <nl> nla_put_u16 ( skb , IFLA_BRPORT_ID , p -> port_id ) || <nl> - nla_put_u16 ( skb , IFLA_BRPORT_NO , p -> port_no )) <nl> + nla_put_u16 ( skb , IFLA_BRPORT_NO , p -> port_no ) || <nl> + nla_put_u8 ( skb , IFLA_BRPORT_TOPOLOGY_CHANGE_ACK , <nl> + p -> topology_change_ack ) || <nl> + nla_put_u8 ( skb , IFLA_BRPORT_CONFIG_PENDING , p -> config_pending )) <nl> return - EMSGSIZE ; <nl>  <nl> return 0 ; <nl> enum { <nl> IFLA_BRPORT_DESIGNATED_COST , <nl> IFLA_BRPORT_ID , <nl> IFLA_BRPORT_NO , <nl> + IFLA_BRPORT_TOPOLOGY_CHANGE_ACK , <nl> + IFLA_BRPORT_CONFIG_PENDING , <nl> __IFLA_BRPORT_MAX <nl> }; <nl> # define IFLA_BRPORT_MAX ( __IFLA_BRPORT_MAX - 1 )
mmm drivers / net / ethernet / broadcom / bnxt / bnxt_ethtool . c <nl> ppp drivers / net / ethernet / broadcom / bnxt / bnxt_ethtool . c <nl> static int bnxt_get_nvram_item ( struct net_device * dev , u32 index , u32 offset , <nl> dma_addr_t dma_handle ; <nl> struct hwrm_nvm_read_input req = { 0 }; <nl>  <nl> + if (! length ) <nl> + return - EINVAL ; <nl> + <nl> buf = dma_alloc_coherent (& bp -> pdev -> dev , length , & dma_handle , <nl> GFP_KERNEL ); <nl> if (! buf ) {
mmm drivers / infiniband / hw / i40iw / i40iw_cm . c <nl> ppp drivers / infiniband / hw / i40iw / i40iw_cm . c <nl> static void i40iw_cm_disconn_true ( struct i40iw_qp * iwqp ) <nl> /* Flush the queues */ <nl> i40iw_flush_wqes ( iwdev , iwqp ); <nl>  <nl> - if ( qp -> term_flags ) { <nl> + if ( qp -> term_flags && iwqp -> ibqp . event_handler ) { <nl> ibevent . device = iwqp -> ibqp . device ; <nl> ibevent . event = ( qp -> eventtype == TERM_EVENT_QP_FATAL ) ? <nl> IB_EVENT_QP_FATAL : IB_EVENT_QP_ACCESS_ERR ;
mmm security / tomoyo / common . c <nl> ppp security / tomoyo / common . c <nl> ssize_t tomoyo_write_control ( struct tomoyo_io_buffer * head , <nl> return - EFAULT ; <nl> if ( mutex_lock_interruptible (& head -> io_sem )) <nl> return - EINTR ; <nl> + head -> read_user_buf_avail = 0 ; <nl> idx = tomoyo_read_lock (); <nl> /* Read a line and dispatch it to the policy handler . */ <nl> while ( avail_len > 0 ) {
mmm drivers / virt / vboxguest / vboxguest_utils . c <nl> ppp drivers / virt / vboxguest / vboxguest_utils . c <nl> static int hgcm_call_preprocess_linaddr ( <nl> if (! bounce_buf ) <nl> return - ENOMEM ; <nl>  <nl> + * bounce_buf_ret = bounce_buf ; <nl> + <nl> if ( copy_in ) { <nl> ret = copy_from_user ( bounce_buf , ( void __user *) buf , len ); <nl> if ( ret ) <nl> static int hgcm_call_preprocess_linaddr ( <nl> memset ( bounce_buf , 0 , len ); <nl> } <nl>  <nl> - * bounce_buf_ret = bounce_buf ; <nl> hgcm_call_add_pagelist_size ( bounce_buf , len , extra ); <nl> return 0 ; <nl> }
mmm drivers / acpi / acpi_memhotplug . c <nl> ppp drivers / acpi / acpi_memhotplug . c <nl> static int acpi_memory_device_add ( struct acpi_device * device ) <nl> if (! acpi_memory_check_device ( mem_device )) { <nl> /* call add_memory func */ <nl> result = acpi_memory_enable_device ( mem_device ); <nl> - if ( result ) <nl> + if ( result ) { <nl> printk ( KERN_ERR PREFIX <nl> " Error in acpi_memory_enable_device \ n "); <nl> + acpi_memory_device_free ( mem_device ); <nl> + } <nl> } <nl> return result ; <nl> }
mmm fs / namespace . c <nl> ppp fs / namespace . c <nl> static inline void namespace_lock ( void ) <nl> enum umount_tree_flags { <nl> UMOUNT_SYNC = 1 , <nl> UMOUNT_PROPAGATE = 2 , <nl> + UMOUNT_CONNECTED = 4 , <nl> }; <nl> /* <nl> * mount_lock must be held <nl> static void umount_tree ( struct mount * mnt , enum umount_tree_flags how ) <nl> if ( how & UMOUNT_SYNC ) <nl> p -> mnt . mnt_flags |= MNT_SYNC_UMOUNT ; <nl>  <nl> - disconnect = ! IS_MNT_LOCKED_AND_LAZY ( p ); <nl> + disconnect = !((( how & UMOUNT_CONNECTED ) && <nl> + mnt_has_parent ( p ) && <nl> + ( p -> mnt_parent -> mnt . mnt_flags & MNT_UMOUNT )) || <nl> + IS_MNT_LOCKED_AND_LAZY ( p )); <nl>  <nl> pin_insert_group (& p -> mnt_umount , & p -> mnt_parent -> mnt , <nl> disconnect ? & unmounted : NULL ); <nl> void __detach_mounts ( struct dentry * dentry ) <nl> umount_mnt ( p ); <nl> } <nl> } <nl> - else umount_tree ( mnt , 0 ); <nl> + else umount_tree ( mnt , UMOUNT_CONNECTED ); <nl> } <nl> unlock_mount_hash (); <nl> put_mountpoint ( mp );
mmm drivers / gpu / drm / i915 / intel_display . c <nl> ppp drivers / gpu / drm / i915 / intel_display . c <nl> static int intel_modeset_checks ( struct drm_atomic_state * state ) <nl>  <nl> DRM_DEBUG_KMS (" New cdclk calculated to be atomic % u , actual % u \ n ", <nl> intel_state -> cdclk , intel_state -> dev_cdclk ); <nl> - } else <nl> + } else { <nl> to_intel_atomic_state ( state )-> cdclk = dev_priv -> atomic_cdclk_freq ; <nl> + } <nl>  <nl> intel_modeset_clear_plls ( state ); <nl>  <nl> static int intel_atomic_check ( struct drm_device * dev , <nl>  <nl> if ( ret ) <nl> return ret ; <nl> - } else <nl> - intel_state -> cdclk = dev_priv -> cdclk_freq ; <nl> + } else { <nl> + intel_state -> cdclk = dev_priv -> atomic_cdclk_freq ; <nl> + } <nl>  <nl> ret = drm_atomic_helper_check_planes ( dev , state ); <nl> if ( ret )
mmm fs / ocfs2 / super . c <nl> ppp fs / ocfs2 / super . c <nl> static int ocfs2_initialize_super ( struct super_block * sb , <nl> sb -> s_fs_info = osb ; <nl> sb -> s_op = & ocfs2_sops ; <nl> sb -> s_export_op = & ocfs2_export_ops ; <nl> + sb -> s_time_gran = 1 ; <nl> sb -> s_flags |= MS_NOATIME ; <nl> /* this is needed to support O_LARGEFILE */ <nl> cbits = le32_to_cpu ( di -> id2 . i_super . s_clustersize_bits );
mmm drivers / video / via / hw . c <nl> ppp drivers / video / via / hw . c <nl> static void set_display_channel ( void ) <nl> } <nl> } <nl>  <nl> - static u8 get_sync ( struct fb_info * info ) <nl> + static u8 get_sync ( struct fb_var_screeninfo * var ) <nl> { <nl> u8 polarity = 0 ; <nl>  <nl> - if (!( info -> var . sync & FB_SYNC_HOR_HIGH_ACT )) <nl> + if (!( var -> sync & FB_SYNC_HOR_HIGH_ACT )) <nl> polarity |= VIA_HSYNC_NEGATIVE ; <nl> - if (!( info -> var . sync & FB_SYNC_VERT_HIGH_ACT )) <nl> + if (!( var -> sync & FB_SYNC_VERT_HIGH_ACT )) <nl> polarity |= VIA_VSYNC_NEGATIVE ; <nl> return polarity ; <nl> } <nl> int viafb_setmode ( int video_bpp , int video_bpp1 ) <nl> viafb_DeviceStatus = CRT_Device ; <nl> } <nl> device_on (); <nl> - if (! viafb_dual_fb ) <nl> - via_set_sync_polarity ( devices , get_sync ( viafbinfo )); <nl> + if (! viafb_SAMM_ON ) <nl> + via_set_sync_polarity ( devices , get_sync (& viafbinfo -> var )); <nl> else { <nl> via_set_sync_polarity ( viaparinfo -> shared -> iga1_devices , <nl> - get_sync ( viafbinfo )); <nl> + get_sync (& viafbinfo -> var )); <nl> via_set_sync_polarity ( viaparinfo -> shared -> iga2_devices , <nl> - get_sync ( viafbinfo1 )); <nl> + get_sync (& var2 )); <nl> } <nl>  <nl> clock . set_engine_pll_state ( VIA_STATE_ON );
mmm net / netlink / af_netlink . c <nl> ppp net / netlink / af_netlink . c <nl> static int netlink_sendmsg ( struct kiocb * kiocb , struct socket * sock , <nl> if ( NULL == siocb -> scm ) <nl> siocb -> scm = & scm ; <nl>  <nl> - err = scm_send ( sock , msg , siocb -> scm ); <nl> + err = scm_send ( sock , msg , siocb -> scm , true ); <nl> if ( err < 0 ) <nl> return err ; <nl> mmm net / unix / af_unix . c <nl> ppp net / unix / af_unix . c <nl> static int netlink_sendmsg ( struct kiocb * kiocb , struct socket * sock , <nl> if ( NULL == siocb -> scm ) <nl> siocb -> scm = & scm ; <nl>  <nl> - err = scm_send ( sock , msg , siocb -> scm ); <nl> + err = scm_send ( sock , msg , siocb -> scm , true ); <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> static int unix_dgram_sendmsg ( struct kiocb * kiocb , struct socket * sock , <nl> if ( NULL == siocb -> scm ) <nl> siocb -> scm = & tmp_scm ; <nl> wait_for_unix_gc (); <nl> - err = scm_send ( sock , msg , siocb -> scm ); <nl> + err = scm_send ( sock , msg , siocb -> scm , false ); <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> static int unix_stream_sendmsg ( struct kiocb * kiocb , struct socket * sock , <nl> if ( NULL == siocb -> scm ) <nl> siocb -> scm = & tmp_scm ; <nl> wait_for_unix_gc (); <nl> - err = scm_send ( sock , msg , siocb -> scm ); <nl> + err = scm_send ( sock , msg , siocb -> scm , false ); <nl> if ( err < 0 ) <nl> return err ; <nl> mmm include / net / scm . h <nl> ppp include / net / scm . h <nl> static int netlink_sendmsg ( struct kiocb * kiocb , struct socket * sock , <nl> if ( NULL == siocb -> scm ) <nl> siocb -> scm = & scm ; <nl>  <nl> - err = scm_send ( sock , msg , siocb -> scm ); <nl> + err = scm_send ( sock , msg , siocb -> scm , true ); <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> static int unix_dgram_sendmsg ( struct kiocb * kiocb , struct socket * sock , <nl> if ( NULL == siocb -> scm ) <nl> siocb -> scm = & tmp_scm ; <nl> wait_for_unix_gc (); <nl> - err = scm_send ( sock , msg , siocb -> scm ); <nl> + err = scm_send ( sock , msg , siocb -> scm , false ); <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> static int unix_stream_sendmsg ( struct kiocb * kiocb , struct socket * sock , <nl> if ( NULL == siocb -> scm ) <nl> siocb -> scm = & tmp_scm ; <nl> wait_for_unix_gc (); <nl> - err = scm_send ( sock , msg , siocb -> scm ); <nl> + err = scm_send ( sock , msg , siocb -> scm , false ); <nl> if ( err < 0 ) <nl> return err ; <nl>  <nl> static __inline__ void scm_destroy ( struct scm_cookie * scm ) <nl> } <nl>  <nl> static __inline__ int scm_send ( struct socket * sock , struct msghdr * msg , <nl> - struct scm_cookie * scm ) <nl> + struct scm_cookie * scm , bool forcecreds ) <nl> { <nl> memset ( scm , 0 , sizeof (* scm )); <nl> + if ( forcecreds ) <nl> + scm_set_cred ( scm , task_tgid ( current ), current_cred ()); <nl> unix_get_peersec_dgram ( sock , scm ); <nl> if ( msg -> msg_controllen <= 0 ) <nl> return 0 ;
mmm kernel / sched . c <nl> ppp kernel / sched . c <nl> static int wake_idle ( int cpu , task_t * p ) <nl>  <nl> for_each_domain ( cpu , sd ) { <nl> if ( sd -> flags & SD_WAKE_IDLE ) { <nl> - cpus_and ( tmp , sd -> span , cpu_online_map ); <nl> - cpus_and ( tmp , tmp , p -> cpus_allowed ); <nl> + cpus_and ( tmp , sd -> span , p -> cpus_allowed ); <nl> for_each_cpu_mask ( i , tmp ) { <nl> if ( idle_cpu ( i )) <nl> return i ; <nl> } <nl> } <nl> - else break ; <nl> + else <nl> + break ; <nl> } <nl> return cpu ; <nl> }
mmm drivers / infiniband / hw / mthca / mthca_srq . c <nl> ppp drivers / infiniband / hw / mthca / mthca_srq . c <nl> int mthca_alloc_srq ( struct mthca_dev * dev , struct mthca_pd * pd , <nl> srq -> first_free = 0 ; <nl> srq -> last_free = srq -> max - 1 ; <nl>  <nl> - attr -> max_wr = srq -> max ; <nl> + attr -> max_wr = ( mthca_is_memfree ( dev )) ? srq -> max - 1 : srq -> max ; <nl> attr -> max_sge = srq -> max_gs ; <nl>  <nl> return 0 ; <nl> int mthca_query_srq ( struct ib_srq * ibsrq , struct ib_srq_attr * srq_attr ) <nl> } else <nl> srq_attr -> srq_limit = 0 ; <nl>  <nl> - srq_attr -> max_wr = srq -> max ; <nl> + srq_attr -> max_wr = ( mthca_is_memfree ( dev )) ? srq -> max - 1 : srq -> max ; <nl> srq_attr -> max_sge = srq -> max_gs ; <nl>  <nl> out :
mmm net / bluetooth / rfcomm / sock . c <nl> ppp net / bluetooth / rfcomm / sock . c <nl> static int rfcomm_sock_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl>  <nl> if ( test_and_clear_bit ( RFCOMM_DEFER_SETUP , & d -> flags )) { <nl> rfcomm_dlc_accept ( d ); <nl> + msg -> msg_namelen = 0 ; <nl> return 0 ; <nl> } <nl> 
mmm fs / xfs / xfs_reflink . c <nl> ppp fs / xfs / xfs_reflink . c <nl> xfs_reflink_end_cow ( <nl> /* If there is a hole at end_fsb - 1 go to the previous extent */ <nl> if (! xfs_iext_lookup_extent ( ip , ifp , end_fsb - 1 , & idx , & got ) || <nl> got . br_startoff > end_fsb ) { <nl> - ASSERT ( idx > 0 ); <nl> + /* <nl> + * In case of racing , overlapping AIO writes no COW extents <nl> + * might be left by the time I / O completes for the loser of <nl> + * the race . In that case we are done . <nl> + */ <nl> + if ( idx <= 0 ) <nl> + goto out_cancel ; <nl> xfs_iext_get_extent ( ifp , -- idx , & got ); <nl> } <nl>  <nl> xfs_reflink_end_cow ( <nl>  <nl> out_defer : <nl> xfs_defer_cancel (& dfops ); <nl> + out_cancel : <nl> xfs_trans_cancel ( tp ); <nl> xfs_iunlock ( ip , XFS_ILOCK_EXCL ); <nl> out :
mmm drivers / of / unittest . c <nl> ppp drivers / of / unittest . c <nl> static int __init unittest_data_add ( void ) <nl> of_fdt_unflatten_tree ( unittest_data , NULL , & unittest_data_node ); <nl> if (! unittest_data_node ) { <nl> pr_warn ("% s : No tree to attach ; not running tests \ n ", __func__ ); <nl> + kfree ( unittest_data ); <nl> return - ENODATA ; <nl> } <nl> 
mmm arch / blackfin / kernel / debug - mmrs . c <nl> ppp arch / blackfin / kernel / debug - mmrs . c <nl> bfin_debug_mmrs_dma ( struct dentry * parent , unsigned long base , int num , char mdm <nl> __DMA ( CURR_DESC_PTR , curr_desc_ptr ); <nl> __DMA ( CURR_ADDR , curr_addr ); <nl> __DMA ( IRQ_STATUS , irq_status ); <nl> - __DMA ( PERIPHERAL_MAP , peripheral_map ); <nl> + if ( strcmp ( pfx , " IMDMA ") != 0 ) <nl> + __DMA ( PERIPHERAL_MAP , peripheral_map ); <nl> __DMA ( CURR_X_COUNT , curr_x_count ); <nl> __DMA ( CURR_Y_COUNT , curr_y_count ); <nl> }
mmm fs / nfs / pnfs . c <nl> ppp fs / nfs / pnfs . c <nl> pnfs_choose_layoutget_stateid ( nfs4_stateid * dst , struct pnfs_layout_hdr * lo , <nl> static struct pnfs_layout_segment * <nl> send_layoutget ( struct pnfs_layout_hdr * lo , <nl> struct nfs_open_context * ctx , <nl> - struct pnfs_layout_range * range , <nl> + const struct pnfs_layout_range * range , <nl> gfp_t gfp_flags ) <nl> { <nl> struct inode * ino = lo -> plh_inode ; <nl> send_layoutget ( struct pnfs_layout_hdr * lo , <nl> lgp -> args . minlength = i_size - range -> offset ; <nl> } <nl> lgp -> args . maxcount = PNFS_LAYOUT_MAXSIZE ; <nl> - lgp -> args . range = * range ; <nl> + pnfs_copy_range (& lgp -> args . range , range ); <nl> lgp -> args . type = server -> pnfs_curr_ld -> id ; <nl> lgp -> args . inode = ino ; <nl> lgp -> args . ctx = get_nfs_open_context ( ctx );mmm fs / nfs / pnfs . h <nl> ppp fs / nfs / pnfs . h <nl> pnfs_choose_layoutget_stateid ( nfs4_stateid * dst , struct pnfs_layout_hdr * lo , <nl> static struct pnfs_layout_segment * <nl> send_layoutget ( struct pnfs_layout_hdr * lo , <nl> struct nfs_open_context * ctx , <nl> - struct pnfs_layout_range * range , <nl> + const struct pnfs_layout_range * range , <nl> gfp_t gfp_flags ) <nl> { <nl> struct inode * ino = lo -> plh_inode ; <nl> send_layoutget ( struct pnfs_layout_hdr * lo , <nl> lgp -> args . minlength = i_size - range -> offset ; <nl> } <nl> lgp -> args . maxcount = PNFS_LAYOUT_MAXSIZE ; <nl> - lgp -> args . range = * range ; <nl> + pnfs_copy_range (& lgp -> args . range , range ); <nl> lgp -> args . type = server -> pnfs_curr_ld -> id ; <nl> lgp -> args . inode = ino ; <nl> lgp -> args . ctx = get_nfs_open_context ( ctx ); <nl> pnfs_mark_layout_returned_if_empty ( struct pnfs_layout_hdr * lo ) <nl> set_bit ( NFS_LAYOUT_INVALID_STID , & lo -> plh_flags ); <nl> } <nl>  <nl> + static inline void <nl> + pnfs_copy_range ( struct pnfs_layout_range * dst , <nl> + const struct pnfs_layout_range * src ) <nl> +{ <nl> + memcpy ( dst , src , sizeof (* dst )); <nl> +} <nl> + <nl> extern unsigned int layoutstats_timer ; <nl>  <nl> # ifdef NFS_DEBUG
mmm mm / vmpressure . c <nl> ppp mm / vmpressure . c <nl> static enum vmpressure_levels vmpressure_calc_level ( unsigned long scanned , <nl> unsigned long reclaimed ) <nl> { <nl> unsigned long scale = scanned + reclaimed ; <nl> - unsigned long pressure ; <nl> + unsigned long pressure = 0 ; <nl>  <nl> + /* <nl> + * reclaimed can be greater than scanned in cases <nl> + * like THP , where the scanned is 1 and reclaimed <nl> + * could be 512 <nl> + */ <nl> + if ( reclaimed >= scanned ) <nl> + goto out ; <nl> /* <nl> * We calculate the ratio ( in percents ) of how many pages were <nl> * scanned vs . reclaimed in a given time frame ( window ). Note that <nl> static enum vmpressure_levels vmpressure_calc_level ( unsigned long scanned , <nl> pressure = scale - ( reclaimed * scale / scanned ); <nl> pressure = pressure * 100 / scale ; <nl>  <nl> + out : <nl> pr_debug ("% s : % 3lu ( s : % lu r : % lu )\ n ", __func__ , pressure , <nl> scanned , reclaimed ); <nl> 
mmm fs / udf / inode . c <nl> ppp fs / udf / inode . c <nl> static int udf_read_inode ( struct inode * inode , bool hidden_inode ) <nl> } <nl> inode -> i_generation = iinfo -> i_unique ; <nl>  <nl> + /* Sanity checks for files in ICB so that we don ' t get confused later */ <nl> + if ( iinfo -> i_alloc_type == ICBTAG_FLAG_AD_IN_ICB ) { <nl> + /* <nl> + * For file in ICB data is stored in allocation descriptor <nl> + * so sizes should match <nl> + */ <nl> + if ( iinfo -> i_lenAlloc != inode -> i_size ) <nl> + goto out ; <nl> + /* File in ICB has to fit in there ... */ <nl> + if ( inode -> i_size > inode -> i_sb -> s_blocksize - <nl> + udf_file_entry_alloc_offset ( inode )) <nl> + goto out ; <nl> + } <nl> + <nl> switch ( fe -> icbTag . fileType ) { <nl> case ICBTAG_FILE_TYPE_DIRECTORY : <nl> inode -> i_op = & udf_dir_inode_operations ;
mmm net / bluetooth / hci_sock . c <nl> ppp net / bluetooth / hci_sock . c <nl> static int hci_sock_getsockopt ( struct socket * sock , int level , int optname , <nl> { <nl> struct hci_filter * f = & hci_pi ( sk )-> filter ; <nl>  <nl> + memset (& uf , 0 , sizeof ( uf )); <nl> uf . type_mask = f -> type_mask ; <nl> uf . opcode = f -> opcode ; <nl> uf . event_mask [ 0 ] = *(( u32 *) f -> event_mask + 0 );
mmm drivers / media / usb / gspca / topro . c <nl> ppp drivers / media / usb / gspca / topro . c <nl> static void sd_pkt_scan ( struct gspca_dev * gspca_dev , <nl> } <nl> data ++; <nl> len --; <nl> + if ( len < 2 ) { <nl> + gspca_dev -> last_packet_type = DISCARD_PACKET ; <nl> + return ; <nl> + } <nl> if (* data == 0xff && data [ 1 ] == 0xd8 ) { <nl> /* fixme : there may be information in the 4 high bits */ <nl> + if ( len < 7 ) { <nl> + gspca_dev -> last_packet_type = DISCARD_PACKET ; <nl> + return ; <nl> + } <nl> if (( data [ 6 ] & 0x0f ) != sd -> quality ) <nl> set_dqt ( gspca_dev , data [ 6 ] & 0x0f ); <nl> gspca_frame_add ( gspca_dev , FIRST_PACKET , <nl> static void sd_pkt_scan ( struct gspca_dev * gspca_dev , <nl> gspca_dev -> last_packet_type = DISCARD_PACKET ; <nl> break ; <nl> case 0xcc : <nl> - if ( data [ 1 ] != 0xff || data [ 2 ] != 0xd8 ) <nl> + if ( len >= 3 && ( data [ 1 ] != 0xff || data [ 2 ] != 0xd8 )) <nl> gspca_frame_add ( gspca_dev , INTER_PACKET , <nl> data + 1 , len - 1 ); <nl> else
mmm net / bluetooth / af_bluetooth . c <nl> ppp net / bluetooth / af_bluetooth . c <nl> void bt_accept_enqueue ( struct sock * parent , struct sock * sk ) <nl> BT_DBG (" parent % p , sk % p ", parent , sk ); <nl>  <nl> sock_hold ( sk ); <nl> + lock_sock ( sk ); <nl> list_add_tail (& bt_sk ( sk )-> accept_q , & bt_sk ( parent )-> accept_q ); <nl> bt_sk ( sk )-> parent = parent ; <nl> + release_sock ( sk ); <nl> parent -> sk_ack_backlog ++; <nl> } <nl> EXPORT_SYMBOL ( bt_accept_enqueue );
mmm include / uapi / linux / usb / ch9 . h <nl> ppp include / uapi / linux / usb / ch9 . h <nl> # define USB_REQ_LOOPBACK_DATA_READ 0x16 <nl> # define USB_REQ_SET_INTERFACE_DS 0x17 <nl>  <nl> +/* specific requests for USB Power Delivery */ <nl> +# define USB_REQ_GET_PARTNER_PDO 20 <nl> +# define USB_REQ_GET_BATTERY_STATUS 21 <nl> +# define USB_REQ_SET_PDO 22 <nl> +# define USB_REQ_GET_VDM 23 <nl> +# define USB_REQ_SEND_VDM 24 <nl> + <nl> /* The Link Power Management ( LPM ) ECN defines USB_REQ_TEST_AND_SET command , <nl> * used by hubs to put ports into a new L1 suspend state , except that it <nl> * forgot to define its number ...
mmm fs / orangefs / file . c <nl> ppp fs / orangefs / file . c <nl> static ssize_t wait_for_direct_io ( enum ORANGEFS_io_type type , struct inode * inod <nl> */ <nl> if ( ret == - EAGAIN && op_state_purged ( new_op )) { <nl> orangefs_bufmap_put ( bufmap , buffer_index ); <nl> + buffer_index = - 1 ; <nl> gossip_debug ( GOSSIP_FILE_DEBUG , <nl> "% s : going to repopulate_shared_memory .\ n ", <nl> __func__ );
mmm arch / x86 / kvm / i8254 . c <nl> ppp arch / x86 / kvm / i8254 . c <nl> struct kvm_pit * kvm_create_pit ( struct kvm * kvm ) <nl> mutex_lock (& kvm -> lock ); <nl> pit -> irq_source_id = kvm_request_irq_source_id ( kvm ); <nl> mutex_unlock (& kvm -> lock ); <nl> - if ( pit -> irq_source_id < 0 ) <nl> + if ( pit -> irq_source_id < 0 ) { <nl> + kfree ( pit ); <nl> return NULL ; <nl> + } <nl>  <nl> mutex_init (& pit -> pit_state . lock ); <nl> mutex_lock (& pit -> pit_state . lock );
mmm drivers / hid / hid - axff . c <nl> ppp drivers / hid / hid - axff . c <nl> static int axff_init ( struct hid_device * hid ) <nl> } <nl> } <nl>  <nl> - if ( field_count < 4 ) { <nl> + if ( field_count < 4 && hid -> product != 0xf705 ) { <nl> hid_err ( hid , " not enough fields in the report : % d \ n ", <nl> field_count ); <nl> return - ENODEV ; <nl> static void ax_remove ( struct hid_device * hdev ) <nl>  <nl> static const struct hid_device_id ax_devices [] = { <nl> { HID_USB_DEVICE ( USB_VENDOR_ID_ACRUX , 0x0802 ), }, <nl> + { HID_USB_DEVICE ( USB_VENDOR_ID_ACRUX , 0xf705 ), }, <nl> { } <nl> }; <nl> MODULE_DEVICE_TABLE ( hid , ax_devices );mmm drivers / hid / hid - core . c <nl> ppp drivers / hid / hid - core . c <nl> static int axff_init ( struct hid_device * hid ) <nl> } <nl> } <nl>  <nl> - if ( field_count < 4 ) { <nl> + if ( field_count < 4 && hid -> product != 0xf705 ) { <nl> hid_err ( hid , " not enough fields in the report : % d \ n ", <nl> field_count ); <nl> return - ENODEV ; <nl> static void ax_remove ( struct hid_device * hdev ) <nl>  <nl> static const struct hid_device_id ax_devices [] = { <nl> { HID_USB_DEVICE ( USB_VENDOR_ID_ACRUX , 0x0802 ), }, <nl> + { HID_USB_DEVICE ( USB_VENDOR_ID_ACRUX , 0xf705 ), }, <nl> { } <nl> }; <nl> MODULE_DEVICE_TABLE ( hid , ax_devices ); <nl> static const struct hid_device_id hid_have_special_driver [] = { <nl> { HID_USB_DEVICE ( USB_VENDOR_ID_A4TECH , USB_DEVICE_ID_A4TECH_X5_005D ) }, <nl> { HID_USB_DEVICE ( USB_VENDOR_ID_A4TECH , USB_DEVICE_ID_A4TECH_RP_649 ) }, <nl> { HID_USB_DEVICE ( USB_VENDOR_ID_ACRUX , 0x0802 ) }, <nl> + { HID_USB_DEVICE ( USB_VENDOR_ID_ACRUX , 0xf705 ) }, <nl> { HID_USB_DEVICE ( USB_VENDOR_ID_APPLE , USB_DEVICE_ID_APPLE_MIGHTYMOUSE ) }, <nl> { HID_BLUETOOTH_DEVICE ( USB_VENDOR_ID_APPLE , USB_DEVICE_ID_APPLE_MAGICMOUSE ) }, <nl> { HID_BLUETOOTH_DEVICE ( USB_VENDOR_ID_APPLE , USB_DEVICE_ID_APPLE_MAGICTRACKPAD ) },
mmm drivers / media / usb / dvb - usb - v2 / rtl28xxu . c <nl> ppp drivers / media / usb / dvb - usb - v2 / rtl28xxu . c <nl> static int rtl2832u_tuner_attach ( struct dvb_usb_adapter * adap ) <nl> struct i2c_board_info info ; <nl> struct i2c_client * client ; <nl> struct v4l2_subdev * subdev = NULL ; <nl> + struct platform_device * pdev ; <nl> + struct rtl2832_sdr_platform_data pdata ; <nl>  <nl> dev_dbg (& d -> intf -> dev , "\ n "); <nl>  <nl> memset (& info , 0 , sizeof ( struct i2c_board_info )); <nl> + memset (& pdata , 0 , sizeof ( pdata )); <nl>  <nl> switch ( dev -> tuner ) { <nl> case TUNER_RTL2832_FC0012 : <nl> static int rtl2832u_tuner_attach ( struct dvb_usb_adapter * adap ) <nl>  <nl> /* register SDR */ <nl> switch ( dev -> tuner ) { <nl> - struct platform_device * pdev ; <nl> - struct rtl2832_sdr_platform_data pdata = {}; <nl> - <nl> case TUNER_RTL2832_FC0012 : <nl> case TUNER_RTL2832_FC0013 : <nl> case TUNER_RTL2832_E4000 :
mmm drivers / vfio / pci / vfio_pci . c <nl> ppp drivers / vfio / pci / vfio_pci . c <nl> static int vfio_pci_mmap ( void * device_data , struct vm_area_struct * vma ) <nl> return ret ; <nl>  <nl> vdev -> barmap [ index ] = pci_iomap ( pdev , index , 0 ); <nl> + if (! vdev -> barmap [ index ]) { <nl> + pci_release_selected_regions ( pdev , 1 << index ); <nl> + return - ENOMEM ; <nl> + } <nl> } <nl>  <nl> vma -> vm_private_data = vdev ;
mmm drivers / scsi / qla4xxx / ql4_init . c <nl> ppp drivers / scsi / qla4xxx / ql4_init . c <nl> void qla4xxx_free_ddb_index ( struct scsi_qla_host * ha ) <nl> ret = qla4xxx_get_fwddb_entry ( ha , idx , NULL , 0 , NULL , <nl> & next_idx , & state , & conn_err , <nl> NULL , NULL ); <nl> - if ( ret == QLA_ERROR ) <nl> + if ( ret == QLA_ERROR ) { <nl> + next_idx ++; <nl> continue ; <nl> + } <nl> if ( state == DDB_DS_NO_CONNECTION_ACTIVE || <nl> state == DDB_DS_SESSION_FAILED ) { <nl> DEBUG2 ( ql4_printk ( KERN_INFO , ha ,
mmm net / sched / cls_basic . c <nl> ppp net / sched / cls_basic . c <nl> static int basic_dump ( struct tcf_proto * tp , unsigned long fh , <nl> rta = ( struct rtattr *) b ; <nl> RTA_PUT ( skb , TCA_OPTIONS , 0 , NULL ); <nl>  <nl> + if ( f -> res . classid ) <nl> + RTA_PUT ( skb , TCA_BASIC_CLASSID , sizeof ( u32 ), & f -> res . classid ); <nl> + <nl> if ( tcf_exts_dump ( skb , & f -> exts , & basic_ext_map ) < 0 || <nl> tcf_em_tree_dump ( skb , & f -> ematches , TCA_BASIC_EMATCHES ) < 0 ) <nl> goto rtattr_failure ;
mmm drivers / gpu / drm / radeon / radeon_atombios . c <nl> ppp drivers / gpu / drm / radeon / radeon_atombios . c <nl> static bool radeon_atom_apply_quirks ( struct drm_device * dev , <nl> if (( supported_device == ATOM_DEVICE_CRT1_SUPPORT ) || <nl> ( supported_device == ATOM_DEVICE_DFP2_SUPPORT )) <nl> return false ; <nl> + if ( supported_device == ATOM_DEVICE_CRT2_SUPPORT ) <nl> + * line_mux = 0x90 ; <nl> } <nl>  <nl> /* ASUS HD 3600 XT board lists the DVI port as HDMI */
mmm drivers / s390 / net / qeth_core_main . c <nl> ppp drivers / s390 / net / qeth_core_main . c <nl> static inline void __qeth_fill_buffer ( struct sk_buff * skb , <nl> struct qdio_buffer * buffer , int is_tso , int * next_element_to_fill , <nl> int offset ) <nl> { <nl> - int length = skb -> len - offset ; <nl> + int length = skb -> len ; <nl> int length_here ; <nl> int element ; <nl> char * data ; <nl> static inline void __qeth_fill_buffer ( struct sk_buff * skb , <nl>  <nl> if ( offset >= 0 ) { <nl> data = skb -> data + offset ; <nl> + length -= offset ; <nl> first_lap = 0 ; <nl> } <nl> 
mmm net / openvswitch / vport - internal_dev . c <nl> ppp net / openvswitch / vport - internal_dev . c <nl> static int internal_dev_recv ( struct vport * vport , struct sk_buff * skb ) <nl> struct net_device * netdev = netdev_vport_priv ( vport )-> dev ; <nl> int len ; <nl>  <nl> + if ( unlikely (!( netdev -> flags & IFF_UP ))) { <nl> + kfree_skb ( skb ); <nl> + return 0 ; <nl> + } <nl> + <nl> len = skb -> len ; <nl>  <nl> skb_dst_drop ( skb );
mmm drivers / net / wireless / ath / ath10k / wmi . c <nl> ppp drivers / net / wireless / ath / ath10k / wmi . c <nl> int ath10k_wmi_beacon_send_nowait ( struct ath10k * ar , <nl> { <nl> struct wmi_bcn_tx_cmd * cmd ; <nl> struct sk_buff * skb ; <nl> + int ret ; <nl>  <nl> skb = ath10k_wmi_alloc_skb ( sizeof (* cmd ) + arg -> bcn_len ); <nl> if (! skb ) <nl> int ath10k_wmi_beacon_send_nowait ( struct ath10k * ar , <nl> cmd -> hdr . bcn_len = __cpu_to_le32 ( arg -> bcn_len ); <nl> memcpy ( cmd -> bcn , arg -> bcn , arg -> bcn_len ); <nl>  <nl> - return ath10k_wmi_cmd_send_nowait ( ar , skb , ar -> wmi . cmd -> bcn_tx_cmdid ); <nl> + ret = ath10k_wmi_cmd_send_nowait ( ar , skb , ar -> wmi . cmd -> bcn_tx_cmdid ); <nl> + if ( ret ) <nl> + dev_kfree_skb ( skb ); <nl> + <nl> + return ret ; <nl> } <nl>  <nl> static void ath10k_wmi_pdev_set_wmm_param ( struct wmi_wmm_params * params ,
mmm kernel / sched / rt . c <nl> ppp kernel / sched / rt . c <nl> static int do_sched_rt_period_timer ( struct rt_bandwidth * rt_b , int overrun ) <nl> const struct cpumask * span ; <nl>  <nl> span = sched_rt_period_mask (); <nl> +# ifdef CONFIG_RT_GROUP_SCHED <nl> + /* <nl> + * FIXME : isolated CPUs should really leave the root task group , <nl> + * whether they are isolcpus or were isolated via cpusets , lest <nl> + * the timer run on a CPU which does not service all runqueues , <nl> + * potentially leaving other CPUs indefinitely throttled . If <nl> + * isolation is really required , the user will turn the throttle <nl> + * off to kill the perturbations it causes anyway . Meanwhile , <nl> + * this maintains functionality for boot and / or troubleshooting . <nl> + */ <nl> + if ( rt_b == & root_task_group . rt_bandwidth ) <nl> + span = cpu_online_mask ; <nl> +# endif <nl> for_each_cpu ( i , span ) { <nl> int enqueue = 0 ; <nl> struct rt_rq * rt_rq = sched_rt_period_rt_rq ( rt_b , i );
mmm drivers / media / usb / usbtv / usbtv - video . c <nl> ppp drivers / media / usb / usbtv / usbtv - video . c <nl> static struct urb * usbtv_setup_iso_transfer ( struct usbtv * usbtv ) <nl> ip -> transfer_flags = URB_ISO_ASAP ; <nl> ip -> transfer_buffer = kzalloc ( size * USBTV_ISOC_PACKETS , <nl> GFP_KERNEL ); <nl> + if (! ip -> transfer_buffer ) { <nl> + usb_free_urb ( ip ); <nl> + return NULL ; <nl> + } <nl> ip -> complete = usbtv_iso_cb ; <nl> ip -> number_of_packets = USBTV_ISOC_PACKETS ; <nl> ip -> transfer_buffer_length = size * USBTV_ISOC_PACKETS ;
mmm fs / udf / symlink . c <nl> ppp fs / udf / symlink . c <nl> static int udf_pc_to_char ( struct super_block * sb , unsigned char * from , <nl> tolen --; <nl> while ( elen < fromlen ) { <nl> pc = ( struct pathComponent *)( from + elen ); <nl> + elen += sizeof ( struct pathComponent ); <nl> switch ( pc -> componentType ) { <nl> case 1 : <nl> /* <nl> * Symlink points to some place which should be agreed <nl> * upon between originator and receiver of the media . Ignore . <nl> */ <nl> - if ( pc -> lengthComponentIdent > 0 ) <nl> + if ( pc -> lengthComponentIdent > 0 ) { <nl> + elen += pc -> lengthComponentIdent ; <nl> break ; <nl> + } <nl> /* Fall through */ <nl> case 2 : <nl> if ( tolen == 0 ) <nl> static int udf_pc_to_char ( struct super_block * sb , unsigned char * from , <nl> /* that would be . - just ignore */ <nl> break ; <nl> case 5 : <nl> + elen += pc -> lengthComponentIdent ; <nl> + if ( elen > fromlen ) <nl> + return - EIO ; <nl> comp_len = udf_get_filename ( sb , pc -> componentIdent , <nl> pc -> lengthComponentIdent , <nl> p , tolen ); <nl> static int udf_pc_to_char ( struct super_block * sb , unsigned char * from , <nl> tolen --; <nl> break ; <nl> } <nl> - elen += sizeof ( struct pathComponent ) + pc -> lengthComponentIdent ; <nl> } <nl> if ( p > to + 1 ) <nl> p [- 1 ] = '\ 0 ';
mmm arch / avr32 / mm / init . c <nl> ppp arch / avr32 / mm / init . c <nl> void __init paging_init ( void ) <nl>  <nl> mem_map = NODE_DATA ( 0 )-> node_mem_map ; <nl>  <nl> - memset ( zero_page , 0 , PAGE_SIZE ); <nl> empty_zero_page = virt_to_page ( zero_page ); <nl> flush_dcache_page ( empty_zero_page ); <nl> }
mmm drivers / infiniband / hw / cxgb4 / cq . c <nl> ppp drivers / infiniband / hw / cxgb4 / cq . c <nl> struct ib_cq * c4iw_create_cq ( struct ib_device * ibdev , int entries , <nl> if (! mm2 ) <nl> goto err4 ; <nl>  <nl> + memset (& uresp , 0 , sizeof ( uresp )); <nl> uresp . qid_mask = rhp -> rdev . cqmask ; <nl> uresp . cqid = chp -> cq . cqid ; <nl> uresp . size = chp -> cq . size ;
mmm drivers / net / wireless / intel / iwlwifi / pcie / rx . c <nl> ppp drivers / net / wireless / intel / iwlwifi / pcie / rx . c <nl> int iwl_pcie_rx_init ( struct iwl_trans * trans ) <nl> else <nl> list_add (& rxb -> list , & def_rxq -> rx_used ); <nl> trans_pcie -> global_table [ i ] = rxb ; <nl> - rxb -> vid = ( u16 ) i ; <nl> + rxb -> vid = ( u16 )( i + 1 ); <nl> } <nl>  <nl> iwl_pcie_rxq_alloc_rbs ( trans , GFP_KERNEL , def_rxq ); <nl> static void iwl_pcie_rx_handle ( struct iwl_trans * trans , int queue ) <nl> */ <nl> u16 vid = le32_to_cpu ( rxq -> used_bd [ i ]) & 0x0FFF ; <nl>  <nl> - if ( WARN ( vid >= ARRAY_SIZE ( trans_pcie -> global_table ), <nl> - " Invalid rxb index from HW % u \ n ", ( u32 ) vid )) <nl> + if ( WARN (! vid || <nl> + vid > ARRAY_SIZE ( trans_pcie -> global_table ), <nl> + " Invalid rxb index from HW % u \ n ", ( u32 ) vid )) { <nl> + iwl_force_nmi ( trans ); <nl> goto out ; <nl> - rxb = trans_pcie -> global_table [ vid ]; <nl> + } <nl> + rxb = trans_pcie -> global_table [ vid - 1 ]; <nl> } else { <nl> rxb = rxq -> queue [ i ]; <nl> rxq -> queue [ i ] = NULL ;
mmm drivers / gpu / drm / radeon / r600_cs . c <nl> ppp drivers / gpu / drm / radeon / r600_cs . c <nl> static int r600_cs_packet_next_reloc_nomm ( struct radeon_cs_parser * p , <nl> idx , relocs_chunk -> length_dw ); <nl> return - EINVAL ; <nl> } <nl> - * cs_reloc = & p -> relocs [ 0 ]; <nl> + * cs_reloc = p -> relocs ; <nl> (* cs_reloc )-> lobj . gpu_offset = ( u64 ) relocs_chunk -> kdata [ idx + 3 ] << 32 ; <nl> (* cs_reloc )-> lobj . gpu_offset |= relocs_chunk -> kdata [ idx + 0 ]; <nl> return 0 ; <nl> static int r600_cs_parser_relocs_legacy ( struct radeon_cs_parser * p ) <nl> if ( p -> chunk_relocs_idx == - 1 ) { <nl> return 0 ; <nl> } <nl> - p -> relocs = kcalloc ( 1 , sizeof ( struct radeon_cs_reloc ), GFP_KERNEL ); <nl> + p -> relocs = kzalloc ( sizeof ( struct radeon_cs_reloc ), GFP_KERNEL ); <nl> if ( p -> relocs == NULL ) { <nl> return - ENOMEM ; <nl> }
mmm net / mac80211 / main . c <nl> ppp net / mac80211 / main . c <nl> struct ieee80211_hw * ieee80211_alloc_hw ( size_t priv_data_len , <nl> if ( WARN_ON ( ops -> sta_state && ( ops -> sta_add || ops -> sta_remove ))) <nl> return NULL ; <nl>  <nl> + /* check all or no channel context operations exist */ <nl> + i = !! ops -> add_chanctx + !! ops -> remove_chanctx + <nl> + !! ops -> change_chanctx + !! ops -> assign_vif_chanctx + <nl> + !! ops -> unassign_vif_chanctx ; <nl> + if ( WARN_ON ( i != 0 && i != 5 )) <nl> + return NULL ; <nl> + <nl> /* Ensure 32 - byte alignment of our private data and hw private data . <nl> * We use the wiphy priv data for both our ieee80211_local and for <nl> * the driver ' s private data
mmm drivers / net / macvlan . c <nl> ppp drivers / net / macvlan . c <nl> static int macvlan_set_mac_address ( struct net_device * dev , void * p ) <nl> if (! is_valid_ether_addr ( addr -> sa_data )) <nl> return - EADDRNOTAVAIL ; <nl>  <nl> + /* If the addresses are the same , this is a no - op */ <nl> + if ( ether_addr_equal ( dev -> dev_addr , addr -> sa_data )) <nl> + return 0 ; <nl> + <nl> if ( vlan -> mode == MACVLAN_MODE_PASSTHRU ) { <nl> dev_set_mac_address ( vlan -> lowerdev , addr ); <nl> return 0 ;
mmm fs / btrfs / tree - log . c <nl> ppp fs / btrfs / tree - log . c <nl> static int btrfs_add_log_tree ( struct btrfs_trans_handle * trans , <nl> */ <nl> new_root -> ref_cows = 0 ; <nl> new_root -> last_trans = trans -> transid ; <nl> + <nl> + /* <nl> + * we need to make sure the root block for this new tree <nl> + * is marked as dirty in the dirty_log_pages tree . This <nl> + * is how it gets flushed down to disk at tree log commit time . <nl> + * <nl> + * the tree logging mutex keeps others from coming in and changing <nl> + * the new_root -> node , so we can safely access it here <nl> + */ <nl> + set_extent_dirty (& new_root -> dirty_log_pages , new_root -> node -> start , <nl> + new_root -> node -> start + new_root -> node -> len - 1 , <nl> + GFP_NOFS ); <nl> + <nl> fail : <nl> return ret ; <nl> }
mmm drivers / acpi / video . c <nl> ppp drivers / acpi / video . c <nl> static int acpi_video_bus_put_one_device ( struct acpi_video_device * device ) <nl> status = acpi_remove_notify_handler ( device -> dev -> handle , <nl> ACPI_DEVICE_NOTIFY , <nl> acpi_video_device_notify ); <nl> - sysfs_remove_link (& device -> backlight -> dev . kobj , " device "); <nl> - backlight_device_unregister ( device -> backlight ); <nl> + if ( device -> backlight ) { <nl> + sysfs_remove_link (& device -> backlight -> dev . kobj , " device "); <nl> + backlight_device_unregister ( device -> backlight ); <nl> + device -> backlight = NULL ; <nl> + } <nl> if ( device -> cdev ) { <nl> sysfs_remove_link (& device -> dev -> dev . kobj , <nl> " thermal_cooling ");
mmm drivers / regulator / core . c <nl> ppp drivers / regulator / core . c <nl> static int set_supply ( struct regulator_dev * rdev , <nl>  <nl> rdev_info ( rdev , " supplied by % s \ n ", rdev_get_name ( supply_rdev )); <nl>  <nl> + if (! try_module_get ( supply_rdev -> owner )) <nl> + return - ENODEV ; <nl> + <nl> rdev -> supply = create_regulator ( supply_rdev , & rdev -> dev , " SUPPLY "); <nl> if ( rdev -> supply == NULL ) { <nl> err = - ENOMEM ;
mmm drivers / media / usb / usbvision / usbvision - video . c <nl> ppp drivers / media / usb / usbvision / usbvision - video . c <nl> static int usbvision_v4l2_close ( struct file * file ) <nl> usbvision_scratch_free ( usbvision ); <nl>  <nl> usbvision -> user --; <nl> + mutex_unlock (& usbvision -> v4l2_lock ); <nl>  <nl> if ( usbvision -> remove_pending ) { <nl> printk ( KERN_INFO "% s : Final disconnect \ n ", __func__ ); <nl> usbvision_release ( usbvision ); <nl> return 0 ; <nl> } <nl> - mutex_unlock (& usbvision -> v4l2_lock ); <nl>  <nl> PDEBUG ( DBG_IO , " success "); <nl> return v4l2_fh_release ( file );
mmm net / bluetooth / hci_request . c <nl> ppp net / bluetooth / hci_request . c <nl> int hci_req_sync ( struct hci_dev * hdev , int (* req )( struct hci_request * req , <nl> { <nl> int ret ; <nl>  <nl> - if (! test_bit ( HCI_UP , & hdev -> flags )) <nl> - return - ENETDOWN ; <nl> - <nl> /* Serialize all requests */ <nl> hci_req_sync_lock ( hdev ); <nl> - ret = __hci_req_sync ( hdev , req , opt , timeout , hci_status ); <nl> + /* check the state after obtaing the lock to protect the HCI_UP <nl> + * against any races from hci_dev_do_close when the controller <nl> + * gets removed . <nl> + */ <nl> + if ( test_bit ( HCI_UP , & hdev -> flags )) <nl> + ret = __hci_req_sync ( hdev , req , opt , timeout , hci_status ); <nl> + else <nl> + ret = - ENETDOWN ; <nl> hci_req_sync_unlock ( hdev ); <nl>  <nl> return ret ;
mmm drivers / staging / cx25821 / cx25821 - video . c <nl> ppp drivers / staging / cx25821 / cx25821 - video . c <nl> static int video_open ( struct file * file ) <nl>  <nl> if ( NULL == dev ) { <nl> mutex_unlock (& cx25821_devlist_mutex ); <nl> + kfree ( fh ); <nl> return - ENODEV ; <nl> } <nl> 
mmm drivers / video / modedb . c <nl> ppp drivers / video / modedb . c <nl> int fb_find_mode ( struct fb_var_screeninfo * var , <nl> "", ( margins ) ? " with margins " : "", ( interlace ) ? <nl> " interlaced " : ""); <nl>  <nl> + memset (& cvt_mode , 0 , sizeof ( cvt_mode )); <nl> cvt_mode . xres = xres ; <nl> cvt_mode . yres = yres ; <nl> cvt_mode . refresh = ( refresh ) ? refresh : 60 ;
mmm fs / btrfs / inode . c <nl> ppp fs / btrfs / inode . c <nl> struct extent_map * btrfs_get_extent ( struct inode * inode , struct page * page , <nl> goto not_found ; <nl> if ( start + len <= found_key . offset ) <nl> goto not_found ; <nl> + if ( start > found_key . offset ) <nl> + goto next ; <nl> em -> start = start ; <nl> em -> orig_start = start ; <nl> em -> len = found_key . offset - start ;
mmm include / linux / fs . h <nl> ppp include / linux / fs . h <nl> struct super_block { <nl> */ <nl> struct list_lru s_dentry_lru ____cacheline_aligned_in_smp ; <nl> struct list_lru s_inode_lru ____cacheline_aligned_in_smp ; <nl> + struct rcu_head rcu ; <nl> }; <nl>  <nl> extern struct timespec current_fs_time ( struct super_block * sb );mmm fs / super . c <nl> ppp fs / super . c <nl> struct super_block { <nl> */ <nl> struct list_lru s_dentry_lru ____cacheline_aligned_in_smp ; <nl> struct list_lru s_inode_lru ____cacheline_aligned_in_smp ; <nl> + struct rcu_head rcu ; <nl> }; <nl>  <nl> extern struct timespec current_fs_time ( struct super_block * sb ); <nl> static void destroy_super ( struct super_block * s ) <nl> WARN_ON (! list_empty (& s -> s_mounts )); <nl> kfree ( s -> s_subtype ); <nl> kfree ( s -> s_options ); <nl> - kfree ( s ); <nl> + kfree_rcu ( s , rcu ); <nl> } <nl>  <nl> /**
mmm drivers / gpio / gpio - rcar . c <nl> ppp drivers / gpio / gpio - rcar . c <nl> static struct irq_domain_ops gpio_rcar_irq_domain_ops = { <nl> static void gpio_rcar_parse_pdata ( struct gpio_rcar_priv * p ) <nl> { <nl> struct gpio_rcar_config * pdata = p -> pdev -> dev . platform_data ; <nl> -# ifdef CONFIG_OF <nl> struct device_node * np = p -> pdev -> dev . of_node ; <nl> struct of_phandle_args args ; <nl> int ret ; <nl> -# endif <nl>  <nl> - if ( pdata ) <nl> + if ( pdata ) { <nl> p -> config = * pdata ; <nl> -# ifdef CONFIG_OF <nl> - else if ( np ) { <nl> + } else if ( IS_ENABLED ( CONFIG_OF ) && np ) { <nl> ret = of_parse_phandle_with_args ( np , " gpio - ranges ", <nl> "# gpio - range - cells ", 0 , & args ); <nl> p -> config . number_of_pins = ret == 0 && args . args_count == 3 <nl> static void gpio_rcar_parse_pdata ( struct gpio_rcar_priv * p ) <nl> : RCAR_MAX_GPIO_PER_BANK ; <nl> p -> config . gpio_base = - 1 ; <nl> } <nl> -# endif <nl>  <nl> if ( p -> config . number_of_pins == 0 || <nl> p -> config . number_of_pins > RCAR_MAX_GPIO_PER_BANK ) {
mmm drivers / staging / cx25821 / cx25821 - video . c <nl> ppp drivers / staging / cx25821 / cx25821 - video . c <nl> struct cx25821_fmt * cx25821_format_by_fourcc ( unsigned int fourcc ) <nl> { <nl> unsigned int i ; <nl>  <nl> - if ( fourcc == V4L2_PIX_FMT_Y41P || fourcc == V4L2_PIX_FMT_YUV411P ) { <nl> + if ( fourcc == V4L2_PIX_FMT_Y41P || fourcc == V4L2_PIX_FMT_YUV411P ) <nl> return formats + 1 ; <nl> - } <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( formats ); i ++) <nl> if ( formats [ i ]. fourcc == fourcc ) <nl> void cx25821_video_wakeup ( struct cx25821_dev * dev , struct cx25821_dmaqueue * q , <nl> /* count comes from the hw and it is 16bit wide -- <nl> * this trick handles wrap - arounds correctly for <nl> * up to 32767 buffers in flight ... */ <nl> - if (( s16 ) ( count - buf -> count ) < 0 ) { <nl> + if (( s16 ) ( count - buf -> count ) < 0 ) <nl> break ; <nl> - } <nl>  <nl> do_gettimeofday (& buf -> vb . ts ); <nl> buf -> vb . state = VIDEOBUF_DONE ;
mmm fs / f2fs / segment . c <nl> ppp fs / f2fs / segment . c <nl> static void f2fs_submit_discard_endio ( struct bio * bio ) <nl>  <nl> dc -> error = bio -> bi_error ; <nl> dc -> state = D_DONE ; <nl> - complete (& dc -> wait ); <nl> + complete_all (& dc -> wait ); <nl> bio_put ( bio ); <nl> } <nl> 
mmm kernel / user_namespace . c <nl> ppp kernel / user_namespace . c <nl> static bool new_idmap_permitted ( const struct file * file , <nl> u32 id = new_map -> extent [ 0 ]. lower_first ; <nl> if ( cap_setid == CAP_SETUID ) { <nl> kuid_t uid = make_kuid ( ns -> parent , id ); <nl> - if ( uid_eq ( uid , current_fsuid ())) <nl> + if ( uid_eq ( uid , file -> f_cred -> fsuid )) <nl> return true ; <nl> } <nl> else if ( cap_setid == CAP_SETGID ) { <nl> kgid_t gid = make_kgid ( ns -> parent , id ); <nl> - if ( gid_eq ( gid , current_fsgid ())) <nl> + if ( gid_eq ( gid , file -> f_cred -> fsgid )) <nl> return true ; <nl> } <nl> }
mmm net / sched / act_police . c <nl> ppp net / sched / act_police . c <nl> static int tcf_act_police_dump ( struct sk_buff * skb , struct tc_action * a , <nl> struct tcf_police * police = to_police ( a ); <nl> struct tc_police opt = { <nl> . index = police -> tcf_index , <nl> - . action = police -> tcf_action , <nl> - . mtu = police -> tcfp_mtu , <nl> - . burst = PSCHED_NS2TICKS ( police -> tcfp_burst ), <nl> . refcnt = refcount_read (& police -> tcf_refcnt ) - ref , <nl> . bindcnt = atomic_read (& police -> tcf_bindcnt ) - bind , <nl> }; <nl> struct tcf_t t ; <nl>  <nl> + spin_lock_bh (& police -> tcf_lock ); <nl> + opt . action = police -> tcf_action ; <nl> + opt . mtu = police -> tcfp_mtu ; <nl> + opt . burst = PSCHED_NS2TICKS ( police -> tcfp_burst ); <nl> if ( police -> rate_present ) <nl> psched_ratecfg_getrate (& opt . rate , & police -> rate ); <nl> if ( police -> peak_present ) <nl> static int tcf_act_police_dump ( struct sk_buff * skb , struct tc_action * a , <nl> t . expires = jiffies_to_clock_t ( police -> tcf_tm . expires ); <nl> if ( nla_put_64bit ( skb , TCA_POLICE_TM , sizeof ( t ), & t , TCA_POLICE_PAD )) <nl> goto nla_put_failure ; <nl> + spin_unlock_bh (& police -> tcf_lock ); <nl>  <nl> return skb -> len ; <nl>  <nl> nla_put_failure : <nl> + spin_unlock_bh (& police -> tcf_lock ); <nl> nlmsg_trim ( skb , b ); <nl> return - 1 ; <nl> }
mmm drivers / media / video / msp3400 - kthreads . c <nl> ppp drivers / media / video / msp3400 - kthreads . c <nl> int msp34xxg_thread ( void * data ) <nl> /* setup the chip */ <nl> msp34xxg_reset ( client ); <nl> state -> std = state -> radio ? 0x40 : msp_standard ; <nl> - if ( state -> std != 1 ) <nl> - goto unmute ; <nl> /* start autodetect */ <nl> msp_write_dem ( client , 0x20 , state -> std ); <nl> + if ( state -> std != 1 ) <nl> + goto unmute ; <nl>  <nl> /* watch autodetect */ <nl> v4l_dbg ( 1 , msp_debug , client , " started autodetect , waiting for result \ n ");
mmm drivers / acpi / acpi_video . c <nl> ppp drivers / acpi / acpi_video . c <nl> static int acpi_video_device_enumerate ( struct acpi_video_bus * video ) <nl> union acpi_object * dod = NULL ; <nl> union acpi_object * obj ; <nl>  <nl> + if (! video -> cap . _DOD ) <nl> + return AE_NOT_EXIST ; <nl> + <nl> status = acpi_evaluate_object ( video -> device -> handle , " _DOD ", NULL , & buffer ); <nl> if (! ACPI_SUCCESS ( status )) { <nl> ACPI_EXCEPTION (( AE_INFO , status , " Evaluating _DOD "));
mmm drivers / extcon / extcon - palmas . c <nl> ppp drivers / extcon / extcon - palmas . c <nl> static int palmas_usb_probe ( struct platform_device * pdev ) <nl> status = devm_extcon_dev_register (& pdev -> dev , palmas_usb -> edev ); <nl> if ( status ) { <nl> dev_err (& pdev -> dev , " failed to register extcon device \ n "); <nl> - kfree ( palmas_usb -> edev -> name ); <nl> return status ; <nl> } <nl>  <nl> static int palmas_usb_probe ( struct platform_device * pdev ) <nl> if ( status < 0 ) { <nl> dev_err (& pdev -> dev , " can ' t get IRQ % d , err % d \ n ", <nl> palmas_usb -> id_irq , status ); <nl> - kfree ( palmas_usb -> edev -> name ); <nl> return status ; <nl> } <nl> } <nl> static int palmas_usb_probe ( struct platform_device * pdev ) <nl> if ( status < 0 ) { <nl> dev_err (& pdev -> dev , " can ' t get IRQ % d , err % d \ n ", <nl> palmas_usb -> vbus_irq , status ); <nl> - kfree ( palmas_usb -> edev -> name ); <nl> return status ; <nl> } <nl> } <nl> static int palmas_usb_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl> } <nl>  <nl> - static int palmas_usb_remove ( struct platform_device * pdev ) <nl> -{ <nl> - struct palmas_usb * palmas_usb = platform_get_drvdata ( pdev ); <nl> - <nl> - kfree ( palmas_usb -> edev -> name ); <nl> - <nl> - return 0 ; <nl> -} <nl> - <nl> # ifdef CONFIG_PM_SLEEP <nl> static int palmas_usb_suspend ( struct device * dev ) <nl> { <nl> static const struct of_device_id of_palmas_match_tbl [] = { <nl>  <nl> static struct platform_driver palmas_usb_driver = { <nl> . probe = palmas_usb_probe , <nl> - . remove = palmas_usb_remove , <nl> . driver = { <nl> . name = " palmas - usb ", <nl> . of_match_table = of_palmas_match_tbl ,
mmm net / bluetooth / mgmt . c <nl> ppp net / bluetooth / mgmt . c <nl> static int set_connectable ( struct sock * sk , struct hci_dev * hdev , void * data , <nl>  <nl> hci_req_add (& req , HCI_OP_WRITE_SCAN_ENABLE , 1 , & scan ); <nl>  <nl> + if (! cp -> val && test_bit ( HCI_FAST_CONNECTABLE , & hdev -> dev_flags )) <nl> + write_fast_connectable (& req , false ); <nl> + <nl> err = hci_req_run (& req , set_connectable_complete ); <nl> if ( err < 0 ) <nl> mgmt_pending_remove ( cmd );
mmm drivers / staging / tm6000 / tm6000 - cards . c <nl> ppp drivers / staging / tm6000 / tm6000 - cards . c <nl> static void tm6000_config_tuner ( struct tm6000_core * dev ) <nl>  <nl> ctl . mts = 1 ; <nl> ctl . read_not_reliable = 1 ; <nl> + ctl . msleep = 10 ; <nl>  <nl> xc2028_cfg . tuner = TUNER_XC2028 ; <nl> xc2028_cfg . priv = & ctl ;
mmm drivers / gpu / drm / i915 / i915_gem . c <nl> ppp drivers / gpu / drm / i915 / i915_gem . c <nl> int i915_gem_init ( struct drm_device * dev ) <nl> i915_gem_init_global_gtt ( dev ); <nl>  <nl> ret = i915_gem_context_init ( dev ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + mutex_unlock (& dev -> struct_mutex ); <nl> return ret ; <nl> + } <nl>  <nl> ret = i915_gem_init_hw ( dev ); <nl> mutex_unlock (& dev -> struct_mutex );
mmm drivers / rtc / rtc - ftrtc010 . c <nl> ppp drivers / rtc / rtc - ftrtc010 . c <nl> static int ftrtc010_rtc_probe ( struct platform_device * pdev ) <nl> if (! rtc -> rtc_base ) <nl> return - ENOMEM ; <nl>  <nl> + rtc -> rtc_dev = devm_rtc_allocate_device ( dev ); <nl> + if ( IS_ERR ( rtc -> rtc_dev )) <nl> + return PTR_ERR ( rtc -> rtc_dev ); <nl> + <nl> + rtc -> rtc_dev -> ops = & ftrtc010_rtc_ops ; <nl> + <nl> ret = devm_request_irq ( dev , rtc -> rtc_irq , ftrtc010_rtc_interrupt , <nl> IRQF_SHARED , pdev -> name , dev ); <nl> if ( unlikely ( ret )) <nl> return ret ; <nl>  <nl> - rtc -> rtc_dev = rtc_device_register ( pdev -> name , dev , <nl> - & ftrtc010_rtc_ops , THIS_MODULE ); <nl> - return PTR_ERR_OR_ZERO ( rtc -> rtc_dev ); <nl> + return rtc_register_device ( rtc -> rtc_dev ); <nl> } <nl>  <nl> static int ftrtc010_rtc_remove ( struct platform_device * pdev ) <nl> static int ftrtc010_rtc_remove ( struct platform_device * pdev ) <nl> clk_disable_unprepare ( rtc -> extclk ); <nl> if (! IS_ERR ( rtc -> pclk )) <nl> clk_disable_unprepare ( rtc -> pclk ); <nl> - rtc_device_unregister ( rtc -> rtc_dev ); <nl>  <nl> return 0 ; <nl> }
mmm sound / soc / intel / boards / bytcr_rt5651 . c <nl> ppp sound / soc / intel / boards / bytcr_rt5651 . c <nl> static int snd_byt_rt5651_mc_probe ( struct platform_device * pdev ) <nl>  <nl> /* fixup codec name based on HID */ <nl> i2c_name = acpi_dev_get_first_match_name ( mach -> id , NULL , - 1 ); <nl> - if ( i2c_name ) { <nl> - snprintf ( byt_rt5651_codec_name , sizeof ( byt_rt5651_codec_name ), <nl> - "% s % s ", " i2c -", i2c_name ); <nl> - <nl> - byt_rt5651_dais [ dai_index ]. codec_name = byt_rt5651_codec_name ; <nl> + if (! i2c_name ) { <nl> + dev_err (& pdev -> dev , " Error cannot find '% s ' dev \ n ", mach -> id ); <nl> + return - ENODEV ; <nl> } <nl> + snprintf ( byt_rt5651_codec_name , sizeof ( byt_rt5651_codec_name ), <nl> + "% s % s ", " i2c -", i2c_name ); <nl> + byt_rt5651_dais [ dai_index ]. codec_name = byt_rt5651_codec_name ; <nl>  <nl> /* check quirks before creating card */ <nl> dmi_check_system ( byt_rt5651_quirk_table );
mmm drivers / staging / vt6656 / wpactl . c <nl> ppp drivers / staging / vt6656 / wpactl . c <nl> int wpa_ioctl ( PSDevice pDevice , struct iw_point * p ) <nl> default : <nl> DBG_PRT ( MSG_LEVEL_DEBUG , KERN_INFO " wpa_ioctl : unknown cmd =% d \ n ", <nl> param -> cmd ); <nl> + kfree ( param ); <nl> return - EOPNOTSUPP ; <nl> } <nl> 
mmm drivers / acpi / ioapic . c <nl> ppp drivers / acpi / ioapic . c <nl> static acpi_status setup_res ( struct acpi_resource * acpi_res , void * data ) <nl> struct resource * res = data ; <nl> struct resource_win win ; <nl>  <nl> + /* <nl> + * We might assign this to ' res ' later , make sure all pointers are <nl> + * cleared before the resource is added to the global list <nl> + */ <nl> + memset (& win , 0 , sizeof ( win )); <nl> + <nl> res -> flags = 0 ; <nl> if ( acpi_dev_filter_resource_type ( acpi_res , IORESOURCE_MEM )) <nl> return AE_OK ;
mmm drivers / net / wireless / intel / iwlwifi / mvm / sta . c <nl> ppp drivers / net / wireless / intel / iwlwifi / mvm / sta . c <nl> static int iwl_mvm_free_inactive_queue ( struct iwl_mvm * mvm , int queue , <nl> spin_unlock_bh (& mvm -> queue_info_lock ); <nl>  <nl> mvmsta = iwl_mvm_sta_from_staid_protected ( mvm , sta_id ); <nl> + if ( WARN_ON (! mvmsta )) <nl> + return - EINVAL ; <nl>  <nl> disable_agg_tids = iwl_mvm_remove_sta_queue_marking ( mvm , queue ); <nl> /* Disable the queue */
mmm net / sctp / sm_make_chunk . c <nl> ppp net / sctp / sm_make_chunk . c <nl> static int sctp_process_param ( struct sctp_association * asoc , <nl> addr_param = param . v + sizeof ( sctp_addip_param_t ); <nl>  <nl> af = sctp_get_af_specific ( param_type2af ( param . p -> type )); <nl> + if ( af == NULL ) <nl> + break ; <nl> + <nl> af -> from_addr_param (& addr , addr_param , <nl> htons ( asoc -> peer . port ), 0 ); <nl> 
mmm drivers / gpu / drm / arm / malidp_planes . c <nl> ppp drivers / gpu / drm / arm / malidp_planes . c <nl> static void malidp_de_set_plane_pitches ( struct malidp_plane * mp , <nl> static void malidp_de_plane_update ( struct drm_plane * plane , <nl> struct drm_plane_state * old_state ) <nl> { <nl> - struct drm_gem_cma_object * obj ; <nl> struct malidp_plane * mp ; <nl> const struct malidp_hw_regmap * map ; <nl> struct malidp_plane_state * ms = to_malidp_plane_state ( plane -> state ); <nl> - u16 ptr ; <nl> u32 src_w , src_h , dest_w , dest_h , val ; <nl> int i ; <nl>  <nl> static void malidp_de_plane_update ( struct drm_plane * plane , <nl>  <nl> for ( i = 0 ; i < ms -> n_planes ; i ++) { <nl> /* calculate the offset for the layer ' s plane registers */ <nl> - ptr = mp -> layer -> ptr + ( i << 4 ); <nl> + u16 ptr = mp -> layer -> ptr + ( i << 4 ); <nl> + dma_addr_t fb_addr = drm_fb_cma_get_gem_addr ( plane -> state -> fb , <nl> + plane -> state , i ); <nl>  <nl> - obj = drm_fb_cma_get_gem_obj ( plane -> state -> fb , i ); <nl> - obj -> paddr += plane -> state -> fb -> offsets [ i ]; <nl> - malidp_hw_write ( mp -> hwdev , lower_32_bits ( obj -> paddr ), ptr ); <nl> - malidp_hw_write ( mp -> hwdev , upper_32_bits ( obj -> paddr ), ptr + 4 ); <nl> + malidp_hw_write ( mp -> hwdev , lower_32_bits ( fb_addr ), ptr ); <nl> + malidp_hw_write ( mp -> hwdev , upper_32_bits ( fb_addr ), ptr + 4 ); <nl> } <nl> malidp_de_set_plane_pitches ( mp , ms -> n_planes , <nl> plane -> state -> fb -> pitches );
mmm drivers / media / video / tvaudio . c <nl> ppp drivers / media / video / tvaudio . c <nl> static int tvaudio_probe ( struct i2c_client * client , const struct i2c_device_id * <nl> } <nl>  <nl> chip -> thread = NULL ; <nl> + init_timer (& chip -> wt ); <nl> if ( desc -> flags & CHIP_NEED_CHECKMODE ) { <nl> if (! desc -> getmode || ! desc -> setmode ) { <nl> /* This shouldn ' t be happen . Warn user , but keep working <nl> static int tvaudio_probe ( struct i2c_client * client , const struct i2c_device_id * <nl> return 0 ; <nl> } <nl> /* start async thread */ <nl> - init_timer (& chip -> wt ); <nl> chip -> wt . function = chip_thread_wake ; <nl> chip -> wt . data = ( unsigned long ) chip ; <nl> chip -> thread = kthread_run ( chip_thread , chip , client -> name );
mmm drivers / gpu / drm / sun4i / sun4i_tv . c <nl> ppp drivers / gpu / drm / sun4i / sun4i_tv . c <nl> static int sun4i_tv_comp_get_modes ( struct drm_connector * connector ) <nl> int i ; <nl>  <nl> for ( i = 0 ; i < ARRAY_SIZE ( tv_modes ); i ++) { <nl> - struct drm_display_mode * mode = drm_mode_create ( connector -> dev ); <nl> + struct drm_display_mode * mode ; <nl> const struct tv_mode * tv_mode = & tv_modes [ i ]; <nl>  <nl> + mode = drm_mode_create ( connector -> dev ); <nl> + if (! mode ) { <nl> + DRM_ERROR (" Failed to create a new display mode \ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> strcpy ( mode -> name , tv_mode -> name ); <nl>  <nl> sun4i_tv_mode_to_drm_mode ( tv_mode , mode );
mmm drivers / mmc / host / renesas_sdhi_internal_dmac . c <nl> ppp drivers / mmc / host / renesas_sdhi_internal_dmac . c <nl> static const struct soc_device_attribute gen3_soc_whitelist [] = { <nl> /* generic ones */ <nl> { . soc_id = " r8a7795 " }, <nl> { . soc_id = " r8a7796 " }, <nl> + { . soc_id = " r8a77980 " }, <nl> { . soc_id = " r8a77995 " }, <nl> { /* sentinel */ } <nl> };
mmm net / ipv6 / ip6_gre . c <nl> ppp net / ipv6 / ip6_gre . c <nl> static netdev_tx_t ip6erspan_tunnel_xmit ( struct sk_buff * skb , <nl> truncate = true ; <nl> } <nl>  <nl> + if ( skb_cow_head ( skb , dev -> needed_headroom )) <nl> + goto tx_err ; <nl> + <nl> t -> parms . o_flags &= ~ TUNNEL_KEY ; <nl> IPCB ( skb )-> flags = 0 ; <nl> 
mmm drivers / media / video / videobuf - core . c <nl> ppp drivers / media / video / videobuf - core . c <nl> int videobuf_dqbuf ( struct videobuf_queue * q , <nl> goto done ; <nl> } <nl> buf = list_entry ( q -> stream . next , struct videobuf_buffer , stream ); <nl> + mutex_unlock (& q -> vb_lock ); <nl> retval = videobuf_waiton ( buf , nonblocking , 1 ); <nl> + mutex_lock (& q -> vb_lock ); <nl> if ( retval < 0 ) { <nl> dprintk ( 1 , " dqbuf : waiton returned % d \ n ", retval ); <nl> goto done ;
mmm fs / afs / volume . c <nl> ppp fs / afs / volume . c <nl> static struct afs_volume * afs_alloc_volume ( struct afs_mount_params * params , <nl> error_2 : <nl> afs_put_serverlist ( params -> net , slist ); <nl> error_1 : <nl> + afs_put_cell ( params -> net , volume -> cell ); <nl> kfree ( volume ); <nl> error_0 : <nl> return ERR_PTR ( ret );
mmm drivers / staging / comedi / drivers / ni_6527 . c <nl> ppp drivers / staging / comedi / drivers / ni_6527 . c <nl> static void ni6527_reset ( struct comedi_device * dev ) <nl> /* disable deglitch filters on all channels */ <nl> ni6527_set_filter_enable ( dev , 0 ); <nl>  <nl> + /* disable edge detection */ <nl> + ni6527_set_edge_detection ( dev , 0xffffffff , 0 , 0 ); <nl> + <nl> writeb ( NI6527_CLR_IRQS | NI6527_CLR_RESET_FILT , <nl> mmio + NI6527_CLR_REG ); <nl> writeb ( NI6527_CTRL_DISABLE_IRQS , mmio + NI6527_CTRL_REG );
mmm arch / powerpc / kvm / book3s_64_vio . c <nl> ppp arch / powerpc / kvm / book3s_64_vio . c <nl> long kvm_vm_ioctl_create_spapr_tce ( struct kvm * kvm , <nl> int ret = - ENOMEM ; <nl> int i ; <nl>  <nl> - if (! args -> size ) <nl> + if (! args -> size || args -> page_shift < 12 || args -> page_shift > 34 || <nl> + ( args -> offset + args -> size > ( ULLONG_MAX >> args -> page_shift ))) <nl> return - EINVAL ; <nl>  <nl> size = _ALIGN_UP ( args -> size , PAGE_SIZE >> 3 );
mmm fs / btrfs / volumes . c <nl> ppp fs / btrfs / volumes . c <nl> int btrfs_rm_device ( struct btrfs_fs_info * fs_info , const char * device_path , <nl>  <nl> if ( IS_ERR ( device )) { <nl> if ( PTR_ERR ( device ) == - ENOENT && <nl> - strcmp ( device_path , " missing ") == 0 ) <nl> + device_path && strcmp ( device_path , " missing ") == 0 ) <nl> ret = BTRFS_ERROR_DEV_MISSING_NOT_FOUND ; <nl> else <nl> ret = PTR_ERR ( device );
mmm include / linux / phy . h <nl> ppp include / linux / phy . h <nl> static inline bool phy_is_internal ( struct phy_device * phydev ) <nl> return phydev -> is_internal ; <nl> } <nl>  <nl> +/** <nl> + * phy_interface_is_rgmii - Convenience function for testing if a PHY interface <nl> + * is RGMII ( all variants ) <nl> + * @ phydev : the phy_device struct <nl> + */ <nl> + static inline bool phy_interface_is_rgmii ( struct phy_device * phydev ) <nl> +{ <nl> + return phydev -> interface >= PHY_INTERFACE_MODE_RGMII && <nl> + phydev -> interface <= PHY_INTERFACE_MODE_RGMII_TXID ; <nl> +} <nl> + <nl> /** <nl> * phy_write_mmd - Convenience function for writing a register <nl> * on an MMD on a given PHY .
mmm drivers / net / wireless / ti / wlcore / io . h <nl> ppp drivers / net / wireless / ti / wlcore / io . h <nl> static inline int __must_check wlcore_write_reg ( struct wl1271 * wl , int reg , <nl>  <nl> static inline void wl1271_power_off ( struct wl1271 * wl ) <nl> { <nl> - int ret ; <nl> + int ret = 0 ; <nl>  <nl> if (! test_bit ( WL1271_FLAG_GPIO_POWER , & wl -> flags )) <nl> return ; <nl>  <nl> - ret = wl -> if_ops -> power ( wl -> dev , false ); <nl> + if ( wl -> if_ops -> power ) <nl> + ret = wl -> if_ops -> power ( wl -> dev , false ); <nl> if (! ret ) <nl> clear_bit ( WL1271_FLAG_GPIO_POWER , & wl -> flags ); <nl> } <nl>  <nl> static inline int wl1271_power_on ( struct wl1271 * wl ) <nl> { <nl> - int ret = wl -> if_ops -> power ( wl -> dev , true ); <nl> + int ret = 0 ; <nl> + <nl> + if ( wl -> if_ops -> power ) <nl> + ret = wl -> if_ops -> power ( wl -> dev , true ); <nl> if ( ret == 0 ) <nl> set_bit ( WL1271_FLAG_GPIO_POWER , & wl -> flags ); <nl> 
mmm arch / powerpc / platforms / powernv / pci - ioda . c <nl> ppp arch / powerpc / platforms / powernv / pci - ioda . c <nl> void __init pnv_pci_init_ioda1_phb ( struct device_node * np ) <nl> /* Allocate aux data & arrays */ <nl> size = _ALIGN_UP ( phb -> ioda . total_pe / 8 , sizeof ( unsigned long )); <nl> m32map_off = size ; <nl> - size += phb -> ioda . total_pe ; <nl> + size += phb -> ioda . total_pe * sizeof ( phb -> ioda . m32_segmap [ 0 ]); <nl> iomap_off = size ; <nl> - size += phb -> ioda . total_pe ; <nl> + size += phb -> ioda . total_pe * sizeof ( phb -> ioda . io_segmap [ 0 ]); <nl> pemap_off = size ; <nl> size += phb -> ioda . total_pe * sizeof ( struct pnv_ioda_pe ); <nl> aux = alloc_bootmem ( size );
mmm net / mac802154 / main . c <nl> ppp net / mac802154 / main . c <nl> EXPORT_SYMBOL ( ieee802154_free_hw ); <nl> int ieee802154_register_hw ( struct ieee802154_hw * hw ) <nl> { <nl> struct ieee802154_local * local = hw_to_local ( hw ); <nl> + struct net_device * dev ; <nl> int rc = - ENOSYS ; <nl>  <nl> local -> workqueue = <nl> int ieee802154_register_hw ( struct ieee802154_hw * hw ) <nl> if ( rc < 0 ) <nl> goto out_wq ; <nl>  <nl> + rtnl_lock (); <nl> + <nl> + dev = ieee802154_if_add ( local , " wpan % d ", NULL , IEEE802154_DEV_WPAN ); <nl> + if ( IS_ERR ( dev )) { <nl> + rtnl_unlock (); <nl> + rc = PTR_ERR ( dev ); <nl> + goto out_wq ; <nl> + } <nl> + <nl> + rtnl_unlock (); <nl> + <nl> return 0 ; <nl>  <nl> out_wq :
mmm fs / befs / linuxvfs . c <nl> ppp fs / befs / linuxvfs . c <nl> static void init_once ( void * foo ) <nl>  <nl> static struct inode * befs_iget ( struct super_block * sb , unsigned long ino ) <nl> { <nl> - struct buffer_head * bh = NULL ; <nl> + struct buffer_head * bh ; <nl> befs_inode * raw_inode = NULL ; <nl> struct befs_sb_info * befs_sb = BEFS_SB ( sb ); <nl> struct befs_inode_info * befs_ino = NULL ;
mmm drivers / net / ethernet / mellanox / mlxsw / pci . c <nl> ppp drivers / net / ethernet / mellanox / mlxsw / pci . c <nl> static void mlxsw_pci_eq_tasklet ( unsigned long data ) <nl> { <nl> struct mlxsw_pci_queue * q = ( struct mlxsw_pci_queue *) data ; <nl> struct mlxsw_pci * mlxsw_pci = q -> pci ; <nl> - unsigned long active_cqns [ BITS_TO_LONGS ( MLXSW_PCI_CQS_COUNT )]; <nl> + u8 cq_count = mlxsw_pci_cq_count ( mlxsw_pci ); <nl> + unsigned long active_cqns [ BITS_TO_LONGS ( MLXSW_PCI_CQS_MAX )]; <nl> char * eqe ; <nl> u8 cqn ; <nl> bool cq_handle = false ; <nl> static void mlxsw_pci_eq_tasklet ( unsigned long data ) <nl>  <nl> if (! cq_handle ) <nl> return ; <nl> - for_each_set_bit ( cqn , active_cqns , MLXSW_PCI_CQS_COUNT ) { <nl> + for_each_set_bit ( cqn , active_cqns , cq_count ) { <nl> q = mlxsw_pci_cq_get ( mlxsw_pci , cqn ); <nl> mlxsw_pci_queue_tasklet_schedule ( q ); <nl> } <nl> static int mlxsw_pci_aqs_init ( struct mlxsw_pci * mlxsw_pci , char * mbox ) <nl>  <nl> if (( num_sdqs != MLXSW_PCI_SDQS_COUNT ) || <nl> ( num_rdqs != MLXSW_PCI_RDQS_COUNT ) || <nl> - ( num_cqs != MLXSW_PCI_CQS_COUNT ) || <nl> - ( num_eqs != MLXSW_PCI_EQS_COUNT )) { <nl> + num_cqs > MLXSW_PCI_CQS_MAX || num_eqs != MLXSW_PCI_EQS_COUNT ) { <nl> dev_err (& pdev -> dev , " Unsupported number of queues \ n "); <nl> return - EINVAL ; <nl> }mmm drivers / net / ethernet / mellanox / mlxsw / pci . h <nl> ppp drivers / net / ethernet / mellanox / mlxsw / pci . h <nl> static void mlxsw_pci_eq_tasklet ( unsigned long data ) <nl> { <nl> struct mlxsw_pci_queue * q = ( struct mlxsw_pci_queue *) data ; <nl> struct mlxsw_pci * mlxsw_pci = q -> pci ; <nl> - unsigned long active_cqns [ BITS_TO_LONGS ( MLXSW_PCI_CQS_COUNT )]; <nl> + u8 cq_count = mlxsw_pci_cq_count ( mlxsw_pci ); <nl> + unsigned long active_cqns [ BITS_TO_LONGS ( MLXSW_PCI_CQS_MAX )]; <nl> char * eqe ; <nl> u8 cqn ; <nl> bool cq_handle = false ; <nl> static void mlxsw_pci_eq_tasklet ( unsigned long data ) <nl>  <nl> if (! cq_handle ) <nl> return ; <nl> - for_each_set_bit ( cqn , active_cqns , MLXSW_PCI_CQS_COUNT ) { <nl> + for_each_set_bit ( cqn , active_cqns , cq_count ) { <nl> q = mlxsw_pci_cq_get ( mlxsw_pci , cqn ); <nl> mlxsw_pci_queue_tasklet_schedule ( q ); <nl> } <nl> static int mlxsw_pci_aqs_init ( struct mlxsw_pci * mlxsw_pci , char * mbox ) <nl>  <nl> if (( num_sdqs != MLXSW_PCI_SDQS_COUNT ) || <nl> ( num_rdqs != MLXSW_PCI_RDQS_COUNT ) || <nl> - ( num_cqs != MLXSW_PCI_CQS_COUNT ) || <nl> - ( num_eqs != MLXSW_PCI_EQS_COUNT )) { <nl> + num_cqs > MLXSW_PCI_CQS_MAX || num_eqs != MLXSW_PCI_EQS_COUNT ) { <nl> dev_err (& pdev -> dev , " Unsupported number of queues \ n "); <nl> return - EINVAL ; <nl> } <nl>  <nl> # define MLXSW_PCI_RDQS_COUNT 24 <nl> # define MLXSW_PCI_SDQS_COUNT 24 <nl> -# define MLXSW_PCI_CQS_COUNT ( MLXSW_PCI_RDQS_COUNT + MLXSW_PCI_SDQS_COUNT ) <nl> +# define MLXSW_PCI_CQS_MAX 96 <nl> # define MLXSW_PCI_EQS_COUNT 2 <nl> # define MLXSW_PCI_EQ_ASYNC_NUM 0 <nl> # define MLXSW_PCI_EQ_COMP_NUM 1
mmm drivers / staging / lustre / lustre / libcfs / module . c <nl> ppp drivers / staging / lustre / lustre / libcfs / module . c <nl> static int __proc_dobitmasks ( void * data , int write , <nl> } else { <nl> rc = cfs_trace_copyin_string ( tmpstr , tmpstrlen , buffer , nob ); <nl> if ( rc < 0 ) { <nl> - cfs_trace_free_string_buffer ( tmpstr , tmpstrlen ); <nl> + kfree ( tmpstr ); <nl> return rc ; <nl> } <nl>  <nl> static int __proc_dobitmasks ( void * data , int write , <nl> * mask |= D_EMERG ; <nl> } <nl>  <nl> - cfs_trace_free_string_buffer ( tmpstr , tmpstrlen ); <nl> + kfree ( tmpstr ); <nl> return rc ; <nl> } <nl> 
mmm drivers / net / wireless / rndis_wlan . c <nl> ppp drivers / net / wireless / rndis_wlan . c <nl> static void rndis_wlan_media_specific_indication ( struct usbnet * usbdev , <nl> struct rndis_indicate * msg , int buflen ) <nl> { <nl> struct ndis_80211_status_indication * indication ; <nl> - int len , offset ; <nl> + unsigned int len , offset ; <nl>  <nl> offset = offsetof ( struct rndis_indicate , status ) + <nl> le32_to_cpu ( msg -> offset ); <nl> static void rndis_wlan_media_specific_indication ( struct usbnet * usbdev , <nl> return ; <nl> } <nl>  <nl> - if ( offset + len > buflen ) { <nl> + if ( len > buflen || offset > buflen || offset + len > buflen ) { <nl> netdev_info ( usbdev -> net , " media specific indication , too large to fit to buffer (% i > % i )\ n ", <nl> offset + len , buflen ); <nl> return ;
mmm sound / core / timer . c <nl> ppp sound / core / timer . c <nl> static void snd_timer_user_tinterrupt ( struct snd_timer_instance * timeri , <nl> } <nl> if (( tu -> filter & ( 1 << SNDRV_TIMER_EVENT_RESOLUTION )) && <nl> tu -> last_resolution != resolution ) { <nl> + memset (& r1 , 0 , sizeof ( r1 )); <nl> r1 . event = SNDRV_TIMER_EVENT_RESOLUTION ; <nl> r1 . tstamp = tstamp ; <nl> r1 . val = resolution ;
mmm drivers / cdrom / cdrom . c <nl> ppp drivers / cdrom / cdrom . c <nl> static int cdrom_ioctl_select_disc ( struct cdrom_device_info * cdi , <nl> return - ENOSYS ; <nl>  <nl> if ( arg != CDSL_CURRENT && arg != CDSL_NONE ) { <nl> - if (( int ) arg >= cdi -> capacity ) <nl> + if ( arg >= cdi -> capacity ) <nl> return - EINVAL ; <nl> } <nl> 
mmm drivers / net / vmxnet3 / vmxnet3_ethtool . c <nl> ppp drivers / net / vmxnet3 / vmxnet3_ethtool . c <nl> vmxnet3_set_ringparam ( struct net_device * netdev , <nl> VMXNET3_RX_RING_MAX_SIZE ) <nl> return - EINVAL ; <nl>  <nl> + /* if adapter not yet initialized , do nothing */ <nl> + if ( adapter -> rx_buf_per_pkt == 0 ) { <nl> + netdev_err ( netdev , " adapter not completely initialized , " <nl> + " ring size cannot be changed yet \ n "); <nl> + return - EOPNOTSUPP ; <nl> + } <nl>  <nl> /* round it up to a multiple of VMXNET3_RING_SIZE_ALIGN */ <nl> new_tx_ring_size = ( param -> tx_pending + VMXNET3_RING_SIZE_MASK ) &mmm drivers / net / vmxnet3 / vmxnet3_int . h <nl> ppp drivers / net / vmxnet3 / vmxnet3_int . h <nl> vmxnet3_set_ringparam ( struct net_device * netdev , <nl> VMXNET3_RX_RING_MAX_SIZE ) <nl> return - EINVAL ; <nl>  <nl> + /* if adapter not yet initialized , do nothing */ <nl> + if ( adapter -> rx_buf_per_pkt == 0 ) { <nl> + netdev_err ( netdev , " adapter not completely initialized , " <nl> + " ring size cannot be changed yet \ n "); <nl> + return - EOPNOTSUPP ; <nl> + } <nl>  <nl> /* round it up to a multiple of VMXNET3_RING_SIZE_ALIGN */ <nl> new_tx_ring_size = ( param -> tx_pending + VMXNET3_RING_SIZE_MASK ) & <nl> /* <nl> * Version numbers <nl> */ <nl> -# define VMXNET3_DRIVER_VERSION_STRING " 1 . 1 . 29 . 0 - k " <nl> +# define VMXNET3_DRIVER_VERSION_STRING " 1 . 1 . 30 . 0 - k " <nl>  <nl> /* a 32 - bit int , each byte encode a verion number in VMXNET3_DRIVER_VERSION */ <nl> -# define VMXNET3_DRIVER_VERSION_NUM 0x01011D00 <nl> +# define VMXNET3_DRIVER_VERSION_NUM 0x01011E00 <nl>  <nl> # if defined ( CONFIG_PCI_MSI ) <nl> /* RSS only makes sense if MSI - X is supported . */mmm drivers / net / vmxnet3 / vmxnet3_drv . c <nl> ppp drivers / net / vmxnet3 / vmxnet3_drv . c <nl> vmxnet3_set_ringparam ( struct net_device * netdev , <nl> VMXNET3_RX_RING_MAX_SIZE ) <nl> return - EINVAL ; <nl>  <nl> + /* if adapter not yet initialized , do nothing */ <nl> + if ( adapter -> rx_buf_per_pkt == 0 ) { <nl> + netdev_err ( netdev , " adapter not completely initialized , " <nl> + " ring size cannot be changed yet \ n "); <nl> + return - EOPNOTSUPP ; <nl> + } <nl>  <nl> /* round it up to a multiple of VMXNET3_RING_SIZE_ALIGN */ <nl> new_tx_ring_size = ( param -> tx_pending + VMXNET3_RING_SIZE_MASK ) & <nl> /* <nl> * Version numbers <nl> */ <nl> -# define VMXNET3_DRIVER_VERSION_STRING " 1 . 1 . 29 . 0 - k " <nl> +# define VMXNET3_DRIVER_VERSION_STRING " 1 . 1 . 30 . 0 - k " <nl>  <nl> /* a 32 - bit int , each byte encode a verion number in VMXNET3_DRIVER_VERSION */ <nl> -# define VMXNET3_DRIVER_VERSION_NUM 0x01011D00 <nl> +# define VMXNET3_DRIVER_VERSION_NUM 0x01011E00 <nl>  <nl> # if defined ( CONFIG_PCI_MSI ) <nl> /* RSS only makes sense if MSI - X is supported . */ <nl> vmxnet3_probe_device ( struct pci_dev * pdev , <nl>  <nl> adapter -> num_rx_queues = num_rx_queues ; <nl> adapter -> num_tx_queues = num_tx_queues ; <nl> + adapter -> rx_buf_per_pkt = 1 ; <nl>  <nl> size = sizeof ( struct Vmxnet3_TxQueueDesc ) * adapter -> num_tx_queues ; <nl> size += sizeof ( struct Vmxnet3_RxQueueDesc ) * adapter -> num_rx_queues ;
mmm mm / memory . c <nl> ppp mm / memory . c <nl> int handle_mm_fault ( struct mm_struct * mm , struct vm_area_struct * vma , <nl> if ( pmd_trans_huge ( orig_pmd )) { <nl> unsigned int dirty = flags & FAULT_FLAG_WRITE ; <nl>  <nl> + /* <nl> + * If the pmd is splitting , return and retry the <nl> + * the fault . Alternative : wait until the split <nl> + * is done , and goto retry . <nl> + */ <nl> + if ( pmd_trans_splitting ( orig_pmd )) <nl> + return 0 ; <nl> + <nl> if ( pmd_numa ( orig_pmd )) <nl> return do_huge_pmd_numa_page ( mm , vma , address , <nl> orig_pmd , pmd );
mmm drivers / net / hyperv / netvsc . c <nl> ppp drivers / net / hyperv / netvsc . c <nl> static int netvsc_init_buf ( struct hv_device * device ) <nl> net_device -> map_words = DIV_ROUND_UP ( net_device -> send_section_cnt , <nl> BITS_PER_LONG ); <nl>  <nl> - net_device -> send_section_map = <nl> - kzalloc ( net_device -> map_words * sizeof ( ulong ), GFP_KERNEL ); <nl> + net_device -> send_section_map = kcalloc ( net_device -> map_words , <nl> + sizeof ( ulong ), GFP_KERNEL ); <nl> if ( net_device -> send_section_map == NULL ) { <nl> ret = - ENOMEM ; <nl> goto cleanup ;
mmm net / core / sock . c <nl> ppp net / core / sock . c <nl> EXPORT_SYMBOL ( sock_kmalloc ); <nl> */ <nl> void sock_kfree_s ( struct sock * sk , void * mem , int size ) <nl> { <nl> + if ( WARN_ON_ONCE (! mem )) <nl> + return ; <nl> kfree ( mem ); <nl> atomic_sub ( size , & sk -> sk_omem_alloc ); <nl> }
mmm fs / proc / root . c <nl> ppp fs / proc / root . c <nl> static struct dentry * proc_mount ( struct file_system_type * fs_type , <nl> if ( IS_ERR ( sb )) <nl> return ERR_CAST ( sb ); <nl>  <nl> + /* <nl> + * procfs isn ' t actually a stacking filesystem ; however , there is <nl> + * too much magic going on inside it to permit stacking things on <nl> + * top of it <nl> + */ <nl> + sb -> s_stack_depth = FILESYSTEM_MAX_STACK_DEPTH ; <nl> + <nl> if (! proc_parse_options ( options , ns )) { <nl> deactivate_locked_super ( sb ); <nl> return ERR_PTR (- EINVAL );
mmm drivers / net / ethernet / intel / i40e / i40e_main . c <nl> ppp drivers / net / ethernet / intel / i40e / i40e_main . c <nl> static void i40e_service_task ( struct work_struct * work ) <nl> service_task ); <nl> unsigned long start_time = jiffies ; <nl>  <nl> + /* don ' t bother with service tasks if a reset is in progress */ <nl> + if ( test_bit ( __I40E_RESET_RECOVERY_PENDING , & pf -> state )) { <nl> + i40e_service_event_complete ( pf ); <nl> + return ; <nl> + } <nl> + <nl> i40e_reset_subtask ( pf ); <nl> i40e_handle_mdd_event ( pf ); <nl> i40e_vc_process_vflr_event ( pf );
mmm arch / arm / mach - sa1100 / cpu - sa1110 . c <nl> ppp arch / arm / mach - sa1100 / cpu - sa1110 . c <nl> static int __init sa1110_clk_init ( void ) <nl> struct sdram_params * sdram ; <nl> const char * name = sdram_name ; <nl>  <nl> + if (! cpu_is_sa1110 ()) <nl> + return - ENODEV ; <nl> + <nl> if (! name [ 0 ]) { <nl> if ( machine_is_assabet ()) <nl> name = " TC59SM716 - CL3 ";
mmm drivers / mtd / nand / diskonchip . c <nl> ppp drivers / mtd / nand / diskonchip . c <nl> * <nl> * Interface to generic NAND code for M - Systems DiskOnChip devices <nl> * <nl> - * $ Id : diskonchip . c , v 1 . 50 2005 / 03 / 29 20 : 57 : 45 dbrown Exp $ <nl> + * $ Id : diskonchip . c , v 1 . 51 2005 / 04 / 06 18 : 10 : 20 dbrown Exp $ <nl> */ <nl>  <nl> # include < linux / kernel . h > <nl> static struct nand_oobinfo doc200x_oobinfo = { <nl> . useecc = MTD_NANDECC_AUTOPLACE , <nl> . eccbytes = 6 , <nl> . eccpos = { 0 , 1 , 2 , 3 , 4 , 5 }, <nl> - . oobfree = { { 8 , 8 } } <nl> + . oobfree = { { 6 , 10 } } <nl> }; <nl>  <nl> /* Find the ( I ) NFTL Media Header , and optionally also the mirror media header .
mmm drivers / net / wireless / ath / ath10k / wmi . c <nl> ppp drivers / net / wireless / ath / ath10k / wmi . c <nl> ath10k_wmi_10_4_gen_update_fw_tdls_state ( struct ath10k * ar , u32 vdev_id , <nl> if (! skb ) <nl> return ERR_PTR (- ENOMEM ); <nl>  <nl> - if ( test_bit ( WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY , ar -> wmi . svc_map )) <nl> + if ( test_bit ( WMI_SERVICE_TDLS_EXPLICIT_MODE_ONLY , ar -> wmi . svc_map ) && <nl> + state == WMI_TDLS_ENABLE_ACTIVE ) <nl> state = WMI_TDLS_ENABLE_PASSIVE ; <nl>  <nl> if ( test_bit ( WMI_SERVICE_TDLS_UAPSD_BUFFER_STA , ar -> wmi . svc_map ))
mmm drivers / power / max17042_battery . c <nl> ppp drivers / power / max17042_battery . c <nl> static int __devinit max17042_probe ( struct i2c_client * client , <nl> reg |= CONFIG_ALRT_BIT_ENBL ; <nl> max17042_write_reg ( client , MAX17042_CONFIG , reg ); <nl> max17042_set_soc_threshold ( chip , 1 ); <nl> - } else <nl> + } else { <nl> + client -> irq = 0 ; <nl> dev_err (& client -> dev , "% s (): cannot get IRQ \ n ", <nl> __func__ ); <nl> + } <nl> } <nl>  <nl> reg = max17042_read_reg ( chip -> client , MAX17042_STATUS );
mmm drivers / mtd / mtdpart . c <nl> ppp drivers / mtd / mtdpart . c <nl> int add_mtd_partitions ( struct mtd_info * master , <nl>  <nl> for ( i = 0 ; i < nbparts ; i ++) { <nl> slave = allocate_partition ( master , parts + i , i , cur_offset ); <nl> - if ( IS_ERR ( slave )) <nl> + if ( IS_ERR ( slave )) { <nl> + del_mtd_partitions ( master ); <nl> return PTR_ERR ( slave ); <nl> + } <nl>  <nl> mutex_lock (& mtd_partitions_mutex ); <nl> list_add (& slave -> list , & mtd_partitions );
mmm arch / powerpc / platforms / cell / spufs / sched . c <nl> ppp arch / powerpc / platforms / cell / spufs / sched . c <nl> void spu_deactivate ( struct spu_context * ctx ) <nl> */ <nl> void spu_yield ( struct spu_context * ctx ) <nl> { <nl> - mutex_lock (& ctx -> state_mutex ); <nl> - __spu_deactivate ( ctx , 0 , MAX_PRIO ); <nl> - mutex_unlock (& ctx -> state_mutex ); <nl> + if (!( ctx -> flags & SPU_CREATE_NOSCHED )) { <nl> + mutex_lock (& ctx -> state_mutex ); <nl> + __spu_deactivate ( ctx , 0 , MAX_PRIO ); <nl> + mutex_unlock (& ctx -> state_mutex ); <nl> + } <nl> } <nl>  <nl> void spu_sched_tick ( struct work_struct * work )
mmm drivers / gpu / drm / vmwgfx / vmwgfx_kms . c <nl> ppp drivers / gpu / drm / vmwgfx / vmwgfx_kms . c <nl> int vmw_du_crtc_cursor_set ( struct drm_crtc * crtc , struct drm_file * file_priv , <nl> if (! ret ) { <nl> if (! surface -> snooper . image ) { <nl> DRM_ERROR (" surface not suitable for cursor \ n "); <nl> + vmw_surface_unreference (& surface ); <nl> return - EINVAL ; <nl> } <nl> } else {
mmm drivers / net / ethernet / mellanox / mlx4 / resource_tracker . c <nl> ppp drivers / net / ethernet / mellanox / mlx4 / resource_tracker . c <nl> static void adjust_proxy_tun_qkey ( struct mlx4_dev * dev , struct mlx4_vhcr * vhcr , <nl> context -> qkey = cpu_to_be32 ( qkey ); <nl> } <nl>  <nl> + static int adjust_qp_sched_queue ( struct mlx4_dev * dev , int slave , <nl> + struct mlx4_qp_context * qpc , <nl> + struct mlx4_cmd_mailbox * inbox ); <nl> + <nl> int mlx4_RST2INIT_QP_wrapper ( struct mlx4_dev * dev , int slave , <nl> struct mlx4_vhcr * vhcr , <nl> struct mlx4_cmd_mailbox * inbox , <nl> int mlx4_RST2INIT_QP_wrapper ( struct mlx4_dev * dev , int slave , <nl> struct res_srq * srq ; <nl> int local_qpn = be32_to_cpu ( qpc -> local_qpn ) & 0xffffff ; <nl>  <nl> + err = adjust_qp_sched_queue ( dev , slave , qpc , inbox ); <nl> + if ( err ) <nl> + return err ; <nl> + <nl> err = qp_res_start_move_to ( dev , slave , qpn , RES_QP_HW , & qp , 0 ); <nl> if ( err ) <nl> return err ;
mmm drivers / usb / core / quirks . c <nl> ppp drivers / usb / core / quirks . c <nl> static const struct usb_device_id usb_quirk_list [] = { <nl> { USB_DEVICE ( 0x0b05 , 0x17e0 ), . driver_info = <nl> USB_QUIRK_IGNORE_REMOTE_WAKEUP }, <nl>  <nl> + /* Protocol and OTG Electrical Test Device */ <nl> + { USB_DEVICE ( 0x1a0a , 0x0200 ), . driver_info = <nl> + USB_QUIRK_LINEAR_UFRAME_INTR_BINTERVAL }, <nl> + <nl> { } /* terminating entry must be last */ <nl> }; <nl> mmm drivers / usb / core / otg_whitelist . h <nl> ppp drivers / usb / core / otg_whitelist . h <nl> static const struct usb_device_id usb_quirk_list [] = { <nl> { USB_DEVICE ( 0x0b05 , 0x17e0 ), . driver_info = <nl> USB_QUIRK_IGNORE_REMOTE_WAKEUP }, <nl>  <nl> + /* Protocol and OTG Electrical Test Device */ <nl> + { USB_DEVICE ( 0x1a0a , 0x0200 ), . driver_info = <nl> + USB_QUIRK_LINEAR_UFRAME_INTR_BINTERVAL }, <nl> + <nl> { } /* terminating entry must be last */ <nl> }; <nl>  <nl> static int is_targeted ( struct usb_device * dev ) <nl> le16_to_cpu ( dev -> descriptor . idProduct ) == 0xbadd )) <nl> return 0 ; <nl>  <nl> + /* OTG PET device is always targeted ( see OTG 2 . 0 ECN 6 . 4 . 2 ) */ <nl> + if (( le16_to_cpu ( dev -> descriptor . idVendor ) == 0x1a0a && <nl> + le16_to_cpu ( dev -> descriptor . idProduct ) == 0x0200 )) <nl> + return 1 ; <nl> + <nl> /* NOTE : can ' t use usb_match_id () since interface caches <nl> * aren ' t set up yet . this is cut / paste from that code . <nl> */
mmm net / netfilter / nf_tables_api . c <nl> ppp net / netfilter / nf_tables_api . c <nl> static int nf_tables_delflowtable ( struct net * net , struct sock * nlsk , <nl> struct nft_table * table ; <nl> struct nft_ctx ctx ; <nl>  <nl> + if (! nla [ NFTA_FLOWTABLE_TABLE ] || <nl> + (! nla [ NFTA_FLOWTABLE_NAME ] && <nl> + ! nla [ NFTA_FLOWTABLE_HANDLE ])) <nl> + return - EINVAL ; <nl> + <nl> table = nf_tables_table_lookup ( net , nla [ NFTA_FLOWTABLE_TABLE ], <nl> family , genmask ); <nl> if ( IS_ERR ( table ))
mmm sound / soc / soc - jack . c <nl> ppp sound / soc / soc - jack . c <nl> int snd_soc_jack_add_gpios ( struct snd_soc_jack * jack , int count , <nl> goto undo ; <nl> } <nl>  <nl> - if ( gpios [ i ]. gpiod_dev ) { <nl> - /* GPIO descriptor */ <nl> + if ( gpios [ i ]. desc ) { <nl> + /* Already have a GPIO descriptor . */ <nl> + goto got_gpio ; <nl> + } else if ( gpios [ i ]. gpiod_dev ) { <nl> + /* Get a GPIO descriptor */ <nl> gpios [ i ]. desc = gpiod_get_index ( gpios [ i ]. gpiod_dev , <nl> gpios [ i ]. name , <nl> gpios [ i ]. idx , GPIOD_IN ); <nl> int snd_soc_jack_add_gpios ( struct snd_soc_jack * jack , int count , <nl>  <nl> gpios [ i ]. desc = gpio_to_desc ( gpios [ i ]. gpio ); <nl> } <nl> - <nl> + got_gpio : <nl> INIT_DELAYED_WORK (& gpios [ i ]. work , gpio_work ); <nl> gpios [ i ]. jack = jack ; <nl> 
mmm net / netfilter / xt_time . c <nl> ppp net / netfilter / xt_time . c <nl> static struct xt_match xt_time_mt_reg __read_mostly = { <nl>  <nl> static int __init time_mt_init ( void ) <nl> { <nl> + int minutes = sys_tz . tz_minuteswest ; <nl> + <nl> + if ( minutes < 0 ) /* east of Greenwich */ <nl> + printk ( KERN_INFO KBUILD_MODNAME <nl> + ": kernel timezone is +% 02d % 02d \ n ", <nl> + - minutes / 60 , - minutes % 60 ); <nl> + else /* west of Greenwich */ <nl> + printk ( KERN_INFO KBUILD_MODNAME <nl> + ": kernel timezone is -% 02d % 02d \ n ", <nl> + minutes / 60 , minutes % 60 ); <nl> + <nl> return xt_register_match (& xt_time_mt_reg ); <nl> } <nl> 
mmm drivers / media / platform / coda / coda - bit . c <nl> ppp drivers / media / platform / coda / coda - bit . c <nl> static int coda_alloc_framebuffers ( struct coda_ctx * ctx , <nl> dev -> devtype -> product != CODA_DX6 ) <nl> size += ysize / 4 ; <nl> name = kasprintf ( GFP_KERNEL , " fb % d ", i ); <nl> + if (! name ) { <nl> + coda_free_framebuffers ( ctx ); <nl> + return - ENOMEM ; <nl> + } <nl> ret = coda_alloc_context_buf ( ctx , & ctx -> internal_frames [ i ], <nl> size , name ); <nl> kfree ( name );
mmm net / mac80211 / iface . c <nl> ppp net / mac80211 / iface . c <nl> int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl>  <nl> ret = dev_alloc_name ( ndev , ndev -> name ); <nl> if ( ret < 0 ) { <nl> - free_netdev ( ndev ); <nl> + ieee80211_if_free ( ndev ); <nl> return ret ; <nl> } <nl>  <nl> int ieee80211_if_add ( struct ieee80211_local * local , const char * name , <nl>  <nl> ret = register_netdevice ( ndev ); <nl> if ( ret ) { <nl> - free_netdev ( ndev ); <nl> + ieee80211_if_free ( ndev ); <nl> return ret ; <nl> } <nl> }
mmm arch / x86_64 / kernel / pci - swiotlb . c <nl> ppp arch / x86_64 / kernel / pci - swiotlb . c <nl> int swiotlb __read_mostly ; <nl> EXPORT_SYMBOL ( swiotlb ); <nl>  <nl> - struct dma_mapping_ops swiotlb_dma_ops = { <nl> + const struct dma_mapping_ops swiotlb_dma_ops = { <nl> . mapping_error = swiotlb_dma_mapping_error , <nl> . alloc_coherent = swiotlb_alloc_coherent , <nl> . free_coherent = swiotlb_free_coherent ,mmm arch / x86_64 / kernel / pci - calgary . c <nl> ppp arch / x86_64 / kernel / pci - calgary . c <nl> int swiotlb __read_mostly ; <nl> EXPORT_SYMBOL ( swiotlb ); <nl>  <nl> - struct dma_mapping_ops swiotlb_dma_ops = { <nl> + const struct dma_mapping_ops swiotlb_dma_ops = { <nl> . mapping_error = swiotlb_dma_mapping_error , <nl> . alloc_coherent = swiotlb_alloc_coherent , <nl> . free_coherent = swiotlb_free_coherent , <nl> void * calgary_alloc_coherent ( struct device * dev , size_t size , <nl> return ret ; <nl> } <nl>  <nl> - static struct dma_mapping_ops calgary_dma_ops = { <nl> + static const struct dma_mapping_ops calgary_dma_ops = { <nl> . alloc_coherent = calgary_alloc_coherent , <nl> . map_single = calgary_map_single , <nl> . unmap_single = calgary_unmap_single ,mmm arch / x86_64 / kernel / pci - gart . c <nl> ppp arch / x86_64 / kernel / pci - gart . c <nl> int swiotlb __read_mostly ; <nl> EXPORT_SYMBOL ( swiotlb ); <nl>  <nl> - struct dma_mapping_ops swiotlb_dma_ops = { <nl> + const struct dma_mapping_ops swiotlb_dma_ops = { <nl> . mapping_error = swiotlb_dma_mapping_error , <nl> . alloc_coherent = swiotlb_alloc_coherent , <nl> . free_coherent = swiotlb_free_coherent , <nl> void * calgary_alloc_coherent ( struct device * dev , size_t size , <nl> return ret ; <nl> } <nl>  <nl> - static struct dma_mapping_ops calgary_dma_ops = { <nl> + static const struct dma_mapping_ops calgary_dma_ops = { <nl> . alloc_coherent = calgary_alloc_coherent , <nl> . map_single = calgary_map_single , <nl> . unmap_single = calgary_unmap_single , <nl> static __init int init_k8_gatt ( struct agp_kern_info * info ) <nl>  <nl> extern int agp_amd64_init ( void ); <nl>  <nl> - static struct dma_mapping_ops gart_dma_ops = { <nl> + static const struct dma_mapping_ops gart_dma_ops = { <nl> . mapping_error = NULL , <nl> . map_single = gart_map_single , <nl> . map_simple = gart_map_simple ,mmm arch / x86_64 / kernel / pci - nommu . c <nl> ppp arch / x86_64 / kernel / pci - nommu . c <nl> int swiotlb __read_mostly ; <nl> EXPORT_SYMBOL ( swiotlb ); <nl>  <nl> - struct dma_mapping_ops swiotlb_dma_ops = { <nl> + const struct dma_mapping_ops swiotlb_dma_ops = { <nl> . mapping_error = swiotlb_dma_mapping_error , <nl> . alloc_coherent = swiotlb_alloc_coherent , <nl> . free_coherent = swiotlb_free_coherent , <nl> void * calgary_alloc_coherent ( struct device * dev , size_t size , <nl> return ret ; <nl> } <nl>  <nl> - static struct dma_mapping_ops calgary_dma_ops = { <nl> + static const struct dma_mapping_ops calgary_dma_ops = { <nl> . alloc_coherent = calgary_alloc_coherent , <nl> . map_single = calgary_map_single , <nl> . unmap_single = calgary_unmap_single , <nl> static __init int init_k8_gatt ( struct agp_kern_info * info ) <nl>  <nl> extern int agp_amd64_init ( void ); <nl>  <nl> - static struct dma_mapping_ops gart_dma_ops = { <nl> + static const struct dma_mapping_ops gart_dma_ops = { <nl> . mapping_error = NULL , <nl> . map_single = gart_map_single , <nl> . map_simple = gart_map_simple , <nl> void nommu_unmap_sg ( struct device * dev , struct scatterlist * sg , <nl> { <nl> } <nl>  <nl> - struct dma_mapping_ops nommu_dma_ops = { <nl> + const struct dma_mapping_ops nommu_dma_ops = { <nl> . map_single = nommu_map_single , <nl> . unmap_single = nommu_unmap_single , <nl> . map_sg = nommu_map_sg ,mmm arch / x86_64 / mm / init . c <nl> ppp arch / x86_64 / mm / init . c <nl> int swiotlb __read_mostly ; <nl> EXPORT_SYMBOL ( swiotlb ); <nl>  <nl> - struct dma_mapping_ops swiotlb_dma_ops = { <nl> + const struct dma_mapping_ops swiotlb_dma_ops = { <nl> . mapping_error = swiotlb_dma_mapping_error , <nl> . alloc_coherent = swiotlb_alloc_coherent , <nl> . free_coherent = swiotlb_free_coherent , <nl> void * calgary_alloc_coherent ( struct device * dev , size_t size , <nl> return ret ; <nl> } <nl>  <nl> - static struct dma_mapping_ops calgary_dma_ops = { <nl> + static const struct dma_mapping_ops calgary_dma_ops = { <nl> . alloc_coherent = calgary_alloc_coherent , <nl> . map_single = calgary_map_single , <nl> . unmap_single = calgary_unmap_single , <nl> static __init int init_k8_gatt ( struct agp_kern_info * info ) <nl>  <nl> extern int agp_amd64_init ( void ); <nl>  <nl> - static struct dma_mapping_ops gart_dma_ops = { <nl> + static const struct dma_mapping_ops gart_dma_ops = { <nl> . mapping_error = NULL , <nl> . map_single = gart_map_single , <nl> . map_simple = gart_map_simple , <nl> void nommu_unmap_sg ( struct device * dev , struct scatterlist * sg , <nl> { <nl> } <nl>  <nl> - struct dma_mapping_ops nommu_dma_ops = { <nl> + const struct dma_mapping_ops nommu_dma_ops = { <nl> . map_single = nommu_map_single , <nl> . unmap_single = nommu_unmap_single , <nl> . map_sg = nommu_map_sg , <nl> # define Dprintk ( x ...) <nl> # endif <nl>  <nl> - struct dma_mapping_ops * dma_ops ; <nl> + const struct dma_mapping_ops * dma_ops ; <nl> EXPORT_SYMBOL ( dma_ops ); <nl>  <nl> static unsigned long dma_reserve __initdata ;mmm include / asm - x86_64 / dma - mapping . h <nl> ppp include / asm - x86_64 / dma - mapping . h <nl> int swiotlb __read_mostly ; <nl> EXPORT_SYMBOL ( swiotlb ); <nl>  <nl> - struct dma_mapping_ops swiotlb_dma_ops = { <nl> + const struct dma_mapping_ops swiotlb_dma_ops = { <nl> . mapping_error = swiotlb_dma_mapping_error , <nl> . alloc_coherent = swiotlb_alloc_coherent , <nl> . free_coherent = swiotlb_free_coherent , <nl> void * calgary_alloc_coherent ( struct device * dev , size_t size , <nl> return ret ; <nl> } <nl>  <nl> - static struct dma_mapping_ops calgary_dma_ops = { <nl> + static const struct dma_mapping_ops calgary_dma_ops = { <nl> . alloc_coherent = calgary_alloc_coherent , <nl> . map_single = calgary_map_single , <nl> . unmap_single = calgary_unmap_single , <nl> static __init int init_k8_gatt ( struct agp_kern_info * info ) <nl>  <nl> extern int agp_amd64_init ( void ); <nl>  <nl> - static struct dma_mapping_ops gart_dma_ops = { <nl> + static const struct dma_mapping_ops gart_dma_ops = { <nl> . mapping_error = NULL , <nl> . map_single = gart_map_single , <nl> . map_simple = gart_map_simple , <nl> void nommu_unmap_sg ( struct device * dev , struct scatterlist * sg , <nl> { <nl> } <nl>  <nl> - struct dma_mapping_ops nommu_dma_ops = { <nl> + const struct dma_mapping_ops nommu_dma_ops = { <nl> . map_single = nommu_map_single , <nl> . unmap_single = nommu_unmap_single , <nl> . map_sg = nommu_map_sg , <nl> # define Dprintk ( x ...) <nl> # endif <nl>  <nl> - struct dma_mapping_ops * dma_ops ; <nl> + const struct dma_mapping_ops * dma_ops ; <nl> EXPORT_SYMBOL ( dma_ops ); <nl>  <nl> static unsigned long dma_reserve __initdata ; <nl> struct dma_mapping_ops { <nl> }; <nl>  <nl> extern dma_addr_t bad_dma_address ; <nl> - extern struct dma_mapping_ops * dma_ops ; <nl> + extern const struct dma_mapping_ops * dma_ops ; <nl> extern int iommu_merge ; <nl>  <nl> static inline int dma_mapping_error ( dma_addr_t dma_addr )
mmm drivers / media / video / cx23885 / cx23885 - dvb . c <nl> ppp drivers / media / video / cx23885 / cx23885 - dvb . c <nl> int cx23885_dvb_unregister ( struct cx23885_tsport * port ) <nl> * implement MFE support . <nl> */ <nl> fe0 = videobuf_dvb_get_frontend (& port -> frontends , 1 ); <nl> - if ( fe0 -> dvb . frontend ) <nl> + if ( fe0 && fe0 -> dvb . frontend ) <nl> videobuf_dvb_unregister_bus (& port -> frontends ); <nl>  <nl> switch ( port -> dev -> board ) {
mmm drivers / video / console / fbcon . c <nl> ppp drivers / video / console / fbcon . c <nl> static void fbcon_deinit ( struct vc_data * vc ) <nl> finished : <nl>  <nl> fbcon_free_font ( p , free_font ); <nl> + if ( free_font ) <nl> + vc -> vc_font . data = NULL ; <nl>  <nl> if (! con_is_bound (& fb_con )) <nl> fbcon_exit ();
mmm drivers / gpu / drm / amd / display / dc / core / dc_resource . c <nl> ppp drivers / gpu / drm / amd / display / dc / core / dc_resource . c <nl> static void set_avi_info_frame ( <nl> info_packet -> hb2 = <nl> info_frame . avi_info_packet . info_packet_hdmi . packet_raw_data . hb2 ; <nl>  <nl> - for ( byte_index = 0 ; byte_index < sizeof ( info_packet -> sb ); byte_index ++) <nl> + for ( byte_index = 0 ; byte_index < sizeof ( info_frame . avi_info_packet . <nl> + info_packet_hdmi . packet_raw_data . sb ); byte_index ++) <nl> info_packet -> sb [ byte_index ] = info_frame . avi_info_packet . <nl> - info_packet_hdmi . packet_raw_data . sb [ byte_index ]; <nl> + info_packet_hdmi . packet_raw_data . sb [ byte_index ]; <nl>  <nl> info_packet -> valid = true ; <nl> }
mmm kernel / fork . c <nl> ppp kernel / fork . c <nl> static struct task_struct * copy_process ( unsigned long clone_flags , <nl> if (( clone_flags & ( CLONE_NEWNS | CLONE_FS )) == ( CLONE_NEWNS | CLONE_FS )) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> + if (( clone_flags & ( CLONE_NEWUSER | CLONE_FS )) == ( CLONE_NEWUSER | CLONE_FS )) <nl> + return ERR_PTR (- EINVAL ); <nl> + <nl> /* <nl> * Thread groups must share signals as well , and detached threads <nl> * can only be started up within the thread group . <nl> SYSCALL_DEFINE1 ( unshare , unsigned long , unshare_flags ) <nl> * If unsharing a user namespace must also unshare the thread . <nl> */ <nl> if ( unshare_flags & CLONE_NEWUSER ) <nl> - unshare_flags |= CLONE_THREAD ; <nl> + unshare_flags |= CLONE_THREAD | CLONE_FS ; <nl> /* <nl> * If unsharing a pid namespace must also unshare the thread . <nl> */mmm kernel / user_namespace . c <nl> ppp kernel / user_namespace . c <nl> static struct task_struct * copy_process ( unsigned long clone_flags , <nl> if (( clone_flags & ( CLONE_NEWNS | CLONE_FS )) == ( CLONE_NEWNS | CLONE_FS )) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> + if (( clone_flags & ( CLONE_NEWUSER | CLONE_FS )) == ( CLONE_NEWUSER | CLONE_FS )) <nl> + return ERR_PTR (- EINVAL ); <nl> + <nl> /* <nl> * Thread groups must share signals as well , and detached threads <nl> * can only be started up within the thread group . <nl> SYSCALL_DEFINE1 ( unshare , unsigned long , unshare_flags ) <nl> * If unsharing a user namespace must also unshare the thread . <nl> */ <nl> if ( unshare_flags & CLONE_NEWUSER ) <nl> - unshare_flags |= CLONE_THREAD ; <nl> + unshare_flags |= CLONE_THREAD | CLONE_FS ; <nl> /* <nl> * If unsharing a pid namespace must also unshare the thread . <nl> */ <nl> # include < linux / uaccess . h > <nl> # include < linux / ctype . h > <nl> # include < linux / projid . h > <nl> +# include < linux / fs_struct . h > <nl>  <nl> static struct kmem_cache * user_ns_cachep __read_mostly ; <nl>  <nl> static int userns_install ( struct nsproxy * nsproxy , void * ns ) <nl> if ( atomic_read (& current -> mm -> mm_users ) > 1 ) <nl> return - EINVAL ; <nl>  <nl> + if ( current -> fs -> users != 1 ) <nl> + return - EINVAL ; <nl> + <nl> if (! ns_capable ( user_ns , CAP_SYS_ADMIN )) <nl> return - EPERM ; <nl> 
mmm drivers / gpu / drm / i915 / intel_panel . c <nl> ppp drivers / gpu / drm / i915 / intel_panel . c <nl> static int intel_backlight_device_update_status ( struct backlight_device * bd ) <nl> */ <nl> if ( panel -> backlight . enabled ) { <nl> if ( panel -> backlight_power ) { <nl> - bool enable = bd -> props . power == FB_BLANK_UNBLANK ; <nl> + bool enable = bd -> props . power == FB_BLANK_UNBLANK && <nl> + bd -> props . brightness != 0 ; <nl> panel -> backlight_power ( connector , enable ); <nl> } <nl> } else {
mmm drivers / net / macvlan . c <nl> ppp drivers / net / macvlan . c <nl> static void macvlan_process_broadcast ( struct work_struct * w ) <nl> static void macvlan_broadcast_enqueue ( struct macvlan_port * port , <nl> struct sk_buff * skb ) <nl> { <nl> + struct sk_buff * nskb ; <nl> int err = - ENOMEM ; <nl>  <nl> - skb = skb_clone ( skb , GFP_ATOMIC ); <nl> - if (! skb ) <nl> + nskb = skb_clone ( skb , GFP_ATOMIC ); <nl> + if (! nskb ) <nl> goto err ; <nl>  <nl> spin_lock (& port -> bc_queue . lock ); <nl> if ( skb_queue_len (& port -> bc_queue ) < skb -> dev -> tx_queue_len ) { <nl> - __skb_queue_tail (& port -> bc_queue , skb ); <nl> + __skb_queue_tail (& port -> bc_queue , nskb ); <nl> err = 0 ; <nl> } <nl> spin_unlock (& port -> bc_queue . lock ); <nl>  <nl> if ( err ) <nl> - goto err ; <nl> + goto free_nskb ; <nl>  <nl> schedule_work (& port -> bc_work ); <nl> return ; <nl>  <nl> + free_nskb : <nl> + kfree_skb ( nskb ); <nl> err : <nl> atomic_long_inc (& skb -> dev -> rx_dropped ); <nl> }
mmm fs / io_uring . c <nl> ppp fs / io_uring . c <nl> static __cold void io_flush_timeouts ( struct io_ring_ctx * ctx ) <nl> __must_hold (& ctx -> completion_lock ) <nl> { <nl> u32 seq = ctx -> cached_cq_tail - atomic_read (& ctx -> cq_timeouts ); <nl> + struct io_kiocb * req , * tmp ; <nl>  <nl> spin_lock_irq (& ctx -> timeout_lock ); <nl> - while (! list_empty (& ctx -> timeout_list )) { <nl> + list_for_each_entry_safe ( req , tmp , & ctx -> timeout_list , timeout . list ) { <nl> u32 events_needed , events_got ; <nl> - struct io_kiocb * req = list_first_entry (& ctx -> timeout_list , <nl> - struct io_kiocb , timeout . list ); <nl>  <nl> if ( io_is_timeout_noseq ( req )) <nl> break ; <nl> static __cold void io_flush_timeouts ( struct io_ring_ctx * ctx ) <nl> if ( events_got < events_needed ) <nl> break ; <nl>  <nl> - list_del_init (& req -> timeout . list ); <nl> io_kill_timeout ( req , 0 ); <nl> } <nl> ctx -> cq_last_tm_flush = seq ; <nl> static int io_timeout_prep ( struct io_kiocb * req , const struct io_uring_sqe * sqe , <nl> if ( data -> ts . tv_sec < 0 || data -> ts . tv_nsec < 0 ) <nl> return - EINVAL ; <nl>  <nl> + INIT_LIST_HEAD (& req -> timeout . list ); <nl> data -> mode = io_translate_timeout_mode ( flags ); <nl> hrtimer_init (& data -> timer , io_timeout_get_clock ( data ), data -> mode ); <nl> 
mmm net / socket . c <nl> ppp net / socket . c <nl> static ssize_t sock_sendpage ( struct file * file , struct page * page , <nl> if ( more ) <nl> flags |= MSG_MORE ; <nl>  <nl> - return sock -> ops -> sendpage ( sock , page , offset , size , flags ); <nl> + return kernel_sendpage ( sock , page , offset , size , flags ); <nl> } <nl>  <nl> static ssize_t sock_splice_read ( struct file * file , loff_t * ppos ,
mmm kernel / sched . c <nl> ppp kernel / sched . c <nl> static inline void ttwu_post_activation ( struct task_struct * p , struct rq * rq , <nl> if ( p -> sched_class -> task_woken ) <nl> p -> sched_class -> task_woken ( rq , p ); <nl>  <nl> - if ( unlikely ( rq -> idle_stamp )) { <nl> + if ( rq -> idle_stamp ) { <nl> u64 delta = rq -> clock - rq -> idle_stamp ; <nl> u64 max = 2 * sysctl_sched_migration_cost ; <nl> 
mmm drivers / media / media - device . c <nl> ppp drivers / media / media - device . c <nl> static long media_device_enum_entities ( struct media_device * mdev , <nl> struct media_entity * ent ; <nl> struct media_entity_desc u_ent ; <nl>  <nl> + memset (& u_ent , 0 , sizeof ( u_ent )); <nl> if ( copy_from_user (& u_ent . id , & uent -> id , sizeof ( u_ent . id ))) <nl> return - EFAULT ; <nl> 
mmm net / can / gw . c <nl> ppp net / can / gw . c <nl> struct cgw_job { <nl> struct rcu_head rcu ; <nl> u32 handled_frames ; <nl> u32 dropped_frames ; <nl> + u32 deleted_frames ; <nl> struct cf_mod mod ; <nl> union { <nl> /* CAN frame data source */ <nl> static void can_can_gw_rcv ( struct sk_buff * skb , void * data ) <nl>  <nl> BUG_ON ( skb -> ip_summed != CHECKSUM_UNNECESSARY ); <nl>  <nl> - if ( cgw_hops ( skb ) >= max_hops ) <nl> + if ( cgw_hops ( skb ) >= max_hops ) { <nl> + /* indicate deleted frames due to misconfiguration */ <nl> + gwj -> deleted_frames ++; <nl> return ; <nl> + } <nl>  <nl> if (!( gwj -> dst . dev -> flags & IFF_UP )) { <nl> gwj -> dropped_frames ++; <nl> static int cgw_put_job ( struct sk_buff * skb , struct cgw_job * gwj , int type , <nl> goto cancel ; <nl> } <nl>  <nl> + if ( gwj -> deleted_frames ) { <nl> + if ( nla_put_u32 ( skb , CGW_DELETED , gwj -> deleted_frames ) < 0 ) <nl> + goto cancel ; <nl> + } <nl> + <nl> /* check non default settings of attributes */ <nl>  <nl> if ( gwj -> mod . modtype . and ) { <nl> static int cgw_create_job ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl>  <nl> gwj -> handled_frames = 0 ; <nl> gwj -> dropped_frames = 0 ; <nl> + gwj -> deleted_frames = 0 ; <nl> gwj -> flags = r -> flags ; <nl> gwj -> gwtype = r -> gwtype ; <nl> mmm include / uapi / linux / can / gw . h <nl> ppp include / uapi / linux / can / gw . h <nl> struct cgw_job { <nl> struct rcu_head rcu ; <nl> u32 handled_frames ; <nl> u32 dropped_frames ; <nl> + u32 deleted_frames ; <nl> struct cf_mod mod ; <nl> union { <nl> /* CAN frame data source */ <nl> static void can_can_gw_rcv ( struct sk_buff * skb , void * data ) <nl>  <nl> BUG_ON ( skb -> ip_summed != CHECKSUM_UNNECESSARY ); <nl>  <nl> - if ( cgw_hops ( skb ) >= max_hops ) <nl> + if ( cgw_hops ( skb ) >= max_hops ) { <nl> + /* indicate deleted frames due to misconfiguration */ <nl> + gwj -> deleted_frames ++; <nl> return ; <nl> + } <nl>  <nl> if (!( gwj -> dst . dev -> flags & IFF_UP )) { <nl> gwj -> dropped_frames ++; <nl> static int cgw_put_job ( struct sk_buff * skb , struct cgw_job * gwj , int type , <nl> goto cancel ; <nl> } <nl>  <nl> + if ( gwj -> deleted_frames ) { <nl> + if ( nla_put_u32 ( skb , CGW_DELETED , gwj -> deleted_frames ) < 0 ) <nl> + goto cancel ; <nl> + } <nl> + <nl> /* check non default settings of attributes */ <nl>  <nl> if ( gwj -> mod . modtype . and ) { <nl> static int cgw_create_job ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl>  <nl> gwj -> handled_frames = 0 ; <nl> gwj -> dropped_frames = 0 ; <nl> + gwj -> deleted_frames = 0 ; <nl> gwj -> flags = r -> flags ; <nl> gwj -> gwtype = r -> gwtype ; <nl>  <nl> enum { <nl> CGW_SRC_IF , /* ifindex of source network interface */ <nl> CGW_DST_IF , /* ifindex of destination network interface */ <nl> CGW_FILTER , /* specify struct can_filter on source CAN device */ <nl> + CGW_DELETED , /* number of deleted CAN frames ( see max_hops param ) */ <nl> __CGW_MAX <nl> }; <nl> 
mmm drivers / ssb / main . c <nl> ppp drivers / ssb / main . c <nl> static int __init ssb_modinit ( void ) <nl> ssb_buses_lock (); <nl> err = ssb_attach_queued_buses (); <nl> ssb_buses_unlock (); <nl> - if ( err ) <nl> + if ( err ) { <nl> bus_unregister (& ssb_bustype ); <nl> + goto out ; <nl> + } <nl>  <nl> err = b43_pci_ssb_bridge_init (); <nl> if ( err ) { <nl> static int __init ssb_modinit ( void ) <nl> /* don ' t fail SSB init because of this */ <nl> err = 0 ; <nl> } <nl> - <nl> + out : <nl> return err ; <nl> } <nl> /* ssb must be initialized after PCI but before the ssb drivers .
mmm include / linux / fsnotify_backend . h <nl> ppp include / linux / fsnotify_backend . h <nl> static inline void __fsnotify_update_dcache_flags ( struct dentry * dentry ) <nl> assert_spin_locked (& dentry -> d_lock ); <nl>  <nl> parent = dentry -> d_parent ; <nl> - if ( fsnotify_inode_watches_children ( parent -> d_inode )) <nl> + if ( parent -> d_inode && fsnotify_inode_watches_children ( parent -> d_inode )) <nl> dentry -> d_flags |= DCACHE_FSNOTIFY_PARENT_WATCHED ; <nl> else <nl> dentry -> d_flags &= ~ DCACHE_FSNOTIFY_PARENT_WATCHED ;
mmm arch / mips / mm / tlb - r4k . c <nl> ppp arch / mips / mm / tlb - r4k . c <nl> void local_flush_tlb_all ( void ) <nl>  <nl> entry = read_c0_wired (); <nl>  <nl> - /* Blast ' em all away . */ <nl> - if ( cpu_has_tlbinv ) { <nl> + /* <nl> + * Blast ' em all away . <nl> + * If there are any wired entries , fall back to iterating <nl> + */ <nl> + if ( cpu_has_tlbinv && ! entry ) { <nl> if ( current_cpu_data . tlbsizevtlb ) { <nl> write_c0_index ( 0 ); <nl> mtc0_tlbw_hazard ();
mmm drivers / cpufreq / pcc - cpufreq . c <nl> ppp drivers / cpufreq / pcc - cpufreq . c <nl> static int pcc_get_offset ( int cpu ) <nl> pr = per_cpu ( processors , cpu ); <nl> pcc_cpu_data = per_cpu_ptr ( pcc_cpu_info , cpu ); <nl>  <nl> + if (! pr ) <nl> + return - ENODEV ; <nl> + <nl> status = acpi_evaluate_object ( pr -> handle , " PCCP ", NULL , & buffer ); <nl> if ( ACPI_FAILURE ( status )) <nl> return - ENODEV ;
mmm drivers / misc / mei / amthif . c <nl> ppp drivers / misc / mei / amthif . c <nl> int mei_amthif_host_init ( struct mei_device * dev , struct mei_me_client * me_cl ) <nl> struct mei_cl * cl = & dev -> iamthif_cl ; <nl> int ret ; <nl>  <nl> - if ( mei_cl_is_connected ( cl )) <nl> - return 0 ; <nl> + mutex_lock (& dev -> device_lock ); <nl> + <nl> + if ( mei_cl_is_connected ( cl )) { <nl> + ret = 0 ; <nl> + goto out ; <nl> + } <nl>  <nl> dev -> iamthif_state = MEI_IAMTHIF_IDLE ; <nl>  <nl> int mei_amthif_host_init ( struct mei_device * dev , struct mei_me_client * me_cl ) <nl> ret = mei_cl_link ( cl ); <nl> if ( ret < 0 ) { <nl> dev_err ( dev -> dev , " amthif : failed cl_link % d \ n ", ret ); <nl> - return ret ; <nl> + goto out ; <nl> } <nl>  <nl> ret = mei_cl_connect ( cl , me_cl , NULL ); <nl>  <nl> + out : <nl> + mutex_unlock (& dev -> device_lock ); <nl> return ret ; <nl> } <nl> mmm drivers / misc / mei / bus . c <nl> ppp drivers / misc / mei / bus . c <nl> int mei_amthif_host_init ( struct mei_device * dev , struct mei_me_client * me_cl ) <nl> struct mei_cl * cl = & dev -> iamthif_cl ; <nl> int ret ; <nl>  <nl> - if ( mei_cl_is_connected ( cl )) <nl> - return 0 ; <nl> + mutex_lock (& dev -> device_lock ); <nl> + <nl> + if ( mei_cl_is_connected ( cl )) { <nl> + ret = 0 ; <nl> + goto out ; <nl> + } <nl>  <nl> dev -> iamthif_state = MEI_IAMTHIF_IDLE ; <nl>  <nl> int mei_amthif_host_init ( struct mei_device * dev , struct mei_me_client * me_cl ) <nl> ret = mei_cl_link ( cl ); <nl> if ( ret < 0 ) { <nl> dev_err ( dev -> dev , " amthif : failed cl_link % d \ n ", ret ); <nl> - return ret ; <nl> + goto out ; <nl> } <nl>  <nl> ret = mei_cl_connect ( cl , me_cl , NULL ); <nl>  <nl> + out : <nl> + mutex_unlock (& dev -> device_lock ); <nl> return ret ; <nl> } <nl>  <nl> void mei_cl_bus_rescan_work ( struct work_struct * work ) <nl> container_of ( work , struct mei_device , bus_rescan_work ); <nl> struct mei_me_client * me_cl ; <nl>  <nl> - mutex_lock (& bus -> device_lock ); <nl> me_cl = mei_me_cl_by_uuid ( bus , & mei_amthif_guid ); <nl> if ( me_cl ) <nl> mei_amthif_host_init ( bus , me_cl ); <nl> mei_me_cl_put ( me_cl ); <nl> - mutex_unlock (& bus -> device_lock ); <nl>  <nl> mei_cl_bus_rescan ( bus ); <nl> }
mmm net / ipv4 / fib_frontend . c <nl> ppp net / ipv4 / fib_frontend . c <nl> __be32 fib_compute_spec_dst ( struct sk_buff * skb ) <nl> if (! ipv4_is_zeronet ( ip_hdr ( skb )-> saddr )) { <nl> struct flowi4 fl4 = { <nl> . flowi4_iif = LOOPBACK_IFINDEX , <nl> + . flowi4_oif = l3mdev_master_ifindex_rcu ( dev ), <nl> . daddr = ip_hdr ( skb )-> saddr , <nl> . flowi4_tos = RT_TOS ( ip_hdr ( skb )-> tos ), <nl> . flowi4_scope = scope ,
mmm drivers / input / input . c <nl> ppp drivers / input / input . c <nl> void input_unregister_device ( struct input_dev * dev ) <nl> sysfs_remove_group (& dev -> cdev . kobj , & input_dev_caps_attr_group ); <nl> sysfs_remove_group (& dev -> cdev . kobj , & input_dev_id_attr_group ); <nl> sysfs_remove_group (& dev -> cdev . kobj , & input_dev_attr_group ); <nl> - class_device_unregister (& dev -> cdev ); <nl>  <nl> mutex_lock (& dev -> mutex ); <nl> dev -> name = dev -> phys = dev -> uniq = NULL ; <nl> mutex_unlock (& dev -> mutex ); <nl>  <nl> + class_device_unregister (& dev -> cdev ); <nl> + <nl> input_wakeup_procfs_readers (); <nl> } <nl> EXPORT_SYMBOL ( input_unregister_device );
mmm drivers / input / misc / uinput . c <nl> ppp drivers / input / misc / uinput . c <nl> static inline int uinput_request_reserve_slot ( struct uinput_device * udev , struct <nl>  <nl> static void uinput_request_done ( struct uinput_device * udev , struct uinput_request * request ) <nl> { <nl> - complete (& request -> done ); <nl> - <nl> /* Mark slot as available */ <nl> udev -> requests [ request -> id ] = NULL ; <nl> wake_up_interruptible (& udev -> requests_waitq ); <nl> + <nl> + complete (& request -> done ); <nl> } <nl>  <nl> static int uinput_request_submit ( struct input_dev * dev , struct uinput_request * request )
mmm include / asm - um / mmu_context . h <nl> ppp include / asm - um / mmu_context . h <nl> static inline void activate_mm ( struct mm_struct * old , struct mm_struct * new ) <nl> * possible . <nl> */ <nl> if ( old != new && ( current -> flags & PF_BORROWED_MM )) <nl> - force_flush_all (); <nl> + CHOOSE_MODE ( force_flush_all (), <nl> + switch_mm_skas (& new -> context . skas . id )); <nl> } <nl>  <nl> static inline void switch_mm ( struct mm_struct * prev , struct mm_struct * next ,
mmm arch / arm / mach - mx2 / eukrea_mbimx27 - baseboard . c <nl> ppp arch / arm / mach - mx2 / eukrea_mbimx27 - baseboard . c <nl> static struct platform_device * platform_devices [] __initdata = { <nl> & leds_gpio , <nl> }; <nl>  <nl> + static struct imxmmc_platform_data sdhc_pdata = { <nl> + . dat3_card_detect = 1 , <nl> +}; <nl> + <nl> /* <nl> * system init for baseboard usage . Will be called by cpuimx27 init . <nl> * <nl> void __init eukrea_mbimx27_baseboard_init ( void ) <nl> # endif <nl>  <nl> mxc_register_device (& mxc_fb_device , & eukrea_mbimx27_fb_data ); <nl> - mxc_register_device (& mxc_sdhc_device0 , NULL ); <nl> + mxc_register_device (& mxc_sdhc_device0 , & sdhc_pdata ); <nl>  <nl> # if defined ( CONFIG_TOUCHSCREEN_ADS7846 ) \ <nl> || defined ( CONFIG_TOUCHSCREEN_ADS7846_MODULE )
mmm drivers / ata / libata - core . c <nl> ppp drivers / ata / libata - core . c <nl> unsigned ata_exec_internal_sg ( struct ata_device * dev , <nl> qc -> tf = * tf ; <nl> if ( cdb ) <nl> memcpy ( qc -> cdb , cdb , ATAPI_CDB_LEN ); <nl> + <nl> + /* some SATA bridges need us to indicate data xfer direction */ <nl> + if ( tf -> protocol == ATAPI_PROT_DMA && ( dev -> flags & ATA_DFLAG_DMADIR ) && <nl> + dma_dir == DMA_FROM_DEVICE ) <nl> + qc -> tf . feature |= ATAPI_DMADIR ; <nl> + <nl> qc -> flags |= ATA_QCFLAG_RESULT_TF ; <nl> qc -> dma_dir = dma_dir ; <nl> if ( dma_dir != DMA_NONE ) {
mmm net / wireless / nl80211 . c <nl> ppp net / wireless / nl80211 . c <nl> static int nl80211_set_rekey_data ( struct sk_buff * skb , struct genl_info * info ) <nl> if ( err ) <nl> return err ; <nl>  <nl> + if (! tb [ NL80211_REKEY_DATA_REPLAY_CTR ] || ! tb [ NL80211_REKEY_DATA_KEK ] || <nl> + ! tb [ NL80211_REKEY_DATA_KCK ]) <nl> + return - EINVAL ; <nl> if ( nla_len ( tb [ NL80211_REKEY_DATA_REPLAY_CTR ]) != NL80211_REPLAY_CTR_LEN ) <nl> return - ERANGE ; <nl> if ( nla_len ( tb [ NL80211_REKEY_DATA_KEK ]) != NL80211_KEK_LEN )
mmm drivers / iio / potentiostat / lmp91000 . c <nl> ppp drivers / iio / potentiostat / lmp91000 . c <nl> static int lmp91000_probe ( struct i2c_client * client , <nl> indio_dev -> channels = lmp91000_channels ; <nl> indio_dev -> num_channels = ARRAY_SIZE ( lmp91000_channels ); <nl> indio_dev -> name = LMP91000_DRV_NAME ; <nl> + indio_dev -> dev . parent = & client -> dev ; <nl> indio_dev -> modes = INDIO_DIRECT_MODE ; <nl> i2c_set_clientdata ( client , indio_dev ); <nl> 
mmm drivers / scsi / sg . c <nl> ppp drivers / scsi / sg . c <nl> sg_unlink_reserve ( Sg_fd * sfp , Sg_request * srp ) <nl> req_schp -> page_order = 0 ; <nl> req_schp -> sglist_len = 0 ; <nl> srp -> res_used = 0 ; <nl> + /* Called without mutex lock to avoid deadlock */ <nl> + sfp -> res_in_use = 0 ; <nl> } <nl>  <nl> static Sg_request *
mmm drivers / hid / hid - corsair . c <nl> ppp drivers / hid / hid - corsair . c <nl> static int corsair_input_mapping ( struct hid_device * dev , <nl> { <nl> int gkey ; <nl>  <nl> + if (( usage -> hid & HID_USAGE_PAGE ) != HID_UP_KEYBOARD ) <nl> + return 0 ; <nl> + <nl> gkey = corsair_usage_to_gkey ( usage -> hid & HID_USAGE ); <nl> if ( gkey != 0 ) { <nl> hid_map_usage_clear ( input , usage , bit , max , EV_KEY ,
mmm drivers / hwmon / ina2xx . c <nl> ppp drivers / hwmon / ina2xx . c <nl> static int ina2xx_probe ( struct i2c_client * client , <nl> data -> config = & ina2xx_config [ data -> kind ]; <nl> data -> client = client ; <nl>  <nl> - if ( data -> rshunt <= 0 ) <nl> + if ( data -> rshunt <= 0 || <nl> + data -> rshunt > data -> config -> calibration_factor ) <nl> return - ENODEV ; <nl>  <nl> ret = ina2xx_init ( data );
mmm drivers / net / cxgb4vf / cxgb4vf_main . c <nl> ppp drivers / net / cxgb4vf / cxgb4vf_main . c <nl> static int cxgb4vf_open ( struct net_device * dev ) <nl> if ( err ) <nl> return err ; <nl> set_bit ( pi -> port_id , & adapter -> open_device_map ); <nl> - link_start ( dev ); <nl> + err = link_start ( dev ); <nl> + if ( err ) <nl> + return err ; <nl> netif_tx_start_all_queues ( dev ); <nl> return 0 ; <nl> }
mmm sound / core / timer . c <nl> ppp sound / core / timer . c <nl> int snd_timer_open ( struct snd_timer_instance ** ti , <nl> goto unlock ; <nl> } <nl> if (! list_empty (& timer -> open_list_head )) { <nl> - timeri = list_entry ( timer -> open_list_head . next , <nl> + struct snd_timer_instance * t = <nl> + list_entry ( timer -> open_list_head . next , <nl> struct snd_timer_instance , open_list ); <nl> - if ( timeri -> flags & SNDRV_TIMER_IFLG_EXCLUSIVE ) { <nl> + if ( t -> flags & SNDRV_TIMER_IFLG_EXCLUSIVE ) { <nl> err = - EBUSY ; <nl> - timeri = NULL ; <nl> goto unlock ; <nl> } <nl> }
mmm drivers / staging / rtl8192u / ieee80211 / ieee80211_tx . c <nl> ppp drivers / staging / rtl8192u / ieee80211 / ieee80211_tx . c <nl> static struct ieee80211_txb * ieee80211_alloc_txb ( int nr_frags , int txb_size , <nl>  <nl> memset ( txb , 0 , sizeof ( struct ieee80211_txb )); <nl> txb -> nr_frags = nr_frags ; <nl> - txb -> frag_size = txb_size ; <nl> + txb -> frag_size = __cpu_to_le16 ( txb_size ); <nl>  <nl> for ( i = 0 ; i < nr_frags ; i ++) { <nl> txb -> fragments [ i ] = dev_alloc_skb ( txb_size ); <nl> int ieee80211_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> goto failed ; <nl> } <nl> txb -> encrypted = encrypt ; <nl> - txb -> payload_size = bytes ; <nl> + txb -> payload_size = __cpu_to_le16 ( bytes ); <nl>  <nl> // if ( ieee -> current_network . QoS_Enable ) <nl> if ( qos_actived ) <nl> int ieee80211_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> } <nl>  <nl> txb -> encrypted = 0 ; <nl> - txb -> payload_size = skb -> len ; <nl> + txb -> payload_size = __cpu_to_le16 ( skb -> len ); <nl> memcpy ( skb_put ( txb -> fragments [ 0 ], skb -> len ), skb -> data , skb -> len ); <nl> } <nl>  <nl> int ieee80211_xmit ( struct sk_buff * skb , struct net_device * dev ) <nl> } else { <nl> if ((* ieee -> hard_start_xmit )( txb , dev ) == 0 ) { <nl> stats -> tx_packets ++; <nl> - stats -> tx_bytes += txb -> payload_size ; <nl> + stats -> tx_bytes += __le16_to_cpu ( txb -> payload_size ); <nl> return 0 ; <nl> } <nl> ieee80211_txb_free ( txb );
mmm kernel / cgroup . c <nl> ppp kernel / cgroup . c <nl> int cgroup_scan_tasks ( struct cgroup_scanner * scan ) <nl> */ <nl> static int pid_array_load ( pid_t * pidarray , int npids , struct cgroup * cgrp ) <nl> { <nl> - int n = 0 ; <nl> + int n = 0 , pid ; <nl> struct cgroup_iter it ; <nl> struct task_struct * tsk ; <nl> cgroup_iter_start ( cgrp , & it ); <nl> while (( tsk = cgroup_iter_next ( cgrp , & it ))) { <nl> if ( unlikely ( n == npids )) <nl> break ; <nl> - pidarray [ n ++] = task_pid_vnr ( tsk ); <nl> + pid = task_pid_vnr ( tsk ); <nl> + if ( pid > 0 ) <nl> + pidarray [ n ++] = pid ; <nl> } <nl> cgroup_iter_end ( cgrp , & it ); <nl> return n ;
mmm net / ipv6 / addrconf . c <nl> ppp net / ipv6 / addrconf . c <nl> static int modify_prefix_route ( struct inet6_ifaddr * ifp , <nl> unsigned long expires , u32 flags ) <nl> { <nl> struct fib6_info * f6i ; <nl> + u32 prio ; <nl>  <nl> f6i = addrconf_get_prefix_route (& ifp -> addr , <nl> ifp -> prefix_len , <nl> static int modify_prefix_route ( struct inet6_ifaddr * ifp , <nl> if (! f6i ) <nl> return - ENOENT ; <nl>  <nl> - if ( f6i -> fib6_metric != ifp -> rt_priority ) { <nl> + prio = ifp -> rt_priority ? : IP6_RT_PRIO_ADDRCONF ; <nl> + if ( f6i -> fib6_metric != prio ) { <nl> + /* delete old one */ <nl> + ip6_del_rt ( dev_net ( ifp -> idev -> dev ), f6i ); <nl> + <nl> /* add new one */ <nl> addrconf_prefix_route (& ifp -> addr , ifp -> prefix_len , <nl> ifp -> rt_priority , ifp -> idev -> dev , <nl> expires , flags , GFP_KERNEL ); <nl> - /* delete old one */ <nl> - ip6_del_rt ( dev_net ( ifp -> idev -> dev ), f6i ); <nl> } else { <nl> if (! expires ) <nl> fib6_clean_expires ( f6i );
mmm drivers / usb / class / cdc - acm . c <nl> ppp drivers / usb / class / cdc - acm . c <nl> static int acm_probe ( struct usb_interface * intf , <nl> i = device_create_file (& intf -> dev , & dev_attr_wCountryCodes ); <nl> if ( i < 0 ) { <nl> kfree ( acm -> country_codes ); <nl> + acm -> country_codes = NULL ; <nl> + acm -> country_code_size = 0 ; <nl> goto skip_countries ; <nl> } <nl>  <nl> static int acm_probe ( struct usb_interface * intf , <nl> if ( i < 0 ) { <nl> device_remove_file (& intf -> dev , & dev_attr_wCountryCodes ); <nl> kfree ( acm -> country_codes ); <nl> + acm -> country_codes = NULL ; <nl> + acm -> country_code_size = 0 ; <nl> goto skip_countries ; <nl> } <nl> }
mmm include / linux / sched . h <nl> ppp include / linux / sched . h <nl> struct task_struct { <nl> int exit_state ; <nl> int exit_code , exit_signal ; <nl> int pdeath_signal ; /* The signal sent when the parent dies */ <nl> - unsigned int jobctl ; /* JOBCTL_ *, siglock protected */ <nl> + unsigned long jobctl ; /* JOBCTL_ *, siglock protected */ <nl>  <nl> /* Used for emulating ABI behavior of previous Linux versions */ <nl> unsigned int personality ;
mmm sound / usb / endpoint . c <nl> ppp sound / usb / endpoint . c <nl> struct snd_usb_endpoint * snd_usb_add_endpoint ( struct snd_usb_audio * chip , <nl> struct snd_usb_endpoint * ep ; <nl> int is_playback = direction == SNDRV_PCM_STREAM_PLAYBACK ; <nl>  <nl> + if ( WARN_ON (! alts )) <nl> + return NULL ; <nl> + <nl> mutex_lock (& chip -> mutex ); <nl>  <nl> list_for_each_entry ( ep , & chip -> ep_list , list ) {
mmm drivers / mmc / host / mmci . c <nl> ppp drivers / mmc / host / mmci . c <nl> static irqreturn_t mmci_irq ( int irq , void * dev_id ) <nl>  <nl> dev_dbg ( mmc_dev ( host -> mmc ), " irq0 ( data + cmd ) % 08x \ n ", status ); <nl>  <nl> + cmd = host -> cmd ; <nl> + if ( status & ( MCI_CMDCRCFAIL | MCI_CMDTIMEOUT | MCI_CMDSENT | <nl> + MCI_CMDRESPEND ) && cmd ) <nl> + mmci_cmd_irq ( host , cmd , status ); <nl> + <nl> data = host -> data ; <nl> if ( status & ( MCI_DATACRCFAIL | MCI_DATATIMEOUT | MCI_STARTBITERR | <nl> MCI_TXUNDERRUN | MCI_RXOVERRUN | MCI_DATAEND | <nl> MCI_DATABLOCKEND ) && data ) <nl> mmci_data_irq ( host , data , status ); <nl>  <nl> - cmd = host -> cmd ; <nl> - if ( status & ( MCI_CMDCRCFAIL | MCI_CMDTIMEOUT | MCI_CMDSENT | MCI_CMDRESPEND ) && cmd ) <nl> - mmci_cmd_irq ( host , cmd , status ); <nl> - <nl> ret = 1 ; <nl> } while ( status ); <nl> 
mmm arch / x86 / kvm / vmx . c <nl> ppp arch / x86 / kvm / vmx . c <nl> static void add_atomic_switch_msr ( struct vcpu_vmx * vmx , unsigned msr , <nl> if ( m -> guest [ i ]. index == msr ) <nl> break ; <nl>  <nl> - if ( i == m -> nr ) { <nl> + if ( i == NR_AUTOLOAD_MSRS ) { <nl> + printk_once ( KERN_WARNING " Not enough mst switch entries . " <nl> + " Can ' t add msr % x \ n ", msr ); <nl> + return ; <nl> + } else if ( i == m -> nr ) { <nl> ++ m -> nr ; <nl> vmcs_write32 ( VM_ENTRY_MSR_LOAD_COUNT , m -> nr ); <nl> vmcs_write32 ( VM_EXIT_MSR_LOAD_COUNT , m -> nr );
mmm kernel / trace / trace_events . c <nl> ppp kernel / trace / trace_events . c <nl> subsystem_filter_write ( struct file * filp , const char __user * ubuf , size_t cnt , <nl>  <nl> err = filter_add_subsystem_pred ( system , pred ); <nl> if ( err < 0 ) { <nl> - filter_free_subsystem_preds ( system ); <nl> filter_free_pred ( pred ); <nl> return err ; <nl> }
mmm drivers / atm / iphase . c <nl> ppp drivers / atm / iphase . c <nl> static int tx_init ( struct atm_dev * dev ) <nl> buf_desc_ptr ++; <nl> tx_pkt_start += iadev -> tx_buf_sz ; <nl> } <nl> - iadev -> tx_buf = kmalloc ( iadev -> num_tx_desc * sizeof ( struct cpcs_trailer_desc ), GFP_KERNEL ); <nl> + iadev -> tx_buf = kmalloc_array ( iadev -> num_tx_desc , <nl> + sizeof (* iadev -> tx_buf ), <nl> + GFP_KERNEL ); <nl> if (! iadev -> tx_buf ) { <nl> printk ( KERN_ERR DEV_LABEL " couldn ' t get mem \ n "); <nl> goto err_free_dle ; <nl> static int tx_init ( struct atm_dev * dev ) <nl> sizeof (* cpcs ), <nl> DMA_TO_DEVICE ); <nl> } <nl> - iadev -> desc_tbl = kmalloc ( iadev -> num_tx_desc * <nl> - sizeof ( struct desc_tbl_t ), GFP_KERNEL ); <nl> + iadev -> desc_tbl = kmalloc_array ( iadev -> num_tx_desc , <nl> + sizeof (* iadev -> desc_tbl ), <nl> + GFP_KERNEL ); <nl> if (! iadev -> desc_tbl ) { <nl> printk ( KERN_ERR DEV_LABEL " couldn ' t get mem \ n "); <nl> goto err_free_all_tx_bufs ; <nl> static int tx_init ( struct atm_dev * dev ) <nl> memset (( caddr_t )( iadev -> seg_ram + i ), 0 , iadev -> num_vc * 4 ); <nl> vc = ( struct main_vc *) iadev -> MAIN_VC_TABLE_ADDR ; <nl> evc = ( struct ext_vc *) iadev -> EXT_VC_TABLE_ADDR ; <nl> - iadev -> testTable = kmalloc ( sizeof ( long )* iadev -> num_vc , GFP_KERNEL ); <nl> + iadev -> testTable = kmalloc_array ( iadev -> num_vc , <nl> + sizeof (* iadev -> testTable ), <nl> + GFP_KERNEL ); <nl> if (! iadev -> testTable ) { <nl> printk (" Get freepage failed \ n "); <nl> goto err_free_desc_tbl ;
mmm arch / arm / kvm / arm . c <nl> ppp arch / arm / kvm / arm . c <nl> static void vcpu_pause ( struct kvm_vcpu * vcpu ) <nl> wait_event_interruptible (* wq , ! vcpu -> arch . pause ); <nl> } <nl>  <nl> + static int kvm_vcpu_initialized ( struct kvm_vcpu * vcpu ) <nl> +{ <nl> + return vcpu -> arch . target >= 0 ; <nl> +} <nl> + <nl> /** <nl> * kvm_arch_vcpu_ioctl_run - the main VCPU run function to execute guest code <nl> * @ vcpu : The VCPU pointer <nl> int kvm_arch_vcpu_ioctl_run ( struct kvm_vcpu * vcpu , struct kvm_run * run ) <nl> int ret ; <nl> sigset_t sigsaved ; <nl>  <nl> - /* Make sure they initialize the vcpu with KVM_ARM_VCPU_INIT */ <nl> - if ( unlikely ( vcpu -> arch . target < 0 )) <nl> + if ( unlikely (! kvm_vcpu_initialized ( vcpu ))) <nl> return - ENOEXEC ; <nl>  <nl> ret = kvm_vcpu_first_run_init ( vcpu ); <nl> long kvm_arch_vcpu_ioctl ( struct file * filp , <nl> case KVM_SET_ONE_REG : <nl> case KVM_GET_ONE_REG : { <nl> struct kvm_one_reg reg ; <nl> + <nl> + if ( unlikely (! kvm_vcpu_initialized ( vcpu ))) <nl> + return - ENOEXEC ; <nl> + <nl> if ( copy_from_user (& reg , argp , sizeof ( reg ))) <nl> return - EFAULT ; <nl> if ( ioctl == KVM_SET_ONE_REG ) <nl> long kvm_arch_vcpu_ioctl ( struct file * filp , <nl> struct kvm_reg_list reg_list ; <nl> unsigned n ; <nl>  <nl> + if ( unlikely (! kvm_vcpu_initialized ( vcpu ))) <nl> + return - ENOEXEC ; <nl> + <nl> if ( copy_from_user (& reg_list , user_list , sizeof ( reg_list ))) <nl> return - EFAULT ; <nl> n = reg_list . n ;
mmm drivers / net / ethernet / micrel / ks8851 . c <nl> ppp drivers / net / ethernet / micrel / ks8851 . c <nl> static int __devinit ks8851_probe ( struct spi_device * spi ) <nl>  <nl>  <nl> err_netdev : <nl> - free_irq ( ndev -> irq , ndev ); <nl> + free_irq ( ndev -> irq , ks ); <nl>  <nl> err_id : <nl> err_irq :
mmm drivers / gpu / drm / i915 / i915_gem_gtt . c <nl> ppp drivers / gpu / drm / i915 / i915_gem_gtt . c <nl> static bool gen8_ppgtt_clear_pt ( struct i915_address_space * vm , <nl> GEM_BUG_ON ( pte_end > GEN8_PTES ); <nl>  <nl> bitmap_clear ( pt -> used_ptes , pte , num_entries ); <nl> - <nl> - if ( bitmap_empty ( pt -> used_ptes , GEN8_PTES )) <nl> - return true ; <nl> + if ( USES_FULL_PPGTT ( vm -> i915 )) { <nl> + if ( bitmap_empty ( pt -> used_ptes , GEN8_PTES )) <nl> + return true ; <nl> + } <nl>  <nl> pt_vaddr = kmap_px ( pt ); <nl> 
mmm scripts / mod / modpost . c <nl> ppp scripts / mod / modpost . c <nl> static int init_section_ref_ok ( const char * name ) <nl> if ( strncmp (* s , name , strlen (* s )) == 0 ) <nl> return 1 ; <nl> for ( s = namelist3 ; * s ; s ++) <nl> - if ( strstr (* s , name ) != NULL ) <nl> + if ( strstr ( name , * s ) != NULL ) <nl> return 1 ; <nl> return 0 ; <nl> } <nl> static int exit_section_ref_ok ( const char * name ) <nl> if ( strncmp (* s , name , strlen (* s )) == 0 ) <nl> return 1 ; <nl> for ( s = namelist3 ; * s ; s ++) <nl> - if ( strstr (* s , name ) != NULL ) <nl> + if ( strstr ( name , * s ) != NULL ) <nl> return 1 ; <nl> return 0 ; <nl> }
mmm drivers / net / ethernet / broadcom / bcmsysport . c <nl> ppp drivers / net / ethernet / broadcom / bcmsysport . c <nl> static u16 bcm_sysport_select_queue ( struct net_device * dev , struct sk_buff * skb , <nl> port = BRCM_TAG_GET_PORT ( queue ); <nl> tx_ring = priv -> ring_map [ q + port * priv -> per_port_num_tx_queues ]; <nl>  <nl> + if ( unlikely (! tx_ring )) <nl> + return fallback ( dev , skb ); <nl> + <nl> return tx_ring -> index ; <nl> } <nl> 
mmm fs / nfsd / nfssvc . c <nl> ppp fs / nfsd / nfssvc . c <nl> static int nfsd_startup ( unsigned short port , int nrservs ) <nl> ret = nfs4_state_start (); <nl> if ( ret ) <nl> goto out_lockd ; <nl> - nfsd_reset_versions (); <nl> nfsd_up = true ; <nl> return 0 ; <nl> out_lockd : <nl> int nfsd_create_serv ( void ) <nl> nfsd_max_blksize >= 8 * 1024 * 2 ) <nl> nfsd_max_blksize /= 2 ; <nl> } <nl> + nfsd_reset_versions (); <nl>  <nl> nfsd_serv = svc_create_pooled (& nfsd_program , nfsd_max_blksize , <nl> nfsd_last_thread , nfsd , THIS_MODULE );
mmm fs / dax . c <nl> ppp fs / dax . c <nl> int __dax_zero_page_range ( struct block_device * bdev , <nl> void * kaddr ; <nl> pfn_t pfn ; <nl>  <nl> - rc = bdev_dax_pgoff ( bdev , sector , size , & pgoff ); <nl> + rc = bdev_dax_pgoff ( bdev , sector , PAGE_SIZE , & pgoff ); <nl> if ( rc ) <nl> return rc ; <nl>  <nl> id = dax_read_lock (); <nl> - rc = dax_direct_access ( dax_dev , pgoff , PHYS_PFN ( size ), & kaddr , <nl> + rc = dax_direct_access ( dax_dev , pgoff , 1 , & kaddr , <nl> & pfn ); <nl> if ( rc < 0 ) { <nl> dax_read_unlock ( id );
mmm net / atm / common . c <nl> ppp net / atm / common . c <nl> int vcc_getsockopt ( struct socket * sock , int level , int optname , <nl>  <nl> if (! vcc -> dev || ! test_bit ( ATM_VF_ADDR , & vcc -> flags )) <nl> return - ENOTCONN ; <nl> + memset (& pvc , 0 , sizeof ( pvc )); <nl> pvc . sap_family = AF_ATMPVC ; <nl> pvc . sap_addr . itf = vcc -> dev -> number ; <nl> pvc . sap_addr . vpi = vcc -> vpi ;
mmm fs / btrfs / transaction . c <nl> ppp fs / btrfs / transaction . c <nl> static struct btrfs_trans_handle * start_transaction ( struct btrfs_root * root , <nl> */ <nl> if ( type != TRANS_JOIN_NOLOCK && <nl> ! __sb_start_write ( root -> fs_info -> sb , SB_FREEZE_FS , false )) { <nl> - if ( type == TRANS_JOIN_FREEZE ) <nl> + if ( type == TRANS_JOIN_FREEZE ) { <nl> + kmem_cache_free ( btrfs_trans_handle_cachep , h ); <nl> return ERR_PTR (- EPERM ); <nl> + } <nl> sb_start_intwrite ( root -> fs_info -> sb ); <nl> } <nl> 
mmm net / batman - adv / bridge_loop_avoidance . c <nl> ppp net / batman - adv / bridge_loop_avoidance . c <nl> int batadv_bla_tx ( struct batadv_priv * bat_priv , struct sk_buff * skb , <nl> if (! atomic_read (& bat_priv -> bridge_loop_avoidance )) <nl> goto allow ; <nl>  <nl> - /* in VLAN case , the mac header might not be set . */ <nl> - skb_reset_mac_header ( skb ); <nl> - <nl> if ( batadv_bla_process_claim ( bat_priv , primary_if , skb )) <nl> goto handled ; <nl> 
mmm drivers / cpufreq / cpufreq_conservative . c <nl> ppp drivers / cpufreq / cpufreq_conservative . c <nl> static int cpufreq_governor_dbs ( struct cpufreq_policy * policy , <nl> if ( latency == 0 ) <nl> latency = 1 ; <nl>  <nl> - def_sampling_rate = latency * <nl> + def_sampling_rate = 10 * latency * <nl> DEF_SAMPLING_RATE_LATENCY_MULTIPLIER ; <nl>  <nl> if ( def_sampling_rate < MIN_STAT_SAMPLING_RATE )
mmm drivers / gpu / drm / i915 / intel_runtime_pm . c <nl> ppp drivers / gpu / drm / i915 / intel_runtime_pm . c <nl> static void assert_can_enable_dc9 ( struct drm_i915_private * dev_priv ) <nl> " DC9 already programmed to be enabled .\ n "); <nl> WARN_ONCE ( I915_READ ( DC_STATE_EN ) & DC_STATE_EN_UPTO_DC5 , <nl> " DC5 still not disabled to enable DC9 .\ n "); <nl> - WARN_ONCE ( I915_READ ( HSW_PWR_WELL_DRIVER ), " Power well on .\ n "); <nl> + WARN_ONCE ( I915_READ ( HSW_PWR_WELL_DRIVER ) & <nl> + SKL_POWER_WELL_REQ ( SKL_DISP_PW_2 ), <nl> + " Power well 2 on .\ n "); <nl> WARN_ONCE ( intel_irqs_enabled ( dev_priv ), <nl> " Interrupts not disabled yet .\ n "); <nl> 
mmm drivers / net / ntb_netdev . c <nl> ppp drivers / net / ntb_netdev . c <nl> static int ntb_netdev_open ( struct net_device * ndev ) <nl>  <nl> rc = ntb_transport_rx_enqueue ( dev -> qp , skb , skb -> data , <nl> ndev -> mtu + ETH_HLEN ); <nl> - if ( rc == - EINVAL ) <nl> + if ( rc == - EINVAL ) { <nl> + dev_kfree_skb ( skb ); <nl> goto err ; <nl> + } <nl> } <nl>  <nl> netif_carrier_off ( ndev );
mmm sound / soc / intel / skylake / skl - topology . c <nl> ppp sound / soc / intel / skylake / skl - topology . c <nl> static int skl_tplg_tlv_control_get ( struct snd_kcontrol * kcontrol , <nl> if ( bc -> params ) { <nl> if ( copy_to_user ( data , & bc -> param_id , sizeof ( u32 ))) <nl> return - EFAULT ; <nl> - if ( copy_to_user ( data + sizeof ( u32 ), & size , sizeof ( u32 ))) <nl> + if ( copy_to_user ( data + 1 , & size , sizeof ( u32 ))) <nl> return - EFAULT ; <nl> - if ( copy_to_user ( data + 2 * sizeof ( u32 ), bc -> params , size )) <nl> + if ( copy_to_user ( data + 2 , bc -> params , size )) <nl> return - EFAULT ; <nl> } <nl> 
mmm drivers / scsi / sd_zbc . c <nl> ppp drivers / scsi / sd_zbc . c <nl> static int sd_zbc_check_capacity ( struct scsi_disk * sdkp , unsigned char * buf ) <nl> return 0 ; <nl> } <nl>  <nl> -# define SD_ZBC_BUF_SIZE 131072 <nl> +# define SD_ZBC_BUF_SIZE 131072U <nl>  <nl> /** <nl> * sd_zbc_check_zone_size - Check the device zone sizes <nl> static int sd_zbc_check_zone_size ( struct scsi_disk * sdkp ) <nl> /* Parse REPORT ZONES header */ <nl> list_length = get_unaligned_be32 (& buf [ 0 ]) + 64 ; <nl> rec = buf + 64 ; <nl> - if ( list_length < SD_ZBC_BUF_SIZE ) <nl> - buf_len = list_length ; <nl> - else <nl> - buf_len = SD_ZBC_BUF_SIZE ; <nl> + buf_len = min ( list_length , SD_ZBC_BUF_SIZE ); <nl>  <nl> /* Parse zone descriptors */ <nl> while ( rec < buf + buf_len ) { <nl> static int sd_zbc_setup ( struct scsi_disk * sdkp ) <nl> /* chunk_sectors indicates the zone size */ <nl> blk_queue_chunk_sectors ( sdkp -> disk -> queue , <nl> logical_to_sectors ( sdkp -> device , sdkp -> zone_blocks )); <nl> - sdkp -> nr_zones = sdkp -> capacity >> sdkp -> zone_shift ; <nl> - if ( sdkp -> capacity & ( sdkp -> zone_blocks - 1 )) <nl> - sdkp -> nr_zones ++; <nl> + sdkp -> nr_zones = <nl> + round_up ( sdkp -> capacity , sdkp -> zone_blocks ) >> sdkp -> zone_shift ; <nl>  <nl> if (! sdkp -> zones_wlock ) { <nl> sdkp -> zones_wlock = kcalloc ( BITS_TO_LONGS ( sdkp -> nr_zones ),
mmm net / rxrpc / ar - internal . h <nl> ppp net / rxrpc / ar - internal . h <nl> static inline bool rxrpc_set_call_completion ( struct rxrpc_call * call , <nl> u32 abort_code , <nl> int error ) <nl> { <nl> - int ret ; <nl> + bool ret ; <nl>  <nl> write_lock_bh (& call -> state_lock ); <nl> ret = __rxrpc_set_call_completion ( call , compl , abort_code , error ); <nl> static inline bool rxrpc_set_call_completion ( struct rxrpc_call * call , <nl> /* <nl> * Record that a call successfully completed . <nl> */ <nl> - static inline void __rxrpc_call_completed ( struct rxrpc_call * call ) <nl> + static inline bool __rxrpc_call_completed ( struct rxrpc_call * call ) <nl> { <nl> - __rxrpc_set_call_completion ( call , RXRPC_CALL_SUCCEEDED , 0 , 0 ); <nl> + return __rxrpc_set_call_completion ( call , RXRPC_CALL_SUCCEEDED , 0 , 0 ); <nl> } <nl>  <nl> - static inline void rxrpc_call_completed ( struct rxrpc_call * call ) <nl> + static inline bool rxrpc_call_completed ( struct rxrpc_call * call ) <nl> { <nl> + bool ret ; <nl> + <nl> write_lock_bh (& call -> state_lock ); <nl> - __rxrpc_call_completed ( call ); <nl> + ret = __rxrpc_call_completed ( call ); <nl> write_unlock_bh (& call -> state_lock ); <nl> + return ret ; <nl> } <nl>  <nl> /*
mmm kernel / rcu / rcutorture . c <nl> ppp kernel / rcu / rcutorture . c <nl> rcu_torture_init ( void ) <nl> if ( firsterr ) <nl> goto unwind ; <nl> } <nl> - if ( test_no_idle_hz ) { <nl> + if ( test_no_idle_hz && shuffle_interval > 0 ) { <nl> firsterr = torture_shuffle_init ( shuffle_interval * HZ ); <nl> if ( firsterr ) <nl> goto unwind ;
mmm drivers / net / hyperv / netvsc . c <nl> ppp drivers / net / hyperv / netvsc . c <nl> int netvsc_send ( struct hv_device * device , <nl> if (! net_device ) <nl> return - ENODEV ; <nl>  <nl> + /* We may race with netvsc_connect_vsp ()/ netvsc_init_buf () and get <nl> + * here before the negotiation with the host is finished and <nl> + * send_section_map may not be allocated yet . <nl> + */ <nl> + if (! net_device -> send_section_map ) <nl> + return - EAGAIN ; <nl> + <nl> out_channel = net_device -> chn_table [ q_idx ]; <nl>  <nl> packet -> send_buf_index = NETVSC_INVALID_INDEX ;
mmm drivers / media / rc / ir - lirc - codec . c <nl> ppp drivers / media / rc / ir - lirc - codec . c <nl> static long ir_lirc_ioctl ( struct file * filep , unsigned int cmd , <nl> return 0 ; <nl>  <nl> case LIRC_GET_REC_RESOLUTION : <nl> + if (! dev -> rx_resolution ) <nl> + return - ENOTTY ; <nl> + <nl> val = dev -> rx_resolution ; <nl> break ; <nl>  <nl> static int ir_lirc_register ( struct rc_dev * dev ) <nl> if ( rc ) <nl> goto rbuf_init_failed ; <nl>  <nl> - if ( dev -> driver_type != RC_DRIVER_IR_RAW_TX ) <nl> + if ( dev -> driver_type != RC_DRIVER_IR_RAW_TX ) { <nl> features |= LIRC_CAN_REC_MODE2 ; <nl> + if ( dev -> rx_resolution ) <nl> + features |= LIRC_CAN_GET_REC_RESOLUTION ; <nl> + } <nl> if ( dev -> tx_ir ) { <nl> features |= LIRC_CAN_SEND_PULSE ; <nl> if ( dev -> s_tx_mask )
mmm drivers / net / ethernet / mellanox / mlxsw / spectrum . c <nl> ppp drivers / net / ethernet / mellanox / mlxsw / spectrum . c <nl> mlxsw_sp_port_add_cls_matchall_mirror ( struct mlxsw_sp_port * mlxsw_sp_port , <nl>  <nl> if (! mlxsw_sp_port_dev_check ( to_dev )) { <nl> netdev_err ( mlxsw_sp_port -> dev , " Cannot mirror to a non - spectrum port "); <nl> - return - ENOTSUPP ; <nl> + return - EOPNOTSUPP ; <nl> } <nl> to_port = netdev_priv ( to_dev ); <nl>  <nl> static int mlxsw_sp_port_add_cls_matchall ( struct mlxsw_sp_port * mlxsw_sp_port , <nl>  <nl> if (! tc_single_action ( cls -> exts )) { <nl> netdev_err ( mlxsw_sp_port -> dev , " only singular actions are supported \ n "); <nl> - return - ENOTSUPP ; <nl> + return - EOPNOTSUPP ; <nl> } <nl>  <nl> mall_tc_entry = kzalloc ( sizeof (* mall_tc_entry ), GFP_KERNEL ); <nl> static int mlxsw_sp_setup_tc ( struct net_device * dev , u32 handle , <nl> } <nl> } <nl>  <nl> - return - ENOTSUPP ; <nl> + return - EOPNOTSUPP ; <nl> } <nl>  <nl> static const struct net_device_ops mlxsw_sp_port_netdev_ops = { <nl> mlxsw_sp_get_hw_stats_by_group ( struct mlxsw_sp_port_hw_stats ** p_hw_stats , <nl> break ; <nl> default : <nl> WARN_ON ( 1 ); <nl> - return - ENOTSUPP ; <nl> + return - EOPNOTSUPP ; <nl> } <nl> return 0 ; <nl> }
mmm drivers / net / ethernet / intel / i40e / i40e_main . c <nl> ppp drivers / net / ethernet / intel / i40e / i40e_main . c <nl> void i40e_vsi_reset_stats ( struct i40e_vsi * vsi ) <nl> **/ <nl> void i40e_pf_reset_stats ( struct i40e_pf * pf ) <nl> { <nl> + int i ; <nl> + <nl> memset (& pf -> stats , 0 , sizeof ( pf -> stats )); <nl> memset (& pf -> stats_offsets , 0 , sizeof ( pf -> stats_offsets )); <nl> pf -> stat_offsets_loaded = false ; <nl> + <nl> + for ( i = 0 ; i < I40E_MAX_VEB ; i ++) { <nl> + if ( pf -> veb [ i ]) { <nl> + memset (& pf -> veb [ i ]-> stats , 0 , <nl> + sizeof ( pf -> veb [ i ]-> stats )); <nl> + memset (& pf -> veb [ i ]-> stats_offsets , 0 , <nl> + sizeof ( pf -> veb [ i ]-> stats_offsets )); <nl> + pf -> veb [ i ]-> stat_offsets_loaded = false ; <nl> + } <nl> + } <nl> } <nl>  <nl> /**
mmm include / linux / mm . h <nl> ppp include / linux / mm . h <nl> struct shrink_control { <nl> struct shrinker { <nl> int (* shrink )( struct shrinker *, struct shrink_control * sc ); <nl> int seeks ; /* seeks to recreate an obj */ <nl> + long batch ; /* reclaim batch size , 0 = default */ <nl>  <nl> /* These are for internal use */ <nl> struct list_head list ;mmm mm / vmscan . c <nl> ppp mm / vmscan . c <nl> struct shrink_control { <nl> struct shrinker { <nl> int (* shrink )( struct shrinker *, struct shrink_control * sc ); <nl> int seeks ; /* seeks to recreate an obj */ <nl> + long batch ; /* reclaim batch size , 0 = default */ <nl>  <nl> /* These are for internal use */ <nl> struct list_head list ; <nl> unsigned long shrink_slab ( struct shrink_control * shrink , <nl> int shrink_ret = 0 ; <nl> long nr ; <nl> long new_nr ; <nl> + long batch_size = shrinker -> batch ? shrinker -> batch <nl> + : SHRINK_BATCH ; <nl>  <nl> /* <nl> * copy the current shrinker scan count into a local variable <nl> unsigned long shrink_slab ( struct shrink_control * shrink , <nl> nr_pages_scanned , lru_pages , <nl> max_pass , delta , total_scan ); <nl>  <nl> - while ( total_scan >= SHRINK_BATCH ) { <nl> - long this_scan = SHRINK_BATCH ; <nl> + while ( total_scan >= batch_size ) { <nl> int nr_before ; <nl>  <nl> nr_before = do_shrinker_shrink ( shrinker , shrink , 0 ); <nl> shrink_ret = do_shrinker_shrink ( shrinker , shrink , <nl> - this_scan ); <nl> + batch_size ); <nl> if ( shrink_ret == - 1 ) <nl> break ; <nl> if ( shrink_ret < nr_before ) <nl> ret += nr_before - shrink_ret ; <nl> - count_vm_events ( SLABS_SCANNED , this_scan ); <nl> - total_scan -= this_scan ; <nl> + count_vm_events ( SLABS_SCANNED , batch_size ); <nl> + total_scan -= batch_size ; <nl>  <nl> cond_resched (); <nl> }
mmm fs / aio . c <nl> ppp fs / aio . c <nl> static int read_events ( struct kioctx * ctx , <nl> break ; <nl> if ( min_nr <= i ) <nl> break ; <nl> - ret = 0 ; <nl> + if ( unlikely ( ctx -> dead )) { <nl> + ret = - EINVAL ; <nl> + break ; <nl> + } <nl> if ( to . timed_out ) /* Only check after read evt */ <nl> break ; <nl> /* Try to only show up in io wait if there are ops <nl> static void io_destroy ( struct kioctx * ioctx ) <nl>  <nl> aio_cancel_all ( ioctx ); <nl> wait_for_all_aios ( ioctx ); <nl> + <nl> + /* <nl> + * Wake up any waiters . The setting of ctx -> dead must be seen <nl> + * by other CPUs at this point . Right now , we rely on the <nl> + * locking done by the above calls to ensure this consistency . <nl> + */ <nl> + wake_up (& ioctx -> wait ); <nl> put_ioctx ( ioctx ); /* once for the lookup */ <nl> } <nl> 
mmm net / ipv4 / ip_output . c <nl> ppp net / ipv4 / ip_output . c <nl> static inline int ip_ufo_append_data ( struct sock * sk , <nl> /* initialize protocol header pointer */ <nl> skb -> transport_header = skb -> network_header + fragheaderlen ; <nl>  <nl> - skb -> ip_summed = CHECKSUM_PARTIAL ; <nl> skb -> csum = 0 ; <nl>  <nl> - /* specify the length of each IP datagram fragment */ <nl> - skb_shinfo ( skb )-> gso_size = maxfraglen - fragheaderlen ; <nl> - skb_shinfo ( skb )-> gso_type = SKB_GSO_UDP ; <nl> + <nl> __skb_queue_tail ( queue , skb ); <nl> + } else if ( skb_is_gso ( skb )) { <nl> + goto append ; <nl> } <nl>  <nl> + skb -> ip_summed = CHECKSUM_PARTIAL ; <nl> + /* specify the length of each IP datagram fragment */ <nl> + skb_shinfo ( skb )-> gso_size = maxfraglen - fragheaderlen ; <nl> + skb_shinfo ( skb )-> gso_type = SKB_GSO_UDP ; <nl> + <nl> + append : <nl> return skb_append_datato_frags ( sk , skb , getfrag , from , <nl> ( length - transhdrlen )); <nl> }
mmm include / linux / rpmsg . h <nl> ppp include / linux / rpmsg . h <nl> struct rpmsg_channel_info { <nl> * rpmsg_device - device that belong to the rpmsg bus <nl> * @ dev : the device struct <nl> * @ id : device id ( used to match between rpmsg drivers and devices ) <nl> + * @ driver_override : driver name to force a match <nl> * @ src : local address <nl> * @ dst : destination address <nl> * @ ept : the rpmsg endpoint of this channel <nl> struct rpmsg_channel_info { <nl> struct rpmsg_device { <nl> struct device dev ; <nl> struct rpmsg_device_id id ; <nl> + char * driver_override ; <nl> u32 src ; <nl> u32 dst ; <nl> struct rpmsg_endpoint * ept ;mmm drivers / rpmsg / rpmsg_core . c <nl> ppp drivers / rpmsg / rpmsg_core . c <nl> struct rpmsg_channel_info { <nl> * rpmsg_device - device that belong to the rpmsg bus <nl> * @ dev : the device struct <nl> * @ id : device id ( used to match between rpmsg drivers and devices ) <nl> + * @ driver_override : driver name to force a match <nl> * @ src : local address <nl> * @ dst : destination address <nl> * @ ept : the rpmsg endpoint of this channel <nl> struct rpmsg_channel_info { <nl> struct rpmsg_device { <nl> struct device dev ; <nl> struct rpmsg_device_id id ; <nl> + char * driver_override ; <nl> u32 src ; <nl> u32 dst ; <nl> struct rpmsg_endpoint * ept ; <nl> static int rpmsg_dev_match ( struct device * dev , struct device_driver * drv ) <nl> const struct rpmsg_device_id * ids = rpdrv -> id_table ; <nl> unsigned int i ; <nl>  <nl> + if ( rpdev -> driver_override ) <nl> + return ! strcmp ( rpdev -> driver_override , drv -> name ); <nl> + <nl> if ( ids ) <nl> for ( i = 0 ; ids [ i ]. name [ 0 ]; i ++) <nl> if ( rpmsg_id_match ( rpdev , & ids [ i ]))
mmm fs / xfs / xfs_sysfs . c <nl> ppp fs / xfs / xfs_sysfs . c <nl> xfs_error_get_cfg ( <nl> { <nl> struct xfs_error_cfg * cfg ; <nl>  <nl> + if ( error < 0 ) <nl> + error = - error ; <nl> + <nl> switch ( error ) { <nl> case EIO : <nl> cfg = & mp -> m_error_cfg [ error_class ][ XFS_ERR_EIO ];
mmm arch / arm / mach - omap2 / board - omap3beagle . c <nl> ppp arch / arm / mach - omap2 / board - omap3beagle . c <nl> static int beagle_twl_gpio_setup ( struct device * dev , <nl>  <nl> /* TWL4030_GPIO_MAX + 0 == ledA , EHCI nEN_USB_PWR ( out , active low ) */ <nl> gpio_request ( gpio + TWL4030_GPIO_MAX , " nEN_USB_PWR "); <nl> - gpio_direction_output ( gpio + TWL4030_GPIO_MAX , 1 ); <nl> + gpio_direction_output ( gpio + TWL4030_GPIO_MAX , 0 ); <nl>  <nl> /* TWL4030_GPIO_MAX + 1 == ledB , PMU_STAT ( out , active low LED ) */ <nl> gpio_leds [ 2 ]. gpio = gpio + TWL4030_GPIO_MAX + 1 ;
mmm net / tipc / discover . c <nl> ppp net / tipc / discover . c <nl> void tipc_disc_rcv ( struct net * net , struct sk_buff * skb , <nl> u16 caps = msg_node_capabilities ( hdr ); <nl> bool respond = false ; <nl> bool dupl_addr = false ; <nl> + int err ; <nl>  <nl> - bearer -> media -> msg2addr ( bearer , & maddr , msg_media_addr ( hdr )); <nl> + err = bearer -> media -> msg2addr ( bearer , & maddr , msg_media_addr ( hdr )); <nl> kfree_skb ( skb ); <nl> + if ( err ) <nl> + return ; <nl>  <nl> /* Ensure message from node is valid and communication is permitted */ <nl> if ( net_id != tn -> net_id )
mmm drivers / rtc / rtc - sun6i . c <nl> ppp drivers / rtc / rtc - sun6i . c <nl> static void __init sun6i_rtc_clk_init ( struct device_node * node ) <nl>  <nl> clk_data = kzalloc ( sizeof (* clk_data ) + ( sizeof (* clk_data -> hws ) * 2 ), <nl> GFP_KERNEL ); <nl> - if (! clk_data ) <nl> + if (! clk_data ) { <nl> + kfree ( rtc ); <nl> return ; <nl> + } <nl>  <nl> spin_lock_init (& rtc -> lock ); <nl> 
mmm fs / logfs / logfs . h <nl> ppp fs / logfs / logfs . h <nl> static inline int logfs_get_sb_bdev ( struct logfs_super * s , <nl>  <nl> /* dev_mtd . c */ <nl> # ifdef CONFIG_MTD <nl> - int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ) <nl> + int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ); <nl> # else <nl> static inline int logfs_get_sb_mtd ( struct logfs_super * s , int mtdnr ) <nl> {
mmm drivers / w1 / masters / ds2490 . c <nl> ppp drivers / w1 / masters / ds2490 . c <nl> static int ds_recv_status_nodump ( struct ds_device * dev , struct ds_status * st , <nl> { <nl> int count , err ; <nl>  <nl> - memset ( st , 0 , sizeof ( st )); <nl> + memset ( st , 0 , sizeof (* st )); <nl>  <nl> count = 0 ; <nl> err = usb_bulk_msg ( dev -> udev , usb_rcvbulkpipe ( dev -> udev , dev -> ep [ EP_STATUS ]), buf , size , & count , 100 );
mmm fs / namespace . c <nl> ppp fs / namespace . c <nl> static int do_loopback ( struct path * path , const char * old_name , <nl>  <nl> if ( IS_ERR ( mnt )) { <nl> err = PTR_ERR ( mnt ); <nl> - goto out ; <nl> + goto out2 ; <nl> } <nl>  <nl> err = graft_tree ( mnt , path );
mmm net / wireless / util . c <nl> ppp net / wireless / util . c <nl> int cfg80211_validate_key_settings ( struct cfg80211_registered_device * rdev , <nl> struct key_params * params , int key_idx , <nl> bool pairwise , const u8 * mac_addr ) <nl> { <nl> - if ( key_idx > 5 ) <nl> + if ( key_idx < 0 || key_idx > 5 ) <nl> return - EINVAL ; <nl>  <nl> if (! pairwise && mac_addr && !( rdev -> wiphy . flags & WIPHY_FLAG_IBSS_RSN )) <nl> int cfg80211_validate_key_settings ( struct cfg80211_registered_device * rdev , <nl> /* Disallow BIP ( group - only ) cipher as pairwise cipher */ <nl> if ( pairwise ) <nl> return - EINVAL ; <nl> + if ( key_idx < 4 ) <nl> + return - EINVAL ; <nl> break ; <nl> + case WLAN_CIPHER_SUITE_WEP40 : <nl> + case WLAN_CIPHER_SUITE_WEP104 : <nl> + if ( key_idx > 3 ) <nl> + return - EINVAL ; <nl> default : <nl> break ; <nl> }
mmm kernel / sched / core . c <nl> ppp kernel / sched / core . c <nl> migration_call ( struct notifier_block * nfb , unsigned long action , void * hcpu ) <nl> migrate_tasks ( rq ); <nl> BUG_ON ( rq -> nr_running != 1 ); /* the migration thread */ <nl> raw_spin_unlock_irqrestore (& rq -> lock , flags ); <nl> - break ; <nl> - <nl> - case CPU_DEAD : <nl> calc_load_migrate ( rq ); <nl> break ; <nl> # endif
mmm drivers / idle / intel_idle . c <nl> ppp drivers / idle / intel_idle . c <nl> static int __init intel_idle_init ( void ) <nl> if ( retval ) <nl> return retval ; <nl>  <nl> + intel_idle_cpuidle_devices = alloc_percpu ( struct cpuidle_device ); <nl> + if ( intel_idle_cpuidle_devices == NULL ) <nl> + return - ENOMEM ; <nl> + <nl> intel_idle_cpuidle_driver_init (); <nl> retval = cpuidle_register_driver (& intel_idle_driver ); <nl> if ( retval ) { <nl> struct cpuidle_driver * drv = cpuidle_get_driver (); <nl> printk ( KERN_DEBUG PREFIX " intel_idle yielding to % s ", <nl> drv ? drv -> name : " none "); <nl> + free_percpu ( intel_idle_cpuidle_devices ); <nl> return retval ; <nl> } <nl>  <nl> - intel_idle_cpuidle_devices = alloc_percpu ( struct cpuidle_device ); <nl> - if ( intel_idle_cpuidle_devices == NULL ) <nl> - return - ENOMEM ; <nl> - <nl> cpu_notifier_register_begin (); <nl>  <nl> for_each_online_cpu ( i ) {
mmm tools / perf / util / symbol . c <nl> ppp tools / perf / util / symbol . c <nl> static int dso__load_sym ( struct dso * self , int fd , const char * name , <nl>  <nl> nr_syms = shdr . sh_size / shdr . sh_entsize ; <nl>  <nl> + memset (& sym , 0 , sizeof ( sym )); <nl> + <nl> elf_symtab__for_each_symbol ( syms , nr_syms , index , sym ) { <nl> struct symbol * f ; <nl> uint64_t obj_start ;
mmm drivers / nvme / host / core . c <nl> ppp drivers / nvme / host / core . c <nl> int __nvme_submit_user_cmd ( struct request_queue * q , struct nvme_command * cmd , <nl> goto out_unmap ; <nl> } <nl>  <nl> - if ( meta_buffer ) { <nl> + if ( meta_buffer && meta_len ) { <nl> struct bio_integrity_payload * bip ; <nl>  <nl> meta = kmalloc ( meta_len , GFP_KERNEL );
mmm fs / btrfs / extent - tree . c <nl> ppp fs / btrfs / extent - tree . c <nl> struct btrfs_block_group_cache * btrfs_find_block_group ( struct btrfs_root * root , <nl> if ( search_start ) { <nl> struct btrfs_block_group_cache * shint ; <nl> shint = btrfs_lookup_block_group ( info , search_start ); <nl> - if ( shint -> data == data ) { <nl> + if ( shint && shint -> data == data ) { <nl> used = btrfs_block_group_used (& shint -> item ); <nl> if ( used + shint -> pinned < <nl> div_factor ( shint -> key . offset , factor )) { <nl> struct buffer_head * btrfs_alloc_free_block ( struct btrfs_trans_handle * trans , <nl> struct buffer_head * buf ; <nl>  <nl> ret = btrfs_alloc_extent ( trans , root , root -> root_key . objectid , <nl> - 1 , empty_size , hint , <nl> - ( unsigned long )- 1 , & ins , 0 ); <nl> + 1 , empty_size , hint , ( u64 )- 1 , & ins , 0 ); <nl> if ( ret ) { <nl> BUG_ON ( ret > 0 ); <nl> return ERR_PTR ( ret );
mmm kernel / bpf / verifier . c <nl> ppp kernel / bpf / verifier . c <nl> static int check_stack_boundary ( struct bpf_verifier_env * env , int regno , <nl> tnum_strn ( tn_buf , sizeof ( tn_buf ), regs [ regno ]. var_off ); <nl> verbose ( env , " invalid variable stack read R % d var_off =% s \ n ", <nl> regno , tn_buf ); <nl> + return - EACCES ; <nl> } <nl> off = regs [ regno ]. off + regs [ regno ]. var_off . value ; <nl> if ( off >= 0 || off < - MAX_BPF_STACK || off + access_size > 0 ||
mmm drivers / staging / unisys / visorbus / visorchipset . c <nl> ppp drivers / staging / unisys / visorbus / visorchipset . c <nl> static ssize_t error_store ( struct device * dev , struct device_attribute * attr , <nl> const char * buf , size_t count ) <nl> { <nl> u32 error ; <nl> - int ret ; <nl> + int err ; <nl>  <nl> if ( kstrtou32 ( buf , 10 , & error )) <nl> return - EINVAL ; <nl>  <nl> - ret = visorchannel_write <nl> + err = visorchannel_write <nl> ( chipset_dev -> controlvm_channel , <nl> offsetof ( struct spar_controlvm_channel_protocol , <nl> installation_error ), <nl> & error , sizeof ( u32 )); <nl> - if ( ret ) <nl> - return ret ; <nl> + if ( err ) <nl> + return err ; <nl> return count ; <nl> } <nl> static DEVICE_ATTR_RW ( error );
mmm include / linux / topology . h <nl> ppp include / linux / topology . h <nl> void arch_update_cpu_topology ( void ); <nl> . busy_idx = 3 , \ <nl> . idle_idx = 3 , \ <nl> . flags = SD_LOAD_BALANCE \ <nl> - | SD_SERIALIZE , \ <nl> + | SD_BALANCE_NEWIDLE \ <nl> + | SD_WAKE_AFFINE \ <nl> + | SD_SERIALIZE , \ <nl> . last_balance = jiffies , \ <nl> . balance_interval = 64 , \ <nl> }
mmm block / partitions / efi . c <nl> ppp block / partitions / efi . c <nl> static gpt_entry * alloc_read_gpt_entries ( struct parsed_partitions * state , <nl> le32_to_cpu ( gpt -> sizeof_partition_entry ); <nl> if (! count ) <nl> return NULL ; <nl> - pte = kzalloc ( count , GFP_KERNEL ); <nl> + pte = kmalloc ( count , GFP_KERNEL ); <nl> if (! pte ) <nl> return NULL ; <nl>  <nl> static gpt_header * alloc_read_gpt_header ( struct parsed_partitions * state , <nl> gpt_header * gpt ; <nl> unsigned ssz = bdev_logical_block_size ( state -> bdev ); <nl>  <nl> - gpt = kzalloc ( ssz , GFP_KERNEL ); <nl> + gpt = kmalloc ( ssz , GFP_KERNEL ); <nl> if (! gpt ) <nl> return NULL ; <nl> 
mmm fs / cifs / transport . c <nl> ppp fs / cifs / transport . c <nl> smb_send_kvec ( struct TCP_Server_Info * server , struct kvec * iov , size_t n_vec , <nl>  <nl> * sent = 0 ; <nl>  <nl> - if ( ssocket == NULL ) <nl> - return - ENOTSOCK ; /* BB eventually add reconnect code here */ <nl> - <nl> smb_msg . msg_name = ( struct sockaddr *) & server -> dstaddr ; <nl> smb_msg . msg_namelen = sizeof ( struct sockaddr ); <nl> smb_msg . msg_control = NULL ; <nl> smb_send_rqst ( struct TCP_Server_Info * server , struct smb_rqst * rqst ) <nl> struct socket * ssocket = server -> ssocket ; <nl> int val = 1 ; <nl>  <nl> + if ( ssocket == NULL ) <nl> + return - ENOTSOCK ; <nl> + <nl> cFYI ( 1 , " Sending smb : smb_len =% u ", smb_buf_length ); <nl> dump_smb ( iov [ 0 ]. iov_base , iov [ 0 ]. iov_len ); <nl> 
mmm drivers / irqchip / irq - stm32 - exti . c <nl> ppp drivers / irqchip / irq - stm32 - exti . c <nl> static void stm32_exti_free ( struct irq_domain * d , unsigned int virq , <nl> irq_domain_reset_irq_data ( data ); <nl> } <nl>  <nl> - struct irq_domain_ops irq_exti_domain_ops = { <nl> + static const struct irq_domain_ops irq_exti_domain_ops = { <nl> . map = irq_map_generic_chip , <nl> . alloc = stm32_exti_alloc , <nl> . free = stm32_exti_free , <nl> __init stm32_exti_init ( const struct stm32_exti_bank ** stm32_exti_banks , <nl> handle_edge_irq , clr , 0 , 0 ); <nl> if ( ret ) { <nl> pr_err ("% pOF : Could not allocate generic interrupt chip .\ n ", <nl> - node ); <nl> + node ); <nl> goto out_free_domain ; <nl> } <nl> 
mmm drivers / scsi / lpfc / lpfc_attr . c <nl> ppp drivers / scsi / lpfc / lpfc_attr . c <nl> lpfc_nvme_info_show ( struct device * dev , struct device_attribute * attr , <nl> wwn_to_u64 ( vport -> fc_nodename . u . wwn ), <nl> phba -> targetport -> port_id ); <nl>  <nl> - len += snprintf ( buf + len , PAGE_SIZE , <nl> + len += snprintf ( buf + len , PAGE_SIZE - len , <nl> "\ nNVME Target : Statistics \ n "); <nl> tgtp = ( struct lpfc_nvmet_tgtport *) phba -> targetport -> private ; <nl> len += snprintf ( buf + len , PAGE_SIZE - len , <nl> lpfc_nvme_info_show ( struct device * dev , struct device_attribute * attr , <nl> } <nl> spin_unlock_irq ( shost -> host_lock ); <nl>  <nl> - len += snprintf ( buf + len , PAGE_SIZE , "\ nNVME Statistics \ n "); <nl> + len += snprintf ( buf + len , PAGE_SIZE - len , "\ nNVME Statistics \ n "); <nl> len += snprintf ( buf + len , PAGE_SIZE - len , <nl> " LS : Xmt % 016llx Cmpl % 016llx \ n ", <nl> phba -> fc4NvmeLsRequests ,
mmm net / netfilter / nf_tables_api . c <nl> ppp net / netfilter / nf_tables_api . c <nl> static int nf_tables_getset ( struct net * net , struct sock * nlsk , <nl> /* Only accept unspec with dump */ <nl> if ( nfmsg -> nfgen_family == NFPROTO_UNSPEC ) <nl> return - EAFNOSUPPORT ; <nl> + if (! nla [ NFTA_SET_TABLE ]) <nl> + return - EINVAL ; <nl>  <nl> set = nf_tables_set_lookup ( ctx . table , nla [ NFTA_SET_NAME ]); <nl> if ( IS_ERR ( set ))
mmm sound / pci / hda / hda_codec . c <nl> ppp sound / pci / hda / hda_codec . c <nl> int snd_hda_multi_out_analog_open ( struct hda_codec * codec , <nl> if ( mout -> spdif_maxbps < hinfo -> maxbps ) <nl> hinfo -> maxbps = mout -> spdif_maxbps ; <nl> } <nl> + mutex_unlock (& codec -> spdif_mutex ); <nl> } <nl> - mutex_unlock (& codec -> spdif_mutex ); <nl> return snd_pcm_hw_constraint_step ( substream -> runtime , 0 , <nl> SNDRV_PCM_HW_PARAM_CHANNELS , 2 ); <nl> }
mmm drivers / regulator / max8973 - regulator . c <nl> ppp drivers / regulator / max8973 - regulator . c <nl> static int max8973_probe ( struct i2c_client * client , <nl> } <nl>  <nl> if ( pdata ) { <nl> - max -> dvs_gpio = pdata -> dvs_gpio ; <nl> + max -> dvs_gpio = ( pdata -> dvs_gpio ) ? pdata -> dvs_gpio : - EINVAL ; <nl> max -> enable_external_control = pdata -> enable_ext_control ; <nl> max -> curr_gpio_val = pdata -> dvs_def_state ; <nl> max -> curr_vout_reg = MAX8973_VOUT + pdata -> dvs_def_state ;
mmm drivers / scsi / scsi_debug . c <nl> ppp drivers / scsi / scsi_debug . c <nl> static int inquiry_evpd_b0 ( unsigned char * arr ) <nl> return sizeof ( vpdb0_data ); <nl> } <nl>  <nl> + static int inquiry_evpd_b1 ( unsigned char * arr ) <nl> +{ <nl> + memset ( arr , 0 , 0x3c ); <nl> + arr [ 0 ] = 0 ; <nl> + arr [ 1 ] = 1 ; <nl> + <nl> + return 0x3c ; <nl> +} <nl>  <nl> # define SDEBUG_LONG_INQ_SZ 96 <nl> # define SDEBUG_MAX_INQ_ARR_SZ 584 <nl> static int resp_inquiry ( struct scsi_cmnd * scp , int target , <nl> arr [ n ++] = 0x88 ; /* SCSI ports */ <nl> arr [ n ++] = 0x89 ; /* ATA information */ <nl> arr [ n ++] = 0xb0 ; /* Block limits ( SBC ) */ <nl> + arr [ n ++] = 0xb1 ; /* Block characteristics ( SBC ) */ <nl> arr [ 3 ] = n - 4 ; /* number of supported VPD pages */ <nl> } else if ( 0x80 == cmd [ 2 ]) { /* unit serial number */ <nl> arr [ 1 ] = cmd [ 2 ]; /* sanity */ <nl> static int resp_inquiry ( struct scsi_cmnd * scp , int target , <nl> } else if ( 0xb0 == cmd [ 2 ]) { /* Block limits ( SBC ) */ <nl> arr [ 1 ] = cmd [ 2 ]; /* sanity */ <nl> arr [ 3 ] = inquiry_evpd_b0 (& arr [ 4 ]); <nl> + } else if ( 0xb1 == cmd [ 2 ]) { /* Block characteristics ( SBC ) */ <nl> + arr [ 1 ] = cmd [ 2 ]; /* sanity */ <nl> + arr [ 3 ] = inquiry_evpd_b1 (& arr [ 4 ]); <nl> } else { <nl> /* Illegal request , invalid field in cdb */ <nl> mk_sense_buffer ( devip , ILLEGAL_REQUEST ,
mmm drivers / gpu / drm / nouveau / nouveau_bios . c <nl> ppp drivers / gpu / drm / nouveau / nouveau_bios . c <nl> int get_pll_limits ( struct drm_device * dev , uint32_t limit_match , struct pll_lims <nl> break ; <nl> } <nl>  <nl> + if (( dev_priv -> card_type >= NV_50 ) && ( pllindex == 0 )) { <nl> + NV_ERROR ( dev , " Register 0x % 08x not found in PLL " <nl> + " limits table ", pll_lim -> reg ); <nl> + return - ENOENT ; <nl> + } <nl> + <nl> pll_rec = & bios -> data [ plloffs + recordlen * pllindex ]; <nl>  <nl> BIOSLOG ( bios , " Loading PLL limits for reg 0x % 08x \ n ",
mmm drivers / infiniband / core / uverbs_main . c <nl> ppp drivers / infiniband / core / uverbs_main . c <nl> static ssize_t ib_uverbs_write ( struct file * filp , const char __user * buf , <nl> goto out ; <nl> } <nl>  <nl> + if (! file -> ucontext && <nl> + command != IB_USER_VERBS_CMD_GET_CONTEXT ) { <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> flags = ( hdr . command & <nl> IB_USER_VERBS_CMD_FLAGS_MASK ) >> IB_USER_VERBS_CMD_FLAGS_SHIFT ; <nl>  <nl> static ssize_t ib_uverbs_write ( struct file * filp , const char __user * buf , <nl> goto out ; <nl> } <nl>  <nl> - if (! file -> ucontext && <nl> - command != IB_USER_VERBS_CMD_GET_CONTEXT ) { <nl> - ret = - EINVAL ; <nl> - goto out ; <nl> - } <nl> - <nl> if ( hdr . in_words * 4 != count ) { <nl> ret = - EINVAL ; <nl> goto out ;
mmm fs / ubifs / lpt . c <nl> ppp fs / ubifs / lpt . c <nl> int ubifs_lpt_scan_nolock ( struct ubifs_info * c , int start_lnum , int end_lnum , <nl>  <nl> if ( path [ h ]. in_tree ) <nl> continue ; <nl> - nnode = kmalloc ( sz , GFP_NOFS ); <nl> + nnode = kmemdup (& path [ h ]. nnode , sz , GFP_NOFS ); <nl> if (! nnode ) { <nl> err = - ENOMEM ; <nl> goto out ; <nl> } <nl> - memcpy ( nnode , & path [ h ]. nnode , sz ); <nl> parent = nnode -> parent ; <nl> parent -> nbranch [ nnode -> iip ]. nnode = nnode ; <nl> path [ h ]. ptr . nnode = nnode ; <nl> int ubifs_lpt_scan_nolock ( struct ubifs_info * c , int start_lnum , int end_lnum , <nl> const size_t sz = sizeof ( struct ubifs_pnode ); <nl> struct ubifs_nnode * parent ; <nl>  <nl> - pnode = kmalloc ( sz , GFP_NOFS ); <nl> + pnode = kmemdup (& path [ h ]. pnode , sz , GFP_NOFS ); <nl> if (! pnode ) { <nl> err = - ENOMEM ; <nl> goto out ; <nl> } <nl> - memcpy ( pnode , & path [ h ]. pnode , sz ); <nl> parent = pnode -> parent ; <nl> parent -> nbranch [ pnode -> iip ]. pnode = pnode ; <nl> path [ h ]. ptr . pnode = pnode ;mmm fs / ubifs / xattr . c <nl> ppp fs / ubifs / xattr . c <nl> int ubifs_lpt_scan_nolock ( struct ubifs_info * c , int start_lnum , int end_lnum , <nl>  <nl> if ( path [ h ]. in_tree ) <nl> continue ; <nl> - nnode = kmalloc ( sz , GFP_NOFS ); <nl> + nnode = kmemdup (& path [ h ]. nnode , sz , GFP_NOFS ); <nl> if (! nnode ) { <nl> err = - ENOMEM ; <nl> goto out ; <nl> } <nl> - memcpy ( nnode , & path [ h ]. nnode , sz ); <nl> parent = nnode -> parent ; <nl> parent -> nbranch [ nnode -> iip ]. nnode = nnode ; <nl> path [ h ]. ptr . nnode = nnode ; <nl> int ubifs_lpt_scan_nolock ( struct ubifs_info * c , int start_lnum , int end_lnum , <nl> const size_t sz = sizeof ( struct ubifs_pnode ); <nl> struct ubifs_nnode * parent ; <nl>  <nl> - pnode = kmalloc ( sz , GFP_NOFS ); <nl> + pnode = kmemdup (& path [ h ]. pnode , sz , GFP_NOFS ); <nl> if (! pnode ) { <nl> err = - ENOMEM ; <nl> goto out ; <nl> } <nl> - memcpy ( pnode , & path [ h ]. pnode , sz ); <nl> parent = pnode -> parent ; <nl> parent -> nbranch [ pnode -> iip ]. pnode = pnode ; <nl> path [ h ]. ptr . pnode = pnode ; <nl> static int create_xattr ( struct ubifs_info * c , struct inode * host , <nl> ui = ubifs_inode ( inode ); <nl> ui -> xattr = 1 ; <nl> ui -> flags |= UBIFS_XATTR_FL ; <nl> - ui -> data = kmalloc ( size , GFP_NOFS ); <nl> + ui -> data = kmemdup ( value , size , GFP_NOFS ); <nl> if (! ui -> data ) { <nl> err = - ENOMEM ; <nl> goto out_free ; <nl> } <nl> - memcpy ( ui -> data , value , size ); <nl> inode -> i_size = ui -> ui_size = size ; <nl> ui -> data_len = size ; <nl>  <nl> static int change_xattr ( struct ubifs_info * c , struct inode * host , <nl> return err ; <nl>  <nl> kfree ( ui -> data ); <nl> - ui -> data = kmalloc ( size , GFP_NOFS ); <nl> + ui -> data = kmemdup ( value , size , GFP_NOFS ); <nl> if (! ui -> data ) { <nl> err = - ENOMEM ; <nl> goto out_free ; <nl> } <nl> - memcpy ( ui -> data , value , size ); <nl> inode -> i_size = ui -> ui_size = size ; <nl> ui -> data_len = size ; <nl> mmm fs / ubifs / tnc . c <nl> ppp fs / ubifs / tnc . c <nl> int ubifs_lpt_scan_nolock ( struct ubifs_info * c , int start_lnum , int end_lnum , <nl>  <nl> if ( path [ h ]. in_tree ) <nl> continue ; <nl> - nnode = kmalloc ( sz , GFP_NOFS ); <nl> + nnode = kmemdup (& path [ h ]. nnode , sz , GFP_NOFS ); <nl> if (! nnode ) { <nl> err = - ENOMEM ; <nl> goto out ; <nl> } <nl> - memcpy ( nnode , & path [ h ]. nnode , sz ); <nl> parent = nnode -> parent ; <nl> parent -> nbranch [ nnode -> iip ]. nnode = nnode ; <nl> path [ h ]. ptr . nnode = nnode ; <nl> int ubifs_lpt_scan_nolock ( struct ubifs_info * c , int start_lnum , int end_lnum , <nl> const size_t sz = sizeof ( struct ubifs_pnode ); <nl> struct ubifs_nnode * parent ; <nl>  <nl> - pnode = kmalloc ( sz , GFP_NOFS ); <nl> + pnode = kmemdup (& path [ h ]. pnode , sz , GFP_NOFS ); <nl> if (! pnode ) { <nl> err = - ENOMEM ; <nl> goto out ; <nl> } <nl> - memcpy ( pnode , & path [ h ]. pnode , sz ); <nl> parent = pnode -> parent ; <nl> parent -> nbranch [ pnode -> iip ]. pnode = pnode ; <nl> path [ h ]. ptr . pnode = pnode ; <nl> static int create_xattr ( struct ubifs_info * c , struct inode * host , <nl> ui = ubifs_inode ( inode ); <nl> ui -> xattr = 1 ; <nl> ui -> flags |= UBIFS_XATTR_FL ; <nl> - ui -> data = kmalloc ( size , GFP_NOFS ); <nl> + ui -> data = kmemdup ( value , size , GFP_NOFS ); <nl> if (! ui -> data ) { <nl> err = - ENOMEM ; <nl> goto out_free ; <nl> } <nl> - memcpy ( ui -> data , value , size ); <nl> inode -> i_size = ui -> ui_size = size ; <nl> ui -> data_len = size ; <nl>  <nl> static int change_xattr ( struct ubifs_info * c , struct inode * host , <nl> return err ; <nl>  <nl> kfree ( ui -> data ); <nl> - ui -> data = kmalloc ( size , GFP_NOFS ); <nl> + ui -> data = kmemdup ( value , size , GFP_NOFS ); <nl> if (! ui -> data ) { <nl> err = - ENOMEM ; <nl> goto out_free ; <nl> } <nl> - memcpy ( ui -> data , value , size ); <nl> inode -> i_size = ui -> ui_size = size ; <nl> ui -> data_len = size ; <nl>  <nl> static int lnc_add ( struct ubifs_info * c , struct ubifs_zbranch * zbr , <nl> return err ; <nl> } <nl>  <nl> - lnc_node = kmalloc ( zbr -> len , GFP_NOFS ); <nl> + lnc_node = kmemdup ( node , zbr -> len , GFP_NOFS ); <nl> if (! lnc_node ) <nl> /* We don ' t have to have the cache , so no error */ <nl> return 0 ; <nl>  <nl> - memcpy ( lnc_node , node , zbr -> len ); <nl> zbr -> leaf = lnc_node ; <nl> return 0 ; <nl> }
mmm arch / arm / mach - omap2 / usb - tusb6010 . c <nl> ppp arch / arm / mach - omap2 / usb - tusb6010 . c <nl> int tusb6010_platform_retime ( unsigned is_refclk ) <nl> unsigned sysclk_ps ; <nl> int status ; <nl>  <nl> - if (! refclk_psec || sysclk_ps == 0 ) <nl> + if (! refclk_psec || fclk_ps == 0 ) <nl> return - ENODEV ; <nl>  <nl> sysclk_ps = is_refclk ? refclk_psec : TUSB6010_OSCCLK_60 ;
mmm drivers / isdn / i4l / isdn_common . c <nl> ppp drivers / isdn / i4l / isdn_common . c <nl> isdn_ioctl ( struct inode * inode , struct file * file , uint cmd , ulong arg ) <nl> if ( copy_from_user (& iocts , argp , <nl> sizeof ( isdn_ioctl_struct ))) <nl> return - EFAULT ; <nl> + iocts . drvid [ sizeof ( iocts . drvid )- 1 ] = 0 ; <nl> if ( strlen ( iocts . drvid )) { <nl> if (( p = strchr ( iocts . drvid , ','))) <nl> * p = 0 ; <nl> isdn_ioctl ( struct inode * inode , struct file * file , uint cmd , ulong arg ) <nl> if ( copy_from_user (& iocts , argp , <nl> sizeof ( isdn_ioctl_struct ))) <nl> return - EFAULT ; <nl> + iocts . drvid [ sizeof ( iocts . drvid )- 1 ] = 0 ; <nl> if ( strlen ( iocts . drvid )) { <nl> drvidx = - 1 ; <nl> for ( i = 0 ; i < ISDN_MAX_DRIVERS ; i ++) <nl> isdn_ioctl ( struct inode * inode , struct file * file , uint cmd , ulong arg ) <nl> } else { <nl> p = ( char __user *) iocts . arg ; <nl> for ( i = 0 ; i < 10 ; i ++) { <nl> - sprintf ( bname , "% s % s ", <nl> + snprintf ( bname , sizeof ( bname ), "% s % s ", <nl> strlen ( dev -> drv [ drvidx ]-> msn2eaz [ i ]) ? <nl> dev -> drv [ drvidx ]-> msn2eaz [ i ] : " _ ", <nl> ( i < 9 ) ? "," : "\ 0 "); <nl> isdn_ioctl ( struct inode * inode , struct file * file , uint cmd , ulong arg ) <nl> char * p ; <nl> if ( copy_from_user (& iocts , argp , sizeof ( isdn_ioctl_struct ))) <nl> return - EFAULT ; <nl> + iocts . drvid [ sizeof ( iocts . drvid )- 1 ] = 0 ; <nl> if ( strlen ( iocts . drvid )) { <nl> if (( p = strchr ( iocts . drvid , ','))) <nl> * p = 0 ;
mmm mm / hugetlb . c <nl> ppp mm / hugetlb . c <nl> static int dissolve_free_huge_page ( struct page * page ) <nl> int dissolve_free_huge_pages ( unsigned long start_pfn , unsigned long end_pfn ) <nl> { <nl> unsigned long pfn ; <nl> + struct page * page ; <nl> int rc = 0 ; <nl>  <nl> if (! hugepages_supported ()) <nl> return rc ; <nl>  <nl> - for ( pfn = start_pfn ; pfn < end_pfn ; pfn += 1 << minimum_order ) <nl> - if ( rc = dissolve_free_huge_page ( pfn_to_page ( pfn ))) <nl> - break ; <nl> + for ( pfn = start_pfn ; pfn < end_pfn ; pfn += 1 << minimum_order ) { <nl> + page = pfn_to_page ( pfn ); <nl> + if ( PageHuge ( page ) && ! page_count ( page )) { <nl> + rc = dissolve_free_huge_page ( page ); <nl> + if ( rc ) <nl> + break ; <nl> + } <nl> + } <nl>  <nl> return rc ; <nl> }
mmm drivers / gpu / drm / amd / amdgpu / amdgpu_cs . c <nl> ppp drivers / gpu / drm / amd / amdgpu / amdgpu_cs . c <nl> static int amdgpu_cs_wait_any_fence ( struct amdgpu_device * adev , <nl> wait -> out . status = ( r > 0 ); <nl> wait -> out . first_signaled = first ; <nl>  <nl> - if ( array [ first ]) <nl> + if ( first < fence_count && array [ first ]) <nl> r = array [ first ]-> error ; <nl> else <nl> r = 0 ;
mmm fs / ceph / mds_client . c <nl> ppp fs / ceph / mds_client . c <nl> static int __do_request ( struct ceph_mds_client * mdsc , <nl> int mds = - 1 ; <nl> int err = - EAGAIN ; <nl>  <nl> - if ( req -> r_err || req -> r_got_result ) <nl> + if ( req -> r_err || req -> r_got_result ) { <nl> + if ( req -> r_aborted ) <nl> + __unregister_request ( mdsc , req ); <nl> goto out ; <nl> + } <nl>  <nl> if ( req -> r_timeout && <nl> time_after_eq ( jiffies , req -> r_started + req -> r_timeout )) {
mmm fs / btrfs / volumes . c <nl> ppp fs / btrfs / volumes . c <nl> static int __btrfs_map_block ( struct btrfs_fs_info * fs_info , int rw , <nl> } <nl> bbio = kzalloc ( btrfs_bio_size ( num_alloc_stripes ), GFP_NOFS ); <nl> if (! bbio ) { <nl> + kfree ( raid_map ); <nl> ret = - ENOMEM ; <nl> goto out ; <nl> }
mmm drivers / media / platform / soc_camera / soc_camera . c <nl> ppp drivers / media / platform / soc_camera / soc_camera . c <nl> static int soc_camera_s_selection ( struct file * file , void * fh , <nl>  <nl> /* In all these cases cropping emulation will not help */ <nl> if ( s -> type != V4L2_BUF_TYPE_VIDEO_CAPTURE || <nl> - ( s -> target != V4L2_SEL_TGT_COMPOSE_ACTIVE && <nl> - s -> target != V4L2_SEL_TGT_CROP_ACTIVE )) <nl> + ( s -> target != V4L2_SEL_TGT_COMPOSE && <nl> + s -> target != V4L2_SEL_TGT_CROP )) <nl> return - EINVAL ; <nl>  <nl> - if ( s -> target == V4L2_SEL_TGT_COMPOSE_ACTIVE ) { <nl> + if ( s -> target == V4L2_SEL_TGT_COMPOSE ) { <nl> /* No output size change during a running capture ! */ <nl> if ( is_streaming ( ici , icd ) && <nl> ( icd -> user_width != s -> r . width || <nl> static int soc_camera_s_selection ( struct file * file , void * fh , <nl>  <nl> ret = ici -> ops -> set_selection ( icd , s ); <nl> if (! ret && <nl> - s -> target == V4L2_SEL_TGT_COMPOSE_ACTIVE ) { <nl> + s -> target == V4L2_SEL_TGT_COMPOSE ) { <nl> icd -> user_width = s -> r . width ; <nl> icd -> user_height = s -> r . height ; <nl> if (! icd -> streamer )
mmm kernel / cgroup . c <nl> ppp kernel / cgroup . c <nl> static struct dentry * cgroup_mount ( struct file_system_type * fs_type , <nl> mutex_lock (& cgroup_mutex ); <nl> mutex_lock (& cgroup_root_mutex ); <nl>  <nl> - root_cgrp -> id = idr_alloc (& root -> cgroup_idr , root_cgrp , <nl> - 0 , 1 , GFP_KERNEL ); <nl> - if ( root_cgrp -> id < 0 ) <nl> + ret = idr_alloc (& root -> cgroup_idr , root_cgrp , 0 , 1 , GFP_KERNEL ); <nl> + if ( ret < 0 ) <nl> goto unlock_drop ; <nl> + root_cgrp -> id = ret ; <nl>  <nl> /* Check for name clashes with existing mounts */ <nl> ret = - EBUSY ;
mmm drivers / gpu / drm / i915 / intel_pm . c <nl> ppp drivers / gpu / drm / i915 / intel_pm . c <nl> static void valleyview_disable_rps ( struct drm_device * dev ) <nl>  <nl> int intel_enable_rc6 ( const struct drm_device * dev ) <nl> { <nl> + /* No RC6 before Ironlake */ <nl> + if ( INTEL_INFO ( dev )-> gen < 5 ) <nl> + return 0 ; <nl> + <nl> /* Respect the kernel parameter if it is set */ <nl> if ( i915_enable_rc6 >= 0 ) <nl> return i915_enable_rc6 ;
mmm sound / soc / atmel / atmel_ssc_dai . c <nl> ppp sound / soc / atmel / atmel_ssc_dai . c <nl> static int atmel_ssc_set_dai_clkdiv ( struct snd_soc_dai * cpu_dai , <nl> * transmit and receive , so if a value has already <nl> * been set , it must match this value . <nl> */ <nl> - if ( ssc_p -> cmr_div == 0 ) <nl> + if ( ssc_p -> dir_mask != <nl> + ( SSC_DIR_MASK_PLAYBACK | SSC_DIR_MASK_CAPTURE )) <nl> + ssc_p -> cmr_div = div ; <nl> + else if ( ssc_p -> cmr_div == 0 ) <nl> ssc_p -> cmr_div = div ; <nl> else <nl> if ( div != ssc_p -> cmr_div )
mmm net / rds / tcp_connect . c <nl> ppp net / rds / tcp_connect . c <nl> int rds_tcp_conn_connect ( struct rds_connection * conn ) <nl> rds_tcp_set_callbacks ( sock , conn ); <nl> ret = sock -> ops -> connect ( sock , ( struct sockaddr *)& dest , sizeof ( dest ), <nl> O_NONBLOCK ); <nl> - sock = NULL ; <nl>  <nl> rdsdebug (" connect to address % pI4 returned % d \ n ", & conn -> c_faddr , ret ); <nl> if ( ret == - EINPROGRESS ) <nl> ret = 0 ; <nl> + if ( ret == 0 ) <nl> + sock = NULL ; <nl> + else <nl> + rds_tcp_restore_callbacks ( sock , conn -> c_transport_data ); <nl>  <nl> out : <nl> if ( sock )
mmm include / linux / pm . h <nl> ppp include / linux / pm . h <nl> typedef int __bitwise suspend_disk_method_t ; <nl>  <nl> struct pm_ops { <nl> suspend_disk_method_t pm_disk_mode ; <nl> + int (* valid )( suspend_state_t state ); <nl> int (* prepare )( suspend_state_t state ); <nl> int (* enter )( suspend_state_t state ); <nl> int (* finish )( suspend_state_t state );mmm drivers / acpi / sleep / main . c <nl> ppp drivers / acpi / sleep / main . c <nl> typedef int __bitwise suspend_disk_method_t ; <nl>  <nl> struct pm_ops { <nl> suspend_disk_method_t pm_disk_mode ; <nl> + int (* valid )( suspend_state_t state ); <nl> int (* prepare )( suspend_state_t state ); <nl> int (* enter )( suspend_state_t state ); <nl> int (* finish )( suspend_state_t state ); <nl> int acpi_suspend ( u32 acpi_state ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + static int acpi_pm_state_valid ( suspend_state_t pm_state ) <nl> +{ <nl> + u32 acpi_state = acpi_suspend_states [ pm_state ]; <nl> + <nl> + return sleep_states [ acpi_state ]; <nl> +} <nl> + <nl> static struct pm_ops acpi_pm_ops = { <nl> + . valid = acpi_pm_state_valid , <nl> . prepare = acpi_pm_prepare , <nl> . enter = acpi_pm_enter , <nl> . finish = acpi_pm_finish ,mmm kernel / power / main . c <nl> ppp kernel / power / main . c <nl> typedef int __bitwise suspend_disk_method_t ; <nl>  <nl> struct pm_ops { <nl> suspend_disk_method_t pm_disk_mode ; <nl> + int (* valid )( suspend_state_t state ); <nl> int (* prepare )( suspend_state_t state ); <nl> int (* enter )( suspend_state_t state ); <nl> int (* finish )( suspend_state_t state ); <nl> int acpi_suspend ( u32 acpi_state ) <nl> return - EINVAL ; <nl> } <nl>  <nl> + static int acpi_pm_state_valid ( suspend_state_t pm_state ) <nl> +{ <nl> + u32 acpi_state = acpi_suspend_states [ pm_state ]; <nl> + <nl> + return sleep_states [ acpi_state ]; <nl> +} <nl> + <nl> static struct pm_ops acpi_pm_ops = { <nl> + . valid = acpi_pm_state_valid , <nl> . prepare = acpi_pm_prepare , <nl> . enter = acpi_pm_enter , <nl> . finish = acpi_pm_finish , <nl> static int enter_state ( suspend_state_t state ) <nl> { <nl> int error ; <nl>  <nl> + if ( pm_ops -> valid && ! pm_ops -> valid ( state )) <nl> + return - ENODEV ; <nl> if ( down_trylock (& pm_sem )) <nl> return - EBUSY ; <nl>  <nl> static ssize_t state_show ( struct subsystem * subsys , char * buf ) <nl> char * s = buf ; <nl>  <nl> for ( i = 0 ; i < PM_SUSPEND_MAX ; i ++) { <nl> - if ( pm_states [ i ]) <nl> + if ( pm_states [ i ] && pm_ops && (! pm_ops -> valid <nl> + ||( pm_ops -> valid && pm_ops -> valid ( i )))) <nl> s += sprintf ( s ,"% s ", pm_states [ i ]); <nl> } <nl> s += sprintf ( s ,"\ n ");
mmm sound / pci / hda / patch_realtek . c <nl> ppp sound / pci / hda / patch_realtek . c <nl> static const char * alc_get_line_out_pfx ( const struct auto_pin_cfg * cfg , <nl>  <nl> switch ( cfg -> line_out_type ) { <nl> case AUTO_PIN_SPEAKER_OUT : <nl> - return " Speaker "; <nl> + if ( cfg -> line_outs == 1 ) <nl> + return " Speaker "; <nl> + break ; <nl> case AUTO_PIN_HP_OUT : <nl> return " Headphone "; <nl> default :
mmm drivers / net / hyperv / netvsc . c <nl> ppp drivers / net / hyperv / netvsc . c <nl> static u32 netvsc_copy_to_send_buf ( struct netvsc_device * net_device , <nl> packet -> page_buf_cnt ; <nl>  <nl> /* Add padding */ <nl> - if ( skb && skb -> xmit_more && remain && <nl> - ! packet -> cp_partial ) { <nl> + if ( skb -> xmit_more && remain && ! packet -> cp_partial ) { <nl> padding = net_device -> pkt_align - remain ; <nl> rndis_msg -> msg_len += padding ; <nl> packet -> total_data_buflen += padding ; <nl> int netvsc_send ( struct hv_device * device , <nl> if ( msdp -> pkt ) <nl> msd_len = msdp -> pkt -> total_data_buflen ; <nl>  <nl> - try_batch = ( skb != NULL ) && msd_len > 0 && msdp -> count < <nl> - net_device -> max_pkt ; <nl> - <nl> + try_batch = msd_len > 0 && msdp -> count < net_device -> max_pkt ; <nl> if ( try_batch && msd_len + pktlen + net_device -> pkt_align < <nl> net_device -> send_section_size ) { <nl> section_index = msdp -> pkt -> send_buf_index ; <nl> int netvsc_send ( struct hv_device * device , <nl> section_index = msdp -> pkt -> send_buf_index ; <nl> packet -> cp_partial = true ; <nl>  <nl> - } else if (( skb != NULL ) && pktlen + net_device -> pkt_align < <nl> + } else if ( pktlen + net_device -> pkt_align < <nl> net_device -> send_section_size ) { <nl> section_index = netvsc_get_next_send_section ( net_device ); <nl> if ( section_index != NETVSC_INVALID_INDEX ) {
mmm drivers / input / serio / ams_delta_serio . c <nl> ppp drivers / input / serio / ams_delta_serio . c <nl> static void __exit ams_delta_serio_exit ( void ) <nl> free_irq ( OMAP_GPIO_IRQ ( AMS_DELTA_GPIO_PIN_KEYBRD_CLK ), 0 ); <nl> gpio_free ( AMS_DELTA_GPIO_PIN_KEYBRD_CLK ); <nl> gpio_free ( AMS_DELTA_GPIO_PIN_KEYBRD_DATA ); <nl> - kfree ( ams_delta_serio ); <nl> } <nl> module_exit ( ams_delta_serio_exit );
mmm drivers / staging / ccree / hash_defs . h <nl> ppp drivers / staging / ccree / hash_defs . h <nl> enum HashCipherDoPadding { <nl>  <nl> typedef struct SepHashPrivateContext { <nl> /* The current length is placed at the end of the context buffer because the hash <nl> - context is used for all HMAC operations as well . HMAC context includes a 64 bytes <nl> - K0 field . The size of struct drv_ctx_hash reserved field is 88 / 184 bytes depend if t <nl> - he SHA512 is supported ( in this case teh context size is 256 bytes ). <nl> - The size of struct drv_ctx_hash reseved field is 20 or 52 depend if the SHA512 is supported . <nl> - This means that this structure size ( without the reserved field can be up to 20 bytes , <nl> - in case sha512 is not suppported it is 20 bytes ( SEP_HASH_LENGTH_WORDS define to 2 ) and in the other <nl> - case it is 28 ( SEP_HASH_LENGTH_WORDS define to 4 ) */ <nl> + * context is used for all HMAC operations as well . HMAC context includes a 64 bytes <nl> + * K0 field . The size of struct drv_ctx_hash reserved field is 88 / 184 bytes depend if t <nl> + * he SHA512 is supported ( in this case teh context size is 256 bytes ). <nl> + * The size of struct drv_ctx_hash reseved field is 20 or 52 depend if the SHA512 is supported . <nl> + * This means that this structure size ( without the reserved field can be up to 20 bytes , <nl> + * in case sha512 is not suppported it is 20 bytes ( SEP_HASH_LENGTH_WORDS define to 2 ) and in the other <nl> + * case it is 28 ( SEP_HASH_LENGTH_WORDS define to 4 ) <nl> + */ <nl> u32 reserved [( sizeof ( struct drv_ctx_hash )/ sizeof ( u32 )) - SEP_HASH_LENGTH_WORDS - 3 ]; <nl> u32 CurrentDigestedLength [ SEP_HASH_LENGTH_WORDS ]; <nl> u32 KeyType ;
mmm net / ipv6 / esp6 . c <nl> ppp net / ipv6 / esp6 . c <nl> int esp6_output_head ( struct xfrm_state * x , struct sk_buff * skb , struct esp_info <nl> struct page * page ; <nl> struct sk_buff * trailer ; <nl> int tailen = esp -> tailen ; <nl> + unsigned int allocsz ; <nl>  <nl> if ( x -> encap ) { <nl> int err = esp6_output_encap ( x , skb , esp ); <nl> int esp6_output_head ( struct xfrm_state * x , struct sk_buff * skb , struct esp_info <nl> return err ; <nl> } <nl>  <nl> + allocsz = ALIGN ( skb -> data_len + tailen , L1_CACHE_BYTES ); <nl> + if ( allocsz > ESP_SKB_FRAG_MAXSIZE ) <nl> + goto cow ; <nl> + <nl> if (! skb_cloned ( skb )) { <nl> if ( tailen <= skb_tailroom ( skb )) { <nl> nfrags = 1 ;mmm include / net / esp . h <nl> ppp include / net / esp . h <nl> int esp6_output_head ( struct xfrm_state * x , struct sk_buff * skb , struct esp_info <nl> struct page * page ; <nl> struct sk_buff * trailer ; <nl> int tailen = esp -> tailen ; <nl> + unsigned int allocsz ; <nl>  <nl> if ( x -> encap ) { <nl> int err = esp6_output_encap ( x , skb , esp ); <nl> int esp6_output_head ( struct xfrm_state * x , struct sk_buff * skb , struct esp_info <nl> return err ; <nl> } <nl>  <nl> + allocsz = ALIGN ( skb -> data_len + tailen , L1_CACHE_BYTES ); <nl> + if ( allocsz > ESP_SKB_FRAG_MAXSIZE ) <nl> + goto cow ; <nl> + <nl> if (! skb_cloned ( skb )) { <nl> if ( tailen <= skb_tailroom ( skb )) { <nl> nfrags = 1 ; <nl>  <nl> # include < linux / skbuff . h > <nl>  <nl> +# define ESP_SKB_FRAG_MAXSIZE ( PAGE_SIZE << SKB_FRAG_PAGE_ORDER ) <nl> + <nl> struct ip_esp_hdr ; <nl>  <nl> static inline struct ip_esp_hdr * ip_esp_hdr ( const struct sk_buff * skb )mmm net / ipv4 / esp4 . c <nl> ppp net / ipv4 / esp4 . c <nl> int esp6_output_head ( struct xfrm_state * x , struct sk_buff * skb , struct esp_info <nl> struct page * page ; <nl> struct sk_buff * trailer ; <nl> int tailen = esp -> tailen ; <nl> + unsigned int allocsz ; <nl>  <nl> if ( x -> encap ) { <nl> int err = esp6_output_encap ( x , skb , esp ); <nl> int esp6_output_head ( struct xfrm_state * x , struct sk_buff * skb , struct esp_info <nl> return err ; <nl> } <nl>  <nl> + allocsz = ALIGN ( skb -> data_len + tailen , L1_CACHE_BYTES ); <nl> + if ( allocsz > ESP_SKB_FRAG_MAXSIZE ) <nl> + goto cow ; <nl> + <nl> if (! skb_cloned ( skb )) { <nl> if ( tailen <= skb_tailroom ( skb )) { <nl> nfrags = 1 ; <nl>  <nl> # include < linux / skbuff . h > <nl>  <nl> +# define ESP_SKB_FRAG_MAXSIZE ( PAGE_SIZE << SKB_FRAG_PAGE_ORDER ) <nl> + <nl> struct ip_esp_hdr ; <nl>  <nl> static inline struct ip_esp_hdr * ip_esp_hdr ( const struct sk_buff * skb ) <nl> int esp_output_head ( struct xfrm_state * x , struct sk_buff * skb , struct esp_info * <nl> struct page * page ; <nl> struct sk_buff * trailer ; <nl> int tailen = esp -> tailen ; <nl> + unsigned int allocsz ; <nl>  <nl> /* this is non - NULL only with TCP / UDP Encapsulation */ <nl> if ( x -> encap ) { <nl> int esp_output_head ( struct xfrm_state * x , struct sk_buff * skb , struct esp_info * <nl> return err ; <nl> } <nl>  <nl> + allocsz = ALIGN ( skb -> data_len + tailen , L1_CACHE_BYTES ); <nl> + if ( allocsz > ESP_SKB_FRAG_MAXSIZE ) <nl> + goto cow ; <nl> + <nl> if (! skb_cloned ( skb )) { <nl> if ( tailen <= skb_tailroom ( skb )) { <nl> nfrags = 1 ;
mmm net / sched / act_api . c <nl> ppp net / sched / act_api . c <nl> tc_dump_action ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> nla_nest_end ( skb , nest ); <nl> ret = skb -> len ; <nl> } else <nl> - nla_nest_cancel ( skb , nest ); <nl> + nlmsg_trim ( skb , b ); <nl>  <nl> nlh -> nlmsg_len = skb_tail_pointer ( skb ) - b ; <nl> if ( NETLINK_CB ( cb -> skb ). portid && ret )
mmm scripts / mod / file2alias . c <nl> ppp scripts / mod / file2alias . c <nl> static int do_vmbus_entry ( const char * filename , struct hv_vmbus_device_id * id , <nl> char * alias ) <nl> { <nl> int i ; <nl> - char guid_name [(( sizeof ( struct hv_vmbus_device_id ) + 1 )) * 2 ]; <nl> + char guid_name [(( sizeof ( id -> guid ) + 1 )) * 2 ]; <nl>  <nl> - for ( i = 0 ; i < ( sizeof ( struct hv_vmbus_device_id ) * 2 ); i += 2 ) <nl> + for ( i = 0 ; i < ( sizeof ( id -> guid ) * 2 ); i += 2 ) <nl> sprintf (& guid_name [ i ], "% 02x ", id -> guid [ i / 2 ]); <nl>  <nl> strcpy ( alias , " vmbus :");
mmm net / phonet / af_phonet . c <nl> ppp net / phonet / af_phonet . c <nl> static int pn_send ( struct sk_buff * skb , struct net_device * dev , <nl> struct phonethdr * ph ; <nl> int err ; <nl>  <nl> - if ( skb -> len + 2 > 0xffff ) { <nl> - /* Phonet length field would overflow */ <nl> + if ( skb -> len + 2 > 0xffff /* Phonet length field limit */ || <nl> + skb -> len + sizeof ( struct phonethdr ) > dev -> mtu ) { <nl> err = - EMSGSIZE ; <nl> goto drop ; <nl> }
mmm net / sctp / sm_statefuns . c <nl> ppp net / sctp / sm_statefuns . c <nl> sctp_disposition_t sctp_sf_do_5_1D_ce ( struct net * net , <nl> struct sctp_chunk auth ; <nl> sctp_ierror_t ret ; <nl>  <nl> + /* Make sure that we and the peer are AUTH capable */ <nl> + if (! net -> sctp . auth_enable || ! new_asoc -> peer . auth_capable ) { <nl> + kfree_skb ( chunk -> auth_chunk ); <nl> + sctp_association_free ( new_asoc ); <nl> + return sctp_sf_pdiscard ( net , ep , asoc , type , arg , commands ); <nl> + } <nl> + <nl> /* set - up our fake chunk so that we can process it */ <nl> auth . skb = chunk -> auth_chunk ; <nl> auth . asoc = chunk -> asoc ;
mmm drivers / regulator / tps65023 - regulator . c <nl> ppp drivers / regulator / tps65023 - regulator . c <nl> static int __devinit tps_65023_probe ( struct i2c_client * client , <nl>  <nl> tps -> desc [ i ]. name = info -> name ; <nl> tps -> desc [ i ]. id = i ; <nl> - tps -> desc [ i ]. n_voltages = info -> table_len ; <nl> + if ( info -> fixed ) <nl> + tps -> desc [ i ]. n_voltages = 1 ; <nl> + else <nl> + tps -> desc [ i ]. n_voltages = info -> table_len ; <nl> tps -> desc [ i ]. ops = ( i > TPS65023_DCDC_3 ? <nl> & tps65023_ldo_ops : & tps65023_dcdc_ops ); <nl> tps -> desc [ i ]. type = REGULATOR_VOLTAGE ;
mmm drivers / scsi / qla2xxx / qla_mr . c <nl> ppp drivers / scsi / qla2xxx / qla_mr . c <nl> qlafx00_soc_cpu_reset ( scsi_qla_host_t * vha ) <nl> /* Kick in Core0 to start boot process */ <nl> QLAFX00_SET_HBA_SOC_REG ( ha , SOC_SW_RST_CONTROL_REG_CORE0 , ( 0xF00 )); <nl>  <nl> + spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> + <nl> /* Wait 10secs for soft - reset to complete . */ <nl> for ( cnt = 10 ; cnt ; cnt --) { <nl> msleep ( 1000 ); <nl> barrier (); <nl> } <nl> - spin_unlock_irqrestore (& ha -> hardware_lock , flags ); <nl> } <nl>  <nl> /**
mmm drivers / phy / rockchip / phy - rockchip - typec . c <nl> ppp drivers / phy / rockchip / phy - rockchip - typec . c <nl> static int tcphy_get_mode ( struct rockchip_typec_phy * tcphy ) <nl> u8 mode ; <nl> int ret ; <nl>  <nl> + if (! edev ) <nl> + return MODE_DFP_USB ; <nl> + <nl> ufp = extcon_get_state ( edev , EXTCON_USB ); <nl> dp = extcon_get_state ( edev , EXTCON_DISP_DP ); <nl>  <nl> static int rockchip_typec_phy_probe ( struct platform_device * pdev ) <nl>  <nl> tcphy -> extcon = extcon_get_edev_by_phandle ( dev , 0 ); <nl> if ( IS_ERR ( tcphy -> extcon )) { <nl> - if ( PTR_ERR ( tcphy -> extcon ) != - EPROBE_DEFER ) <nl> - dev_err ( dev , " Invalid or missing extcon \ n "); <nl> - return PTR_ERR ( tcphy -> extcon ); <nl> + if ( PTR_ERR ( tcphy -> extcon ) == - ENODEV ) { <nl> + tcphy -> extcon = NULL ; <nl> + } else { <nl> + if ( PTR_ERR ( tcphy -> extcon ) != - EPROBE_DEFER ) <nl> + dev_err ( dev , " Invalid or missing extcon \ n "); <nl> + return PTR_ERR ( tcphy -> extcon ); <nl> + } <nl> } <nl>  <nl> pm_runtime_enable ( dev );
mmm drivers / gpu / drm / ttm / ttm_memory . c <nl> ppp drivers / gpu / drm / ttm / ttm_memory . c <nl> static int ttm_mem_init_dma32_zone ( struct ttm_mem_global * glob , <nl> * No special dma32 zone needed . <nl> */ <nl>  <nl> - if ( mem <= (( uint64_t ) 1ULL << 32 )) <nl> + if ( mem <= (( uint64_t ) 1ULL << 32 )) { <nl> + kfree ( zone ); <nl> return 0 ; <nl> + } <nl>  <nl> /* <nl> * Limit max dma32 memory to 4GB for now
mmm drivers / staging / tidspbridge / dynload / cload . c <nl> ppp drivers / staging / tidspbridge / dynload / cload . c <nl> static void dload_symbols ( struct dload_state * dlthis ) <nl> struct local_symbol * sp ; <nl> struct dynload_symbol * symp ; <nl> struct dynload_symbol * newsym ; <nl> + struct doff_syment_t * my_sym_buf ; <nl>  <nl> sym_count = dlthis -> dfile_hdr . df_no_syms ; <nl> if ( sym_count == 0 ) <nl> static void dload_symbols ( struct dload_state * dlthis ) <nl> become defined from the global symbol table */ <nl> checks = dlthis -> verify . dv_sym_tab_checksum ; <nl> symbols_left = sym_count ; <nl> + <nl> + my_sym_buf = kzalloc ( sizeof (* my_sym_buf ) * MY_SYM_BUF_SIZ , GFP_KERNEL ); <nl> + if (! my_sym_buf ) <nl> + return ; <nl> + <nl> do { /* read all symbols */ <nl> char * sname ; <nl> u32 val ; <nl> s32 delta ; <nl> struct doff_syment_t * input_sym ; <nl> unsigned syms_in_buf ; <nl> - struct doff_syment_t my_sym_buf [ MY_SYM_BUF_SIZ ]; <nl> + <nl> input_sym = my_sym_buf ; <nl> syms_in_buf = symbols_left > MY_SYM_BUF_SIZ ? <nl> MY_SYM_BUF_SIZ : symbols_left ; <nl> static void dload_symbols ( struct dload_state * dlthis ) <nl> if ( dlthis -> strm -> read_buffer ( dlthis -> strm , input_sym , siz ) != <nl> siz ) { <nl> DL_ERROR ( readstrm , sym_errid ); <nl> - return ; <nl> + goto free_sym_buf ; <nl> } <nl> if ( dlthis -> reorder_map ) <nl> dload_reorder ( input_sym , siz , dlthis -> reorder_map ); <nl> static void dload_symbols ( struct dload_state * dlthis ) <nl> DL_ERROR (" Absolute symbol % s is " <nl> " defined multiple times with " <nl> " different values ", sname ); <nl> - return ; <nl> + goto free_sym_buf ; <nl> } <nl> } <nl> loop_itr : <nl> static void dload_symbols ( struct dload_state * dlthis ) <nl> if (~ checks ) <nl> dload_error ( dlthis , " Checksum of symbols failed "); <nl>  <nl> + free_sym_buf : <nl> + kfree ( my_sym_buf ); <nl> + return ; <nl> } /* dload_symbols */ <nl>  <nl> /*****************************************************************************
mmm drivers / net / slip / slip . c <nl> ppp drivers / net / slip / slip . c <nl> static void sl_tx_timeout ( struct net_device * dev , unsigned int txqueue ) <nl> spin_lock (& sl -> lock ); <nl>  <nl> if ( netif_queue_stopped ( dev )) { <nl> - if (! netif_running ( dev )) <nl> + if (! netif_running ( dev ) || ! sl -> tty ) <nl> goto out ; <nl>  <nl> /* May be we must check transmitter timeout here ?
mmm drivers / iommu / amd_iommu . c <nl> ppp drivers / iommu / amd_iommu . c <nl> static struct protection_domain * get_domain ( struct device * dev ) <nl> domain = to_pdomain ( io_domain ); <nl> attach_device ( dev , domain ); <nl> } <nl> + if ( domain == NULL ) <nl> + return ERR_PTR (- EBUSY ); <nl> + <nl> if (! dma_ops_domain ( domain )) <nl> return ERR_PTR (- EBUSY ); <nl> 
mmm drivers / s390 / cio / qdio_main . c <nl> ppp drivers / s390 / cio / qdio_main . c <nl> static int qdio_siga_output ( struct qdio_q * q , unsigned int * busy_bit , <nl> int retries = 0 , cc ; <nl> unsigned long laob = 0 ; <nl>  <nl> + WARN_ON_ONCE ( aob && (( queue_type ( q ) != QDIO_IQDIO_QFMT ) || <nl> + ! q -> u . out . use_cq )); <nl> if ( q -> u . out . use_cq && aob != 0 ) { <nl> fc = QDIO_SIGA_WRITEQ ; <nl> laob = aob ; <nl> static int qdio_siga_output ( struct qdio_q * q , unsigned int * busy_bit , <nl> fc |= QDIO_SIGA_QEBSM_FLAG ; <nl> } <nl> again : <nl> - WARN_ON_ONCE (( aob && queue_type ( q ) != QDIO_IQDIO_QFMT ) || <nl> - ( aob && fc != QDIO_SIGA_WRITEQ )); <nl> cc = do_siga_output ( schid , q -> mask , busy_bit , fc , laob ); <nl>  <nl> /* hipersocket busy condition */
mmm drivers / vfio / pci / vfio_pci_intrs . c <nl> ppp drivers / vfio / pci / vfio_pci_intrs . c <nl> int vfio_pci_set_irqs_ioctl ( struct vfio_pci_device * vdev , uint32_t flags , <nl> func = vfio_pci_set_err_trigger ; <nl> break ; <nl> } <nl> + break ; <nl> case VFIO_PCI_REQ_IRQ_INDEX : <nl> switch ( flags & VFIO_IRQ_SET_ACTION_TYPE_MASK ) { <nl> case VFIO_IRQ_SET_ACTION_TRIGGER : <nl> func = vfio_pci_set_req_trigger ; <nl> break ; <nl> } <nl> + break ; <nl> } <nl>  <nl> if (! func )
mmm drivers / gpu / drm / vc4 / vc4_dsi . c <nl> ppp drivers / gpu / drm / vc4 / vc4_dsi . c <nl> static void vc4_dsi_latch_ulps ( struct vc4_dsi * dsi , bool latch ) <nl> /* Enters or exits Ultra Low Power State . */ <nl> static void vc4_dsi_ulps ( struct vc4_dsi * dsi , bool ulps ) <nl> { <nl> - bool continuous = dsi -> mode_flags & MIPI_DSI_CLOCK_NON_CONTINUOUS ; <nl> - u32 phyc_ulps = (( continuous ? DSI_PORT_BIT ( PHYC_CLANE_ULPS ) : 0 ) | <nl> + bool non_continuous = dsi -> mode_flags & MIPI_DSI_CLOCK_NON_CONTINUOUS ; <nl> + u32 phyc_ulps = (( non_continuous ? DSI_PORT_BIT ( PHYC_CLANE_ULPS ) : 0 ) | <nl> DSI_PHYC_DLANE0_ULPS | <nl> ( dsi -> lanes > 1 ? DSI_PHYC_DLANE1_ULPS : 0 ) | <nl> ( dsi -> lanes > 2 ? DSI_PHYC_DLANE2_ULPS : 0 ) | <nl> ( dsi -> lanes > 3 ? DSI_PHYC_DLANE3_ULPS : 0 )); <nl> - u32 stat_ulps = (( continuous ? DSI1_STAT_PHY_CLOCK_ULPS : 0 ) | <nl> + u32 stat_ulps = (( non_continuous ? DSI1_STAT_PHY_CLOCK_ULPS : 0 ) | <nl> DSI1_STAT_PHY_D0_ULPS | <nl> ( dsi -> lanes > 1 ? DSI1_STAT_PHY_D1_ULPS : 0 ) | <nl> ( dsi -> lanes > 2 ? DSI1_STAT_PHY_D2_ULPS : 0 ) | <nl> ( dsi -> lanes > 3 ? DSI1_STAT_PHY_D3_ULPS : 0 )); <nl> - u32 stat_stop = (( continuous ? DSI1_STAT_PHY_CLOCK_STOP : 0 ) | <nl> + u32 stat_stop = (( non_continuous ? DSI1_STAT_PHY_CLOCK_STOP : 0 ) | <nl> DSI1_STAT_PHY_D0_STOP | <nl> ( dsi -> lanes > 1 ? DSI1_STAT_PHY_D1_STOP : 0 ) | <nl> ( dsi -> lanes > 2 ? DSI1_STAT_PHY_D2_STOP : 0 ) |
mmm drivers / ieee1394 / nodemgr . c <nl> ppp drivers / ieee1394 / nodemgr . c <nl> static int nodemgr_host_thread ( void * data ) <nl> g = get_hpsb_generation ( host ); <nl> for ( i = 0 ; i < 4 ; i ++) { <nl> msleep_interruptible ( 63 ); <nl> + try_to_freeze (); <nl> if ( kthread_should_stop ()) <nl> goto exit ; <nl>  <nl> static int nodemgr_host_thread ( void * data ) <nl> /* Sleep 3 seconds */ <nl> for ( i = 3000 / 200 ; i ; i --) { <nl> msleep_interruptible ( 200 ); <nl> + try_to_freeze (); <nl> if ( kthread_should_stop ()) <nl> goto exit ; <nl> 
mmm drivers / net / ethernet / rocker / rocker_main . c <nl> ppp drivers / net / ethernet / rocker / rocker_main . c <nl> static void rocker_switchdev_event_work ( struct work_struct * work ) <nl> switch ( switchdev_work -> event ) { <nl> case SWITCHDEV_FDB_ADD_TO_DEVICE : <nl> fdb_info = & switchdev_work -> fdb_info ; <nl> + if (! fdb_info -> added_by_user ) <nl> + break ; <nl> err = rocker_world_port_fdb_add ( rocker_port , fdb_info ); <nl> if ( err ) { <nl> netdev_dbg ( rocker_port -> dev , " fdb add failed err =% d \ n ", err ); <nl> static void rocker_switchdev_event_work ( struct work_struct * work ) <nl> break ; <nl> case SWITCHDEV_FDB_DEL_TO_DEVICE : <nl> fdb_info = & switchdev_work -> fdb_info ; <nl> + if (! fdb_info -> added_by_user ) <nl> + break ; <nl> err = rocker_world_port_fdb_del ( rocker_port , fdb_info ); <nl> if ( err ) <nl> netdev_dbg ( rocker_port -> dev , " fdb add failed err =% d \ n ", err ); <nl> static int rocker_switchdev_event ( struct notifier_block * unused , <nl> switch ( event ) { <nl> case SWITCHDEV_FDB_ADD_TO_DEVICE : /* fall through */ <nl> case SWITCHDEV_FDB_DEL_TO_DEVICE : <nl> - if (! fdb_info -> added_by_user ) <nl> - break ; <nl> memcpy (& switchdev_work -> fdb_info , ptr , <nl> sizeof ( switchdev_work -> fdb_info )); <nl> switchdev_work -> fdb_info . addr = kzalloc ( ETH_ALEN , GFP_ATOMIC );
mmm arch / x86 / crypto / salsa20_glue . c <nl> ppp arch / x86 / crypto / salsa20_glue . c <nl> static int encrypt ( struct blkcipher_desc * desc , <nl>  <nl> salsa20_ivsetup ( ctx , walk . iv ); <nl>  <nl> - if ( likely ( walk . nbytes == nbytes )) <nl> - { <nl> - salsa20_encrypt_bytes ( ctx , walk . src . virt . addr , <nl> - walk . dst . virt . addr , nbytes ); <nl> - return blkcipher_walk_done ( desc , & walk , 0 ); <nl> - } <nl> - <nl> while ( walk . nbytes >= 64 ) { <nl> salsa20_encrypt_bytes ( ctx , walk . src . virt . addr , <nl> walk . dst . virt . addr ,mmm crypto / salsa20_generic . c <nl> ppp crypto / salsa20_generic . c <nl> static int encrypt ( struct blkcipher_desc * desc , <nl>  <nl> salsa20_ivsetup ( ctx , walk . iv ); <nl>  <nl> - if ( likely ( walk . nbytes == nbytes )) <nl> - { <nl> - salsa20_encrypt_bytes ( ctx , walk . src . virt . addr , <nl> - walk . dst . virt . addr , nbytes ); <nl> - return blkcipher_walk_done ( desc , & walk , 0 ); <nl> - } <nl> - <nl> while ( walk . nbytes >= 64 ) { <nl> salsa20_encrypt_bytes ( ctx , walk . src . virt . addr , <nl> walk . dst . virt . addr , <nl> static int encrypt ( struct blkcipher_desc * desc , <nl>  <nl> salsa20_ivsetup ( ctx , walk . iv ); <nl>  <nl> - if ( likely ( walk . nbytes == nbytes )) <nl> - { <nl> - salsa20_encrypt_bytes ( ctx , walk . dst . virt . addr , <nl> - walk . src . virt . addr , nbytes ); <nl> - return blkcipher_walk_done ( desc , & walk , 0 ); <nl> - } <nl> - <nl> while ( walk . nbytes >= 64 ) { <nl> salsa20_encrypt_bytes ( ctx , walk . dst . virt . addr , <nl> walk . src . virt . addr ,
mmm fs / ecryptfs / crypto . c <nl> ppp fs / ecryptfs / crypto . c <nl> ecryptfs_process_key_cipher ( struct crypto_blkcipher ** key_tfm , <nl> char * cipher_name , size_t * key_size ) <nl> { <nl> char dummy_key [ ECRYPTFS_MAX_KEY_BYTES ]; <nl> - char * full_alg_name ; <nl> + char * full_alg_name = NULL ; <nl> int rc ; <nl>  <nl> * key_tfm = NULL ; <nl> ecryptfs_process_key_cipher ( struct crypto_blkcipher ** key_tfm , <nl> if ( rc ) <nl> goto out ; <nl> * key_tfm = crypto_alloc_blkcipher ( full_alg_name , 0 , CRYPTO_ALG_ASYNC ); <nl> - kfree ( full_alg_name ); <nl> if ( IS_ERR (* key_tfm )) { <nl> rc = PTR_ERR (* key_tfm ); <nl> printk ( KERN_ERR " Unable to allocate crypto cipher with name " <nl> ecryptfs_process_key_cipher ( struct crypto_blkcipher ** key_tfm , <nl> goto out ; <nl> } <nl> out : <nl> + kfree ( full_alg_name ); <nl> return rc ; <nl> } <nl> 
mmm drivers / i2c / busses / i2c - pxa . c <nl> ppp drivers / i2c / busses / i2c - pxa . c <nl> static int i2c_pxa_xfer ( struct i2c_adapter * adap , struct i2c_msg msgs [], int num <nl> struct pxa_i2c * i2c = adap -> algo_data ; <nl> int ret , i ; <nl>  <nl> + /* If the I2C controller is disabled we need to reset it ( probably due <nl> + to a suspend / resume destroying state ). We do this here as we can then <nl> + avoid worrying about resuming the controller before its users . */ <nl> + if (!( ICR & ICR_IUE )) <nl> + i2c_pxa_reset ( i2c ); <nl> + <nl> for ( i = adap -> retries ; i >= 0 ; i --) { <nl> ret = i2c_pxa_do_xfer ( i2c , msgs , num ); <nl> if ( ret != I2C_RETRY ) <nl> static struct pxa_i2c i2c_pxa = { <nl> static int i2c_pxa_probe ( struct platform_device * dev ) <nl> { <nl> struct pxa_i2c * i2c = & i2c_pxa ; <nl> +# ifdef CONFIG_I2C_PXA_SLAVE <nl> struct i2c_pxa_platform_data * plat = dev -> dev . platform_data ; <nl> +# endif <nl> int ret ; <nl>  <nl> # ifdef CONFIG_PXA27x <nl> static void i2c_adap_pxa_exit ( void ) <nl> return platform_driver_unregister (& i2c_pxa_driver ); <nl> } <nl>  <nl> + MODULE_LICENSE (" GPL "); <nl> + <nl> module_init ( i2c_adap_pxa_init ); <nl> module_exit ( i2c_adap_pxa_exit );
mmm fs / btrfs / ioctl . c <nl> ppp fs / btrfs / ioctl . c <nl> static noinline int btrfs_ioctl_resize ( struct btrfs_root * root , <nl> } <nl> ret = btrfs_grow_device ( trans , device , new_size ); <nl> btrfs_commit_transaction ( trans , root ); <nl> - } else { <nl> + } else if ( new_size < old_size ) { <nl> ret = btrfs_shrink_device ( device , new_size ); <nl> } <nl> 
mmm arch / x86 / kvm / vmx . c <nl> ppp arch / x86 / kvm / vmx . c <nl> static void kvm_do_inject_irq ( struct kvm_vcpu * vcpu ) <nl> clear_bit ( bit_index , & vcpu -> arch . irq_pending [ word_index ]); <nl> if (! vcpu -> arch . irq_pending [ word_index ]) <nl> clear_bit ( word_index , & vcpu -> arch . irq_summary ); <nl> - vmx_inject_irq ( vcpu , irq ); <nl> + kvm_queue_interrupt ( vcpu , irq ); <nl> } <nl>  <nl>  <nl> static void do_interrupt_requests ( struct kvm_vcpu * vcpu , <nl> ( vmcs_read32 ( GUEST_INTERRUPTIBILITY_INFO ) & 3 ) == 0 ); <nl>  <nl> if ( vcpu -> arch . interrupt_window_open && <nl> - vcpu -> arch . irq_summary && <nl> - !( vmcs_read32 ( VM_ENTRY_INTR_INFO_FIELD ) & INTR_INFO_VALID_MASK )) <nl> - /* <nl> - * If interrupts enabled , and not blocked by sti or mov ss . Good . <nl> - */ <nl> + vcpu -> arch . irq_summary && ! vcpu -> arch . interrupt . pending ) <nl> kvm_do_inject_irq ( vcpu ); <nl>  <nl> + if ( vcpu -> arch . interrupt_window_open && vcpu -> arch . interrupt . pending ) <nl> + vmx_inject_irq ( vcpu , vcpu -> arch . interrupt . nr ); <nl> + <nl> cpu_based_vm_exec_control = vmcs_read32 ( CPU_BASED_VM_EXEC_CONTROL ); <nl> if (! vcpu -> arch . interrupt_window_open && <nl> ( vcpu -> arch . irq_summary || kvm_run -> request_interrupt_window ))
mmm drivers / infiniband / hw / nes / nes_cm . c <nl> ppp drivers / infiniband / hw / nes / nes_cm . c <nl> int schedule_nes_timer ( struct nes_cm_node * cm_node , struct sk_buff * skb , <nl> int ret = 0 ; <nl> u32 was_timer_set ; <nl>  <nl> + if (! cm_node ) <nl> + return - EINVAL ; <nl> new_send = kzalloc ( sizeof (* new_send ), GFP_ATOMIC ); <nl> if (! new_send ) <nl> return - 1 ; <nl> - if (! cm_node ) <nl> - return - EINVAL ; <nl>  <nl> /* new_send -> timetosend = currenttime */ <nl> new_send -> retrycount = NES_DEFAULT_RETRYS ;
mmm drivers / acpi / processor_idle . c <nl> ppp drivers / acpi / processor_idle . c <nl> static void acpi_processor_power_verify_c3 ( struct acpi_processor * pr , <nl> } <nl>  <nl> if ( pr -> flags . bm_check ) { <nl> - /* bus mastering control is necessary */ <nl> if (! pr -> flags . bm_control ) { <nl> - /* In this case we enter C3 without bus mastering */ <nl> - ACPI_DEBUG_PRINT (( ACPI_DB_INFO , <nl> - " C3 support without bus mastering control \ n ")); <nl> + if ( pr -> flags . has_cst != 1 ) { <nl> + /* bus mastering control is necessary */ <nl> + ACPI_DEBUG_PRINT (( ACPI_DB_INFO , <nl> + " C3 support requires BM control \ n ")); <nl> + return ; <nl> + } else { <nl> + /* Here we enter C3 without bus mastering */ <nl> + ACPI_DEBUG_PRINT (( ACPI_DB_INFO , <nl> + " C3 support without BM control \ n ")); <nl> + } <nl> } <nl> } else { <nl> /*
mmm drivers / nvdimm / btt . c <nl> ppp drivers / nvdimm / btt . c <nl> int nvdimm_namespace_attach_btt ( struct nd_namespace_common * ndns ) <nl> } <nl>  <nl> btt_sb = devm_kzalloc (& nd_btt -> dev , sizeof (* btt_sb ), GFP_KERNEL ); <nl> + if (! btt_sb ) <nl> + return - ENOMEM ; <nl>  <nl> /* <nl> * If this returns < 0 , that is ok as it just means there wasn ' t
mmm drivers / acpi / debugfs . c <nl> ppp drivers / acpi / debugfs . c <nl> int __init acpi_debugfs_init ( void ) <nl> if (! acpi_dir ) <nl> goto err ; <nl>  <nl> - cm_dentry = debugfs_create_file (" custom_method ", S_IWUGO , <nl> + cm_dentry = debugfs_create_file (" custom_method ", S_IWUSR , <nl> acpi_dir , NULL , & cm_fops ); <nl> if (! cm_dentry ) <nl> goto err ;
mmm arch / powerpc / xmon / xmon . c <nl> ppp arch / powerpc / xmon / xmon . c <nl> static unsigned long nidump = 16 ; <nl> static unsigned long ncsum = 4096 ; <nl> static int termch ; <nl> static char tmpstr [ 128 ]; <nl> + static int tracing_enabled ; <nl>  <nl> static long bus_error_jmp [ JMP_BUF_LEN ]; <nl> static int catch_memory_errors ; <nl> static int xmon_core ( struct pt_regs * regs , int fromipi ) <nl> local_irq_save ( flags ); <nl> hard_irq_disable (); <nl>  <nl> + tracing_enabled = tracing_is_on (); <nl> + tracing_off (); <nl> + <nl> bp = in_breakpoint_table ( regs -> nip , & offset ); <nl> if ( bp != NULL ) { <nl> regs -> nip = bp -> address + offset ; <nl> cmds ( struct pt_regs * excp ) <nl> break ; <nl> case ' x ': <nl> case ' X ': <nl> + if ( tracing_enabled ) <nl> + tracing_on (); <nl> return cmd ; <nl> case EOF : <nl> printf (" < no input ...>\ n "); <nl> static void dump_tracing ( void ) <nl> ftrace_dump ( DUMP_ORIG ); <nl> else <nl> ftrace_dump ( DUMP_ALL ); <nl> - <nl> - tracing_on (); <nl> } <nl>  <nl> # ifdef CONFIG_PPC64
mmm arch / tile / mm / init . c <nl> ppp arch / tile / mm / init . c <nl> static long __write_once initfree = 1 ; <nl> static int __init set_initfree ( char * str ) <nl> { <nl> long val ; <nl> - if ( strict_strtol ( str , 0 , & val )) { <nl> + if ( strict_strtol ( str , 0 , & val ) == 0 ) { <nl> initfree = val ; <nl> pr_info (" initfree : % s free init pages \ n ", <nl> initfree ? " will " : " won ' t ");
mmm arch / x86 / xen / apic . c <nl> ppp arch / x86 / xen / apic . c <nl> static u32 xen_apic_read ( u32 reg ) <nl>  <nl> ret = HYPERVISOR_platform_op (& op ); <nl> if ( ret ) <nl> - return 0 ; <nl> + op . u . pcpu_info . apic_id = BAD_APICID ; <nl>  <nl> return op . u . pcpu_info . apic_id << 24 ; <nl> } <nl> static void xen_silent_inquire ( int apicid ) <nl> { <nl> } <nl>  <nl> + static int xen_cpu_present_to_apicid ( int cpu ) <nl> +{ <nl> + if ( cpu_present ( cpu )) <nl> + return xen_get_apic_id ( xen_apic_read ( APIC_ID )); <nl> + else <nl> + return BAD_APICID ; <nl> +} <nl> + <nl> static struct apic xen_pv_apic = { <nl> . name = " Xen PV ", <nl> . probe = xen_apic_probe_pv , <nl> static struct apic xen_pv_apic = { <nl>  <nl> . ioapic_phys_id_map = default_ioapic_phys_id_map , /* Used on 32 - bit */ <nl> . setup_apic_routing = NULL , <nl> - . cpu_present_to_apicid = default_cpu_present_to_apicid , <nl> + . cpu_present_to_apicid = xen_cpu_present_to_apicid , <nl> . apicid_to_cpu_present = physid_set_mask_of_physid , /* Used on 32 - bit */ <nl> . check_phys_apicid_present = default_check_phys_apicid_present , /* smp_sanity_check needs it */ <nl> . phys_pkg_id = xen_phys_pkg_id , /* detect_ht */
mmm net / ipv4 / tcp_output . c <nl> ppp net / ipv4 / tcp_output . c <nl> static bool tcp_write_xmit ( struct sock * sk , unsigned int mss_now , int nonagle , <nl>  <nl> /* Send one loss probe per tail loss episode . */ <nl> if ( push_one != 2 ) <nl> - tcp_schedule_loss_probe ( sk ); <nl> + tcp_schedule_loss_probe ( sk , false ); <nl> is_cwnd_limited |= ( tcp_packets_in_flight ( tp ) >= tp -> snd_cwnd ); <nl> tcp_cwnd_validate ( sk , is_cwnd_limited ); <nl> return false ; <nl> static bool tcp_write_xmit ( struct sock * sk , unsigned int mss_now , int nonagle , <nl> return ! tp -> packets_out && ! tcp_write_queue_empty ( sk ); <nl> } <nl>  <nl> - bool tcp_schedule_loss_probe ( struct sock * sk ) <nl> + bool tcp_schedule_loss_probe ( struct sock * sk , bool advancing_rto ) <nl> { <nl> struct inet_connection_sock * icsk = inet_csk ( sk ); <nl> struct tcp_sock * tp = tcp_sk ( sk ); <nl> bool tcp_schedule_loss_probe ( struct sock * sk ) <nl> } <nl>  <nl> /* If the RTO formula yields an earlier time , then use that time . */ <nl> - rto_delta_us = tcp_rto_delta_us ( sk ); /* How far in future is RTO ? */ <nl> + rto_delta_us = advancing_rto ? <nl> + jiffies_to_usecs ( inet_csk ( sk )-> icsk_rto ) : <nl> + tcp_rto_delta_us ( sk ); /* How far in future is RTO ? */ <nl> if ( rto_delta_us > 0 ) <nl> timeout = min_t ( u32 , timeout , usecs_to_jiffies ( rto_delta_us )); <nl> mmm include / net / tcp . h <nl> ppp include / net / tcp . h <nl> static bool tcp_write_xmit ( struct sock * sk , unsigned int mss_now , int nonagle , <nl>  <nl> /* Send one loss probe per tail loss episode . */ <nl> if ( push_one != 2 ) <nl> - tcp_schedule_loss_probe ( sk ); <nl> + tcp_schedule_loss_probe ( sk , false ); <nl> is_cwnd_limited |= ( tcp_packets_in_flight ( tp ) >= tp -> snd_cwnd ); <nl> tcp_cwnd_validate ( sk , is_cwnd_limited ); <nl> return false ; <nl> static bool tcp_write_xmit ( struct sock * sk , unsigned int mss_now , int nonagle , <nl> return ! tp -> packets_out && ! tcp_write_queue_empty ( sk ); <nl> } <nl>  <nl> - bool tcp_schedule_loss_probe ( struct sock * sk ) <nl> + bool tcp_schedule_loss_probe ( struct sock * sk , bool advancing_rto ) <nl> { <nl> struct inet_connection_sock * icsk = inet_csk ( sk ); <nl> struct tcp_sock * tp = tcp_sk ( sk ); <nl> bool tcp_schedule_loss_probe ( struct sock * sk ) <nl> } <nl>  <nl> /* If the RTO formula yields an earlier time , then use that time . */ <nl> - rto_delta_us = tcp_rto_delta_us ( sk ); /* How far in future is RTO ? */ <nl> + rto_delta_us = advancing_rto ? <nl> + jiffies_to_usecs ( inet_csk ( sk )-> icsk_rto ) : <nl> + tcp_rto_delta_us ( sk ); /* How far in future is RTO ? */ <nl> if ( rto_delta_us > 0 ) <nl> timeout = min_t ( u32 , timeout , usecs_to_jiffies ( rto_delta_us )); <nl>  <nl> void tcp_push_one ( struct sock *, unsigned int mss_now ); <nl> void tcp_send_ack ( struct sock * sk ); <nl> void tcp_send_delayed_ack ( struct sock * sk ); <nl> void tcp_send_loss_probe ( struct sock * sk ); <nl> - bool tcp_schedule_loss_probe ( struct sock * sk ); <nl> + bool tcp_schedule_loss_probe ( struct sock * sk , bool advancing_rto ); <nl> void tcp_skb_collapse_tstamp ( struct sk_buff * skb , <nl> const struct sk_buff * next_skb ); <nl> mmm net / ipv4 / tcp_input . c <nl> ppp net / ipv4 / tcp_input . c <nl> static bool tcp_write_xmit ( struct sock * sk , unsigned int mss_now , int nonagle , <nl>  <nl> /* Send one loss probe per tail loss episode . */ <nl> if ( push_one != 2 ) <nl> - tcp_schedule_loss_probe ( sk ); <nl> + tcp_schedule_loss_probe ( sk , false ); <nl> is_cwnd_limited |= ( tcp_packets_in_flight ( tp ) >= tp -> snd_cwnd ); <nl> tcp_cwnd_validate ( sk , is_cwnd_limited ); <nl> return false ; <nl> static bool tcp_write_xmit ( struct sock * sk , unsigned int mss_now , int nonagle , <nl> return ! tp -> packets_out && ! tcp_write_queue_empty ( sk ); <nl> } <nl>  <nl> - bool tcp_schedule_loss_probe ( struct sock * sk ) <nl> + bool tcp_schedule_loss_probe ( struct sock * sk , bool advancing_rto ) <nl> { <nl> struct inet_connection_sock * icsk = inet_csk ( sk ); <nl> struct tcp_sock * tp = tcp_sk ( sk ); <nl> bool tcp_schedule_loss_probe ( struct sock * sk ) <nl> } <nl>  <nl> /* If the RTO formula yields an earlier time , then use that time . */ <nl> - rto_delta_us = tcp_rto_delta_us ( sk ); /* How far in future is RTO ? */ <nl> + rto_delta_us = advancing_rto ? <nl> + jiffies_to_usecs ( inet_csk ( sk )-> icsk_rto ) : <nl> + tcp_rto_delta_us ( sk ); /* How far in future is RTO ? */ <nl> if ( rto_delta_us > 0 ) <nl> timeout = min_t ( u32 , timeout , usecs_to_jiffies ( rto_delta_us )); <nl>  <nl> void tcp_push_one ( struct sock *, unsigned int mss_now ); <nl> void tcp_send_ack ( struct sock * sk ); <nl> void tcp_send_delayed_ack ( struct sock * sk ); <nl> void tcp_send_loss_probe ( struct sock * sk ); <nl> - bool tcp_schedule_loss_probe ( struct sock * sk ); <nl> + bool tcp_schedule_loss_probe ( struct sock * sk , bool advancing_rto ); <nl> void tcp_skb_collapse_tstamp ( struct sk_buff * skb , <nl> const struct sk_buff * next_skb ); <nl>  <nl> void tcp_rearm_rto ( struct sock * sk ) <nl> /* Try to schedule a loss probe ; if that doesn ' t work , then schedule an RTO . */ <nl> static void tcp_set_xmit_timer ( struct sock * sk ) <nl> { <nl> - if (! tcp_schedule_loss_probe ( sk )) <nl> + if (! tcp_schedule_loss_probe ( sk , true )) <nl> tcp_rearm_rto ( sk ); <nl> } <nl> 
mmm net / socket . c <nl> ppp net / socket . c <nl> static int do_siocgstamp ( struct net * net , struct socket * sock , <nl> err = sock_do_ioctl ( net , sock , cmd , ( unsigned long )& ktv ); <nl> set_fs ( old_fs ); <nl> if (! err ) <nl> - err = compat_put_timeval ( up , & ktv ); <nl> + err = compat_put_timeval (& ktv , up ); <nl>  <nl> return err ; <nl> } <nl> static int do_siocgstampns ( struct net * net , struct socket * sock , <nl> err = sock_do_ioctl ( net , sock , cmd , ( unsigned long )& kts ); <nl> set_fs ( old_fs ); <nl> if (! err ) <nl> - err = compat_put_timespec ( up , & kts ); <nl> + err = compat_put_timespec (& kts , up ); <nl>  <nl> return err ; <nl> }
mmm drivers / ssb / scan . c <nl> ppp drivers / ssb / scan . c <nl> int ssb_bus_scan ( struct ssb_bus * bus , <nl> bus -> pcicore . dev = dev ; <nl> # endif /* CONFIG_SSB_DRIVER_PCICORE */ <nl> break ; <nl> + case SSB_DEV_ETHERNET : <nl> + if ( bus -> bustype == SSB_BUSTYPE_PCI ) { <nl> + if ( bus -> host_pci -> vendor == PCI_VENDOR_ID_BROADCOM && <nl> + ( bus -> host_pci -> device & 0xFF00 ) == 0x4300 ) { <nl> + /* This is a dangling ethernet core on a <nl> + * wireless device . Ignore it . */ <nl> + continue ; <nl> + } <nl> + } <nl> + break ; <nl> default : <nl> break ; <nl> }
mmm arch / s390 / kernel / kprobes . c <nl> ppp arch / s390 / kernel / kprobes . c <nl> static void copy_instruction ( struct kprobe * p ) <nl> ftrace_generate_nop_insn (( struct ftrace_insn *) p -> ainsn . insn ); <nl> p -> ainsn . is_ftrace_insn = 1 ; <nl> } else <nl> - memcpy ( p -> ainsn . insn , p -> addr , insn_length ( p -> opcode >> 8 )); <nl> + memcpy ( p -> ainsn . insn , p -> addr , insn_length (* p -> addr >> 8 )); <nl> p -> opcode = p -> ainsn . insn [ 0 ]; <nl> if (! probe_is_insn_relative_long ( p -> ainsn . insn )) <nl> return ;
mmm arch / x86_64 / kernel / setup . c <nl> ppp arch / x86_64 / kernel / setup . c <nl> static __init void parse_cmdline_early ( char ** cmdline_p ) <nl> if (! memcmp ( from , " noapic ", 6 )) <nl> skip_ioapic_setup = 1 ; <nl>  <nl> - if (! memcmp ( from , " apic ", 4 )) { <nl> + /* Make sure to not confuse with apic = */ <nl> + if (! memcmp ( from , " apic ", 4 ) && <nl> + ( from [ 4 ] == ' ' || from [ 4 ] == 0 )) { <nl> skip_ioapic_setup = 0 ; <nl> ioapic_force = 1 ; <nl> }
mmm drivers / iommu / intel_irq_remapping . c <nl> ppp drivers / iommu / intel_irq_remapping . c <nl> intel_ioapic_set_affinity ( struct irq_data * data , const struct cpumask * mask , <nl>  <nl> err = apic -> cpu_mask_to_apicid_and ( cfg -> domain , mask , & dest ); <nl> if ( err ) { <nl> - if ( assign_irq_vector ( irq , cfg , data -> affinity )); <nl> + if ( assign_irq_vector ( irq , cfg , data -> affinity )) <nl> pr_err (" Failed to recover vector for irq % d \ n ", irq ); <nl> return err ; <nl> }
mmm drivers / gpu / drm / i915 / i915_gem_execbuffer . c <nl> ppp drivers / gpu / drm / i915 / i915_gem_execbuffer . c <nl> i915_gem_execbuffer2 ( struct drm_device * dev , void * data , <nl> struct drm_i915_gem_exec_object2 * exec2_list = NULL ; <nl> int ret ; <nl>  <nl> - if ( args -> buffer_count < 1 ) { <nl> + if ( args -> buffer_count < 1 || <nl> + args -> buffer_count > UINT_MAX / sizeof (* exec2_list )) { <nl> DRM_DEBUG (" execbuf2 with % d buffers \ n ", args -> buffer_count ); <nl> return - EINVAL ; <nl> }
mmm drivers / media / common / tuners / qt1010 . c <nl> ppp drivers / media / common / tuners / qt1010 . c <nl> static int qt1010_get_bandwidth ( struct dvb_frontend * fe , u32 * bandwidth ) <nl> return 0 ; <nl> } <nl>  <nl> + static int qt1010_get_if_frequency ( struct dvb_frontend * fe , u32 * frequency ) <nl> +{ <nl> + * frequency = 36125000 ; <nl> + return 0 ; <nl> +} <nl> + <nl> static const struct dvb_tuner_ops qt1010_tuner_ops = { <nl> . info = { <nl> . name = " Quantek QT1010 ", <nl> static const struct dvb_tuner_ops qt1010_tuner_ops = { <nl>  <nl> . set_params = qt1010_set_params , <nl> . get_frequency = qt1010_get_frequency , <nl> - . get_bandwidth = qt1010_get_bandwidth <nl> + . get_bandwidth = qt1010_get_bandwidth , <nl> + . get_if_frequency = qt1010_get_if_frequency , <nl> }; <nl>  <nl> struct dvb_frontend * qt1010_attach ( struct dvb_frontend * fe ,
mmm sound / usb / quirks . c <nl> ppp sound / usb / quirks . c <nl> u64 snd_usb_interface_dsd_format_quirks ( struct snd_usb_audio * chip , <nl> } <nl> } <nl> break ; <nl> + case USB_ID ( 0x16d0 , 0x0a23 ): <nl> + if ( fp -> altsetting == 2 ) <nl> + return SNDRV_PCM_FMTBIT_DSD_U32_BE ; <nl> + break ; <nl>  <nl> default : <nl> break ;
mmm drivers / media / platform / vivid / vivid - osd . c <nl> ppp drivers / media / platform / vivid / vivid - osd . c <nl> static int vivid_fb_ioctl ( struct fb_info * info , unsigned cmd , unsigned long arg ) <nl> case FBIOGET_VBLANK : { <nl> struct fb_vblank vblank ; <nl>  <nl> + memset (& vblank , 0 , sizeof ( vblank )); <nl> vblank . flags = FB_VBLANK_HAVE_COUNT | FB_VBLANK_HAVE_VCOUNT | <nl> FB_VBLANK_HAVE_VSYNC ; <nl> vblank . count = 0 ;
mmm sound / isa / opti9xx / miro . c <nl> ppp sound / isa / opti9xx / miro . c <nl> static int __devinit snd_miro_probe ( struct snd_card * card ) <nl>  <nl> error = snd_card_miro_aci_detect ( card , miro ); <nl> if ( error < 0 ) { <nl> - snd_card_free ( card ); <nl> snd_printk ( KERN_ERR " unable to detect aci chip \ n "); <nl> return - ENODEV ; <nl> }
mmm net / packet / af_packet . c <nl> ppp net / packet / af_packet . c <nl> static int tpacket_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> struct timespec ts ; <nl> __u32 ts_status ; <nl> bool is_drop_n_account = false ; <nl> + bool do_vnet = false ; <nl>  <nl> /* struct tpacket { 2 , 3 } _hdr is aligned to a multiple of TPACKET_ALIGNMENT . <nl> * We may add members to them until current aligned size without forcing <nl> static int tpacket_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> netoff = TPACKET_ALIGN ( po -> tp_hdrlen + <nl> ( maclen < 16 ? 16 : maclen )) + <nl> po -> tp_reserve ; <nl> - if ( po -> has_vnet_hdr ) <nl> + if ( po -> has_vnet_hdr ) { <nl> netoff += sizeof ( struct virtio_net_hdr ); <nl> + do_vnet = true ; <nl> + } <nl> macoff = netoff - maclen ; <nl> } <nl> if ( po -> tp_version <= TPACKET_V2 ) { <nl> static int tpacket_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> skb_set_owner_r ( copy_skb , sk ); <nl> } <nl> snaplen = po -> rx_ring . frame_size - macoff ; <nl> - if (( int ) snaplen < 0 ) <nl> + if (( int ) snaplen < 0 ) { <nl> snaplen = 0 ; <nl> + do_vnet = false ; <nl> + } <nl> } <nl> } else if ( unlikely ( macoff + snaplen > <nl> GET_PBDQC_FROM_RB (& po -> rx_ring )-> max_frame_len )) { <nl> static int tpacket_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> if ( unlikely (( int ) snaplen < 0 )) { <nl> snaplen = 0 ; <nl> macoff = GET_PBDQC_FROM_RB (& po -> rx_ring )-> max_frame_len ; <nl> + do_vnet = false ; <nl> } <nl> } <nl> spin_lock (& sk -> sk_receive_queue . lock ); <nl> static int tpacket_rcv ( struct sk_buff * skb , struct net_device * dev , <nl> } <nl> spin_unlock (& sk -> sk_receive_queue . lock ); <nl>  <nl> - if ( po -> has_vnet_hdr ) { <nl> + if ( do_vnet ) { <nl> if ( virtio_net_hdr_from_skb ( skb , h . raw + macoff - <nl> sizeof ( struct virtio_net_hdr ), <nl> vio_le (), true )) {
mmm drivers / gpu / drm / i915 / i915_gem_execbuffer . c <nl> ppp drivers / gpu / drm / i915 / i915_gem_execbuffer . c <nl> static int eb_relocate_vma ( struct i915_execbuffer * eb , struct i915_vma * vma ) <nl> * to read . However , if the array is not writable the user loses <nl> * the updated relocation values . <nl> */ <nl> - if ( unlikely (! access_ok ( VERIFY_READ , urelocs , remain * sizeof ( urelocs )))) <nl> + if ( unlikely (! access_ok ( VERIFY_READ , urelocs , remain * sizeof (* urelocs )))) <nl> return - EFAULT ; <nl>  <nl> do {
mmm fs / ext4 / extents . c <nl> ppp fs / ext4 / extents . c <nl> int ext4_insert_range ( struct inode * inode , loff_t offset , loff_t len ) <nl> up_write (& EXT4_I ( inode )-> i_data_sem ); <nl> goto out_stop ; <nl> } <nl> + } else { <nl> + ext4_ext_drop_refs ( path ); <nl> + kfree ( path ); <nl> } <nl>  <nl> ret = ext4_es_remove_extent ( inode , offset_lblk ,
mmm fs / aio . c <nl> ppp fs / aio . c <nl> static long aio_read_events_ring ( struct kioctx * ctx , <nl> if ( head == tail ) <nl> goto out ; <nl>  <nl> + head %= ctx -> nr_events ; <nl> + tail %= ctx -> nr_events ; <nl> + <nl> while ( ret < nr ) { <nl> long avail ; <nl> struct io_event * ev ;
mmm drivers / cpuidle / cpuidle . c <nl> ppp drivers / cpuidle / cpuidle . c <nl> static cpuidle_enter_t cpuidle_enter_ops ; <nl> /** <nl> * cpuidle_play_dead - cpu off - lining <nl> * <nl> - * Only returns in case of an error <nl> + * Returns in case of an error or no driver <nl> */ <nl> int cpuidle_play_dead ( void ) <nl> { <nl> int cpuidle_play_dead ( void ) <nl> int i , dead_state = - 1 ; <nl> int power_usage = - 1 ; <nl>  <nl> + if (! drv ) <nl> + return - ENODEV ; <nl> + <nl> /* Find lowest - power state that supports long - term idle */ <nl> for ( i = CPUIDLE_DRIVER_STATE_START ; i < drv -> state_count ; i ++) { <nl> struct cpuidle_state * s = & drv -> states [ i ];
mmm net / ipx / af_ipx . c <nl> ppp net / ipx / af_ipx . c <nl> static int ipxitf_ioctl ( unsigned int cmd , void __user * arg ) <nl> sipx -> sipx_network = ipxif -> if_netnum ; <nl> memcpy ( sipx -> sipx_node , ipxif -> if_node , <nl> sizeof ( sipx -> sipx_node )); <nl> - rc = - EFAULT ; <nl> + rc = 0 ; <nl> if ( copy_to_user ( arg , & ifr , sizeof ( ifr ))) <nl> - break ; <nl> + rc = - EFAULT ; <nl> ipxitf_put ( ipxif ); <nl> - rc = 0 ; <nl> break ; <nl> } <nl> case SIOCAIPXITFCRT :
mmm drivers / power / bq20z75 . c <nl> ppp drivers / power / bq20z75 . c <nl> static int __devinit bq20z75_probe ( struct i2c_client * client , <nl>  <nl> INIT_DELAYED_WORK (& bq20z75_device -> work , bq20z75_delayed_work ); <nl>  <nl> + bq20z75_device -> enable_detection = true ; <nl> + <nl> return 0 ; <nl>  <nl> exit_psupply :
mmm drivers / net / skge . c <nl> ppp drivers / net / skge . c <nl> static int skge_xmit_frame ( struct sk_buff * skb , struct net_device * dev ) <nl> } <nl>  <nl> if ( unlikely ( skge -> tx_avail < skb_shinfo ( skb )-> nr_frags + 1 )) { <nl> - netif_stop_queue ( dev ); <nl> - spin_unlock_irqrestore (& skge -> tx_lock , flags ); <nl> + if (! netif_stopped ( dev )) { <nl> + netif_stop_queue ( dev ); <nl>  <nl> - printk ( KERN_WARNING PFX "% s : ring full when queue awake !\ n ", <nl> - dev -> name ); <nl> + printk ( KERN_WARNING PFX "% s : ring full when queue awake !\ n ", <nl> + dev -> name ); <nl> + } <nl> + spin_unlock_irqrestore (& skge -> tx_lock , flags ); <nl> return NETDEV_TX_BUSY ; <nl> } <nl> 
mmm arch / x86 / platform / efi / efi . c <nl> ppp arch / x86 / platform / efi / efi . c <nl> void __init efi_enter_virtual_mode ( void ) <nl> new_memmap = krealloc ( new_memmap , <nl> ( count + 1 ) * memmap . desc_size , <nl> GFP_KERNEL ); <nl> + if (! new_memmap ) <nl> + goto err_out ; <nl> + <nl> memcpy ( new_memmap + ( count * memmap . desc_size ), md , <nl> memmap . desc_size ); <nl> count ++; <nl> void __init efi_enter_virtual_mode ( void ) <nl> EFI_VARIABLE_BOOTSERVICE_ACCESS | <nl> EFI_VARIABLE_RUNTIME_ACCESS , <nl> 0 , NULL ); <nl> + <nl> + return ; <nl> + <nl> + err_out : <nl> + pr_err (" Error reallocating memory , EFI runtime non - functional !\ n "); <nl> } <nl>  <nl> /*
mmm include / asm - generic / pgtable . h <nl> ppp include / asm - generic / pgtable . h <nl> static inline int pmd_none_or_trans_huge_or_clear_bad ( pmd_t * pmd ) <nl> # ifdef CONFIG_TRANSPARENT_HUGEPAGE <nl> barrier (); <nl> # endif <nl> - if ( pmd_none ( pmdval )) <nl> + if ( pmd_none ( pmdval ) || pmd_trans_huge ( pmdval )) <nl> return 1 ; <nl> if ( unlikely ( pmd_bad ( pmdval ))) { <nl> - if (! pmd_trans_huge ( pmdval )) <nl> - pmd_clear_bad ( pmd ); <nl> + pmd_clear_bad ( pmd ); <nl> return 1 ; <nl> } <nl> return 0 ;
mmm net / xfrm / xfrm_user . c <nl> ppp net / xfrm / xfrm_user . c <nl> static int build_expire ( struct sk_buff * skb , struct xfrm_state * x , int hard ) <nl> static int xfrm_exp_state_notify ( struct xfrm_state * x , struct km_event * c ) <nl> { <nl> struct sk_buff * skb ; <nl> + int len = NLMSG_LENGTH ( sizeof ( struct xfrm_user_expire )); <nl>  <nl> - /* fix to do alloc using NLM macros */ <nl> - skb = alloc_skb ( sizeof ( struct xfrm_user_expire ) + 16 , GFP_ATOMIC ); <nl> + skb = alloc_skb ( len , GFP_ATOMIC ); <nl> if ( skb == NULL ) <nl> return - ENOMEM ; <nl> 
mmm drivers / net / bonding / bond_main . c <nl> ppp drivers / net / bonding / bond_main . c <nl> static int __bond_release_one ( struct net_device * bond_dev , <nl> return - EINVAL ; <nl> } <nl>  <nl> - /* release the slave from its bond */ <nl> - bond -> slave_cnt --; <nl> - <nl> bond_sysfs_slave_del ( slave ); <nl>  <nl> bond_upper_dev_unlink ( bond_dev , slave_dev ); <nl> static int __bond_release_one ( struct net_device * bond_dev , <nl>  <nl> unblock_netpoll_tx (); <nl> synchronize_rcu (); <nl> + bond -> slave_cnt --; <nl>  <nl> if (! bond_has_slaves ( bond )) { <nl> call_netdevice_notifiers ( NETDEV_CHANGEADDR , bond -> dev );
mmm arch / x86 / kvm / i8254 . c <nl> ppp arch / x86 / kvm / i8254 . c <nl> static int pit_ioport_read ( struct kvm_io_device * this , <nl> return - EOPNOTSUPP ; <nl>  <nl> addr &= KVM_PIT_CHANNEL_MASK ; <nl> + if ( addr == 3 ) <nl> + return 0 ; <nl> + <nl> s = & pit_state -> channels [ addr ]; <nl>  <nl> mutex_lock (& pit_state -> lock );
mmm drivers / gpu / drm / amd / include / atomfirmware . h <nl> ppp drivers / gpu / drm / amd / include / atomfirmware . h <nl> enum atom_smu11_syspll_id { <nl> SMU11_SYSPLL3_1_ID = 6 , <nl> }; <nl>  <nl> - <nl> enum atom_smu11_syspll0_clock_id { <nl> - SMU11_SYSPLL0_SOCCLK_ID = 0 , // SOCCLK <nl> - SMU11_SYSPLL0_MP0CLK_ID = 1 , // MP0CLK <nl> - SMU11_SYSPLL0_DCLK_ID = 2 , // DCLK <nl> - SMU11_SYSPLL0_VCLK_ID = 3 , // VCLK <nl> - SMU11_SYSPLL0_ECLK_ID = 4 , // ECLK <nl> + SMU11_SYSPLL0_ECLK_ID = 0 , // ECLK <nl> + SMU11_SYSPLL0_SOCCLK_ID = 1 , // SOCCLK <nl> + SMU11_SYSPLL0_MP0CLK_ID = 2 , // MP0CLK <nl> + SMU11_SYSPLL0_DCLK_ID = 3 , // DCLK <nl> + SMU11_SYSPLL0_VCLK_ID = 4 , // VCLK <nl> SMU11_SYSPLL0_DCEFCLK_ID = 5 , // DCEFCLK <nl> }; <nl>  <nl> - <nl> enum atom_smu11_syspll1_0_clock_id { <nl> SMU11_SYSPLL1_0_UCLKA_ID = 0 , // UCLK_a <nl> };
mmm drivers / input / tablet / kbtab . c <nl> ppp drivers / input / tablet / kbtab . c <nl> static int kbtab_probe ( struct usb_interface * intf , const struct usb_device_id * i <nl> return 0 ; <nl>  <nl> fail3 : usb_free_urb ( kbtab -> irq ); <nl> - fail2 : usb_buffer_free ( dev , 10 , kbtab -> data , kbtab -> data_dma ); <nl> + fail2 : usb_buffer_free ( dev , 8 , kbtab -> data , kbtab -> data_dma ); <nl> fail1 : input_free_device ( input_dev ); <nl> kfree ( kbtab ); <nl> return error ; <nl> static void kbtab_disconnect ( struct usb_interface * intf ) <nl> usb_kill_urb ( kbtab -> irq ); <nl> input_unregister_device ( kbtab -> dev ); <nl> usb_free_urb ( kbtab -> irq ); <nl> - usb_buffer_free ( interface_to_usbdev ( intf ), 10 , kbtab -> data , kbtab -> data_dma ); <nl> + usb_buffer_free ( interface_to_usbdev ( intf ), 8 , kbtab -> data , kbtab -> data_dma ); <nl> kfree ( kbtab ); <nl> } <nl> }
mmm sound / core / timer . c <nl> ppp sound / core / timer . c <nl> void snd_timer_interrupt ( struct snd_timer * timer , unsigned long ticks_left ) <nl> } else { <nl> ti -> flags &= ~ SNDRV_TIMER_IFLG_RUNNING ; <nl> if (-- timer -> running ) <nl> - list_del (& ti -> active_list ); <nl> + list_del_init (& ti -> active_list ); <nl> } <nl> if (( timer -> hw . flags & SNDRV_TIMER_HW_TASKLET ) || <nl> ( ti -> flags & SNDRV_TIMER_IFLG_FAST ))
mmm drivers / staging / vt6655 / wroute . c <nl> ppp drivers / staging / vt6655 / wroute . c <nl> BOOL ROUTEbRelay ( PSDevice pDevice , PBYTE pbySkbData , UINT uDataLen , UINT uNodeI <nl> } <nl>  <nl> if ( pDevice -> bEnableHostWEP ) { <nl> - if ( uNodeIndex >= 0 ) { <nl> + if ( uNodeIndex < MAX_NODE_NUM + 1 ) { <nl> pTransmitKey = & STempKey ; <nl> pTransmitKey -> byCipherSuite = pMgmt -> sNodeDBTable [ uNodeIndex ]. byCipherSuite ; <nl> pTransmitKey -> dwKeyIndex = pMgmt -> sNodeDBTable [ uNodeIndex ]. dwKeyIndex ;mmm drivers / staging / vt6656 / rxtx . c <nl> ppp drivers / staging / vt6656 / rxtx . c <nl> BOOL ROUTEbRelay ( PSDevice pDevice , PBYTE pbySkbData , UINT uDataLen , UINT uNodeI <nl> } <nl>  <nl> if ( pDevice -> bEnableHostWEP ) { <nl> - if ( uNodeIndex >= 0 ) { <nl> + if ( uNodeIndex < MAX_NODE_NUM + 1 ) { <nl> pTransmitKey = & STempKey ; <nl> pTransmitKey -> byCipherSuite = pMgmt -> sNodeDBTable [ uNodeIndex ]. byCipherSuite ; <nl> pTransmitKey -> dwKeyIndex = pMgmt -> sNodeDBTable [ uNodeIndex ]. dwKeyIndex ; <nl> bRelayPacketSend ( <nl> } <nl>  <nl> if ( pDevice -> bEnableHostWEP ) { <nl> - if ( uNodeIndex >= 0 ) { <nl> + if ( uNodeIndex < MAX_NODE_NUM + 1 ) { <nl> pTransmitKey = & STempKey ; <nl> pTransmitKey -> byCipherSuite = pMgmt -> sNodeDBTable [ uNodeIndex ]. byCipherSuite ; <nl> pTransmitKey -> dwKeyIndex = pMgmt -> sNodeDBTable [ uNodeIndex ]. dwKeyIndex ;
mmm sound / pci / hda / patch_realtek . c <nl> ppp sound / pci / hda / patch_realtek . c <nl> static int alc662_parse_auto_config ( struct hda_codec * codec ) <nl> if ( codec -> vendor_id == 0x10ec0663 ) <nl> spec -> init_verbs [ spec -> num_init_verbs ++] = <nl> alc663_auto_init_verbs ; <nl> + <nl> + err = alc_auto_add_mic_boost ( codec ); <nl> + if ( err < 0 ) <nl> + return err ; <nl> + <nl> spec -> mixers [ spec -> num_mixers ] = alc662_capture_mixer ; <nl> spec -> num_mixers ++; <nl> return 1 ;
mmm drivers / staging / ste_rmi4 / synaptics_i2c_rmi4 . c <nl> ppp drivers / staging / ste_rmi4 / synaptics_i2c_rmi4 . c <nl> static int synpatics_rmi4_touchpad_report ( struct synaptics_rmi4_data * pdata , <nl> * 10 = finger present but data may not be accurate , <nl> * 11 = reserved for product use . <nl> */ <nl> - finger_registers = ( fingers_supported + 3 )/ 4 ; <nl> + finger_registers = ( fingers_supported + 3 ) / 4 ; <nl> data_base_addr = rfi -> fn_desc . data_base_addr ; <nl> retval = synaptics_rmi4_i2c_block_read ( pdata , data_base_addr , values , <nl> finger_registers ); <nl> static int synpatics_rmi4_touchpad_report ( struct synaptics_rmi4_data * pdata , <nl> data_reg_blk_size = rfi -> size_of_data_register_block ; <nl> for ( finger = 0 ; finger < fingers_supported ; finger ++) { <nl> /* determine which data byte the finger status is in */ <nl> - reg = finger / 4 ; <nl> + reg = finger / 4 ; <nl> /* bit shift to get finger ' s status */ <nl> finger_shift = ( finger % 4 ) * 2 ; <nl> finger_status = ( values [ reg ] >> finger_shift ) & 3 ; <nl> static int synpatics_rmi4_touchpad_detect ( struct synaptics_rmi4_data * pdata , <nl> } <nl> pdata -> fingers_supported = rfi -> num_of_data_points ; <nl> /* Need to get interrupt info for handling interrupts */ <nl> - rfi -> index_to_intr_reg = ( interruptcount + 7 )/ 8 ; <nl> + rfi -> index_to_intr_reg = ( interruptcount + 7 ) / 8 ; <nl> if ( rfi -> index_to_intr_reg != 0 ) <nl> rfi -> index_to_intr_reg -= 1 ; <nl> /*
mmm fs / sysfs / dir . c <nl> ppp fs / sysfs / dir . c <nl> static struct sysfs_dirent * sysfs_new_dirent ( struct sysfs_dirent * parent_sd , <nl>  <nl> memset ( sd , 0 , sizeof (* sd )); <nl> atomic_set (& sd -> s_count , 1 ); <nl> - atomic_set (& sd -> s_event , 0 ); <nl> + atomic_set (& sd -> s_event , 1 ); <nl> INIT_LIST_HEAD (& sd -> s_children ); <nl> list_add (& sd -> s_sibling , & parent_sd -> s_children ); <nl> sd -> s_element = element ;
mmm crypto / md5 . c <nl> ppp crypto / md5 . c <nl> static struct shash_alg alg = { <nl> . export = md5_export , <nl> . import = md5_import , <nl> . descsize = sizeof ( struct md5_state ), <nl> + . statesize = sizeof ( struct md5_state ), <nl> . base = { <nl> . cra_name = " md5 ", <nl> . cra_flags = CRYPTO_ALG_TYPE_SHASH ,
mmm drivers / net / wireless / iwlwifi / iwl - tx . c <nl> ppp drivers / net / wireless / iwlwifi / iwl - tx . c <nl> int iwl_enqueue_hcmd ( struct iwl_priv * priv , struct iwl_host_cmd * cmd ) <nl> return - EIO ; <nl> } <nl>  <nl> + if (( priv -> ucode_owner == IWL_OWNERSHIP_TM ) && <nl> + !( cmd -> flags & CMD_ON_DEMAND )) { <nl> + IWL_DEBUG_HC ( priv , " tm own the uCode , no regular hcmd send \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> copy_size = sizeof ( out_cmd -> hdr ); <nl> cmd_size = sizeof ( out_cmd -> hdr ); <nl> 
mmm security / keys / key . c <nl> ppp security / keys / key . c <nl> static int __key_instantiate_and_link ( struct key * key , <nl>  <nl> /* and link it into the destination keyring */ <nl> if ( keyring ) { <nl> - set_bit ( KEY_FLAG_KEEP , & key -> flags ); <nl> + if ( test_bit ( KEY_FLAG_KEEP , & keyring -> flags )) <nl> + set_bit ( KEY_FLAG_KEEP , & key -> flags ); <nl>  <nl> __key_link ( key , _edit ); <nl> }
mmm fs / ubifs / lprops . c <nl> ppp fs / ubifs / lprops . c <nl> static int scan_check_cb ( struct ubifs_info * c , <nl> } <nl> } <nl>  <nl> - buf = __vmalloc ( c -> leb_size , GFP_NOFS , PAGE_KERNEL ); <nl> - if (! buf ) <nl> - return - ENOMEM ; <nl> - <nl> /* <nl> * After an unclean unmount , empty and freeable LEBs <nl> * may contain garbage - do not scan them . <nl> static int scan_check_cb ( struct ubifs_info * c , <nl> return LPT_SCAN_CONTINUE ; <nl> } <nl>  <nl> + buf = __vmalloc ( c -> leb_size , GFP_NOFS , PAGE_KERNEL ); <nl> + if (! buf ) <nl> + return - ENOMEM ; <nl> + <nl> sleb = ubifs_scan ( c , lnum , 0 , buf , 0 ); <nl> if ( IS_ERR ( sleb )) { <nl> ret = PTR_ERR ( sleb );
mmm drivers / media / platform / s3c - camif / camif - core . c <nl> ppp drivers / media / platform / s3c - camif / camif - core . c <nl> static struct s3c_camif_drvdata s3c6410_camif_drvdata = { <nl> . bus_clk_freq = 133000000UL , <nl> }; <nl>  <nl> - static struct platform_device_id s3c_camif_driver_ids [] = { <nl> + static const struct platform_device_id s3c_camif_driver_ids [] = { <nl> { <nl> . name = " s3c2440 - camif ", <nl> . driver_data = ( unsigned long )& s3c244x_camif_drvdata ,
mmm drivers / pci / hotplug / shpchp_core . c <nl> ppp drivers / pci / hotplug / shpchp_core . c <nl> int shpchp_debug ; <nl> int shpchp_poll_mode ; <nl> int shpchp_poll_time ; <nl> + int shpchp_slot_with_bus ; <nl> struct workqueue_struct * shpchp_wq ; <nl>  <nl> # define DRIVER_VERSION " 0 . 4 " <nl> MODULE_LICENSE (" GPL "); <nl> module_param ( shpchp_debug , bool , 0644 ); <nl> module_param ( shpchp_poll_mode , bool , 0644 ); <nl> module_param ( shpchp_poll_time , int , 0644 ); <nl> + module_param ( shpchp_slot_with_bus , bool , 0644 ); <nl> MODULE_PARM_DESC ( shpchp_debug , " Debugging mode enabled or not "); <nl> MODULE_PARM_DESC ( shpchp_poll_mode , " Using polling mechanism for hot - plug events or not "); <nl> MODULE_PARM_DESC ( shpchp_poll_time , " Polling mechanism frequency , in seconds "); <nl> + MODULE_PARM_DESC ( shpchp_slot_with_bus , " Use bus number in the slot name "); <nl>  <nl> # define SHPC_MODULE_NAME " shpchp " <nl>  <nl> static void release_slot ( struct hotplug_slot * hotplug_slot ) <nl>  <nl> static void make_slot_name ( struct slot * slot ) <nl> { <nl> - snprintf ( slot -> hotplug_slot -> name , SLOT_NAME_SIZE , "% 04d_ % 04d ", <nl> - slot -> bus , slot -> number ); <nl> + if ( shpchp_slot_with_bus ) <nl> + snprintf ( slot -> hotplug_slot -> name , SLOT_NAME_SIZE , "% 04d_ % 04d ", <nl> + slot -> bus , slot -> number ); <nl> + else <nl> + snprintf ( slot -> hotplug_slot -> name , SLOT_NAME_SIZE , "% d ", <nl> + slot -> number ); <nl> } <nl>  <nl> static int init_slots ( struct controller * ctrl )
mmm net / sched / cls_route . c <nl> ppp net / sched / cls_route . c <nl> static int route4_change ( struct net * net , struct sk_buff * in_skb , <nl> fp = & b -> ht [ h ]; <nl> for ( pfp = rtnl_dereference (* fp ); pfp ; <nl> fp = & pfp -> next , pfp = rtnl_dereference (* fp )) { <nl> - if ( pfp == f ) { <nl> - * fp = f -> next ; <nl> + if ( pfp == fold ) { <nl> + rcu_assign_pointer (* fp , fold -> next ); <nl> break ; <nl> } <nl> }
mmm drivers / media / platform / s5p - fimc / mipi - csis . c <nl> ppp drivers / media / platform / s5p - fimc / mipi - csis . c <nl> static void s5pcsis_log_counters ( struct csis_state * state , bool non_errors ) <nl>  <nl> spin_lock_irqsave (& state -> slock , flags ); <nl>  <nl> - for ( i --; i >= 0 ; i --) <nl> - if ( state -> events [ i ]. counter >= 0 ) <nl> + for ( i --; i >= 0 ; i --) { <nl> + if ( state -> events [ i ]. counter > 0 || debug ) <nl> v4l2_info (& state -> sd , "% s events : % d \ n ", <nl> state -> events [ i ]. name , <nl> state -> events [ i ]. counter ); <nl> - <nl> + } <nl> spin_unlock_irqrestore (& state -> slock , flags ); <nl> } <nl> 
mmm net / ax25 / af_ax25 . c <nl> ppp net / ax25 / af_ax25 . c <nl> static int ax25_recvmsg ( struct kiocb * iocb , struct socket * sock , <nl> ax25_address src ; <nl> const unsigned char * mac = skb_mac_header ( skb ); <nl>  <nl> + memset ( sax , 0 , sizeof ( struct full_sockaddr_ax25 )); <nl> ax25_addr_parse ( mac + 1 , skb -> data - mac - 1 , & src , NULL , <nl> & digi , NULL , NULL ); <nl> sax -> sax25_family = AF_AX25 ;
mmm arch / x86 / xen / time . c <nl> ppp arch / x86 / xen / time . c <nl> static const struct clock_event_device xen_vcpuop_clockevent = { <nl>  <nl> static const struct clock_event_device * xen_clockevent = <nl> & xen_timerop_clockevent ; <nl> - static DEFINE_PER_CPU ( struct clock_event_device , xen_clock_events ); <nl> + static DEFINE_PER_CPU ( struct clock_event_device , xen_clock_events ) = { . irq = - 1 }; <nl>  <nl> static irqreturn_t xen_timer_interrupt ( int irq , void * dev_id ) <nl> { <nl> void xen_setup_timer ( int cpu ) <nl> struct clock_event_device * evt ; <nl> int irq ; <nl>  <nl> + evt = & per_cpu ( xen_clock_events , cpu ); <nl> + WARN ( evt -> irq >= 0 , " IRQ % d for CPU % d is already allocated \ n ", evt -> irq , cpu ); <nl> + <nl> printk ( KERN_INFO " installing Xen timer for CPU % d \ n ", cpu ); <nl>  <nl> name = kasprintf ( GFP_KERNEL , " timer % d ", cpu ); <nl> void xen_setup_timer ( int cpu ) <nl> IRQF_FORCE_RESUME , <nl> name , NULL ); <nl>  <nl> - evt = & per_cpu ( xen_clock_events , cpu ); <nl> memcpy ( evt , xen_clockevent , sizeof (* evt )); <nl>  <nl> evt -> cpumask = cpumask_of ( cpu ); <nl> void xen_teardown_timer ( int cpu ) <nl> BUG_ON ( cpu == 0 ); <nl> evt = & per_cpu ( xen_clock_events , cpu ); <nl> unbind_from_irqhandler ( evt -> irq , NULL ); <nl> + evt -> irq = - 1 ; <nl> } <nl>  <nl> void xen_setup_cpu_clockevents ( void )
mmm sound / soc / intel / common / sst - firmware . c <nl> ppp sound / soc / intel / common / sst - firmware . c <nl> void sst_dsp_dma_put_channel ( struct sst_dsp * dsp ) <nl> } <nl> EXPORT_SYMBOL_GPL ( sst_dsp_dma_put_channel ); <nl>  <nl> - int sst_dma_new ( struct sst_dsp * sst ) <nl> + static int sst_dma_new ( struct sst_dsp * sst ) <nl> { <nl> struct sst_pdata * sst_pdata = sst -> pdata ; <nl> struct sst_dma * dma ; <nl> int sst_dma_new ( struct sst_dsp * sst ) <nl> devm_kfree ( sst -> dev , dma ); <nl> return ret ; <nl> } <nl> - EXPORT_SYMBOL ( sst_dma_new ); <nl>  <nl> - void sst_dma_free ( struct sst_dma * dma ) <nl> + static void sst_dma_free ( struct sst_dma * dma ) <nl> { <nl>  <nl> if ( dma == NULL ) <nl> void sst_dma_free ( struct sst_dma * dma ) <nl> dw_remove ( dma -> chip ); <nl>  <nl> } <nl> - EXPORT_SYMBOL ( sst_dma_free ); <nl>  <nl> /* create new generic firmware object */ <nl> struct sst_fw * sst_fw_new ( struct sst_dsp * dsp ,
mmm fs / btrfs / async - thread . c <nl> ppp fs / btrfs / async - thread . c <nl> void btrfs_destroy_workqueue ( struct btrfs_workqueue * wq ) <nl> if ( wq -> high ) <nl> __btrfs_destroy_workqueue ( wq -> high ); <nl> __btrfs_destroy_workqueue ( wq -> normal ); <nl> + kfree ( wq ); <nl> } <nl>  <nl> void btrfs_workqueue_set_max ( struct btrfs_workqueue * wq , int max )
mmm tools / perf / util / ui / browsers / annotate . c <nl> ppp tools / perf / util / ui / browsers / annotate . c <nl> static int annotate_browser__run ( struct annotate_browser * self , int evidx , <nl> nd = self -> curr_hot ; <nl> break ; <nl> case ' H ': <nl> + case ' h ': <nl> nd = self -> curr_hot ; <nl> break ; <nl> case ' S ': <nl> + case ' s ': <nl> if ( annotate_browser__toggle_source ( self )) <nl> ui_helpline__puts ( help ); <nl> continue ;
mmm drivers / block / loop . c <nl> ppp drivers / block / loop . c <nl> static int loop_set_fd ( struct loop_device * lo , fmode_t mode , <nl>  <nl> bio_list_init (& lo -> lo_bio_list ); <nl>  <nl> - /* <nl> - * set queue make_request_fn , and add limits based on lower level <nl> - * device <nl> - */ <nl> - blk_queue_make_request ( lo -> lo_queue , loop_make_request ); <nl> - lo -> lo_queue -> queuedata = lo ; <nl> - <nl> if (!( lo_flags & LO_FLAGS_READ_ONLY ) && file -> f_op -> fsync ) <nl> blk_queue_flush ( lo -> lo_queue , REQ_FLUSH ); <nl>  <nl> static int loop_add ( struct loop_device ** l , int i ) <nl> if (! lo ) <nl> goto out ; <nl>  <nl> + lo -> lo_state = Lo_unbound ; <nl> + <nl> /* allocate id , if @ id >= 0 , we ' re requesting that specific id */ <nl> if ( i >= 0 ) { <nl> err = idr_alloc (& loop_index_idr , lo , i , i + 1 , GFP_KERNEL ); <nl> static int loop_add ( struct loop_device ** l , int i ) <nl> if (! lo -> lo_queue ) <nl> goto out_free_idr ; <nl>  <nl> + /* <nl> + * set queue make_request_fn <nl> + */ <nl> + blk_queue_make_request ( lo -> lo_queue , loop_make_request ); <nl> + lo -> lo_queue -> queuedata = lo ; <nl> + <nl> disk = lo -> lo_disk = alloc_disk ( 1 << part_shift ); <nl> if (! disk ) <nl> goto out_free_queue ;
mmm arch / x86 / kvm / vmx . c <nl> ppp arch / x86 / kvm / vmx . c <nl> static inline bool nested_cpu_has_posted_intr ( struct vmcs12 * vmcs12 ) <nl> return vmcs12 -> pin_based_vm_exec_control & PIN_BASED_POSTED_INTR ; <nl> } <nl>  <nl> - static inline bool is_exception ( u32 intr_info ) <nl> + static inline bool is_nmi ( u32 intr_info ) <nl> { <nl> return ( intr_info & ( INTR_INFO_INTR_TYPE_MASK | INTR_INFO_VALID_MASK )) <nl> - == ( INTR_TYPE_HARD_EXCEPTION | INTR_INFO_VALID_MASK ); <nl> + == ( INTR_TYPE_NMI_INTR | INTR_INFO_VALID_MASK ); <nl> } <nl>  <nl> static void nested_vmx_vmexit ( struct kvm_vcpu * vcpu , u32 exit_reason , <nl> static int handle_exception ( struct kvm_vcpu * vcpu ) <nl> if ( is_machine_check ( intr_info )) <nl> return handle_machine_check ( vcpu ); <nl>  <nl> - if (( intr_info & INTR_INFO_INTR_TYPE_MASK ) == INTR_TYPE_NMI_INTR ) <nl> + if ( is_nmi ( intr_info )) <nl> return 1 ; /* already handled by vmx_vcpu_run () */ <nl>  <nl> if ( is_no_device ( intr_info )) { <nl> static bool nested_vmx_exit_handled ( struct kvm_vcpu * vcpu ) <nl>  <nl> switch ( exit_reason ) { <nl> case EXIT_REASON_EXCEPTION_NMI : <nl> - if (! is_exception ( intr_info )) <nl> + if ( is_nmi ( intr_info )) <nl> return false ; <nl> else if ( is_page_fault ( intr_info )) <nl> return enable_ept ; <nl> static void vmx_complete_atomic_exit ( struct vcpu_vmx * vmx ) <nl> kvm_machine_check (); <nl>  <nl> /* We need to handle NMIs before interrupts are enabled */ <nl> - if (( exit_intr_info & INTR_INFO_INTR_TYPE_MASK ) == INTR_TYPE_NMI_INTR && <nl> - ( exit_intr_info & INTR_INFO_VALID_MASK )) { <nl> + if ( is_nmi ( exit_intr_info )) { <nl> kvm_before_handle_nmi (& vmx -> vcpu ); <nl> asm (" int $ 2 "); <nl> kvm_after_handle_nmi (& vmx -> vcpu );
mmm drivers / block / floppy . c <nl> ppp drivers / block / floppy . c <nl> static int raw_cmd_copyin ( int cmd , void __user * param , <nl> return - ENOMEM ; <nl> * rcmd = ptr ; <nl> ret = copy_from_user ( ptr , param , sizeof (* ptr )); <nl> - if ( ret ) <nl> - return - EFAULT ; <nl> ptr -> next = NULL ; <nl> ptr -> buffer_length = 0 ; <nl> + ptr -> kernel_data = NULL ; <nl> + if ( ret ) <nl> + return - EFAULT ; <nl> param += sizeof ( struct floppy_raw_cmd ); <nl> if ( ptr -> cmd_count > 33 ) <nl> /* the command may now also take up the space <nl> static int raw_cmd_copyin ( int cmd , void __user * param , <nl> for ( i = 0 ; i < 16 ; i ++) <nl> ptr -> reply [ i ] = 0 ; <nl> ptr -> resultcode = 0 ; <nl> - ptr -> kernel_data = NULL ; <nl>  <nl> if ( ptr -> flags & ( FD_RAW_READ | FD_RAW_WRITE )) { <nl> if ( ptr -> length <= 0 )
mmm fs / nfsd / nfs4recover . c <nl> ppp fs / nfsd / nfs4recover . c <nl> cld_pipe_downcall ( struct file * filp , const char __user * src , size_t mlen ) <nl> struct cld_upcall * tmp , * cup ; <nl> struct cld_msg __user * cmsg = ( struct cld_msg __user *) src ; <nl> uint32_t xid ; <nl> - struct nfsd_net * nn = net_generic ( filp -> f_dentry -> d_sb -> s_fs_info , <nl> + struct nfsd_net * nn = net_generic ( file_inode ( filp )-> i_sb -> s_fs_info , <nl> nfsd_net_id ); <nl> struct cld_net * cn = nn -> cld_net ; <nl> 
mmm drivers / scsi / hpsa . c <nl> ppp drivers / scsi / hpsa . c <nl> static int hpsa_eh_device_reset_handler ( struct scsi_cmnd * scsicmd ) <nl> return FAILED ; <nl> } <nl>  <nl> + if ( dev -> devtype == TYPE_ENCLOSURE ) <nl> + return SUCCESS ; <nl> + <nl> /* if controller locked up , we can guarantee command won ' t complete */ <nl> if ( lockup_detected ( h )) { <nl> snprintf ( msg , sizeof ( msg ),
mmm sound / pci / hda / patch_realtek . c <nl> ppp sound / pci / hda / patch_realtek . c <nl> static void alc889_fixup_dac_route ( struct hda_codec * codec , <nl> const struct alc_fixup * fix , int action ) <nl> { <nl> if ( action == ALC_FIXUP_ACT_PRE_PROBE ) { <nl> + /* fake the connections during parsing the tree */ <nl> hda_nid_t conn1 [ 2 ] = { 0x0c , 0x0d }; <nl> hda_nid_t conn2 [ 2 ] = { 0x0e , 0x0f }; <nl> snd_hda_override_conn_list ( codec , 0x14 , 2 , conn1 ); <nl> snd_hda_override_conn_list ( codec , 0x15 , 2 , conn1 ); <nl> snd_hda_override_conn_list ( codec , 0x18 , 2 , conn2 ); <nl> snd_hda_override_conn_list ( codec , 0x1a , 2 , conn2 ); <nl> + } else if ( action == ALC_FIXUP_ACT_PROBE ) { <nl> + /* restore the connections */ <nl> + hda_nid_t conn [ 5 ] = { 0x0c , 0x0d , 0x0e , 0x0f , 0x26 }; <nl> + snd_hda_override_conn_list ( codec , 0x14 , 5 , conn ); <nl> + snd_hda_override_conn_list ( codec , 0x15 , 5 , conn ); <nl> + snd_hda_override_conn_list ( codec , 0x18 , 5 , conn ); <nl> + snd_hda_override_conn_list ( codec , 0x1a , 5 , conn ); <nl> } <nl> } <nl> 
mmm kernel / exit . c <nl> ppp kernel / exit . c <nl> void do_exit ( long code ) <nl>  <nl> module_put ( task_thread_info ( tsk )-> exec_domain -> module ); <nl>  <nl> - proc_exit_connector ( tsk ); <nl> /* <nl> * FIXME : do that only when needed , using sched_exit tracepoint <nl> */ <nl> flush_ptrace_hw_breakpoint ( tsk ); <nl>  <nl> exit_notify ( tsk , group_dead ); <nl> + proc_exit_connector ( tsk ); <nl> # ifdef CONFIG_NUMA <nl> task_lock ( tsk ); <nl> mpol_put ( tsk -> mempolicy );
mmm drivers / staging / comedi / drivers / das16 . c <nl> ppp drivers / staging / comedi / drivers / das16 . c <nl> static struct comedi_driver das16_driver = { <nl> module_comedi_driver ( das16_driver ); <nl>  <nl> MODULE_AUTHOR (" Comedi http :// www . comedi . org "); <nl> - MODULE_DESCRIPTION (" Comedi low - level driver "); <nl> + MODULE_DESCRIPTION (" Comedi driver for DAS16 compatible boards "); <nl> MODULE_LICENSE (" GPL ");
mmm fs / hfsplus / catalog . c <nl> ppp fs / hfsplus / catalog . c <nl> int hfsplus_find_cat ( struct super_block * sb , u32 cnid , <nl> return - EIO ; <nl> } <nl>  <nl> + if ( be16_to_cpu ( tmp . thread . nodeName . length ) > 255 ) { <nl> + printk ( KERN_ERR " hfs : catalog name length corrupted \ n "); <nl> + return - EIO ; <nl> + } <nl> + <nl> hfsplus_cat_build_key_uni ( fd -> search_key , be32_to_cpu ( tmp . thread . parentID ), <nl> & tmp . thread . nodeName ); <nl> return hfs_brec_find ( fd );
mmm drivers / staging / most / dim2 / dim2 . c <nl> ppp drivers / staging / most / dim2 / dim2 . c <nl> static int dim2_probe ( struct platform_device * pdev ) <nl> if ( ret ) <nl> return ret ; <nl>  <nl> - dev -> disable_platform = pdata ? pdata -> disable : 0 ; <nl> + dev -> disable_platform = pdata ? pdata -> disable : NULL ; <nl>  <nl> dev_info (& pdev -> dev , " sync : num of frames per sub - buffer : % u \ n ", fcnt ); <nl> hal_ret = dim_startup ( dev -> io_base , dev -> clk_speed , fcnt );
mmm drivers / net / wireless / marvell / mwifiex / scan . c <nl> ppp drivers / net / wireless / marvell / mwifiex / scan . c <nl> int mwifiex_ret_802_11_scan ( struct mwifiex_private * priv , <nl>  <nl> pmatch = adapter -> nd_info -> matches [ idx ]; <nl>  <nl> - if (! pmatch ) { <nl> + if ( pmatch ) { <nl> memset ( pmatch , 0 , sizeof (* pmatch )); <nl> if ( chan_band_tlv ) { <nl> pmatch -> n_channels = 1 ;
mmm drivers / net / hamradio / 6pack . c <nl> ppp drivers / net / hamradio / 6pack . c <nl> static void sixpack_close ( struct tty_struct * tty ) <nl> */ <nl> netif_stop_queue ( sp -> dev ); <nl>  <nl> + unregister_netdev ( sp -> dev ); <nl> + <nl> del_timer_sync (& sp -> tx_t ); <nl> del_timer_sync (& sp -> resync_t ); <nl>  <nl> - unregister_netdev ( sp -> dev ); <nl> - <nl> /* Free all 6pack frame buffers after unreg . */ <nl> kfree ( sp -> rbuff ); <nl> kfree ( sp -> xbuff );
mmm arch / arm / plat - s3c64xx / s3c6400 - clock . c <nl> ppp arch / arm / plat - s3c64xx / s3c6400 - clock . c <nl> static int s3c64xx_setrate_clksrc ( struct clk * clk , unsigned long rate ) <nl>  <nl> rate = clk_round_rate ( clk , rate ); <nl> div = clk_get_rate ( clk -> parent ) / rate ; <nl> + if ( div > 16 ) <nl> + return - EINVAL ; <nl>  <nl> val = __raw_readl ( reg ); <nl> - val &= ~ sclk -> mask ; <nl> - val |= ( rate - 1 ) << sclk -> shift ; <nl> + val &= ~( 0xf << sclk -> shift ); <nl> + val |= ( div - 1 ) << sclk -> shift ; <nl> __raw_writel ( val , reg ); <nl>  <nl> return 0 ;
mmm drivers / scsi / lpfc / lpfc_init . c <nl> ppp drivers / scsi / lpfc / lpfc_init . c <nl> lpfc_cleanup ( struct lpfc_vport * vport ) <nl> continue ; <nl> } <nl>  <nl> + /* take care of nodes in unused state before the state <nl> + * machine taking action . <nl> + */ <nl> + if ( ndlp -> nlp_state == NLP_STE_UNUSED_NODE ) { <nl> + lpfc_nlp_put ( ndlp ); <nl> + continue ; <nl> + } <nl> + <nl> if ( ndlp -> nlp_type & NLP_FABRIC ) <nl> lpfc_disc_state_machine ( vport , ndlp , NULL , <nl> NLP_EVT_DEVICE_RECOVERY ); <nl>  <nl> lpfc_disc_state_machine ( vport , ndlp , NULL , <nl> NLP_EVT_DEVICE_RM ); <nl> - <nl> } <nl>  <nl> /* At this point , ALL ndlp ' s should be gonemmm drivers / scsi / lpfc / lpfc_hbadisc . c <nl> ppp drivers / scsi / lpfc / lpfc_hbadisc . c <nl> lpfc_cleanup ( struct lpfc_vport * vport ) <nl> continue ; <nl> } <nl>  <nl> + /* take care of nodes in unused state before the state <nl> + * machine taking action . <nl> + */ <nl> + if ( ndlp -> nlp_state == NLP_STE_UNUSED_NODE ) { <nl> + lpfc_nlp_put ( ndlp ); <nl> + continue ; <nl> + } <nl> + <nl> if ( ndlp -> nlp_type & NLP_FABRIC ) <nl> lpfc_disc_state_machine ( vport , ndlp , NULL , <nl> NLP_EVT_DEVICE_RECOVERY ); <nl>  <nl> lpfc_disc_state_machine ( vport , ndlp , NULL , <nl> NLP_EVT_DEVICE_RM ); <nl> - <nl> } <nl>  <nl> /* At this point , ALL ndlp ' s should be gone <nl> lpfc_filter_by_rpi ( struct lpfc_nodelist * ndlp , void * param ) <nl> { <nl> uint16_t * rpi = param ; <nl>  <nl> + /* check for active node */ <nl> + if (! NLP_CHK_NODE_ACT ( ndlp )) <nl> + return 0 ; <nl> + <nl> return ndlp -> nlp_rpi == * rpi ; <nl> } <nl> mmm drivers / scsi / lpfc / lpfc_els . c <nl> ppp drivers / scsi / lpfc / lpfc_els . c <nl> lpfc_cleanup ( struct lpfc_vport * vport ) <nl> continue ; <nl> } <nl>  <nl> + /* take care of nodes in unused state before the state <nl> + * machine taking action . <nl> + */ <nl> + if ( ndlp -> nlp_state == NLP_STE_UNUSED_NODE ) { <nl> + lpfc_nlp_put ( ndlp ); <nl> + continue ; <nl> + } <nl> + <nl> if ( ndlp -> nlp_type & NLP_FABRIC ) <nl> lpfc_disc_state_machine ( vport , ndlp , NULL , <nl> NLP_EVT_DEVICE_RECOVERY ); <nl>  <nl> lpfc_disc_state_machine ( vport , ndlp , NULL , <nl> NLP_EVT_DEVICE_RM ); <nl> - <nl> } <nl>  <nl> /* At this point , ALL ndlp ' s should be gone <nl> lpfc_filter_by_rpi ( struct lpfc_nodelist * ndlp , void * param ) <nl> { <nl> uint16_t * rpi = param ; <nl>  <nl> + /* check for active node */ <nl> + if (! NLP_CHK_NODE_ACT ( ndlp )) <nl> + return 0 ; <nl> + <nl> return ndlp -> nlp_rpi == * rpi ; <nl> } <nl>  <nl> lpfc_plogi_confirm_nport ( struct lpfc_hba * phba , uint32_t * prsp , <nl> memcpy (& ndlp -> active_rrqs . xri_bitmap , <nl> & rrq . xri_bitmap , <nl> sizeof ( ndlp -> active_rrqs . xri_bitmap )); <nl> - lpfc_nlp_set_state ( vport , ndlp , NLP_STE_NPR_NODE ); <nl> /* Since we are swapping the ndlp passed in with the new one <nl> * and the did has already been swapped , copy over the <nl> * state and names . <nl> lpfc_plogi_confirm_nport ( struct lpfc_hba * phba , uint32_t * prsp , <nl> memcpy (& new_ndlp -> nlp_nodename , & ndlp -> nlp_nodename , <nl> sizeof ( struct lpfc_name )); <nl> new_ndlp -> nlp_state = ndlp -> nlp_state ; <nl> + lpfc_nlp_set_state ( vport , ndlp , NLP_STE_NPR_NODE ); <nl> /* Fix up the rport accordingly */ <nl> rport = ndlp -> rport ; <nl> if ( rport ) {
mmm net / ipv4 / route . c <nl> ppp net / ipv4 / route . c <nl> static void __ip_rt_update_pmtu ( struct rtable * rt , struct flowi4 * fl4 , u32 mtu ) <nl> if ( mtu < ip_rt_min_pmtu ) <nl> mtu = ip_rt_min_pmtu ; <nl>  <nl> + if ( rt -> rt_pmtu == mtu && <nl> + time_before ( jiffies , dst -> expires - ip_rt_mtu_expires / 2 )) <nl> + return ; <nl> + <nl> rcu_read_lock (); <nl> if ( fib_lookup ( dev_net ( dst -> dev ), fl4 , & res ) == 0 ) { <nl> struct fib_nh * nh = & FIB_RES_NH ( res );
mmm drivers / gpu / drm / sun4i / sun4i_backend . h <nl> ppp drivers / gpu / drm / sun4i / sun4i_backend . h <nl> # define SUN4I_BACKEND_LAYFB_L32ADD_REG ( l ) ( 0x850 + ( 0x4 * ( l ))) <nl>  <nl> # define SUN4I_BACKEND_LAYFB_H4ADD_REG 0x860 <nl> -# define SUN4I_BACKEND_LAYFB_H4ADD_MSK ( l ) GENMASK ( 3 + (( l ) * 8 ), 0 ) <nl> -# define SUN4I_BACKEND_LAYFB_H4ADD ( l , val ) (( val ) << (( l ) * 8 )) <nl> +# define SUN4I_BACKEND_LAYFB_H4ADD_MSK ( l ) GENMASK ( 3 + (( l ) * 8 ), ( l ) * 8 ) <nl> +# define SUN4I_BACKEND_LAYFB_H4ADD ( l , val ) (( val ) << (( l ) * 8 )) <nl>  <nl> # define SUN4I_BACKEND_REGBUFFCTL_REG 0x870 <nl> # define SUN4I_BACKEND_REGBUFFCTL_AUTOLOAD_DIS BIT ( 1 )
mmm fs / btrfs / inode . c <nl> ppp fs / btrfs / inode . c <nl> static noinline int compress_file_range ( struct inode * inode , <nl> nr_pages = ( end >> PAGE_CACHE_SHIFT ) - ( start >> PAGE_CACHE_SHIFT ) + 1 ; <nl> nr_pages = min ( nr_pages , ( 128 * 1024UL ) / PAGE_CACHE_SIZE ); <nl>  <nl> + /* <nl> + * we don ' t want to send crud past the end of i_size through <nl> + * compression , that ' s just a waste of CPU time . So , if the <nl> + * end of the file is before the start of our current <nl> + * requested range of bytes , we bail out to the uncompressed <nl> + * cleanup code that can deal with all of this . <nl> + * <nl> + * It isn ' t really the fastest way to fix things , but this is a <nl> + * very uncommon corner . <nl> + */ <nl> + if ( actual_end <= start ) <nl> + goto cleanup_and_bail_uncompressed ; <nl> + <nl> total_compressed = actual_end - start ; <nl>  <nl> /* we want to make sure that amount of ram required to uncompress <nl> static noinline int compress_file_range ( struct inode * inode , <nl> goto again ; <nl> } <nl> } else { <nl> + cleanup_and_bail_uncompressed : <nl> /* <nl> * No compression , but we still need to write the pages in <nl> * the file we ' ve been given so far . redirty the locked
mmm drivers / hid / usbhid / hid - core . c <nl> ppp drivers / hid / usbhid / hid - core . c <nl> static int usbhid_parse ( struct hid_device * hid ) <nl> unsigned int rsize = 0 ; <nl> char * rdesc ; <nl> int ret , n ; <nl> + int num_descriptors ; <nl> + size_t offset = offsetof ( struct hid_descriptor , desc ); <nl>  <nl> quirks = usbhid_lookup_quirk ( le16_to_cpu ( dev -> descriptor . idVendor ), <nl> le16_to_cpu ( dev -> descriptor . idProduct )); <nl> static int usbhid_parse ( struct hid_device * hid ) <nl> return - ENODEV ; <nl> } <nl>  <nl> + if ( hdesc -> bLength < sizeof ( struct hid_descriptor )) { <nl> + dbg_hid (" hid descriptor is too short \ n "); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> hid -> version = le16_to_cpu ( hdesc -> bcdHID ); <nl> hid -> country = hdesc -> bCountryCode ; <nl>  <nl> - for ( n = 0 ; n < hdesc -> bNumDescriptors ; n ++) <nl> + num_descriptors = min_t ( int , hdesc -> bNumDescriptors , <nl> + ( hdesc -> bLength - offset ) / sizeof ( struct hid_class_descriptor )); <nl> + <nl> + for ( n = 0 ; n < num_descriptors ; n ++) <nl> if ( hdesc -> desc [ n ]. bDescriptorType == HID_DT_REPORT ) <nl> rsize = le16_to_cpu ( hdesc -> desc [ n ]. wDescriptorLength ); <nl> 
mmm drivers / net / wireless / iwlwifi / iwl - trans - pcie . c <nl> ppp drivers / net / wireless / iwlwifi / iwl - trans - pcie . c <nl> static int iwl_trans_pcie_start_hw ( struct iwl_trans * trans ) <nl> err = iwl_prepare_card_hw ( trans ); <nl> if ( err ) { <nl> IWL_ERR ( trans , " Error while preparing HW : % d ", err ); <nl> - goto error ; <nl> + goto err_free_irq ; <nl> } <nl>  <nl> iwl_apm_init ( trans ); <nl> static int iwl_trans_pcie_start_hw ( struct iwl_trans * trans ) <nl>  <nl> return err ; <nl>  <nl> + err_free_irq : <nl> + free_irq ( trans -> irq , trans ); <nl> error : <nl> iwl_free_isr_ict ( trans ); <nl> tasklet_kill (& trans_pcie -> irq_tasklet );
mmm security / keys / gc . c <nl> ppp security / keys / gc . c <nl> static noinline void key_gc_unused_keys ( struct list_head * keys ) <nl> kdebug ("- % u ", key -> serial ); <nl> key_check ( key ); <nl>  <nl> - /* Throw away the key data */ <nl> - if ( key -> type -> destroy ) <nl> + /* Throw away the key data if the key is instantiated */ <nl> + if ( test_bit ( KEY_FLAG_INSTANTIATED , & key -> flags ) && <nl> + ! test_bit ( KEY_FLAG_NEGATIVE , & key -> flags ) && <nl> + key -> type -> destroy ) <nl> key -> type -> destroy ( key ); <nl>  <nl> security_key_free ( key );
mmm arch / arm / mach - omap2 / clock34xx . c <nl> ppp arch / arm / mach - omap2 / clock34xx . c <nl> static int omap3_noncore_dpll_program ( struct clk * clk , u16 m , u8 n , u16 freqsel ) <nl> /* 3430 ES2 TRM : 4 . 7 . 6 . 9 DPLL Programming Sequence */ <nl> _omap3_noncore_dpll_bypass ( clk ); <nl>  <nl> + /* Set jitter correction */ <nl> + v = __raw_readl ( dd -> control_reg ); <nl> + v &= ~ dd -> freqsel_mask ; <nl> + v |= freqsel << __ffs ( dd -> freqsel_mask ); <nl> + __raw_writel ( v , dd -> control_reg ); <nl> + <nl> + /* Set DPLL multiplier , divider */ <nl> v = __raw_readl ( dd -> mult_div1_reg ); <nl> v &= ~( dd -> mult_mask | dd -> div1_mask ); <nl> - <nl> - /* Set mult ( M ), div1 ( N ), freqsel */ <nl> v |= m << __ffs ( dd -> mult_mask ); <nl> - v |= n << __ffs ( dd -> div1_mask ); <nl> - v |= freqsel << __ffs ( dd -> freqsel_mask ); <nl> - <nl> + v |= ( n - 1 ) << __ffs ( dd -> div1_mask ); <nl> __raw_writel ( v , dd -> mult_div1_reg ); <nl>  <nl> /* We let the clock framework set the other output dividers later */
mmm arch / arm / mach - integrator / integrator_ap . c <nl> ppp arch / arm / mach - integrator / integrator_ap . c <nl> # include < linux / clockchips . h > <nl> # include < linux / interrupt . h > <nl> # include < linux / io . h > <nl> +# include < linux / mtd / physmap . h > <nl>  <nl> # include < mach / hardware . h > <nl> # include < mach / platform . h > <nl> # include < mach / lm . h > <nl>  <nl> # include < asm / mach / arch . h > <nl> -# include < asm / mach / flash . h > <nl> # include < asm / mach / irq . h > <nl> # include < asm / mach / map . h > <nl> # include < asm / mach / time . h > <nl> device_initcall ( irq_init_sysfs ); <nl> # define EBI_CSR1 ( VA_EBI_BASE + INTEGRATOR_EBI_CSR1_OFFSET ) <nl> # define EBI_LOCK ( VA_EBI_BASE + INTEGRATOR_EBI_LOCK_OFFSET ) <nl>  <nl> - static int ap_flash_init ( void ) <nl> + static int ap_flash_init ( struct platform_device * dev ) <nl> { <nl> u32 tmp ; <nl>  <nl> static int ap_flash_init ( void ) <nl> return 0 ; <nl> } <nl>  <nl> - static void ap_flash_exit ( void ) <nl> + static void ap_flash_exit ( struct platform_device * dev ) <nl> { <nl> u32 tmp ; <nl>  <nl> static void ap_flash_exit ( void ) <nl> } <nl> } <nl>  <nl> - static void ap_flash_set_vpp ( int on ) <nl> + static void ap_flash_set_vpp ( struct map_info * map , int on ) <nl> { <nl> void __iomem * reg = on ? SC_CTRLS : SC_CTRLC ; <nl>  <nl> writel ( INTEGRATOR_SC_CTRL_nFLVPPEN , reg ); <nl> } <nl>  <nl> - static struct flash_platform_data ap_flash_data = { <nl> - . map_name = " cfi_probe ", <nl> + static struct physmap_flash_data ap_flash_data = { <nl> . width = 4 , <nl> . init = ap_flash_init , <nl> . exit = ap_flash_exit , <nl> static struct resource cfi_flash_resource = { <nl> }; <nl>  <nl> static struct platform_device cfi_flash_device = { <nl> - . name = " armflash ", <nl> + . name = " physmap - flash ", <nl> . id = 0 , <nl> . dev = { <nl> . platform_data = & ap_flash_data ,
mmm drivers / hwtracing / coresight / coresight - etm - perf . c <nl> ppp drivers / hwtracing / coresight / coresight - etm - perf . c <nl> static void * etm_setup_aux ( int event_cpu , void ** pages , <nl> if (! sink_ops ( sink )-> alloc_buffer ) <nl> goto err ; <nl>  <nl> + cpu = cpumask_first ( mask ); <nl> /* Get the AUX specific data from the sink buffer */ <nl> event_data -> snk_config = <nl> sink_ops ( sink )-> alloc_buffer ( sink , cpu , pages ,
mmm arch / x86 / xen / mmu . c <nl> ppp arch / x86 / xen / mmu . c <nl> __init pgd_t * xen_setup_kernel_pagetable ( pgd_t * pgd , <nl> return pgd ; <nl> } <nl> # else /* ! CONFIG_X86_64 */ <nl> - static pmd_t level2_kernel_pgt [ PTRS_PER_PMD ] __page_aligned_bss ; <nl> + static RESERVE_BRK_ARRAY ( pmd_t , level2_kernel_pgt , PTRS_PER_PMD ); <nl>  <nl> __init pgd_t * xen_setup_kernel_pagetable ( pgd_t * pgd , <nl> unsigned long max_pfn ) <nl> { <nl> pmd_t * kernel_pmd ; <nl>  <nl> + level2_kernel_pgt = extend_brk ( sizeof ( pmd_t *) * PTRS_PER_PMD , PAGE_SIZE ); <nl> + <nl> max_pfn_mapped = PFN_DOWN ( __pa ( xen_start_info -> pt_base ) + <nl> xen_start_info -> nr_pt_frames * PAGE_SIZE + <nl> 512 * 1024 );
mmm drivers / media / video / em28xx / em28xx - reg . h <nl> ppp drivers / media / video / em28xx / em28xx - reg . h <nl>  <nl> /* FIXME : Need to be populated with the other chip ID ' s */ <nl> enum em28xx_chip_id { <nl> + CHIP_ID_EM2820 = 18 , <nl> + CHIP_ID_EM2840 = 20 , <nl> CHIP_ID_EM2860 = 34 , <nl> CHIP_ID_EM2883 = 36 , <nl> CHIP_ID_EM2874 = 65 ,mmm drivers / media / video / em28xx / em28xx - cards . c <nl> ppp drivers / media / video / em28xx / em28xx - cards . c <nl>  <nl> /* FIXME : Need to be populated with the other chip ID ' s */ <nl> enum em28xx_chip_id { <nl> + CHIP_ID_EM2820 = 18 , <nl> + CHIP_ID_EM2840 = 20 , <nl> CHIP_ID_EM2860 = 34 , <nl> CHIP_ID_EM2883 = 36 , <nl> CHIP_ID_EM2874 = 65 , <nl> void em28xx_pre_card_setup ( struct em28xx * dev ) <nl> if ( rc > 0 ) { <nl> dev -> chip_id = rc ; <nl> switch ( rc ) { <nl> + case CHIP_ID_EM2820 : <nl> + em28xx_info (" chip ID is em2820 \ n "); <nl> + break ; <nl> + case CHIP_ID_EM2840 : <nl> + em28xx_info (" chip ID is em2840 \ n "); <nl> + break ; <nl> case CHIP_ID_EM2860 : <nl> em28xx_info (" chip ID is em2860 \ n "); <nl> break ;
mmm net / bridge / br_multicast . c <nl> ppp net / bridge / br_multicast . c <nl> static void __br_multicast_send_query ( struct net_bridge * br , <nl> return ; <nl>  <nl> if ( port ) { <nl> - __skb_push ( skb , sizeof ( struct ethhdr )); <nl> skb -> dev = port -> dev ; <nl> NF_HOOK ( NFPROTO_BRIDGE , NF_BR_LOCAL_OUT , skb , NULL , skb -> dev , <nl> - dev_queue_xmit ); <nl> + br_dev_queue_push_xmit ); <nl> } else { <nl> br_multicast_select_own_querier ( br , ip , skb ); <nl> netif_rx ( skb );
mmm drivers / char / ftape / zftape / zftape - buffers . c <nl> ppp drivers / char / ftape / zftape / zftape - buffers . c <nl> int zft_vmalloc_once ( void * new , size_t size ) <nl> peak_memory = used_memory ; <nl> } <nl> TRACE_ABORT ( 0 , ft_t_noise , <nl> - " allocated buffer @ % p , % d bytes ", *( void **) new , size ); <nl> + " allocated buffer @ % p , % zd bytes ", *( void **) new , size ); <nl> } <nl> int zft_vmalloc_always ( void * new , size_t size ) <nl> { <nl> void zft_vfree ( void * old , size_t size ) <nl> if (*( void **) old ) { <nl> vfree (*( void **) old ); <nl> used_memory -= size ; <nl> - TRACE ( ft_t_noise , " released buffer @ % p , % d bytes ", <nl> + TRACE ( ft_t_noise , " released buffer @ % p , % zd bytes ", <nl> *( void **) old , size ); <nl> *( void **) old = NULL ; <nl> }
mmm drivers / block / paride / pcd . c <nl> ppp drivers / block / paride / pcd . c <nl> static void pcd_init_units ( void ) <nl> disk -> queue = blk_mq_init_sq_queue (& cd -> tag_set , & pcd_mq_ops , <nl> 1 , BLK_MQ_F_SHOULD_MERGE ); <nl> if ( IS_ERR ( disk -> queue )) { <nl> + put_disk ( disk ); <nl> disk -> queue = NULL ; <nl> continue ; <nl> } <nl> static int pcd_detect ( void ) <nl>  <nl> printk ("% s : No CD - ROM drive found \ n ", name ); <nl> for ( unit = 0 , cd = pcd ; unit < PCD_UNITS ; unit ++, cd ++) { <nl> + if (! cd -> disk ) <nl> + continue ; <nl> blk_cleanup_queue ( cd -> disk -> queue ); <nl> cd -> disk -> queue = NULL ; <nl> blk_mq_free_tag_set (& cd -> tag_set ); <nl> static int __init pcd_init ( void ) <nl> pcd_probe_capabilities (); <nl>  <nl> if ( register_blkdev ( major , name )) { <nl> - for ( unit = 0 , cd = pcd ; unit < PCD_UNITS ; unit ++, cd ++) <nl> + for ( unit = 0 , cd = pcd ; unit < PCD_UNITS ; unit ++, cd ++) { <nl> + if (! cd -> disk ) <nl> + continue ; <nl> + <nl> + blk_cleanup_queue ( cd -> disk -> queue ); <nl> + blk_mq_free_tag_set (& cd -> tag_set ); <nl> put_disk ( cd -> disk ); <nl> + } <nl> return - EBUSY ; <nl> } <nl>  <nl> static void __exit pcd_exit ( void ) <nl> int unit ; <nl>  <nl> for ( unit = 0 , cd = pcd ; unit < PCD_UNITS ; unit ++, cd ++) { <nl> + if (! cd -> disk ) <nl> + continue ; <nl> + <nl> if ( cd -> present ) { <nl> del_gendisk ( cd -> disk ); <nl> pi_release ( cd -> pi );
mmm tools / perf / builtin - sched . c <nl> ppp tools / perf / builtin - sched . c <nl> static int perf_sched__read_events ( struct perf_sched * sched ) <nl> struct perf_data_file file = { <nl> . path = input_name , <nl> . mode = PERF_DATA_MODE_READ , <nl> + . force = sched -> force , <nl> }; <nl> int rc = - 1 ; <nl> 
mmm drivers / acpi / tables . c <nl> ppp drivers / acpi / tables . c <nl> int __init acpi_table_init ( void ) <nl>  <nl> static int __init acpi_parse_apic_instance ( char * str ) <nl> { <nl> + if (! str ) <nl> + return - EINVAL ; <nl>  <nl> acpi_apic_instance = simple_strtoul ( str , NULL , 0 ); <nl> 
mmm drivers / staging / media / go7007 / wis - saa7115 . c <nl> ppp drivers / staging / media / go7007 / wis - saa7115 . c <nl> static int wis_saa7115_probe ( struct i2c_client * client , <nl> dec -> hue = 0 ; <nl> i2c_set_clientdata ( client , dec ); <nl>  <nl> - printk ( KERN_DEBUG <nl> + dev_dbg (& client -> dev , <nl> " wis - saa7115 : initializing SAA7115 at address % d on % s \ n ", <nl> client -> addr , adapter -> name ); <nl>  <nl> if ( write_regs ( client , initial_registers ) < 0 ) { <nl> - printk ( KERN_ERR <nl> + dev_err (& client -> dev , <nl> " wis - saa7115 : error initializing SAA7115 \ n "); <nl> kfree ( dec ); <nl> return - ENODEV ;
mmm drivers / usb / serial / option . c <nl> ppp drivers / usb / serial / option . c <nl> static void option_instat_callback ( struct urb * urb ) <nl> dev_dbg ( dev , "% s : type % x req % x \ n ", __func__ , <nl> req_pkt -> bRequestType , req_pkt -> bRequest ); <nl> } <nl> + } else if ( status == - ENOENT || status == - ESHUTDOWN ) { <nl> + dev_dbg ( dev , "% s : urb stopped : % d \ n ", __func__ , status ); <nl> } else <nl> dev_err ( dev , "% s : error % d \ n ", __func__ , status ); <nl> 
mmm kernel / tsacct . c <nl> ppp kernel / tsacct . c <nl> void bacct_add_tsk ( struct taskstats * stats , struct task_struct * tsk ) <nl> */ <nl> void xacct_add_tsk ( struct taskstats * stats , struct task_struct * p ) <nl> { <nl> + struct mm_struct * mm ; <nl> + <nl> /* convert pages - jiffies to Mbyte - usec */ <nl> stats -> coremem = jiffies_to_usecs ( p -> acct_rss_mem1 ) * PAGE_SIZE / MB ; <nl> stats -> virtmem = jiffies_to_usecs ( p -> acct_vm_mem1 ) * PAGE_SIZE / MB ; <nl> - if ( p -> mm ) { <nl> + mm = get_task_mm ( p ); <nl> + if ( mm ) { <nl> /* adjust to KB unit */ <nl> - stats -> hiwater_rss = p -> mm -> hiwater_rss * PAGE_SIZE / KB ; <nl> - stats -> hiwater_vm = p -> mm -> hiwater_vm * PAGE_SIZE / KB ; <nl> + stats -> hiwater_rss = mm -> hiwater_rss * PAGE_SIZE / KB ; <nl> + stats -> hiwater_vm = mm -> hiwater_vm * PAGE_SIZE / KB ; <nl> + mmput ( mm ); <nl> } <nl> stats -> read_char = p -> rchar ; <nl> stats -> write_char = p -> wchar ;
mmm fs / eventpoll . c <nl> ppp fs / eventpoll . c <nl> static void ep_free ( struct eventpoll * ep ) <nl> } <nl>  <nl> mutex_unlock (& epmutex ); <nl> - <nl> mutex_destroy (& ep -> mtx ); <nl> + kfree ( ep ); <nl> } <nl>  <nl> static int ep_eventpoll_release ( struct inode * inode , struct file * file ) <nl> { <nl> struct eventpoll * ep = file -> private_data ; <nl>  <nl> - if ( ep ) { <nl> + if ( ep ) <nl> ep_free ( ep ); <nl> - kfree ( ep ); <nl> - } <nl>  <nl> DNPRINTK ( 3 , ( KERN_INFO "[% p ] eventpoll : close () ep =% p \ n ", current , ep )); <nl> return 0 ; <nl> asmlinkage long sys_epoll_create ( int size ) <nl>  <nl> error_free : <nl> ep_free ( ep ); <nl> - kfree ( ep ); <nl> error_return : <nl> DNPRINTK ( 3 , ( KERN_INFO "[% p ] eventpoll : sys_epoll_create (% d ) = % d \ n ", <nl> current , size , error ));
mmm drivers / staging / android / ion / ion_carveout_heap . c <nl> ppp drivers / staging / android / ion / ion_carveout_heap . c <nl> static int ion_carveout_heap_allocate ( struct ion_heap * heap , <nl> unsigned long size , unsigned long align , <nl> unsigned long flags ) <nl> { <nl> + if ( align > PAGE_SIZE ) <nl> + return - EINVAL ; <nl> + <nl> buffer -> priv_phys = ion_carveout_allocate ( heap , size , align ); <nl> return buffer -> priv_phys == ION_CARVEOUT_ALLOCATE_FAIL ? - ENOMEM : 0 ; <nl> }
mmm drivers / usb / gadget / f_fs . c <nl> ppp drivers / usb / gadget / f_fs . c <nl> static ssize_t ffs_epfile_io ( struct file * file , <nl> char __user * buf , size_t len , int read ) <nl> { <nl> struct ffs_epfile * epfile = file -> private_data ; <nl> - struct usb_gadget * gadget = epfile -> ffs -> gadget ; <nl> struct ffs_ep * ep ; <nl> char * data = NULL ; <nl> ssize_t ret , data_len ; <nl> static ssize_t ffs_epfile_io ( struct file * file , <nl>  <nl> /* Allocate & copy */ <nl> if (! halt ) { <nl> + /* <nl> + * if we _do_ wait above , the epfile -> ffs -> gadget might be NULL <nl> + * before the waiting completes , so do not assign to ' gadget ' earlier <nl> + */ <nl> + struct usb_gadget * gadget = epfile -> ffs -> gadget ; <nl> + <nl> /* <nl> * Controller may require buffer size to be aligned to <nl> * maxpacketsize of an out endpoint .
mmm fs / ecryptfs / file . c <nl> ppp fs / ecryptfs / file . c <nl> static int read_or_initialize_metadata ( struct dentry * dentry ) <nl> return rc ; <nl> } <nl>  <nl> + static int ecryptfs_mmap ( struct file * file , struct vm_area_struct * vma ) <nl> +{ <nl> + struct file * lower_file = ecryptfs_file_to_lower ( file ); <nl> + /* <nl> + * Don ' t allow mmap on top of file systems that don ' t support it <nl> + * natively . If FILESYSTEM_MAX_STACK_DEPTH > 2 or ecryptfs <nl> + * allows recursive mounting , this will need to be extended . <nl> + */ <nl> + if (! lower_file -> f_op -> mmap ) <nl> + return - ENODEV ; <nl> + return generic_file_mmap ( file , vma ); <nl> +} <nl> + <nl> /** <nl> * ecryptfs_open <nl> * @ inode : inode specifying file to open <nl> const struct file_operations ecryptfs_main_fops = { <nl> # ifdef CONFIG_COMPAT <nl> . compat_ioctl = ecryptfs_compat_ioctl , <nl> # endif <nl> - . mmap = generic_file_mmap , <nl> + . mmap = ecryptfs_mmap , <nl> . open = ecryptfs_open , <nl> . flush = ecryptfs_flush , <nl> . release = ecryptfs_release ,
mmm kernel / fork . c <nl> ppp kernel / fork . c <nl> noinline struct pt_regs * __cpuinit __attribute__ (( weak )) idle_regs ( struct pt_re <nl> return regs ; <nl> } <nl>  <nl> + static inline void init_idle_pids ( struct pid_link * links ) <nl> +{ <nl> + enum pid_type type ; <nl> + <nl> + for ( type = PIDTYPE_PID ; type < PIDTYPE_MAX ; ++ type ) { <nl> + INIT_HLIST_NODE (& links [ type ]. node ); /* not really needed */ <nl> + links [ type ]. pid = & init_struct_pid ; <nl> + } <nl> +} <nl> + <nl> struct task_struct * __cpuinit fork_idle ( int cpu ) <nl> { <nl> struct task_struct * task ; <nl> struct task_struct * __cpuinit fork_idle ( int cpu ) <nl>  <nl> task = copy_process ( CLONE_VM , 0 , idle_regs (& regs ), 0 , NULL , <nl> & init_struct_pid , 0 ); <nl> - if (! IS_ERR ( task )) <nl> + if (! IS_ERR ( task )) { <nl> + init_idle_pids ( task -> pids ); <nl> init_idle ( task , cpu ); <nl> + } <nl>  <nl> return task ; <nl> }
mmm net / bridge / br_mdb . c <nl> ppp net / bridge / br_mdb . c <nl> static int __br_mdb_add ( struct net * net , struct net_bridge * br , <nl> if (! p || p -> br != br || p -> state == BR_STATE_DISABLED ) <nl> return - EINVAL ; <nl>  <nl> + memset (& ip , 0 , sizeof ( ip )); <nl> ip . proto = entry -> addr . proto ; <nl> if ( ip . proto == htons ( ETH_P_IP )) <nl> ip . u . ip4 = entry -> addr . u . ip4 ; <nl> static int __br_mdb_del ( struct net_bridge * br , struct br_mdb_entry * entry ) <nl> if (! netif_running ( br -> dev ) || br -> multicast_disabled ) <nl> return - EINVAL ; <nl>  <nl> + memset (& ip , 0 , sizeof ( ip )); <nl> ip . proto = entry -> addr . proto ; <nl> if ( ip . proto == htons ( ETH_P_IP )) { <nl> if ( timer_pending (& br -> ip4_other_query . timer ))
mmm kernel / trace / trace_output . c <nl> ppp kernel / trace / trace_output . c <nl> static enum print_line_t trace_stack_print ( struct trace_iterator * iter , <nl>  <nl> trace_assign_type ( field , iter -> ent ); <nl>  <nl> + if (! trace_seq_puts ( s , "\ n ")) <nl> + goto partial ; <nl> for ( i = 0 ; i < FTRACE_STACK_ENTRIES ; i ++) { <nl> - if (! field -> caller [ i ]) <nl> + if (! field -> caller [ i ] || ( field -> caller [ i ] == ULONG_MAX )) <nl> break ; <nl> - if ( i ) { <nl> - if (! trace_seq_puts ( s , " <= ")) <nl> - goto partial ; <nl> + if (! trace_seq_puts ( s , " => ")) <nl> + goto partial ; <nl>  <nl> - if (! seq_print_ip_sym ( s , field -> caller [ i ], flags )) <nl> - goto partial ; <nl> - } <nl> + if (! seq_print_ip_sym ( s , field -> caller [ i ], flags )) <nl> + goto partial ; <nl> if (! trace_seq_puts ( s , "\ n ")) <nl> goto partial ; <nl> }
mmm fs / btrfs / send . c <nl> ppp fs / btrfs / send . c <nl> static int iterate_dir_item ( struct btrfs_root * root , struct btrfs_path * path , <nl> buf = tmp ; <nl> } <nl> if (! buf ) { <nl> - buf = vmalloc ( buf_len ); <nl> + buf = kvmalloc ( buf_len , GFP_KERNEL ); <nl> if (! buf ) { <nl> ret = - ENOMEM ; <nl> goto out ;
mmm arch / x86 / vdso / vdso2c . h <nl> ppp arch / x86 / vdso / vdso2c . h <nl> static void BITSFUNC ( go )( void * raw_addr , size_t raw_len , <nl>  <nl> /* Validate mapping addresses . */ <nl> for ( i = 0 ; i < sizeof ( special_pages ) / sizeof ( special_pages [ 0 ]); i ++) { <nl> - if (! syms [ i ]) <nl> + INT_BITS symval = syms [ special_pages [ i ]]; <nl> + <nl> + if (! symval ) <nl> continue ; /* The mapping isn ' t used ; ignore it . */ <nl>  <nl> - if ( syms [ i ] % 4096 ) <nl> + if ( symval % 4096 ) <nl> fail ("% s must be a multiple of 4096 \ n ", <nl> required_syms [ i ]. name ); <nl> - if ( syms [ sym_vvar_start ] > syms [ i ] + 4096 ) <nl> - fail ("% s underruns begin_vvar \ n ", <nl> + if ( symval + 4096 < syms [ sym_vvar_start ]) <nl> + fail ("% s underruns vvar_start \ n ", <nl> required_syms [ i ]. name ); <nl> - if ( syms [ i ] + 4096 > 0 ) <nl> + if ( symval + 4096 > 0 ) <nl> fail ("% s is on the wrong side of the vdso text \ n ", <nl> required_syms [ i ]. name ); <nl> }
mmm kernel / events / core . c <nl> ppp kernel / events / core . c <nl> static int perf_copy_attr ( struct perf_event_attr __user * uattr , <nl> if ( ret ) <nl> return - EFAULT ; <nl>  <nl> + attr -> size = size ; <nl> + <nl> if ( attr -> __reserved_1 ) <nl> return - EINVAL ; <nl> 
mmm drivers / spi / spi - imx . c <nl> ppp drivers / spi / spi - imx . c <nl> static int spi_imx_probe ( struct platform_device * pdev ) <nl> goto out_clk_put ; <nl> } <nl>  <nl> + if (! master -> cs_gpios ) { <nl> + dev_err (& pdev -> dev , " No CS GPIOs available \ n "); <nl> + goto out_clk_put ; <nl> + } <nl> + <nl> for ( i = 0 ; i < master -> num_chipselect ; i ++) { <nl> if (! gpio_is_valid ( master -> cs_gpios [ i ])) <nl> continue ;
mmm drivers / target / target_core_configfs . c <nl> ppp drivers / target / target_core_configfs . c <nl> static ssize_t target_core_alua_tg_pt_gp_store_attr_alua_access_state ( <nl> " tg_pt_gp ID : % hu \ n ", tg_pt_gp -> tg_pt_gp_valid_id ); <nl> return - EINVAL ; <nl> } <nl> + if (!( dev -> dev_flags & DF_CONFIGURED )) { <nl> + pr_err (" Unable to set alua_access_state while device is " <nl> + " not configured \ n "); <nl> + return - ENODEV ; <nl> + } <nl>  <nl> ret = kstrtoul ( page , 0 , & tmp ); <nl> if ( ret < 0 ) {
mmm fs / namei . c <nl> ppp fs / namei . c <nl> static struct file * path_openat ( int dfd , struct filename * pathname , <nl>  <nl> if ( unlikely ( file -> f_flags & __O_TMPFILE )) { <nl> error = do_tmpfile ( dfd , pathname , nd , flags , op , file , & opened ); <nl> - goto out ; <nl> + goto out2 ; <nl> } <nl>  <nl> error = path_init ( dfd , pathname , flags , nd ); <nl> static struct file * path_openat ( int dfd , struct filename * pathname , <nl> } <nl> out : <nl> path_cleanup ( nd ); <nl> + out2 : <nl> if (!( opened & FILE_OPENED )) { <nl> BUG_ON (! error ); <nl> put_filp ( file );
mmm drivers / crypto / chelsio / chtls / chtls_main . c <nl> ppp drivers / crypto / chelsio / chtls / chtls_main . c <nl> static int do_chtls_setsockopt ( struct sock * sk , int optname , <nl>  <nl> switch ( tmp_crypto_info . cipher_type ) { <nl> case TLS_CIPHER_AES_GCM_128 : { <nl> - rc = copy_from_user ( crypto_info , optval , <nl> - sizeof ( struct <nl> - tls12_crypto_info_aes_gcm_128 )); <nl> + /* Obtain version and type from previous copy */ <nl> + crypto_info [ 0 ] = tmp_crypto_info ; <nl> + /* Now copy the following data */ <nl> + rc = copy_from_user (( char *) crypto_info + sizeof (* crypto_info ), <nl> + optval + sizeof (* crypto_info ), <nl> + sizeof ( struct tls12_crypto_info_aes_gcm_128 ) <nl> + - sizeof (* crypto_info )); <nl>  <nl> if ( rc ) { <nl> rc = - EFAULT ;
mmm drivers / net / pcnet32 . c <nl> ppp drivers / net / pcnet32 . c <nl> static int pcnet32_phys_id ( struct net_device * dev , u32 data ) <nl> if ((! data ) || ( data > ( u32 )( MAX_SCHEDULE_TIMEOUT / HZ ))) <nl> data = ( u32 )( MAX_SCHEDULE_TIMEOUT / HZ ); <nl>  <nl> - schedule_timeout ( data * HZ ); <nl> + msleep_interruptible ( data * 1000 ); <nl> del_timer_sync (& lp -> blink_timer ); <nl>  <nl> /* Restore the original value of the bcrs */
mmm arch / x86 / kvm / x86 . c <nl> ppp arch / x86 / kvm / x86 . c <nl> int kvm_arch_vcpu_init ( struct kvm_vcpu * vcpu ) <nl> } <nl> vcpu -> arch . mcg_cap = KVM_MAX_MCE_BANKS ; <nl>  <nl> - if (! zalloc_cpumask_var (& vcpu -> arch . wbinvd_dirty_mask , GFP_KERNEL )) <nl> + if (! zalloc_cpumask_var (& vcpu -> arch . wbinvd_dirty_mask , GFP_KERNEL )) { <nl> + r = - ENOMEM ; <nl> goto fail_free_mce_banks ; <nl> + } <nl>  <nl> r = fx_init ( vcpu ); <nl> if ( r )
mmm fs / nfs / flexfilelayout / flexfilelayoutdev . c <nl> ppp fs / nfs / flexfilelayout / flexfilelayoutdev . c <nl> static bool ff_layout_mirror_valid ( struct pnfs_layout_segment * lseg , <nl> } else <nl> goto outerr ; <nl> } <nl> + <nl> + if ( IS_ERR ( mirror -> mirror_ds )) <nl> + goto outerr ; <nl> + <nl> if ( mirror -> mirror_ds -> ds == NULL ) { <nl> struct nfs4_deviceid_node * devid ; <nl> devid = & mirror -> mirror_ds -> id_node ;
mmm drivers / hwmon / scmi - hwmon . c <nl> ppp drivers / hwmon / scmi - hwmon . c <nl> static int scmi_hwmon_probe ( struct scmi_device * sdev ) <nl> scmi_chip_info . info = ptr_scmi_ci ; <nl> chip_info = & scmi_chip_info ; <nl>  <nl> - for ( type = 0 ; type < hwmon_max && nr_count [ type ]; type ++) { <nl> + for ( type = 0 ; type < hwmon_max ; type ++) { <nl> + if (! nr_count [ type ]) <nl> + continue ; <nl> + <nl> scmi_hwmon_add_chan_info ( scmi_hwmon_chan , dev , nr_count [ type ], <nl> type , hwmon_attributes [ type ]); <nl> * ptr_scmi_ci ++ = scmi_hwmon_chan ++;
mmm drivers / media / usb / dvb - usb / gp8psk . c <nl> ppp drivers / media / usb / dvb - usb / gp8psk . c <nl> int gp8psk_usb_in_op ( struct dvb_usb_device * d , u8 req , u16 value , u16 index , u8 <nl> struct gp8psk_state * st = d -> priv ; <nl> int ret = 0 , try = 0 ; <nl>  <nl> + if ( blen > sizeof ( st -> data )) <nl> + return - EIO ; <nl> + <nl> if (( ret = mutex_lock_interruptible (& d -> usb_mutex ))) <nl> return ret ; <nl>  <nl> int gp8psk_usb_out_op ( struct dvb_usb_device * d , u8 req , u16 value , <nl> deb_xfer (" out : req . % x , val : % x , ind : % x , buffer : ", req , value , index ); <nl> debug_dump ( b , blen , deb_xfer ); <nl>  <nl> + if ( blen > sizeof ( st -> data )) <nl> + return - EIO ; <nl> + <nl> if (( ret = mutex_lock_interruptible (& d -> usb_mutex ))) <nl> return ret ; <nl>  <nl> static int gp8psk_load_bcm4500fw ( struct dvb_usb_device * d ) <nl> err (" failed to load bcm4500 firmware ."); <nl> goto out_free ; <nl> } <nl> + if ( buflen > 64 ) { <nl> + err (" firmare chunk size bigger than 64 bytes ."); <nl> + goto out_free ; <nl> + } <nl> + <nl> memcpy ( buf , ptr , buflen ); <nl> if ( dvb_usb_generic_write ( d , buf , buflen )) { <nl> err (" failed to load bcm4500 firmware .");
mmm arch / x86 / kvm / vmx . c <nl> ppp arch / x86 / kvm / vmx . c <nl> static int check_vmentry_postreqs ( struct kvm_vcpu * vcpu , struct vmcs12 * vmcs12 , <nl> return 1 ; <nl> } <nl>  <nl> + if (( vmcs12 -> vm_entry_controls & VM_ENTRY_LOAD_BNDCFGS ) && <nl> + ( is_noncanonical_address ( vmcs12 -> guest_bndcfgs & PAGE_MASK , vcpu ) || <nl> + ( vmcs12 -> guest_bndcfgs & MSR_IA32_BNDCFGS_RSVD ))) <nl> + return 1 ; <nl> + <nl> return 0 ; <nl> } <nl> 
mmm drivers / scsi / aha1542 . c <nl> ppp drivers / scsi / aha1542 . c <nl> static void setup_mailboxes ( int base_io , struct Scsi_Host * shpnt ); <nl> static int aha1542_restart ( struct Scsi_Host * shost ); <nl> static void aha1542_intr_handle ( struct Scsi_Host * shost ); <nl>  <nl> -# define aha1542_intr_reset ( base ) outb ( IRST , CONTROL ( base )) <nl> + static inline void aha1542_intr_reset ( u16 base ) <nl> +{ <nl> + outb ( IRST , CONTROL ( base )); <nl> +} <nl>  <nl> # define WAIT ( port , mask , allof , noneof ) \ <nl> { register int WAITbits ; \
mmm fs / xfs / xfs_itable . c <nl> ppp fs / xfs / xfs_itable . c <nl> xfs_inumbers ( <nl> return error ; <nl>  <nl> bcount = MIN ( left , ( int )( PAGE_SIZE / sizeof (* buffer ))); <nl> - buffer = kmem_alloc ( bcount * sizeof (* buffer ), KM_SLEEP ); <nl> + buffer = kmem_zalloc ( bcount * sizeof (* buffer ), KM_SLEEP ); <nl> do { <nl> struct xfs_inobt_rec_incore r ; <nl> int stat ;
mmm tools / perf / util / intel - pt - decoder / intel - pt - insn - decoder . c <nl> ppp tools / perf / util / intel - pt - decoder / intel - pt - insn - decoder . c <nl> static void intel_pt_insn_decoder ( struct insn * insn , <nl> enum intel_pt_insn_branch branch = INTEL_PT_BR_NO_BRANCH ; <nl> int ext ; <nl>  <nl> + intel_pt_insn -> rel = 0 ; <nl> + <nl> if ( insn_is_avx ( insn )) { <nl> intel_pt_insn -> op = INTEL_PT_OP_OTHER ; <nl> intel_pt_insn -> branch = INTEL_PT_BR_NO_BRANCH ;
mmm mm / backing - dev . c <nl> ppp mm / backing - dev . c <nl> int bdi_register ( struct backing_dev_info * bdi , struct device * parent , <nl> int ret = 0 ; <nl> struct device * dev ; <nl>  <nl> + if ( WARN_ON ( bdi -> dev )) <nl> + goto exit ; <nl> + <nl> va_start ( args , fmt ); <nl> dev = device_create_vargs ( bdi_class , parent , MKDEV ( 0 , 0 ), bdi , fmt , args ); <nl> va_end ( args );
mmm security / keys / request_key_auth . c <nl> ppp security / keys / request_key_auth . c <nl> # include " internal . h " <nl> # include < keys / user - type . h > <nl>  <nl> + static int request_key_auth_preparse ( struct key_preparsed_payload *); <nl> + static void request_key_auth_free_preparse ( struct key_preparsed_payload *); <nl> static int request_key_auth_instantiate ( struct key *, <nl> struct key_preparsed_payload *); <nl> static void request_key_auth_describe ( const struct key *, struct seq_file *); <nl> static long request_key_auth_read ( const struct key *, char __user *, size_t ); <nl> struct key_type key_type_request_key_auth = { <nl> . name = ". request_key_auth ", <nl> . def_datalen = sizeof ( struct request_key_auth ), <nl> + . preparse = request_key_auth_preparse , <nl> + . free_preparse = request_key_auth_free_preparse , <nl> . instantiate = request_key_auth_instantiate , <nl> . describe = request_key_auth_describe , <nl> . revoke = request_key_auth_revoke , <nl> struct key_type key_type_request_key_auth = { <nl> . read = request_key_auth_read , <nl> }; <nl>  <nl> + int request_key_auth_preparse ( struct key_preparsed_payload * prep ) <nl> +{ <nl> + return 0 ; <nl> +} <nl> + <nl> + void request_key_auth_free_preparse ( struct key_preparsed_payload * prep ) <nl> +{ <nl> +} <nl> + <nl> /* <nl> * Instantiate a request - key authorisation key . <nl> */
mmm drivers / gpu / drm / i915 / intel_atomic . c <nl> ppp drivers / gpu / drm / i915 / intel_atomic . c <nl> int intel_atomic_check ( struct drm_device * dev , <nl> state -> allow_modeset = false ; <nl> for ( i = 0 ; i < ncrtcs ; i ++) { <nl> struct intel_crtc * crtc = to_intel_crtc ( state -> crtcs [ i ]); <nl> + if ( crtc ) <nl> + memset (& crtc -> atomic , 0 , sizeof ( crtc -> atomic )); <nl> if ( crtc && crtc -> pipe != nuclear_pipe ) <nl> not_nuclear = true ; <nl> }
mmm drivers / char / cyclades . c <nl> ppp drivers / char / cyclades . c <nl> static irqreturn_t cyy_interrupt ( int irq , void * dev_id ) <nl> card_base_addr = cinfo -> base_addr ; <nl> index = cinfo -> bus_index ; <nl>  <nl> + /* card was not initialized yet ( e . g . DEBUG_SHIRQ ) */ <nl> + if ( unlikely ( card_base_addr == NULL )) <nl> + return IRQ_HANDLED ; <nl> + <nl> /* This loop checks all chips in the card . Make a note whenever <nl> _any_ chip had some work to do , as this is considered an <nl> indication that there will be more to do . Only when no chip
mmm drivers / gpu / drm / i915 / intel_pm . c <nl> ppp drivers / gpu / drm / i915 / intel_pm . c <nl> static int ilk_compute_pipe_wm ( struct intel_crtc * intel_crtc , <nl> return PTR_ERR ( cstate ); <nl>  <nl> pipe_wm = & cstate -> wm . optimal . ilk ; <nl> + memset ( pipe_wm , 0 , sizeof (* pipe_wm )); <nl>  <nl> for_each_intel_plane_on_crtc ( dev , intel_crtc , intel_plane ) { <nl> ps = drm_atomic_get_plane_state ( state ,
mmm include / linux / init_task . h <nl> ppp include / linux / init_task . h <nl> extern struct group_info init_groups ; <nl> # define INIT_STRUCT_PID { \ <nl> . count = ATOMIC_INIT ( 1 ), \ <nl> . tasks = { \ <nl> - { . first = & init_task . pids [ PIDTYPE_PID ]. node }, \ <nl> - { . first = & init_task . pids [ PIDTYPE_PGID ]. node }, \ <nl> - { . first = & init_task . pids [ PIDTYPE_SID ]. node }, \ <nl> + { . first = NULL }, \ <nl> + { . first = NULL }, \ <nl> + { . first = NULL }, \ <nl> }, \ <nl> . level = 0 , \ <nl> . numbers = { { \ <nl> extern struct group_info init_groups ; <nl> { \ <nl> . node = { \ <nl> . next = NULL , \ <nl> - . pprev = & init_struct_pid . tasks [ type ]. first , \ <nl> + . pprev = NULL , \ <nl> }, \ <nl> . pid = & init_struct_pid , \ <nl> }
mmm sound / soc / fsl / mpc5200_psc_i2s . c <nl> ppp sound / soc / fsl / mpc5200_psc_i2s . c <nl> static struct snd_soc_dai_driver psc_i2s_dai [] = {{ <nl> . ops = & psc_i2s_dai_ops , <nl> } }; <nl>  <nl> + static const struct snd_soc_component_driver psc_i2s_component = { <nl> + . name = " mpc5200 - i2s ", <nl> +}; <nl> + <nl> /* --------------------------------------------------------------------- <nl> * OF platform bus binding code : <nl> * - Probe / remove operations <nl> static int psc_i2s_of_probe ( struct platform_device * op ) <nl> if ( rc != 0 ) <nl> return rc ; <nl>  <nl> - rc = snd_soc_register_dais (& op -> dev , psc_i2s_dai , ARRAY_SIZE ( psc_i2s_dai )); <nl> + rc = snd_soc_register_component (& op -> dev , & psc_i2s_component , <nl> + psc_i2s_dai , ARRAY_SIZE ( psc_i2s_dai )); <nl> if ( rc != 0 ) { <nl> pr_err (" Failed to register DAI \ n "); <nl> return rc ; <nl> static int psc_i2s_of_probe ( struct platform_device * op ) <nl> static int psc_i2s_of_remove ( struct platform_device * op ) <nl> { <nl> mpc5200_audio_dma_destroy ( op ); <nl> - snd_soc_unregister_dais (& op -> dev , ARRAY_SIZE ( psc_i2s_dai )); <nl> + snd_soc_unregister_component (& op -> dev ); <nl> return 0 ; <nl> } <nl> 
mmm drivers / mmc / host / mxcmmc . c <nl> ppp drivers / mmc / host / mxcmmc . c <nl> static int mxcmci_probe ( struct platform_device * pdev ) <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> irq = platform_get_irq ( pdev , 0 ); <nl> - if ( irq < 0 ) <nl> - return - EINVAL ; <nl> + if ( irq < 0 ) { <nl> + dev_err (& pdev -> dev , " failed to get IRQ : % d \ n ", irq ); <nl> + return irq ; <nl> + } <nl>  <nl> mmc = mmc_alloc_host ( sizeof (* host ), & pdev -> dev ); <nl> if (! mmc )
mmm include / linux / random . h <nl> ppp include / linux / random . h <nl> # include < linux / kernel . h > <nl> # include < linux / list . h > <nl> # include < linux / once . h > <nl> +# include < linux / percpu . h > <nl>  <nl> # include < uapi / linux / random . h > <nl>  <nl> struct rnd_state { <nl> __u32 s1 , s2 , s3 , s4 ; <nl> }; <nl>  <nl> + DECLARE_PER_CPU ( struct rnd_state , net_rand_state ) __latent_entropy ; <nl> + <nl> u32 prandom_u32_state ( struct rnd_state * state ); <nl> void prandom_bytes_state ( struct rnd_state * state , void * buf , size_t nbytes ); <nl> void prandom_seed_full_state ( struct rnd_state __percpu * pcpu_state );mmm drivers / char / random . c <nl> ppp drivers / char / random . c <nl> # include < linux / kernel . h > <nl> # include < linux / list . h > <nl> # include < linux / once . h > <nl> +# include < linux / percpu . h > <nl>  <nl> # include < uapi / linux / random . h > <nl>  <nl> struct rnd_state { <nl> __u32 s1 , s2 , s3 , s4 ; <nl> }; <nl>  <nl> + DECLARE_PER_CPU ( struct rnd_state , net_rand_state ) __latent_entropy ; <nl> + <nl> u32 prandom_u32_state ( struct rnd_state * state ); <nl> void prandom_bytes_state ( struct rnd_state * state , void * buf , size_t nbytes ); <nl> void prandom_seed_full_state ( struct rnd_state __percpu * pcpu_state ); <nl> void add_interrupt_randomness ( int irq , int irq_flags ) <nl>  <nl> fast_mix ( fast_pool ); <nl> add_interrupt_bench ( cycles ); <nl> + this_cpu_add ( net_rand_state . s1 , fast_pool -> pool [ cycles & 3 ]); <nl>  <nl> if ( unlikely ( crng_init == 0 )) { <nl> if (( fast_pool -> count >= 64 ) &&mmm lib / random32 . c <nl> ppp lib / random32 . c <nl> # include < linux / kernel . h > <nl> # include < linux / list . h > <nl> # include < linux / once . h > <nl> +# include < linux / percpu . h > <nl>  <nl> # include < uapi / linux / random . h > <nl>  <nl> struct rnd_state { <nl> __u32 s1 , s2 , s3 , s4 ; <nl> }; <nl>  <nl> + DECLARE_PER_CPU ( struct rnd_state , net_rand_state ) __latent_entropy ; <nl> + <nl> u32 prandom_u32_state ( struct rnd_state * state ); <nl> void prandom_bytes_state ( struct rnd_state * state , void * buf , size_t nbytes ); <nl> void prandom_seed_full_state ( struct rnd_state __percpu * pcpu_state ); <nl> void add_interrupt_randomness ( int irq , int irq_flags ) <nl>  <nl> fast_mix ( fast_pool ); <nl> add_interrupt_bench ( cycles ); <nl> + this_cpu_add ( net_rand_state . s1 , fast_pool -> pool [ cycles & 3 ]); <nl>  <nl> if ( unlikely ( crng_init == 0 )) { <nl> if (( fast_pool -> count >= 64 ) && <nl> static inline void prandom_state_selftest ( void ) <nl> } <nl> # endif <nl>  <nl> - static DEFINE_PER_CPU ( struct rnd_state , net_rand_state ) __latent_entropy ; <nl> + DEFINE_PER_CPU ( struct rnd_state , net_rand_state ) __latent_entropy ; <nl>  <nl> /** <nl> * prandom_u32_state - seeded pseudo - random number generator .mmm kernel / time / timer . c <nl> ppp kernel / time / timer . c <nl> # include < linux / kernel . h > <nl> # include < linux / list . h > <nl> # include < linux / once . h > <nl> +# include < linux / percpu . h > <nl>  <nl> # include < uapi / linux / random . h > <nl>  <nl> struct rnd_state { <nl> __u32 s1 , s2 , s3 , s4 ; <nl> }; <nl>  <nl> + DECLARE_PER_CPU ( struct rnd_state , net_rand_state ) __latent_entropy ; <nl> + <nl> u32 prandom_u32_state ( struct rnd_state * state ); <nl> void prandom_bytes_state ( struct rnd_state * state , void * buf , size_t nbytes ); <nl> void prandom_seed_full_state ( struct rnd_state __percpu * pcpu_state ); <nl> void add_interrupt_randomness ( int irq , int irq_flags ) <nl>  <nl> fast_mix ( fast_pool ); <nl> add_interrupt_bench ( cycles ); <nl> + this_cpu_add ( net_rand_state . s1 , fast_pool -> pool [ cycles & 3 ]); <nl>  <nl> if ( unlikely ( crng_init == 0 )) { <nl> if (( fast_pool -> count >= 64 ) && <nl> static inline void prandom_state_selftest ( void ) <nl> } <nl> # endif <nl>  <nl> - static DEFINE_PER_CPU ( struct rnd_state , net_rand_state ) __latent_entropy ; <nl> + DEFINE_PER_CPU ( struct rnd_state , net_rand_state ) __latent_entropy ; <nl>  <nl> /** <nl> * prandom_u32_state - seeded pseudo - random number generator . <nl> # include < linux / sched / debug . h > <nl> # include < linux / slab . h > <nl> # include < linux / compat . h > <nl> +# include < linux / random . h > <nl>  <nl> # include < linux / uaccess . h > <nl> # include < asm / unistd . h > <nl> void update_process_times ( int user_tick ) <nl> scheduler_tick (); <nl> if ( IS_ENABLED ( CONFIG_POSIX_TIMERS )) <nl> run_posix_cpu_timers (); <nl> + <nl> + /* The current CPU might make use of net randoms without receiving IRQs <nl> + * to renew them often enough . Let ' s update the net_rand_state from a <nl> + * non - constant value that ' s not affine to the number of calls to make <nl> + * sure it ' s updated when there ' s some activity ( we don ' t care in idle ). <nl> + */ <nl> + this_cpu_add ( net_rand_state . s1 , rol32 ( jiffies , 24 ) + user_tick ); <nl> } <nl>  <nl> /**
mmm drivers / pci / hotplug / pciehp_hpc . c <nl> ppp drivers / pci / hotplug / pciehp_hpc . c <nl> static void pcie_disable_notification ( struct controller * ctrl ) <nl> u16 mask ; <nl> mask = ( PCI_EXP_SLTCTL_PDCE | PCI_EXP_SLTCTL_ABPE | <nl> PCI_EXP_SLTCTL_MRLSCE | PCI_EXP_SLTCTL_PFDE | <nl> - PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_CCIE ); <nl> + PCI_EXP_SLTCTL_HPIE | PCI_EXP_SLTCTL_CCIE | <nl> + PCI_EXP_SLTCTL_DLLSCE ); <nl> if ( pcie_write_cmd ( ctrl , 0 , mask )) <nl> ctrl_warn ( ctrl , " Cannot disable software notification \ n "); <nl> }
mmm kernel / sched / fair . c <nl> ppp kernel / sched / fair . c <nl> ___update_load_avg ( u64 now , int cpu , struct sched_avg * sa , <nl>  <nl> sa -> last_update_time += delta << 10 ; <nl>  <nl> + /* <nl> + * running is a subset of runnable ( weight ) so running can ' t be set if <nl> + * runnable is clear . But there are some corner cases where the current <nl> + * se has been already dequeued but cfs_rq -> curr still points to it . <nl> + * This means that weight will be 0 but not running for a sched_entity <nl> + * but also for a cfs_rq if the latter becomes idle . As an example , <nl> + * this happens during idle_balance () which calls <nl> + * update_blocked_averages () <nl> + */ <nl> + if (! weight ) <nl> + running = 0 ; <nl> + <nl> /* <nl> * Now we know we crossed measurement unit boundaries . The * _avg <nl> * accrues by two steps :
mmm drivers / platform / x86 / acer - wmi . c <nl> ppp drivers / platform / x86 / acer - wmi . c <nl> static acpi_status WMID_set_capabilities ( void ) <nl> devices = *(( u32 *) obj -> buffer . pointer ); <nl> } else if ( obj -> type == ACPI_TYPE_INTEGER ) { <nl> devices = ( u32 ) obj -> integer . value ; <nl> + } else { <nl> + kfree ( out . pointer ); <nl> + return AE_ERROR ; <nl> } <nl> } else { <nl> kfree ( out . pointer );
mmm include / linux / hid . h <nl> ppp include / linux / hid . h <nl> struct hid_collection { <nl> struct hid_usage { <nl> unsigned hid ; /* hid usage code */ <nl> unsigned collection_index ; /* index into collection array */ <nl> + unsigned usage_index ; /* index into usage array */ <nl> /* hidinput data */ <nl> __u16 code ; /* input driver code */ <nl> __u8 type ; /* input driver type */mmm drivers / hid / hid - core . c <nl> ppp drivers / hid / hid - core . c <nl> struct hid_collection { <nl> struct hid_usage { <nl> unsigned hid ; /* hid usage code */ <nl> unsigned collection_index ; /* index into collection array */ <nl> + unsigned usage_index ; /* index into usage array */ <nl> /* hidinput data */ <nl> __u16 code ; /* input driver code */ <nl> __u8 type ; /* input driver type */ <nl> EXPORT_SYMBOL_GPL ( hid_register_report ); <nl> static struct hid_field * hid_register_field ( struct hid_report * report , unsigned usages , unsigned values ) <nl> { <nl> struct hid_field * field ; <nl> + int i ; <nl>  <nl> if ( report -> maxfield == HID_MAX_FIELDS ) { <nl> hid_err ( report -> device , " too many fields in report \ n "); <nl> static struct hid_field * hid_register_field ( struct hid_report * report , unsigned <nl> field -> value = ( s32 *)( field -> usage + usages ); <nl> field -> report = report ; <nl>  <nl> + for ( i = 0 ; i < usages ; i ++) <nl> + field -> usage [ i ]. usage_index = i ; <nl> + <nl> return field ; <nl> } <nl> 
mmm fs / exec . c <nl> ppp fs / exec . c <nl> static int __bprm_mm_init ( struct linux_binprm * bprm ) <nl> if (! vma ) <nl> return - ENOMEM ; <nl>  <nl> - down_write (& mm -> mmap_sem ); <nl> + if ( down_write_killable (& mm -> mmap_sem )) { <nl> + err = - EINTR ; <nl> + goto err_free ; <nl> + } <nl> vma -> vm_mm = mm ; <nl>  <nl> /* <nl> static int __bprm_mm_init ( struct linux_binprm * bprm ) <nl> return 0 ; <nl> err : <nl> up_write (& mm -> mmap_sem ); <nl> + err_free : <nl> bprm -> vma = NULL ; <nl> kmem_cache_free ( vm_area_cachep , vma ); <nl> return err ; <nl> int setup_arg_pages ( struct linux_binprm * bprm , <nl> bprm -> loader -= stack_shift ; <nl> bprm -> exec -= stack_shift ; <nl>  <nl> - down_write (& mm -> mmap_sem ); <nl> + if ( down_write_killable (& mm -> mmap_sem )) <nl> + return - EINTR ; <nl> + <nl> vm_flags = VM_STACK_FLAGS ; <nl>  <nl> /*
mmm drivers / gpu / drm / i915 / i915_drv . h <nl> ppp drivers / gpu / drm / i915 / i915_drv . h <nl> struct drm_i915_file_private { <nl>  <nl> # define HAS_FORCE_WAKE ( dev ) ( INTEL_INFO ( dev )-> has_force_wake ) <nl>  <nl> -# define HAS_L3_GPU_CACHE ( dev ) ( IS_IVYBRIDGE ( dev )) <nl> +# define HAS_L3_GPU_CACHE ( dev ) ( IS_IVYBRIDGE ( dev ) || IS_HASWELL ( dev )) <nl>  <nl> # include " i915_trace . h " <nl> 
mmm net / sctp / sm_statefuns . c <nl> ppp net / sctp / sm_statefuns . c <nl> sctp_disposition_t sctp_sf_do_5_2_4_dupcook ( struct net * net , <nl> } <nl>  <nl> /* Delete the tempory new association . */ <nl> - sctp_add_cmd_sf ( commands , SCTP_CMD_NEW_ASOC , SCTP_ASOC ( new_asoc )); <nl> + sctp_add_cmd_sf ( commands , SCTP_CMD_SET_ASOC , SCTP_ASOC ( new_asoc )); <nl> sctp_add_cmd_sf ( commands , SCTP_CMD_DELETE_TCB , SCTP_NULL ()); <nl>  <nl> /* Restore association pointer to provide SCTP command interpeter
mmm drivers / net / wireless / mac80211_hwsim . c <nl> ppp drivers / net / wireless / mac80211_hwsim . c <nl> static void mac80211_hwsim_tx_frame_nl ( struct ieee80211_hw * hw , <nl> goto nla_put_failure ; <nl>  <nl> genlmsg_end ( skb , msg_head ); <nl> - genlmsg_unicast (& init_net , skb , dst_portid ); <nl> + if ( genlmsg_unicast (& init_net , skb , dst_portid )) <nl> + goto err_free_txskb ; <nl>  <nl> /* Enqueue the packet */ <nl> skb_queue_tail (& data -> pending , my_skb ); <nl> static void mac80211_hwsim_tx_frame_nl ( struct ieee80211_hw * hw , <nl> return ; <nl>  <nl> nla_put_failure : <nl> + nlmsg_free ( skb ); <nl> + err_free_txskb : <nl> printk ( KERN_DEBUG " mac80211_hwsim : error occurred in % s \ n ", __func__ ); <nl> ieee80211_free_txskb ( hw , my_skb ); <nl> data -> tx_failed ++;
mmm drivers / gpu / drm / msm / msm_gem . c <nl> ppp drivers / gpu / drm / msm / msm_gem . c <nl> void msm_gem_free_object ( struct drm_gem_object * obj ) <nl> if ( msm_obj -> pages ) <nl> drm_free_large ( msm_obj -> pages ); <nl>  <nl> + drm_prime_gem_destroy ( obj , msm_obj -> sgt ); <nl> } else { <nl> vunmap ( msm_obj -> vaddr ); <nl> put_pages ( obj );
mmm drivers / media / video / em28xx / em28xx . h <nl> ppp drivers / media / video / em28xx / em28xx . h <nl> struct em28xx { <nl>  <nl> /* locks */ <nl> struct mutex lock ; <nl> + struct mutex ctrl_urb_lock ; /* protects urb_buf */ <nl> /* spinlock_t queue_lock ; */ <nl> struct list_head inqueue , outqueue ; <nl> wait_queue_head_t open , wait_frame , wait_stream ;mmm drivers / media / video / em28xx / em28xx - core . c <nl> ppp drivers / media / video / em28xx / em28xx - core . c <nl> struct em28xx { <nl>  <nl> /* locks */ <nl> struct mutex lock ; <nl> + struct mutex ctrl_urb_lock ; /* protects urb_buf */ <nl> /* spinlock_t queue_lock ; */ <nl> struct list_head inqueue , outqueue ; <nl> wait_queue_head_t open , wait_frame , wait_stream ; <nl> int em28xx_read_reg_req_len ( struct em28xx * dev , u8 req , u16 reg , <nl>  <nl> em28xx_regdbg (" req =% 02x , reg =% 02x ", req , reg ); <nl>  <nl> + mutex_lock (& dev -> ctrl_urb_lock ); <nl> ret = usb_control_msg ( dev -> udev , usb_rcvctrlpipe ( dev -> udev , 0 ), req , <nl> USB_DIR_IN | USB_TYPE_VENDOR | USB_RECIP_DEVICE , <nl> 0x0000 , reg , dev -> urb_buf , len , HZ ); <nl> if ( ret < 0 ) { <nl> if ( reg_debug ) <nl> printk (" failed !\ n "); <nl> + mutex_unlock (& dev -> ctrl_urb_lock ); <nl> return ret ; <nl> } <nl>  <nl> if ( len ) <nl> memcpy ( buf , dev -> urb_buf , len ); <nl>  <nl> + mutex_unlock (& dev -> ctrl_urb_lock ); <nl> + <nl> if ( reg_debug ) { <nl> printk ("% 02x values : ", ret ); <nl> for ( byte = 0 ; byte < len ; byte ++) <nl> int em28xx_read_reg_req ( struct em28xx * dev , u8 req , u16 reg ) <nl>  <nl> em28xx_regdbg (" req =% 02x , reg =% 02x :", req , reg ); <nl>  <nl> + mutex_lock (& dev -> ctrl_urb_lock ); <nl> ret = usb_control_msg ( dev -> udev , usb_rcvctrlpipe ( dev -> udev , 0 ), req , <nl> USB_DIR_IN | USB_TYPE_VENDOR | USB_RECIP_DEVICE , <nl> 0x0000 , reg , dev -> urb_buf , 1 , HZ ); <nl> + val = dev -> urb_buf [ 0 ]; <nl> + mutex_unlock (& dev -> ctrl_urb_lock ); <nl> + <nl> if ( ret < 0 ) { <nl> printk (" failed !\ n "); <nl> return ret ; <nl> } <nl>  <nl> - val = dev -> urb_buf [ 0 ]; <nl> - <nl> if ( reg_debug ) <nl> printk ("% 02x \ n ", ( unsigned char ) val ); <nl>  <nl> int em28xx_write_regs_req ( struct em28xx * dev , u8 req , u16 reg , char * buf , <nl> printk ("\ n "); <nl> } <nl>  <nl> + mutex_lock (& dev -> ctrl_urb_lock ); <nl> memcpy ( dev -> urb_buf , buf , len ); <nl> ret = usb_control_msg ( dev -> udev , usb_sndctrlpipe ( dev -> udev , 0 ), req , <nl> USB_DIR_OUT | USB_TYPE_VENDOR | USB_RECIP_DEVICE , <nl> 0x0000 , reg , dev -> urb_buf , len , HZ ); <nl> + mutex_unlock (& dev -> ctrl_urb_lock ); <nl>  <nl> if ( dev -> wait_after_write ) <nl> msleep ( dev -> wait_after_write );mmm drivers / media / video / em28xx / em28xx - video . c <nl> ppp drivers / media / video / em28xx / em28xx - video . c <nl> struct em28xx { <nl>  <nl> /* locks */ <nl> struct mutex lock ; <nl> + struct mutex ctrl_urb_lock ; /* protects urb_buf */ <nl> /* spinlock_t queue_lock ; */ <nl> struct list_head inqueue , outqueue ; <nl> wait_queue_head_t open , wait_frame , wait_stream ; <nl> int em28xx_read_reg_req_len ( struct em28xx * dev , u8 req , u16 reg , <nl>  <nl> em28xx_regdbg (" req =% 02x , reg =% 02x ", req , reg ); <nl>  <nl> + mutex_lock (& dev -> ctrl_urb_lock ); <nl> ret = usb_control_msg ( dev -> udev , usb_rcvctrlpipe ( dev -> udev , 0 ), req , <nl> USB_DIR_IN | USB_TYPE_VENDOR | USB_RECIP_DEVICE , <nl> 0x0000 , reg , dev -> urb_buf , len , HZ ); <nl> if ( ret < 0 ) { <nl> if ( reg_debug ) <nl> printk (" failed !\ n "); <nl> + mutex_unlock (& dev -> ctrl_urb_lock ); <nl> return ret ; <nl> } <nl>  <nl> if ( len ) <nl> memcpy ( buf , dev -> urb_buf , len ); <nl>  <nl> + mutex_unlock (& dev -> ctrl_urb_lock ); <nl> + <nl> if ( reg_debug ) { <nl> printk ("% 02x values : ", ret ); <nl> for ( byte = 0 ; byte < len ; byte ++) <nl> int em28xx_read_reg_req ( struct em28xx * dev , u8 req , u16 reg ) <nl>  <nl> em28xx_regdbg (" req =% 02x , reg =% 02x :", req , reg ); <nl>  <nl> + mutex_lock (& dev -> ctrl_urb_lock ); <nl> ret = usb_control_msg ( dev -> udev , usb_rcvctrlpipe ( dev -> udev , 0 ), req , <nl> USB_DIR_IN | USB_TYPE_VENDOR | USB_RECIP_DEVICE , <nl> 0x0000 , reg , dev -> urb_buf , 1 , HZ ); <nl> + val = dev -> urb_buf [ 0 ]; <nl> + mutex_unlock (& dev -> ctrl_urb_lock ); <nl> + <nl> if ( ret < 0 ) { <nl> printk (" failed !\ n "); <nl> return ret ; <nl> } <nl>  <nl> - val = dev -> urb_buf [ 0 ]; <nl> - <nl> if ( reg_debug ) <nl> printk ("% 02x \ n ", ( unsigned char ) val ); <nl>  <nl> int em28xx_write_regs_req ( struct em28xx * dev , u8 req , u16 reg , char * buf , <nl> printk ("\ n "); <nl> } <nl>  <nl> + mutex_lock (& dev -> ctrl_urb_lock ); <nl> memcpy ( dev -> urb_buf , buf , len ); <nl> ret = usb_control_msg ( dev -> udev , usb_sndctrlpipe ( dev -> udev , 0 ), req , <nl> USB_DIR_OUT | USB_TYPE_VENDOR | USB_RECIP_DEVICE , <nl> 0x0000 , reg , dev -> urb_buf , len , HZ ); <nl> + mutex_unlock (& dev -> ctrl_urb_lock ); <nl>  <nl> if ( dev -> wait_after_write ) <nl> msleep ( dev -> wait_after_write ); <nl> static int em28xx_init_dev ( struct em28xx ** devhandle , struct usb_device * udev , <nl>  <nl> dev -> udev = udev ; <nl> mutex_init (& dev -> lock ); <nl> + mutex_init (& dev -> ctrl_urb_lock ); <nl> spin_lock_init (& dev -> slock ); <nl> init_waitqueue_head (& dev -> open ); <nl> init_waitqueue_head (& dev -> wait_frame );
mmm sound / soc / codecs / arizona . c <nl> ppp sound / soc / codecs / arizona . c <nl> int arizona_set_sysclk ( struct snd_soc_codec * codec , int clk_id , <nl> case 147456000 : <nl> val |= 6 << ARIZONA_SYSCLK_FREQ_SHIFT ; <nl> break ; <nl> + case 0 : <nl> + dev_dbg ( arizona -> dev , "% s cleared \ n ", name ); <nl> + * clk = freq ; <nl> + return 0 ; <nl> default : <nl> return - EINVAL ; <nl> } <nl> static int arizona_startup ( struct snd_pcm_substream * substream , <nl> return 0 ; <nl> } <nl>  <nl> + if ( base_rate == 0 ) <nl> + return 0 ; <nl> + <nl> if ( base_rate % 8000 ) <nl> constraint = & arizona_44k1_constraint ; <nl> else
mmm drivers / infiniband / hw / mlx4 / cm . c <nl> ppp drivers / infiniband / hw / mlx4 / cm . c <nl> static struct id_map_entry * <nl> id_map_alloc ( struct ib_device * ibdev , int slave_id , u32 sl_cm_id ) <nl> { <nl> int ret ; <nl> - static int next_id ; <nl> struct id_map_entry * ent ; <nl> struct mlx4_ib_sriov * sriov = & to_mdev ( ibdev )-> sriov ; <nl>  <nl> id_map_alloc ( struct ib_device * ibdev , int slave_id , u32 sl_cm_id ) <nl> idr_preload ( GFP_KERNEL ); <nl> spin_lock (& to_mdev ( ibdev )-> sriov . id_map_lock ); <nl>  <nl> - ret = idr_alloc (& sriov -> pv_id_table , ent , next_id , 0 , GFP_NOWAIT ); <nl> + ret = idr_alloc_cyclic (& sriov -> pv_id_table , ent , 0 , 0 , GFP_NOWAIT ); <nl> if ( ret >= 0 ) { <nl> - next_id = max ( ret + 1 , 0 ); <nl> ent -> pv_cm_id = ( u32 ) ret ; <nl> sl_id_map_add ( ibdev , ent ); <nl> list_add_tail (& ent -> list , & sriov -> cm_list );
mmm drivers / media / usb / ttusb - dec / ttusbdecfe . c <nl> ppp drivers / media / usb / ttusb - dec / ttusbdecfe . c <nl> static int ttusbdecfe_dvbs_diseqc_send_master_cmd ( struct dvb_frontend * fe , struc <nl> 0x00 , 0x00 , 0x00 , 0x00 , <nl> 0x00 , 0x00 }; <nl>  <nl> + if ( cmd -> msg_len > sizeof ( b ) - 4 ) <nl> + return - EINVAL ; <nl> + <nl> memcpy (& b [ 4 ], cmd -> msg , cmd -> msg_len ); <nl>  <nl> state -> config -> send_command ( fe , 0x72 ,
mmm include / net / cipso_ipv4 . h <nl> ppp include / net / cipso_ipv4 . h <nl> static inline int cipso_v4_validate ( const struct sk_buff * skb , <nl> unsigned char err_offset = 0 ; <nl> u8 opt_len = opt [ 1 ]; <nl> u8 opt_iter ; <nl> + u8 tag_len ; <nl>  <nl> if ( opt_len < 8 ) { <nl> err_offset = 1 ; <nl> static inline int cipso_v4_validate ( const struct sk_buff * skb , <nl> } <nl>  <nl> for ( opt_iter = 6 ; opt_iter < opt_len ;) { <nl> - if ( opt [ opt_iter + 1 ] > ( opt_len - opt_iter )) { <nl> + tag_len = opt [ opt_iter + 1 ]; <nl> + if (( tag_len == 0 ) || ( opt [ opt_iter + 1 ] > ( opt_len - opt_iter ))) { <nl> err_offset = opt_iter + 1 ; <nl> goto out ; <nl> } <nl> - opt_iter += opt [ opt_iter + 1 ]; <nl> + opt_iter += tag_len ; <nl> } <nl>  <nl> out :
mmm sound / soc / intel / sst - haswell - ipc . c <nl> ppp sound / soc / intel / sst - haswell - ipc . c <nl> int sst_hsw_stream_get_volume ( struct sst_hsw * hsw , struct sst_hsw_stream * stream <nl> return - EINVAL ; <nl>  <nl> sst_dsp_read ( hsw -> dsp , volume , <nl> - stream -> reply . volume_register_address [ channel ], sizeof ( volume )); <nl> + stream -> reply . volume_register_address [ channel ], <nl> + sizeof (* volume )); <nl>  <nl> return 0 ; <nl> }
mmm net / rds / rdma . c <nl> ppp net / rds / rdma . c <nl> static int __rds_rdma_map ( struct rds_sock * rs , struct rds_get_mr_args * args , <nl> long i ; <nl> int ret ; <nl>  <nl> - if ( rs -> rs_bound_addr == 0 ) { <nl> + if ( rs -> rs_bound_addr == 0 || ! rs -> rs_transport ) { <nl> ret = - ENOTCONN ; /* XXX not a great errno */ <nl> goto out ; <nl> }
mmm drivers / gpu / drm / radeon / kv_dpm . c <nl> ppp drivers / gpu / drm / radeon / kv_dpm . c <nl> void kv_dpm_powergate_uvd ( struct radeon_device * rdev , bool gate ) <nl> pi -> uvd_power_gated = gate ; <nl>  <nl> if ( gate ) { <nl> - uvd_v1_0_stop ( rdev ); <nl> - cik_update_cg ( rdev , RADEON_CG_BLOCK_UVD , false ); <nl> + if ( pi -> caps_uvd_pg ) { <nl> + uvd_v1_0_stop ( rdev ); <nl> + cik_update_cg ( rdev , RADEON_CG_BLOCK_UVD , false ); <nl> + } <nl> kv_update_uvd_dpm ( rdev , gate ); <nl> if ( pi -> caps_uvd_pg ) <nl> kv_notify_message_to_smu ( rdev , PPSMC_MSG_UVDPowerOFF ); <nl> } else { <nl> - if ( pi -> caps_uvd_pg ) <nl> + if ( pi -> caps_uvd_pg ) { <nl> kv_notify_message_to_smu ( rdev , PPSMC_MSG_UVDPowerON ); <nl> - uvd_v4_2_resume ( rdev ); <nl> - uvd_v1_0_start ( rdev ); <nl> - cik_update_cg ( rdev , RADEON_CG_BLOCK_UVD , true ); <nl> + uvd_v4_2_resume ( rdev ); <nl> + uvd_v1_0_start ( rdev ); <nl> + cik_update_cg ( rdev , RADEON_CG_BLOCK_UVD , true ); <nl> + } <nl> kv_update_uvd_dpm ( rdev , gate ); <nl> } <nl> }
mmm drivers / bluetooth / hci_ldisc . c <nl> ppp drivers / bluetooth / hci_ldisc . c <nl> static int hci_uart_send_frame ( struct sk_buff * skb ) <nl> */ <nl> static int hci_uart_tty_open ( struct tty_struct * tty ) <nl> { <nl> - struct hci_uart * hu = ( void *) tty -> disc_data ; <nl> + struct hci_uart * hu ; <nl>  <nl> BT_DBG (" tty % p ", tty ); <nl>  <nl> - /* FIXME : This btw is bogus , nothing requires the old ldisc to clear <nl> - the pointer */ <nl> - if ( hu ) <nl> - return - EEXIST ; <nl> - <nl> /* Error if the tty has no write op instead of leaving an exploitable <nl> hole */ <nl> if ( tty -> ops -> write == NULL )
mmm net / wireless / reg . c <nl> ppp net / wireless / reg . c <nl> struct reg_beacon { <nl> struct ieee80211_channel chan ; <nl> }; <nl>  <nl> + static void reg_todo ( struct work_struct * work ); <nl> + static DECLARE_WORK ( reg_work , reg_todo ); <nl> + <nl> /* We keep a static world regulatory domain in case of the absence of CRDA */ <nl> static const struct ieee80211_regdomain world_regdom = { <nl> . n_reg_rules = 5 , <nl> static void reg_todo ( struct work_struct * work ) <nl> reg_process_pending_beacon_hints (); <nl> } <nl>  <nl> - static DECLARE_WORK ( reg_work , reg_todo ); <nl> - <nl> static void queue_regulatory_request ( struct regulatory_request * request ) <nl> { <nl> if ( isalpha ( request -> alpha2 [ 0 ]))
mmm fs / cifs / cifs_dfs_ref . c <nl> ppp fs / cifs / cifs_dfs_ref . c <nl> char * cifs_compose_mount_options ( const char * sb_mountdata , <nl> * string to the length of the original string to allow for worst case . <nl> */ <nl> md_len = strlen ( sb_mountdata ) + INET6_ADDRSTRLEN ; <nl> - mountdata = kzalloc ( md_len + 1 , GFP_KERNEL ); <nl> + mountdata = kzalloc ( md_len + sizeof (" ip =") + 1 , GFP_KERNEL ); <nl> if ( mountdata == NULL ) { <nl> rc = - ENOMEM ; <nl> goto compose_mount_options_err ;
mmm drivers / lguest / lg . h <nl> ppp drivers / lguest / lg . h <nl> struct lg_cpu { <nl> unsigned long regs_page ; <nl> struct lguest_regs * regs ; <nl>  <nl> + struct lguest_pages * last_pages ; <nl> + <nl> int cpu_pgd ; /* which pgd this cpu is currently using */ <nl>  <nl> /* If a hypercall was asked for , this points to the arguments . */ <nl> struct lguest <nl>  <nl> /* Bitmap of what has changed : see CHANGED_ * above . */ <nl> int changed ; <nl> - struct lguest_pages * last_pages ; <nl>  <nl> struct pgdir pgdirs [ 4 ]; <nl> mmm drivers / lguest / lguest_user . c <nl> ppp drivers / lguest / lguest_user . c <nl> struct lg_cpu { <nl> unsigned long regs_page ; <nl> struct lguest_regs * regs ; <nl>  <nl> + struct lguest_pages * last_pages ; <nl> + <nl> int cpu_pgd ; /* which pgd this cpu is currently using */ <nl>  <nl> /* If a hypercall was asked for , this points to the arguments . */ <nl> struct lguest <nl>  <nl> /* Bitmap of what has changed : see CHANGED_ * above . */ <nl> int changed ; <nl> - struct lguest_pages * last_pages ; <nl>  <nl> struct pgdir pgdirs [ 4 ]; <nl>  <nl> static int lg_cpu_start ( struct lg_cpu * cpu , unsigned id , unsigned long start_ip ) <nl> * reference , it is destroyed before close () is called . */ <nl> cpu -> mm = get_task_mm ( cpu -> tsk ); <nl>  <nl> + /* We remember which CPU ' s pages this Guest used last , for optimization <nl> + * when the same Guest runs on the same CPU twice . */ <nl> + cpu -> last_pages = NULL ; <nl> + <nl> return 0 ; <nl> } <nl>  <nl> static int initialize ( struct file * file , const unsigned long __user * input ) <nl> if ( err ) <nl> goto free_regs ; <nl>  <nl> - /* We remember which CPU ' s pages this Guest used last , for optimization <nl> - * when the same Guest runs on the same CPU twice . */ <nl> - lg -> last_pages = NULL ; <nl> - <nl> /* We keep our " struct lguest " in the file ' s private_data . */ <nl> file -> private_data = lg ; <nl> mmm drivers / lguest / x86 / core . c <nl> ppp drivers / lguest / x86 / core . c <nl> struct lg_cpu { <nl> unsigned long regs_page ; <nl> struct lguest_regs * regs ; <nl>  <nl> + struct lguest_pages * last_pages ; <nl> + <nl> int cpu_pgd ; /* which pgd this cpu is currently using */ <nl>  <nl> /* If a hypercall was asked for , this points to the arguments . */ <nl> struct lguest <nl>  <nl> /* Bitmap of what has changed : see CHANGED_ * above . */ <nl> int changed ; <nl> - struct lguest_pages * last_pages ; <nl>  <nl> struct pgdir pgdirs [ 4 ]; <nl>  <nl> static int lg_cpu_start ( struct lg_cpu * cpu , unsigned id , unsigned long start_ip ) <nl> * reference , it is destroyed before close () is called . */ <nl> cpu -> mm = get_task_mm ( cpu -> tsk ); <nl>  <nl> + /* We remember which CPU ' s pages this Guest used last , for optimization <nl> + * when the same Guest runs on the same CPU twice . */ <nl> + cpu -> last_pages = NULL ; <nl> + <nl> return 0 ; <nl> } <nl>  <nl> static int initialize ( struct file * file , const unsigned long __user * input ) <nl> if ( err ) <nl> goto free_regs ; <nl>  <nl> - /* We remember which CPU ' s pages this Guest used last , for optimization <nl> - * when the same Guest runs on the same CPU twice . */ <nl> - lg -> last_pages = NULL ; <nl> - <nl> /* We keep our " struct lguest " in the file ' s private_data . */ <nl> file -> private_data = lg ; <nl>  <nl> static void copy_in_guest_info ( struct lg_cpu * cpu , struct lguest_pages * pages ) <nl> * same Guest we ran last time ( and that Guest hasn ' t run anywhere else <nl> * meanwhile ). If that ' s not the case , we pretend everything in the <nl> * Guest has changed . */ <nl> - if ( __get_cpu_var ( last_cpu ) != cpu || lg -> last_pages != pages ) { <nl> + if ( __get_cpu_var ( last_cpu ) != cpu || cpu -> last_pages != pages ) { <nl> __get_cpu_var ( last_cpu ) = cpu ; <nl> - lg -> last_pages = pages ; <nl> + cpu -> last_pages = pages ; <nl> lg -> changed = CHANGED_ALL ; <nl> } <nl> 
mmm drivers / block / floppy . c <nl> ppp drivers / block / floppy . c <nl> static void setup_format_params ( int track ) <nl> raw_cmd -> kernel_data = floppy_track_buffer ; <nl> raw_cmd -> length = 4 * F_SECT_PER_TRACK ; <nl>  <nl> + if (! F_SECT_PER_TRACK ) <nl> + return ; <nl> + <nl> /* allow for about 30ms for data transport per track */ <nl> head_shift = ( F_SECT_PER_TRACK + 5 ) / 6 ; <nl>  <nl> static int set_geometry ( unsigned int cmd , struct floppy_struct * g , <nl> /* sanity checking for parameters . */ <nl> if ( g -> sect <= 0 || <nl> g -> head <= 0 || <nl> + /* check for zero in F_SECT_PER_TRACK */ <nl> + ( unsigned char )(( g -> sect << 2 ) >> FD_SIZECODE ( g )) == 0 || <nl> g -> track <= 0 || g -> track > UDP -> tracks >> STRETCH ( g ) || <nl> /* check if reserved bits are set */ <nl> ( g -> stretch & ~( FD_STRETCH | FD_SWAPSIDES | FD_SECTBASEMASK )) != 0 )
mmm drivers / net / ethernet / broadcom / cnic . c <nl> ppp drivers / net / ethernet / broadcom / cnic . c <nl> static int cnic_start_hw ( struct cnic_dev * dev ) <nl> return 0 ; <nl>  <nl> err1 : <nl> - cp -> free_resc ( dev ); <nl> + if ( ethdev -> drv_state & CNIC_DRV_STATE_HANDLES_IRQ ) <nl> + cp -> stop_hw ( dev ); <nl> + else <nl> + cp -> free_resc ( dev ); <nl> pci_dev_put ( dev -> pcidev ); <nl> return err ; <nl> }
mmm drivers / scsi / sg . c <nl> ppp drivers / scsi / sg . c <nl> sg_common_write ( Sg_fd * sfp , Sg_request * srp , <nl> return k ; /* probably out of space --> ENOMEM */ <nl> } <nl> if ( atomic_read (& sdp -> detaching )) { <nl> - if ( srp -> bio ) <nl> + if ( srp -> bio ) { <nl> + if ( srp -> rq -> cmd != srp -> rq -> __cmd ) <nl> + kfree ( srp -> rq -> cmd ); <nl> + <nl> blk_end_request_all ( srp -> rq , - EIO ); <nl> + srp -> rq = NULL ; <nl> + } <nl> + <nl> sg_finish_rem_req ( srp ); <nl> return - ENODEV ; <nl> }
mmm drivers / net / wireless / orinoco_cs . c <nl> ppp drivers / net / wireless / orinoco_cs . c <nl> static struct pcmcia_device_id orinoco_cs_ids [] = { <nl> PCMCIA_DEVICE_PROD_ID12 (" Cabletron ", " RoamAbout 802 . 11 DS ", 0x32d445f5 , 0xedeffd90 ), <nl> PCMCIA_DEVICE_PROD_ID12 (" corega K . K .", " Wireless LAN PCC - 11 ", 0x5261440f , 0xa6405584 ), <nl> PCMCIA_DEVICE_PROD_ID12 (" corega K . K .", " Wireless LAN PCCA - 11 ", 0x5261440f , 0xdf6115f9 ), <nl> + PCMCIA_DEVICE_PROD_ID12 (" corega_K . K .", " Wireless_LAN_PCCB - 11 ", 0x29e33311 , 0xee7a27ae ), <nl> PCMCIA_DEVICE_PROD_ID12 (" D ", " Link DRC - 650 11Mbps WLAN Card ", 0x71b18589 , 0xf144e3ac ), <nl> PCMCIA_DEVICE_PROD_ID12 (" D ", " Link DWL - 650 11Mbps WLAN Card ", 0x71b18589 , 0xb6f1b0ab ), <nl> PCMCIA_DEVICE_PROD_ID12 (" ELSA ", " AirLancer MC - 11 ", 0x4507a33a , 0xef54f0e3 ),
mmm drivers / staging / serqt_usb2 / serqt_usb2 . c <nl> ppp drivers / staging / serqt_usb2 / serqt_usb2 . c <nl> static void qt_read_bulk_callback ( struct urb * urb ) <nl> if ( port_paranoia_check ( port , __func__ ) != 0 ) { <nl> dbg ("% s - port_paranoia_check , exiting \ n ", __func__ ); <nl> qt_port -> ReadBulkStopped = 1 ; <nl> - return ; <nl> + goto exit ; <nl> } <nl>  <nl> if (! serial ) { <nl> dbg ("% s - bad serial pointer , exiting \ n ", __func__ ); <nl> - return ; <nl> + goto exit ; <nl> } <nl> if ( qt_port -> closePending == 1 ) { <nl> /* Were closing , stop reading */ <nl> dbg ("% s - ( qt_port -> closepending == 1 \ n ", __func__ ); <nl> qt_port -> ReadBulkStopped = 1 ; <nl> - return ; <nl> + goto exit ; <nl> } <nl>  <nl> /* <nl> static void qt_read_bulk_callback ( struct urb * urb ) <nl> */ <nl> if ( qt_port -> RxHolding == 1 ) { <nl> qt_port -> ReadBulkStopped = 1 ; <nl> - return ; <nl> + goto exit ; <nl> } <nl>  <nl> if ( urb -> status ) { <nl> static void qt_read_bulk_callback ( struct urb * urb ) <nl>  <nl> dbg ("% s - nonzero read bulk status received : % d \ n ", <nl> __func__ , urb -> status ); <nl> - return ; <nl> + goto exit ; <nl> } <nl>  <nl> if ( tty && RxCount ) { <nl> static void qt_read_bulk_callback ( struct urb * urb ) <nl> } <nl>  <nl> schedule_work (& port -> work ); <nl> + exit : <nl> + tty_kref_put ( tty ); <nl> } <nl>  <nl> /* <nl> static void qt_block_until_empty ( struct tty_struct * tty , <nl> } <nl> } <nl>  <nl> - static void qt_close ( struct usb_serial_port * port ) <nl> + static void qt_close ( struct usb_serial_port * port ) <nl> { <nl> struct usb_serial * serial = port -> serial ; <nl> struct quatech_port * qt_port ; <nl> static void qt_close ( struct usb_serial_port * port ) <nl> /* wait up to for transmitter to empty */ <nl> if ( serial -> dev ) <nl> qt_block_until_empty ( tty , qt_port ); <nl> + tty_kref_put ( tty ); <nl>  <nl> /* Close uart channel */ <nl> status = qt_close_channel ( serial , index );
mmm drivers / usb / host / ohci - sm501 . c <nl> ppp drivers / usb / host / ohci - sm501 . c <nl> static int ohci_hcd_sm501_drv_remove ( struct platform_device * pdev ) <nl> static int ohci_sm501_suspend ( struct platform_device * pdev , pm_message_t msg ) <nl> { <nl> struct device * dev = & pdev -> dev ; <nl> - struct ohci_hcd * ohci = hcd_to_ohci ( platform_get_drvdata ( pdev )); <nl> + struct usb_hcd * hcd = platform_get_drvdata ( pdev ); <nl> + struct ohci_hcd * ohci = hcd_to_ohci ( hcd ); <nl> + bool do_wakeup = device_may_wakeup ( dev ); <nl> + int ret ; <nl>  <nl> if ( time_before ( jiffies , ohci -> next_statechange )) <nl> msleep ( 5 ); <nl> ohci -> next_statechange = jiffies ; <nl>  <nl> + ret = ohci_suspend ( hcd , do_wakeup ); <nl> + if ( ret ) <nl> + return ret ; <nl> + <nl> sm501_unit_power ( dev -> parent , SM501_GATE_USB_HOST , 0 ); <nl> - return 0 ; <nl> + return ret ; <nl> } <nl>  <nl> static int ohci_sm501_resume ( struct platform_device * pdev )
mmm drivers / gpu / drm / etnaviv / etnaviv_gpu . c <nl> ppp drivers / gpu / drm / etnaviv / etnaviv_gpu . c <nl> int etnaviv_gpu_submit ( struct etnaviv_gpu * gpu , <nl> goto out_pm_put ; <nl> } <nl>  <nl> + mutex_lock (& gpu -> lock ); <nl> + <nl> fence = etnaviv_gpu_fence_alloc ( gpu ); <nl> if (! fence ) { <nl> event_free ( gpu , event ); <nl> int etnaviv_gpu_submit ( struct etnaviv_gpu * gpu , <nl> goto out_pm_put ; <nl> } <nl>  <nl> - mutex_lock (& gpu -> lock ); <nl> - <nl> gpu -> event [ event ]. fence = fence ; <nl> submit -> fence = fence -> seqno ; <nl> gpu -> active_fence = submit -> fence ;
mmm drivers / staging / rtl8188eu / hal / usb_halinit . c <nl> ppp drivers / staging / rtl8188eu / hal / usb_halinit . c <nl> void rtl8188eu_set_hal_ops ( struct adapter * adapt ) <nl>  <nl>  <nl> adapt -> HalData = kzalloc ( sizeof ( struct hal_data_8188e ), GFP_KERNEL ); <nl> - if ( adapt -> HalData == NULL ) <nl> + if (! adapt -> HalData ) <nl> DBG_88E (" cant not alloc memory for HAL DATA \ n "); <nl>  <nl> halfunc -> hal_power_on = rtl8188eu_InitPowerOn ;
mmm drivers / char / virtio_console . c <nl> ppp drivers / char / virtio_console . c <nl> static ssize_t port_fops_write ( struct file * filp , const char __user * ubuf , <nl> if ( ret < 0 ) <nl> return ret ; <nl> } <nl> + /* Port got hot - unplugged . */ <nl> + if (! port -> guest_connected ) <nl> + return - ENODEV ; <nl>  <nl> count = min (( size_t )( 32 * 1024 ), count ); <nl> 
mmm fs / nfsd / nfs4state . c <nl> ppp fs / nfsd / nfs4state . c <nl> nfsd4_create_session ( struct svc_rqst * rqstp , <nl> return status ; <nl> status = check_backchannel_attrs (& cr_ses -> back_channel ); <nl> if ( status ) <nl> - return status ; <nl> + goto out_release_drc_mem ; <nl> status = nfserr_jukebox ; <nl> new = alloc_session (& cr_ses -> fore_channel ); <nl> if (! new )
mmm drivers / net / hamradio / mkiss . c <nl> ppp drivers / net / hamradio / mkiss . c <nl> static int mkiss_ioctl ( struct tty_struct * tty , struct file * file , <nl> unsigned int cmd , unsigned long arg ) <nl> { <nl> struct mkiss * ax = mkiss_get ( tty ); <nl> - struct net_device * dev = ax -> dev ; <nl> + struct net_device * dev ; <nl> unsigned int tmp , err ; <nl>  <nl> /* First make sure we ' re connected . */ <nl> if ( ax == NULL ) <nl> return - ENXIO ; <nl> + dev = ax -> dev ; <nl>  <nl> switch ( cmd ) { <nl> case SIOCGIFNAME :
mmm drivers / gpu / drm / drm_fops . c <nl> ppp drivers / gpu / drm / drm_fops . c <nl> int drm_stub_open ( struct inode * inode , struct file * filp ) <nl>  <nl> old_fops = filp -> f_op ; <nl> filp -> f_op = fops_get (& dev -> driver -> fops ); <nl> + if ( filp -> f_op == NULL ) { <nl> + filp -> f_op = old_fops ; <nl> + goto out ; <nl> + } <nl> if ( filp -> f_op -> open && ( err = filp -> f_op -> open ( inode , filp ))) { <nl> fops_put ( filp -> f_op ); <nl> filp -> f_op = fops_get ( old_fops );mmm drivers / media / dvb / dvb - core / dvbdev . c <nl> ppp drivers / media / dvb / dvb - core / dvbdev . c <nl> int drm_stub_open ( struct inode * inode , struct file * filp ) <nl>  <nl> old_fops = filp -> f_op ; <nl> filp -> f_op = fops_get (& dev -> driver -> fops ); <nl> + if ( filp -> f_op == NULL ) { <nl> + filp -> f_op = old_fops ; <nl> + goto out ; <nl> + } <nl> if ( filp -> f_op -> open && ( err = filp -> f_op -> open ( inode , filp ))) { <nl> fops_put ( filp -> f_op ); <nl> filp -> f_op = fops_get ( old_fops ); <nl> static int dvb_device_open ( struct inode * inode , struct file * file ) <nl> file -> private_data = dvbdev ; <nl> old_fops = file -> f_op ; <nl> file -> f_op = fops_get ( dvbdev -> fops ); <nl> + if ( file -> f_op == NULL ) { <nl> + file -> f_op = old_fops ; <nl> + goto fail ; <nl> + } <nl> if ( file -> f_op -> open ) <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> static int dvb_device_open ( struct inode * inode , struct file * file ) <nl> unlock_kernel (); <nl> return err ; <nl> } <nl> + fail : <nl> up_read (& minor_rwsem ); <nl> unlock_kernel (); <nl> return - ENODEV ;mmm sound / core / sound . c <nl> ppp sound / core / sound . c <nl> int drm_stub_open ( struct inode * inode , struct file * filp ) <nl>  <nl> old_fops = filp -> f_op ; <nl> filp -> f_op = fops_get (& dev -> driver -> fops ); <nl> + if ( filp -> f_op == NULL ) { <nl> + filp -> f_op = old_fops ; <nl> + goto out ; <nl> + } <nl> if ( filp -> f_op -> open && ( err = filp -> f_op -> open ( inode , filp ))) { <nl> fops_put ( filp -> f_op ); <nl> filp -> f_op = fops_get ( old_fops ); <nl> static int dvb_device_open ( struct inode * inode , struct file * file ) <nl> file -> private_data = dvbdev ; <nl> old_fops = file -> f_op ; <nl> file -> f_op = fops_get ( dvbdev -> fops ); <nl> + if ( file -> f_op == NULL ) { <nl> + file -> f_op = old_fops ; <nl> + goto fail ; <nl> + } <nl> if ( file -> f_op -> open ) <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> static int dvb_device_open ( struct inode * inode , struct file * file ) <nl> unlock_kernel (); <nl> return err ; <nl> } <nl> + fail : <nl> up_read (& minor_rwsem ); <nl> unlock_kernel (); <nl> return - ENODEV ; <nl> static int __snd_open ( struct inode * inode , struct file * file ) <nl> } <nl> old_fops = file -> f_op ; <nl> file -> f_op = fops_get ( mptr -> f_ops ); <nl> + if ( file -> f_op == NULL ) { <nl> + file -> f_op = old_fops ; <nl> + return - ENODEV ; <nl> + } <nl> if ( file -> f_op -> open ) <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) {
mmm net / core / rtnetlink . c <nl> ppp net / core / rtnetlink . c <nl> static bool link_dump_filtered ( struct net_device * dev , <nl> return false ; <nl> } <nl>  <nl> - static struct net * get_target_net ( struct sk_buff * skb , int netnsid ) <nl> + static struct net * get_target_net ( struct sock * sk , int netnsid ) <nl> { <nl> struct net * net ; <nl>  <nl> - net = get_net_ns_by_id ( sock_net ( skb -> sk ), netnsid ); <nl> + net = get_net_ns_by_id ( sock_net ( sk ), netnsid ); <nl> if (! net ) <nl> return ERR_PTR (- EINVAL ); <nl>  <nl> /* For now , the caller is required to have CAP_NET_ADMIN in <nl> * the user namespace owning the target net ns . <nl> */ <nl> - if (! netlink_ns_capable ( skb , net -> user_ns , CAP_NET_ADMIN )) { <nl> + if (! sk_ns_capable ( sk , net -> user_ns , CAP_NET_ADMIN )) { <nl> put_net ( net ); <nl> return ERR_PTR (- EACCES ); <nl> } <nl> static int rtnl_dump_ifinfo ( struct sk_buff * skb , struct netlink_callback * cb ) <nl> ifla_policy , NULL ) >= 0 ) { <nl> if ( tb [ IFLA_IF_NETNSID ]) { <nl> netnsid = nla_get_s32 ( tb [ IFLA_IF_NETNSID ]); <nl> - tgt_net = get_target_net ( skb , netnsid ); <nl> + tgt_net = get_target_net ( skb -> sk , netnsid ); <nl> if ( IS_ERR ( tgt_net )) { <nl> tgt_net = net ; <nl> netnsid = - 1 ; <nl> static int rtnl_getlink ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl>  <nl> if ( tb [ IFLA_IF_NETNSID ]) { <nl> netnsid = nla_get_s32 ( tb [ IFLA_IF_NETNSID ]); <nl> - tgt_net = get_target_net ( skb , netnsid ); <nl> + tgt_net = get_target_net ( NETLINK_CB ( skb ). sk , netnsid ); <nl> if ( IS_ERR ( tgt_net )) <nl> return PTR_ERR ( tgt_net ); <nl> }
mmm drivers / staging / comedi / drivers / unioxx5 . c <nl> ppp drivers / staging / comedi / drivers / unioxx5 . c <nl> static int __unioxx5_subdev_init ( struct comedi_device * dev , <nl> return - ENOMEM ; <nl>  <nl> ret = __comedi_request_region ( dev , iobase , UNIOXX5_SIZE ); <nl> - if ( ret ) <nl> + if ( ret ) { <nl> + kfree ( usp ); <nl> return ret ; <nl> + } <nl> usp -> usp_iobase = iobase ; <nl>  <nl> /* defining modules types */
mmm drivers / soc / tegra / pmc . c <nl> ppp drivers / soc / tegra / pmc . c <nl> static int tegra_io_pad_prepare ( enum tegra_io_pad id , unsigned long * request , <nl> } <nl>  <nl> rate = clk_get_rate ( pmc -> clk ); <nl> + if (! rate ) <nl> + return - ENODEV ; <nl>  <nl> tegra_pmc_writel ( DPD_SAMPLE_ENABLE , DPD_SAMPLE ); <nl> 
mmm drivers / net / ethernet / atheros / atlx / atl2 . c <nl> ppp drivers / net / ethernet / atheros / atlx / atl2 . c <nl> static int atl2_probe ( struct pci_dev * pdev , const struct pci_device_id * ent ) <nl>  <nl> err = - EIO ; <nl>  <nl> - netdev -> hw_features = NETIF_F_SG | NETIF_F_HW_VLAN_CTAG_RX ; <nl> + netdev -> hw_features = NETIF_F_HW_VLAN_CTAG_RX ; <nl> netdev -> features |= ( NETIF_F_HW_VLAN_CTAG_TX | NETIF_F_HW_VLAN_CTAG_RX ); <nl>  <nl> /* Init PHY as early as possible due to power saving issue */
mmm drivers / net / ethernet / intel / ixgbe / ixgbe_main . c <nl> ppp drivers / net / ethernet / intel / ixgbe / ixgbe_main . c <nl> static void ixgbe_tx_map ( struct ixgbe_ring * tx_ring , <nl> tx_buffer_info -> dma = dma ; <nl>  <nl> tx_desc -> read . buffer_addr = cpu_to_le64 ( dma + offset ); <nl> + if ( unlikely ( skb -> no_fcs )) <nl> + cmd_type &= ~( cpu_to_le32 ( IXGBE_ADVTXD_DCMD_IFCS )); <nl> tx_desc -> read . cmd_type_len = cmd_type | cpu_to_le32 ( size ); <nl> tx_desc -> read . olinfo_status = olinfo_status ; <nl>  <nl> static int __devinit ixgbe_probe ( struct pci_dev * pdev , <nl> netdev -> vlan_features |= NETIF_F_SG ; <nl>  <nl> netdev -> priv_flags |= IFF_UNICAST_FLT ; <nl> + netdev -> priv_flags |= IFF_SUPP_NOFCS ; <nl>  <nl> if ( adapter -> flags & IXGBE_FLAG_SRIOV_ENABLED ) <nl> adapter -> flags &= ~( IXGBE_FLAG_RSS_ENABLED |
mmm drivers / cpufreq / cpufreq . c <nl> ppp drivers / cpufreq / cpufreq . c <nl> static int cpufreq_online ( unsigned int cpu ) <nl> if ( new_policy ) { <nl> /* related_cpus should at least include policy -> cpus . */ <nl> cpumask_copy ( policy -> related_cpus , policy -> cpus ); <nl> - /* Clear mask of registered CPUs */ <nl> - cpumask_clear ( policy -> real_cpus ); <nl> } <nl>  <nl> /*
mmm net / rxrpc / af_rxrpc . c <nl> ppp net / rxrpc / af_rxrpc . c <nl> struct rxrpc_call * rxrpc_kernel_begin_call ( struct socket * sock , <nl> struct rxrpc_transport * trans ; <nl> struct rxrpc_call * call ; <nl> struct rxrpc_sock * rx = rxrpc_sk ( sock -> sk ); <nl> + int ret ; <nl>  <nl> _enter (",,% x ,% lx ", key_serial ( key ), user_call_ID ); <nl>  <nl> + ret = rxrpc_validate_address ( rx , srx , sizeof (* srx )); <nl> + if ( ret < 0 ) <nl> + return ERR_PTR ( ret ); <nl> + <nl> lock_sock (& rx -> sk ); <nl>  <nl> if (! key )
mmm drivers / staging / sm750fb / sm750 . c <nl> ppp drivers / staging / sm750fb / sm750 . c <nl> static int lynxfb_ops_cursor ( struct fb_info * info , struct fb_cursor * fbcursor ) <nl> } <nl>  <nl> cursor -> disable ( cursor ); <nl> - if ( fbcursor -> set & FB_CUR_SETSIZE ) { <nl> + if ( fbcursor -> set & FB_CUR_SETSIZE ) <nl> cursor -> setSize ( cursor , fbcursor -> image . width , fbcursor -> image . height ); <nl> - } <nl>  <nl> if ( fbcursor -> set & FB_CUR_SETPOS ) { <nl> cursor -> setPos ( cursor , fbcursor -> image . dx - info -> var . xoffset , <nl> static int lynxfb_ops_cursor ( struct fb_info * info , struct fb_cursor * fbcursor ) <nl> fbcursor -> mask ); <nl> } <nl>  <nl> - if ( fbcursor -> enable ) { <nl> + if ( fbcursor -> enable ) <nl> cursor -> enable ( cursor ); <nl> - } <nl>  <nl> return 0 ; <nl> } <nl> static void lynxfb_ops_fillrect ( struct fb_info * info , const struct fb_fillrect * <nl> unsigned int base , pitch , Bpp , rop ; <nl> u32 color ; <nl>  <nl> - if ( info -> state != FBINFO_STATE_RUNNING ) { <nl> + if ( info -> state != FBINFO_STATE_RUNNING ) <nl> return ; <nl> - } <nl>  <nl> par = info -> par ; <nl> share = par -> share ;
mmm arch / openrisc / mm / init . c <nl> ppp arch / openrisc / mm / init . c <nl> static void __init map_ram ( void ) <nl> set_pmd ( pme , __pmd ( _KERNPG_TABLE + __pa ( pte ))); <nl>  <nl> /* Fill the newly allocated page with PTE ' S */ <nl> - for ( j = 0 ; p < e && j < PTRS_PER_PGD ; <nl> + for ( j = 0 ; p < e && j < PTRS_PER_PTE ; <nl> v += PAGE_SIZE , p += PAGE_SIZE , j ++, pte ++) { <nl> if ( v >= ( u32 ) _e_kernel_ro || <nl> v < ( u32 ) _s_kernel_ro )mmm arch / openrisc / include / asm / pgtable . h <nl> ppp arch / openrisc / include / asm / pgtable . h <nl> static void __init map_ram ( void ) <nl> set_pmd ( pme , __pmd ( _KERNPG_TABLE + __pa ( pte ))); <nl>  <nl> /* Fill the newly allocated page with PTE ' S */ <nl> - for ( j = 0 ; p < e && j < PTRS_PER_PGD ; <nl> + for ( j = 0 ; p < e && j < PTRS_PER_PTE ; <nl> v += PAGE_SIZE , p += PAGE_SIZE , j ++, pte ++) { <nl> if ( v >= ( u32 ) _e_kernel_ro || <nl> v < ( u32 ) _s_kernel_ro ) <nl> extern void paging_init ( void ); <nl> */ <nl> # define PTRS_PER_PTE ( 1UL << ( PAGE_SHIFT - 2 )) <nl>  <nl> -# define PTRS_PER_PGD ( 1UL << ( PAGE_SHIFT - 2 )) <nl> +# define PTRS_PER_PGD ( 1UL << ( 32 - PGDIR_SHIFT )) <nl>  <nl> /* calculate how many PGD entries a user - level program can use <nl> * the first mappable virtual address is 0
mmm drivers / base / firmware_class . c <nl> ppp drivers / base / firmware_class . c <nl> request_firmware ( const struct firmware ** firmware_p , const char * name , <nl> { <nl> return _request_firmware ( firmware_p , name , device , true , false ); <nl> } <nl> + EXPORT_SYMBOL ( request_firmware ); <nl>  <nl> /** <nl> * release_firmware : - release the resource associated with a firmware image <nl> void release_firmware ( const struct firmware * fw ) <nl> kfree ( fw ); <nl> } <nl> } <nl> + EXPORT_SYMBOL ( release_firmware ); <nl>  <nl> /* Async support */ <nl> struct firmware_work { <nl> request_firmware_nowait ( <nl> schedule_work (& fw_work -> work ); <nl> return 0 ; <nl> } <nl> + EXPORT_SYMBOL ( request_firmware_nowait ); <nl>  <nl> /** <nl> * cache_firmware - cache one firmware image in kernel memory space <nl> int cache_firmware ( const char * fw_name ) <nl>  <nl> return ret ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( cache_firmware ); <nl>  <nl> /** <nl> * uncache_firmware - remove one cached firmware image <nl> int uncache_firmware ( const char * fw_name ) <nl>  <nl> return - EINVAL ; <nl> } <nl> + EXPORT_SYMBOL_GPL ( uncache_firmware ); <nl>  <nl> # ifdef CONFIG_PM_SLEEP <nl> static ASYNC_DOMAIN_EXCLUSIVE ( fw_cache_domain ); <nl> static void __exit firmware_class_exit ( void ) <nl>  <nl> fs_initcall ( firmware_class_init ); <nl> module_exit ( firmware_class_exit ); <nl> - <nl> - EXPORT_SYMBOL ( release_firmware ); <nl> - EXPORT_SYMBOL ( request_firmware ); <nl> - EXPORT_SYMBOL ( request_firmware_nowait ); <nl> - EXPORT_SYMBOL_GPL ( cache_firmware ); <nl> - EXPORT_SYMBOL_GPL ( uncache_firmware );
mmm drivers / usb / mon / mon_text . c <nl> ppp drivers / usb / mon / mon_text . c <nl> int __init mon_text_init ( void ) <nl> { <nl> struct dentry * mondir ; <nl>  <nl> - mondir = debugfs_create_dir (" usbmon ", NULL ); <nl> + mondir = debugfs_create_dir (" usbmon ", usb_debug_root ); <nl> if ( IS_ERR ( mondir )) { <nl> printk ( KERN_NOTICE TAG ": debugfs is not available \ n "); <nl> return - ENODEV ;
mmm drivers / net / ethernet / intel / ixgbe / ixgbe_x550 . c <nl> ppp drivers / net / ethernet / intel / ixgbe / ixgbe_x550 . c <nl> static s32 ixgbe_setup_kr_x550em ( struct ixgbe_hw * hw ) <nl> if ( hw -> phy . autoneg_advertised & IXGBE_LINK_SPEED_2_5GB_FULL ) <nl> return 0 ; <nl>  <nl> + if ( ixgbe_check_reset_blocked ( hw )) <nl> + return 0 ; <nl> + <nl> return ixgbe_setup_kr_speed_x550em ( hw , hw -> phy . autoneg_advertised ); <nl> } <nl> 
mmm fs / btrfs / file . c <nl> ppp fs / btrfs / file . c <nl> int btrfs_add_inode_defrag ( struct btrfs_trans_handle * trans , <nl> spin_lock (& root -> fs_info -> defrag_inodes_lock ); <nl> if (! BTRFS_I ( inode )-> in_defrag ) <nl> __btrfs_add_inode_defrag ( inode , defrag ); <nl> + else <nl> + kfree ( defrag ); <nl> spin_unlock (& root -> fs_info -> defrag_inodes_lock ); <nl> return 0 ; <nl> }
mmm drivers / infiniband / hw / ipath / ipath_rc . c <nl> ppp drivers / infiniband / hw / ipath / ipath_rc . c <nl> static int do_rc_ack ( struct ipath_qp * qp , u32 aeth , u32 psn , int opcode , <nl> /* If this is a partial ACK , reset the retransmit timer . */ <nl> if ( qp -> s_last != qp -> s_tail ) { <nl> spin_lock (& dev -> pending_lock ); <nl> - list_add_tail (& qp -> timerwait , <nl> - & dev -> pending [ dev -> pending_index ]); <nl> + if ( list_empty (& qp -> timerwait )) <nl> + list_add_tail (& qp -> timerwait , <nl> + & dev -> pending [ dev -> pending_index ]); <nl> spin_unlock (& dev -> pending_lock ); <nl> /* <nl> * If we get a partial ACK for a resent operation ,
mmm net / ipv4 / netfilter / nf_flow_table_ipv4 . c <nl> ppp net / ipv4 / netfilter / nf_flow_table_ipv4 . c <nl> static int nf_flow_dnat_ip ( const struct flow_offload * flow , struct sk_buff * skb , <nl> default : <nl> return - 1 ; <nl> } <nl> + csum_replace4 (& iph -> check , addr , new_addr ); <nl>  <nl> return nf_flow_nat_ip_l4proto ( skb , iph , thoff , addr , new_addr ); <nl> }
mmm drivers / gpu / drm / i915 / intel_display . c <nl> ppp drivers / gpu / drm / i915 / intel_display . c <nl> int intel_get_load_detect_pipe ( struct drm_connector * connector , <nl> */ <nl> if (! crtc ) { <nl> DRM_DEBUG_KMS (" no pipe available for load - detect \ n "); <nl> + ret = - ENODEV ; <nl> goto fail ; <nl> } <nl>  <nl> int intel_get_load_detect_pipe ( struct drm_connector * connector , <nl> DRM_DEBUG_KMS (" reusing fbdev for load - detection framebuffer \ n "); <nl> if ( IS_ERR ( fb )) { <nl> DRM_DEBUG_KMS (" failed to allocate framebuffer for load - detection \ n "); <nl> + ret = PTR_ERR ( fb ); <nl> goto fail ; <nl> } <nl> 
mmm drivers / rpmsg / rpmsg_core . c <nl> ppp drivers / rpmsg / rpmsg_core . c <nl> static ssize_t modalias_show ( struct device * dev , <nl> struct device_attribute * attr , char * buf ) <nl> { <nl> struct rpmsg_device * rpdev = to_rpmsg_device ( dev ); <nl> + ssize_t len ; <nl> + <nl> + len = of_device_modalias ( dev , buf , PAGE_SIZE ); <nl> + if ( len != - ENODEV ) <nl> + return len ; <nl>  <nl> return sprintf ( buf , RPMSG_DEVICE_MODALIAS_FMT "\ n ", rpdev -> id . name ); <nl> } <nl> static int rpmsg_dev_match ( struct device * dev , struct device_driver * drv ) <nl> static int rpmsg_uevent ( struct device * dev , struct kobj_uevent_env * env ) <nl> { <nl> struct rpmsg_device * rpdev = to_rpmsg_device ( dev ); <nl> + int ret ; <nl> + <nl> + ret = of_device_uevent_modalias ( dev , env ); <nl> + if ( ret != - ENODEV ) <nl> + return ret ; <nl>  <nl> return add_uevent_var ( env , " MODALIAS =" RPMSG_DEVICE_MODALIAS_FMT , <nl> rpdev -> id . name );
mmm drivers / bluetooth / hci_bcm . c <nl> ppp drivers / bluetooth / hci_bcm . c <nl> static int bcm_close ( struct hci_uart * hu ) <nl> if ( IS_ENABLED ( CONFIG_PM ) && bdev -> irq > 0 ) { <nl> devm_free_irq ( bdev -> dev , bdev -> irq , bdev ); <nl> device_init_wakeup ( bdev -> dev , false ); <nl> + pm_runtime_disable ( bdev -> dev ); <nl> } <nl>  <nl> bcm_gpio_set_power ( bdev , false ); <nl> - pm_runtime_disable ( bdev -> dev ); <nl> pm_runtime_set_suspended ( bdev -> dev ); <nl> } <nl> mutex_unlock (& bcm_device_lock );
mmm drivers / usb / net / usbnet . c <nl> ppp drivers / usb / net / usbnet . c <nl> static int blan_mdlm_bind ( struct usbnet * dev , struct usb_interface * intf ) <nl> } <nl> /* expect bcdVersion 1 . 0 , ignore */ <nl> if ( memcmp (& desc -> bGUID , blan_guid , 16 ) <nl> - && memcmp (& desc -> bGUID , blan_guid , 16 ) ) { <nl> + && memcmp (& desc -> bGUID , safe_guid , 16 ) ) { <nl> /* hey , this one might _really_ be MDLM ! */ <nl> dev_dbg (& intf -> dev , " MDLM guid \ n "); <nl> goto bad_desc ;
mmm drivers / gpu / drm / atmel - hlcdc / atmel_hlcdc_dc . c <nl> ppp drivers / gpu / drm / atmel - hlcdc / atmel_hlcdc_dc . c <nl> static int atmel_hlcdc_dc_atomic_commit ( struct drm_device * dev , <nl> dc -> commit . pending = true ; <nl> spin_unlock (& dc -> commit . wait . lock ); <nl>  <nl> - if ( ret ) { <nl> - kfree ( commit ); <nl> - goto error ; <nl> - } <nl> + if ( ret ) <nl> + goto err_free ; <nl>  <nl> - /* Swap the state , this is the point of no return . */ <nl> - drm_atomic_helper_swap_state ( state , true ); <nl> + /* We have our own synchronization through the commit lock . */ <nl> + BUG_ON ( drm_atomic_helper_swap_state ( state , false ) < 0 ); <nl>  <nl> + /* Swap state succeeded , this is the point of no return . */ <nl> drm_atomic_state_get ( state ); <nl> if ( async ) <nl> queue_work ( dc -> wq , & commit -> work ); <nl> static int atmel_hlcdc_dc_atomic_commit ( struct drm_device * dev , <nl>  <nl> return 0 ; <nl>  <nl> + err_free : <nl> + kfree ( commit ); <nl> error : <nl> drm_atomic_helper_cleanup_planes ( dev , state ); <nl> return ret ;
mmm fs / btrfs / ioctl . c <nl> ppp fs / btrfs / ioctl . c <nl> static int btrfs_extent_same ( struct inode * src , u64 loff , u64 olen , <nl> inode_lock ( src ); <nl>  <nl> ret = extent_same_check_offsets ( src , loff , & len , olen ); <nl> + if ( ret ) <nl> + goto out_unlock ; <nl> + ret = extent_same_check_offsets ( src , dst_loff , & len , olen ); <nl> if ( ret ) <nl> goto out_unlock ; <nl> 
mmm drivers / staging / android / ion / ion_system_heap . c <nl> ppp drivers / staging / android / ion / ion_system_heap . c <nl> static struct page_info * alloc_largest_available ( struct ion_system_heap * heap , <nl> struct page_info * info ; <nl> int i ; <nl>  <nl> + info = kmalloc ( sizeof ( struct page_info ), GFP_KERNEL ); <nl> + if (! info ) <nl> + return NULL ; <nl> + <nl> for ( i = 0 ; i < num_orders ; i ++) { <nl> if ( size < order_to_size ( orders [ i ])) <nl> continue ; <nl> static struct page_info * alloc_largest_available ( struct ion_system_heap * heap , <nl> if (! page ) <nl> continue ; <nl>  <nl> - info = kmalloc ( sizeof ( struct page_info ), GFP_KERNEL ); <nl> info -> page = page ; <nl> info -> order = orders [ i ]; <nl> return info ; <nl> } <nl> + kfree ( info ); <nl> + <nl> return NULL ; <nl> } <nl> 
mmm crypto / shash . c <nl> ppp crypto / shash . c <nl> static int shash_update_unaligned ( struct shash_desc * desc , const u8 * data , <nl> u8 buf [ shash_align_buffer_size ( unaligned_len , alignmask )] <nl> __attribute__ (( aligned )); <nl>  <nl> + if ( unaligned_len > len ) <nl> + unaligned_len = len ; <nl> + <nl> memcpy ( buf , data , unaligned_len ); <nl>  <nl> return shash -> update ( desc , buf , unaligned_len ) ?:
mmm drivers / gpu / drm / exynos / exynos_drm_dmabuf . c <nl> ppp drivers / gpu / drm / exynos / exynos_drm_dmabuf . c <nl> struct dma_buf * exynos_dmabuf_prime_export ( struct drm_device * drm_dev , <nl> struct exynos_drm_gem_obj * exynos_gem_obj = to_exynos_gem_obj ( obj ); <nl>  <nl> return dma_buf_export ( exynos_gem_obj , & exynos_dmabuf_ops , <nl> - exynos_gem_obj -> base . size , 0600 ); <nl> + exynos_gem_obj -> base . size , flags ); <nl> } <nl>  <nl> struct drm_gem_object * exynos_dmabuf_prime_import ( struct drm_device * drm_dev ,
mmm drivers / tty / serial / atmel_serial . c <nl> ppp drivers / tty / serial / atmel_serial . c <nl> static int atmel_serial_remove ( struct platform_device * pdev ) <nl> struct atmel_uart_port * atmel_port = to_atmel_uart_port ( port ); <nl> int ret = 0 ; <nl>  <nl> + tasklet_kill (& atmel_port -> tasklet ); <nl> + <nl> device_init_wakeup (& pdev -> dev , 0 ); <nl>  <nl> ret = uart_remove_one_port (& atmel_uart , port ); <nl>  <nl> - tasklet_kill (& atmel_port -> tasklet ); <nl> kfree ( atmel_port -> rx_ring . buf ); <nl>  <nl> /* " port " is allocated statically , so we shouldn ' t free it */
mmm drivers / md / raid1 . c <nl> ppp drivers / md / raid1 . c <nl> int md_raid1_congested ( struct mddev * mddev , int bits ) <nl> return 1 ; <nl>  <nl> rcu_read_lock (); <nl> - for ( i = 0 ; i < conf -> raid_disks ; i ++) { <nl> + for ( i = 0 ; i < conf -> raid_disks * 2 ; i ++) { <nl> struct md_rdev * rdev = rcu_dereference ( conf -> mirrors [ i ]. rdev ); <nl> if ( rdev && ! test_bit ( Faulty , & rdev -> flags )) { <nl> struct request_queue * q = bdev_get_queue ( rdev -> bdev );
mmm fs / namespace . c <nl> ppp fs / namespace . c <nl> void __detach_mounts ( struct dentry * dentry ) <nl>  <nl> namespace_lock (); <nl> mp = lookup_mountpoint ( dentry ); <nl> - if (! mp ) <nl> + if ( IS_ERR_OR_NULL ( mp )) <nl> goto out_unlock ; <nl>  <nl> lock_mount_hash ();
mmm fs / nfs / nfs4xdr . c <nl> ppp fs / nfs / nfs4xdr . c <nl> static int decode_attr_fs_locations ( struct xdr_stream * xdr , uint32_t * bitmap , st <nl> status = 0 ; <nl> if ( unlikely (!( bitmap [ 0 ] & FATTR4_WORD0_FS_LOCATIONS ))) <nl> goto out ; <nl> + bitmap [ 0 ] &= ~ FATTR4_WORD0_FS_LOCATIONS ; <nl> status = - EIO ; <nl> /* Ignore borken servers that return unrequested attrs */ <nl> if ( unlikely ( res == NULL ))
mmm fs / isofs / rock . c <nl> ppp fs / isofs / rock . c <nl> struct rock_state { <nl> int cont_size ; <nl> int cont_extent ; <nl> int cont_offset ; <nl> + int cont_loops ; <nl> struct inode * inode ; <nl> }; <nl>  <nl> static void init_rock_state ( struct rock_state * rs , struct inode * inode ) <nl> rs -> inode = inode ; <nl> } <nl>  <nl> +/* Maximum number of Rock Ridge continuation entries */ <nl> +# define RR_MAX_CE_ENTRIES 32 <nl> + <nl> /* <nl> * Returns 0 if the caller should continue scanning , 1 if the scan must end <nl> * and - ve on error . <nl> static int rock_continue ( struct rock_state * rs ) <nl> goto out ; <nl> } <nl> ret = - EIO ; <nl> + if (++ rs -> cont_loops >= RR_MAX_CE_ENTRIES ) <nl> + goto out ; <nl> bh = sb_bread ( rs -> inode -> i_sb , rs -> cont_extent ); <nl> if ( bh ) { <nl> memcpy ( rs -> buffer , bh -> b_data + rs -> cont_offset ,
mmm lib / mpi / mpi - pow . c <nl> ppp lib / mpi / mpi - pow . c <nl> int mpi_powm ( MPI res , MPI base , MPI exp , MPI mod ) <nl> if (! esize ) { <nl> /* Exponent is zero , result is 1 mod MOD , i . e ., 1 or 0 <nl> * depending on if MOD equals 1 . */ <nl> - rp [ 0 ] = 1 ; <nl> res -> nlimbs = ( msize == 1 && mod -> d [ 0 ] == 1 ) ? 0 : 1 ; <nl> + if ( res -> nlimbs ) { <nl> + if ( mpi_resize ( res , 1 ) < 0 ) <nl> + goto enomem ; <nl> + rp = res -> d ; <nl> + rp [ 0 ] = 1 ; <nl> + } <nl> res -> sign = 0 ; <nl> goto leave ; <nl> }
mmm net / wireless / radiotap . c <nl> ppp net / wireless / radiotap . c <nl> int ieee80211_radiotap_iterator_init ( <nl> struct ieee80211_radiotap_header * radiotap_header , <nl> int max_length , const struct ieee80211_radiotap_vendor_namespaces * vns ) <nl> { <nl> + /* check the radiotap header can actually be present */ <nl> + if ( max_length < sizeof ( struct ieee80211_radiotap_header )) <nl> + return - EINVAL ; <nl> + <nl> /* Linux only supports version 0 radiotap format */ <nl> if ( radiotap_header -> it_version ) <nl> return - EINVAL ; <nl> int ieee80211_radiotap_iterator_init ( <nl> */ <nl>  <nl> if (( unsigned long ) iterator -> _arg - <nl> - ( unsigned long ) iterator -> _rtheader > <nl> + ( unsigned long ) iterator -> _rtheader + <nl> + sizeof ( uint32_t ) > <nl> ( unsigned long ) iterator -> _max_length ) <nl> return - EINVAL ; <nl> }
mmm tools / perf / util / symbol . c <nl> ppp tools / perf / util / symbol . c <nl> static const char * const vmlinux_paths_upd [] = { <nl> "/ boot / vmlinux -% s ", <nl> "/ usr / lib / debug / boot / vmlinux -% s ", <nl> "/ lib / modules /% s / build / vmlinux ", <nl> - "/ usr / lib / debug / lib / modules /% s / vmlinux " <nl> + "/ usr / lib / debug / lib / modules /% s / vmlinux ", <nl> + "/ usr / lib / debug / boot / vmlinux -% s . debug " <nl> }; <nl>  <nl> static int vmlinux_path__add ( const char * new_entry )
mmm drivers / staging / rtl8188eu / core / rtw_efuse . c <nl> ppp drivers / staging / rtl8188eu / core / rtw_efuse . c <nl> int Efuse_PgPacketRead ( struct adapter * pAdapter , u8 offset , u8 * data ) <nl> u8 hoffset = 0 , hworden = 0 ; <nl> u8 tmpidx = 0 ; <nl> u8 tmpdata [ 8 ]; <nl> - u8 max_section = 0 ; <nl> u8 tmp_header = 0 ; <nl>  <nl> - EFUSE_GetEfuseDefinition ( pAdapter , EFUSE_WIFI , TYPE_EFUSE_MAX_SECTION , ( void *)& max_section ); <nl> - <nl> if (! data ) <nl> return false ; <nl> - if ( offset > max_section ) <nl> + if ( offset > EFUSE_MAX_SECTION_88E ) <nl> return false ; <nl>  <nl> memset ( data , 0xff , sizeof ( u8 ) * PGPKT_DATA_SIZE );
mmm drivers / gpu / drm / vmwgfx / vmwgfx_kms . c <nl> ppp drivers / gpu / drm / vmwgfx / vmwgfx_kms . c <nl> int vmw_du_page_flip ( struct drm_crtc * crtc , <nl> struct vmw_private * dev_priv = vmw_priv ( crtc -> dev ); <nl> struct drm_framebuffer * old_fb = crtc -> fb ; <nl> struct vmw_framebuffer * vfb = vmw_framebuffer_to_vfb ( fb ); <nl> - struct drm_file * file_priv = event -> base . file_priv ; <nl> + struct drm_file * file_priv ; <nl> struct vmw_fence_obj * fence = NULL ; <nl> struct drm_clip_rect clips ; <nl> int ret ; <nl>  <nl> + if ( event == NULL ) <nl> + return - EINVAL ; <nl> + <nl> /* require ScreenObject support for page flipping */ <nl> if (! dev_priv -> sou_priv ) <nl> return - ENOSYS ; <nl>  <nl> + file_priv = event -> base . file_priv ; <nl> if (! vmw_kms_screen_object_flippable ( dev_priv , crtc )) <nl> return - EINVAL ; <nl> 
mmm drivers / usb / net / zd1201 . c <nl> ppp drivers / usb / net / zd1201 . c <nl> static int zd1201_resume ( struct usb_interface * interface ) <nl> { <nl> struct zd1201 * zd = usb_get_intfdata ( interface ); <nl>  <nl> + if (! zd || ! zd -> dev ) <nl> + return - ENODEV ; <nl> + <nl> netif_device_attach ( zd -> dev ); <nl>  <nl> if ( zd -> was_enabled )
mmm fs / btrfs / backref . c <nl> ppp fs / btrfs / backref . c <nl> static int find_parent_nodes ( struct btrfs_trans_handle * trans , <nl> } <nl> ret = find_extent_in_eb ( eb , bytenr , <nl> * extent_item_pos , & eie ); <nl> - ref -> inode_list = eie ; <nl> free_extent_buffer ( eb ); <nl> + if ( ret < 0 ) <nl> + goto out ; <nl> + ref -> inode_list = eie ; <nl> } <nl> ret = ulist_add_merge ( refs , ref -> parent , <nl> ( uintptr_t ) ref -> inode_list ,
mmm fs / dax . c <nl> ppp fs / dax . c <nl> static int copy_user_dax ( struct block_device * bdev , struct dax_device * dax_dev , <nl> static void * dax_insert_mapping_entry ( struct address_space * mapping , <nl> struct vm_fault * vmf , <nl> void * entry , sector_t sector , <nl> - unsigned long flags ) <nl> + unsigned long flags , bool dirty ) <nl> { <nl> struct radix_tree_root * page_tree = & mapping -> page_tree ; <nl> void * new_entry ; <nl> pgoff_t index = vmf -> pgoff ; <nl>  <nl> - if ( vmf -> flags & FAULT_FLAG_WRITE ) <nl> + if ( dirty ) <nl> __mark_inode_dirty ( mapping -> host , I_DIRTY_PAGES ); <nl>  <nl> if ( dax_is_zero_entry ( entry ) && !( flags & RADIX_DAX_ZERO_PAGE )) { <nl> static void * dax_insert_mapping_entry ( struct address_space * mapping , <nl> entry = new_entry ; <nl> } <nl>  <nl> - if ( vmf -> flags & FAULT_FLAG_WRITE ) <nl> + if ( dirty ) <nl> radix_tree_tag_set ( page_tree , index , PAGECACHE_TAG_DIRTY ); <nl>  <nl> spin_unlock_irq (& mapping -> tree_lock ); <nl> static int dax_load_hole ( struct address_space * mapping , void * entry , <nl> } <nl>  <nl> entry2 = dax_insert_mapping_entry ( mapping , vmf , entry , 0 , <nl> - RADIX_DAX_ZERO_PAGE ); <nl> + RADIX_DAX_ZERO_PAGE , false ); <nl> if ( IS_ERR ( entry2 )) { <nl> ret = VM_FAULT_SIGBUS ; <nl> goto out ; <nl> static int dax_iomap_pte_fault ( struct vm_fault * vmf , pfn_t * pfnp , <nl>  <nl> entry = dax_insert_mapping_entry ( mapping , vmf , entry , <nl> dax_iomap_sector (& iomap , pos ), <nl> - 0 ); <nl> + 0 , write ); <nl> if ( IS_ERR ( entry )) { <nl> error = PTR_ERR ( entry ); <nl> goto error_finish_iomap ; <nl> static int dax_pmd_load_hole ( struct vm_fault * vmf , struct iomap * iomap , <nl> goto fallback ; <nl>  <nl> ret = dax_insert_mapping_entry ( mapping , vmf , entry , 0 , <nl> - RADIX_DAX_PMD | RADIX_DAX_ZERO_PAGE ); <nl> + RADIX_DAX_PMD | RADIX_DAX_ZERO_PAGE , false ); <nl> if ( IS_ERR ( ret )) <nl> goto fallback ; <nl>  <nl> static int dax_iomap_pmd_fault ( struct vm_fault * vmf , pfn_t * pfnp , <nl>  <nl> entry = dax_insert_mapping_entry ( mapping , vmf , entry , <nl> dax_iomap_sector (& iomap , pos ), <nl> - RADIX_DAX_PMD ); <nl> + RADIX_DAX_PMD , write ); <nl> if ( IS_ERR ( entry )) <nl> goto finish_iomap ; <nl> 
mmm drivers / iio / industrialio - buffer . c <nl> ppp drivers / iio / industrialio - buffer . c <nl> int iio_buffer_register ( struct iio_dev * indio_dev , <nl> if ( channels ) { <nl> /* new magic */ <nl> for ( i = 0 ; i < num_channels ; i ++) { <nl> + if ( channels [ i ]. scan_index < 0 ) <nl> + continue ; <nl> + <nl> /* Establish necessary mask length */ <nl> if ( channels [ i ]. scan_index > <nl> ( int ) indio_dev -> masklength - 1 )
mmm drivers / net / wireless / ti / wlcore / cmd . c <nl> ppp drivers / net / wireless / ti / wlcore / cmd . c <nl> static int __wlcore_cmd_send ( struct wl1271 * wl , u16 id , void * buf , <nl> id != CMD_STOP_FWLOGGER )) <nl> return - EIO ; <nl>  <nl> + if ( WARN_ON_ONCE ( len < sizeof (* cmd ))) <nl> + return - EIO ; <nl> + <nl> cmd = buf ; <nl> cmd -> id = cpu_to_le16 ( id ); <nl> cmd -> status = 0 ; <nl> int wlcore_cmd_configure_failsafe ( struct wl1271 * wl , u16 id , void * buf , <nl>  <nl> wl1271_debug ( DEBUG_CMD , " cmd configure (% d )", id ); <nl>  <nl> + if ( WARN_ON_ONCE ( len < sizeof (* acx ))) <nl> + return - EIO ; <nl> + <nl> acx -> id = cpu_to_le16 ( id ); <nl>  <nl> /* payload length , does not include any headers */
mmm drivers / edac / i82975x_edac . c <nl> ppp drivers / edac / i82975x_edac . c <nl> static int i82975x_probe1 ( struct pci_dev * pdev , int dev_idx ) <nl> } <nl> mchbar &= 0xffffc000 ; /* bits 31 : 14 used for 16K window */ <nl> mch_window = ioremap_nocache ( mchbar , 0x1000 ); <nl> + if (! mch_window ) { <nl> + edac_dbg ( 3 , " error ioremapping MCHBAR !\ n "); <nl> + goto fail0 ; <nl> + } <nl>  <nl> # ifdef i82975x_DEBUG_IOMEM <nl> i82975x_printk ( KERN_INFO , " MCHBAR real = % 0x , remapped = % p \ n ",
mmm drivers / media / video / gspca / ov519 . c <nl> ppp drivers / media / video / gspca / ov519 . c <nl> static int sd_config ( struct gspca_dev * gspca_dev , <nl> break ; <nl> } <nl> sd -> brightness = BRIGHTNESS_DEF ; <nl> - sd -> contrast = CONTRAST_DEF ; <nl> + if ( sd -> sensor == SEN_OV6630 || sd -> sensor == SEN_OV66308AF ) <nl> + sd -> contrast = 200 ; /* The default is too low for the ov6630 */ <nl> + else <nl> + sd -> contrast = CONTRAST_DEF ; <nl> sd -> colors = COLOR_DEF ; <nl> sd -> hflip = HFLIP_DEF ; <nl> sd -> vflip = VFLIP_DEF ;
mmm drivers / scsi / megaraid / mega_common . h <nl> ppp drivers / scsi / megaraid / mega_common . h <nl> typedef struct { <nl> uint8_t max_lun ; <nl>  <nl> uint32_t unique_id ; <nl> - uint8_t irq ; <nl> + int irq ; <nl> uint8_t ito ; <nl> caddr_t ibuf ; <nl> dma_addr_t ibuf_dma_h ;
mmm fs / btrfs / send . c <nl> ppp fs / btrfs / send . c <nl> long btrfs_ioctl_send ( struct file * mnt_file , void __user * arg_ ) <nl> goto out ; <nl> } <nl>  <nl> + if ( arg -> clone_sources_count > <nl> + ULLONG_MAX / sizeof (* arg -> clone_sources )) { <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> + } <nl> + <nl> if (! access_ok ( VERIFY_READ , arg -> clone_sources , <nl> sizeof (* arg -> clone_sources ) * <nl> arg -> clone_sources_count )) {
mmm drivers / infiniband / core / uverbs_ioctl . c <nl> ppp drivers / infiniband / core / uverbs_ioctl . c <nl> static int uverbs_validate_kernel_mandatory ( const struct uverbs_method_spec * met <nl> return - EINVAL ; <nl> } <nl>  <nl> + for (; i < method_spec -> num_buckets ; i ++) { <nl> + struct uverbs_attr_spec_hash * attr_spec_bucket = <nl> + method_spec -> attr_buckets [ i ]; <nl> + <nl> + if (! bitmap_empty ( attr_spec_bucket -> mandatory_attrs_bitmask , <nl> + attr_spec_bucket -> num_attrs )) <nl> + return - EINVAL ; <nl> + } <nl> + <nl> return 0 ; <nl> } <nl> 
mmm drivers / media / tuners / r820t . c <nl> ppp drivers / media / tuners / r820t . c <nl> static int r820t_set_tv_standard ( struct r820t_priv * priv , <nl> return rc ; <nl> msleep ( 1 ); <nl> } <nl> - priv -> int_freq = if_khz ; <nl> + priv -> int_freq = if_khz * 1000 ; <nl>  <nl> /* Check if standard changed . If so , filter calibration is needed */ <nl> if ( type != priv -> type )
mmm drivers / infiniband / hw / mlx5 / main . c <nl> ppp drivers / infiniband / hw / mlx5 / main . c <nl> static int get_port_caps ( struct mlx5_ib_dev * dev ) <nl> struct ib_device_attr * dprops = NULL ; <nl> struct ib_port_attr * pprops = NULL ; <nl> struct mlx5_general_caps * gen ; <nl> - int err = 0 ; <nl> + int err = - ENOMEM ; <nl> int port ; <nl>  <nl> gen = & dev -> mdev -> caps . gen ;
mmm fs / btrfs / free - space - cache . c <nl> ppp fs / btrfs / free - space - cache . c <nl> void btrfs_dump_free_space ( struct btrfs_block_group_cache * block_group , <nl>  <nl> for ( n = rb_first (& ctl -> free_space_offset ); n ; n = rb_next ( n )) { <nl> info = rb_entry ( n , struct btrfs_free_space , offset_index ); <nl> - if ( info -> bytes >= bytes ) <nl> + if ( info -> bytes >= bytes && ! block_group -> ro ) <nl> count ++; <nl> printk ( KERN_CRIT " entry offset % llu , bytes % llu , bitmap % s \ n ", <nl> ( unsigned long long ) info -> offset ,
mmm drivers / gpu / drm / radeon / radeon_device . c <nl> ppp drivers / gpu / drm / radeon / radeon_device . c <nl> int radeon_device_init ( struct radeon_device * rdev , <nl> mutex_init (& rdev -> gem . mutex ); <nl> mutex_init (& rdev -> pm . mutex ); <nl> mutex_init (& rdev -> gpu_clock_mutex ); <nl> + mutex_init (& rdev -> srbm_mutex ); <nl> init_rwsem (& rdev -> pm . mclk_lock ); <nl> init_rwsem (& rdev -> exclusive_lock ); <nl> init_waitqueue_head (& rdev -> irq . vblank_queue );mmm drivers / gpu / drm / radeon / cik . c <nl> ppp drivers / gpu / drm / radeon / cik . c <nl> int radeon_device_init ( struct radeon_device * rdev , <nl> mutex_init (& rdev -> gem . mutex ); <nl> mutex_init (& rdev -> pm . mutex ); <nl> mutex_init (& rdev -> gpu_clock_mutex ); <nl> + mutex_init (& rdev -> srbm_mutex ); <nl> init_rwsem (& rdev -> pm . mclk_lock ); <nl> init_rwsem (& rdev -> exclusive_lock ); <nl> init_waitqueue_head (& rdev -> irq . vblank_queue ); <nl> u32 cik_compute_ring_get_rptr ( struct radeon_device * rdev , <nl> if ( rdev -> wb . enabled ) { <nl> rptr = le32_to_cpu ( rdev -> wb . wb [ ring -> rptr_offs / 4 ]); <nl> } else { <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> cik_srbm_select ( rdev , ring -> me , ring -> pipe , ring -> queue , 0 ); <nl> rptr = RREG32 ( CP_HQD_PQ_RPTR ); <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl> } <nl> rptr = ( rptr & ring -> ptr_reg_mask ) >> ring -> ptr_reg_shift ; <nl>  <nl> u32 cik_compute_ring_get_wptr ( struct radeon_device * rdev , <nl> if ( rdev -> wb . enabled ) { <nl> wptr = le32_to_cpu ( rdev -> wb . wb [ ring -> wptr_offs / 4 ]); <nl> } else { <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> cik_srbm_select ( rdev , ring -> me , ring -> pipe , ring -> queue , 0 ); <nl> wptr = RREG32 ( CP_HQD_PQ_WPTR ); <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl> } <nl> wptr = ( wptr & ring -> ptr_reg_mask ) >> ring -> ptr_reg_shift ; <nl>  <nl> static int cik_cp_compute_resume ( struct radeon_device * rdev ) <nl> WREG32 ( CP_CPF_DEBUG , tmp ); <nl>  <nl> /* init the pipes */ <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> for ( i = 0 ; i < ( rdev -> mec . num_pipe * rdev -> mec . num_mec ); i ++) { <nl> int me = ( i < 4 ) ? 1 : 2 ; <nl> int pipe = ( i < 4 ) ? i : ( i - 4 ); <nl> static int cik_cp_compute_resume ( struct radeon_device * rdev ) <nl> WREG32 ( CP_HPD_EOP_CONTROL , tmp ); <nl> } <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl>  <nl> /* init the queues . Just two for now . */ <nl> for ( i = 0 ; i < 2 ; i ++) { <nl> static int cik_cp_compute_resume ( struct radeon_device * rdev ) <nl> mqd -> static_thread_mgmt23 [ 0 ] = 0xffffffff ; <nl> mqd -> static_thread_mgmt23 [ 1 ] = 0xffffffff ; <nl>  <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> cik_srbm_select ( rdev , rdev -> ring [ idx ]. me , <nl> rdev -> ring [ idx ]. pipe , <nl> rdev -> ring [ idx ]. queue , 0 ); <nl> static int cik_cp_compute_resume ( struct radeon_device * rdev ) <nl> WREG32 ( CP_HQD_ACTIVE , mqd -> queue_state . cp_hqd_active ); <nl>  <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl>  <nl> radeon_bo_kunmap ( rdev -> ring [ idx ]. mqd_obj ); <nl> radeon_bo_unreserve ( rdev -> ring [ idx ]. mqd_obj ); <nl> static int cik_pcie_gart_enable ( struct radeon_device * rdev ) <nl>  <nl> /* XXX SH_MEM regs */ <nl> /* where to put LDS , scratch , GPUVM in FSA64 space */ <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> for ( i = 0 ; i < 16 ; i ++) { <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , i ); <nl> /* CP and shaders */ <nl> static int cik_pcie_gart_enable ( struct radeon_device * rdev ) <nl> /* XXX SDMA RLC - todo */ <nl> } <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl>  <nl> cik_pcie_gart_tlb_flush ( rdev ); <nl> DRM_INFO (" PCIE GART of % uM enabled ( table at 0x % 016llX ).\ n ",mmm drivers / gpu / drm / radeon / radeon . h <nl> ppp drivers / gpu / drm / radeon / radeon . h <nl> int radeon_device_init ( struct radeon_device * rdev , <nl> mutex_init (& rdev -> gem . mutex ); <nl> mutex_init (& rdev -> pm . mutex ); <nl> mutex_init (& rdev -> gpu_clock_mutex ); <nl> + mutex_init (& rdev -> srbm_mutex ); <nl> init_rwsem (& rdev -> pm . mclk_lock ); <nl> init_rwsem (& rdev -> exclusive_lock ); <nl> init_waitqueue_head (& rdev -> irq . vblank_queue ); <nl> u32 cik_compute_ring_get_rptr ( struct radeon_device * rdev , <nl> if ( rdev -> wb . enabled ) { <nl> rptr = le32_to_cpu ( rdev -> wb . wb [ ring -> rptr_offs / 4 ]); <nl> } else { <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> cik_srbm_select ( rdev , ring -> me , ring -> pipe , ring -> queue , 0 ); <nl> rptr = RREG32 ( CP_HQD_PQ_RPTR ); <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl> } <nl> rptr = ( rptr & ring -> ptr_reg_mask ) >> ring -> ptr_reg_shift ; <nl>  <nl> u32 cik_compute_ring_get_wptr ( struct radeon_device * rdev , <nl> if ( rdev -> wb . enabled ) { <nl> wptr = le32_to_cpu ( rdev -> wb . wb [ ring -> wptr_offs / 4 ]); <nl> } else { <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> cik_srbm_select ( rdev , ring -> me , ring -> pipe , ring -> queue , 0 ); <nl> wptr = RREG32 ( CP_HQD_PQ_WPTR ); <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl> } <nl> wptr = ( wptr & ring -> ptr_reg_mask ) >> ring -> ptr_reg_shift ; <nl>  <nl> static int cik_cp_compute_resume ( struct radeon_device * rdev ) <nl> WREG32 ( CP_CPF_DEBUG , tmp ); <nl>  <nl> /* init the pipes */ <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> for ( i = 0 ; i < ( rdev -> mec . num_pipe * rdev -> mec . num_mec ); i ++) { <nl> int me = ( i < 4 ) ? 1 : 2 ; <nl> int pipe = ( i < 4 ) ? i : ( i - 4 ); <nl> static int cik_cp_compute_resume ( struct radeon_device * rdev ) <nl> WREG32 ( CP_HPD_EOP_CONTROL , tmp ); <nl> } <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl>  <nl> /* init the queues . Just two for now . */ <nl> for ( i = 0 ; i < 2 ; i ++) { <nl> static int cik_cp_compute_resume ( struct radeon_device * rdev ) <nl> mqd -> static_thread_mgmt23 [ 0 ] = 0xffffffff ; <nl> mqd -> static_thread_mgmt23 [ 1 ] = 0xffffffff ; <nl>  <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> cik_srbm_select ( rdev , rdev -> ring [ idx ]. me , <nl> rdev -> ring [ idx ]. pipe , <nl> rdev -> ring [ idx ]. queue , 0 ); <nl> static int cik_cp_compute_resume ( struct radeon_device * rdev ) <nl> WREG32 ( CP_HQD_ACTIVE , mqd -> queue_state . cp_hqd_active ); <nl>  <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl>  <nl> radeon_bo_kunmap ( rdev -> ring [ idx ]. mqd_obj ); <nl> radeon_bo_unreserve ( rdev -> ring [ idx ]. mqd_obj ); <nl> static int cik_pcie_gart_enable ( struct radeon_device * rdev ) <nl>  <nl> /* XXX SH_MEM regs */ <nl> /* where to put LDS , scratch , GPUVM in FSA64 space */ <nl> + mutex_lock (& rdev -> srbm_mutex ); <nl> for ( i = 0 ; i < 16 ; i ++) { <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , i ); <nl> /* CP and shaders */ <nl> static int cik_pcie_gart_enable ( struct radeon_device * rdev ) <nl> /* XXX SDMA RLC - todo */ <nl> } <nl> cik_srbm_select ( rdev , 0 , 0 , 0 , 0 ); <nl> + mutex_unlock (& rdev -> srbm_mutex ); <nl>  <nl> cik_pcie_gart_tlb_flush ( rdev ); <nl> DRM_INFO (" PCIE GART of % uM enabled ( table at 0x % 016llX ).\ n ", <nl> struct radeon_device { <nl> /* ACPI interface */ <nl> struct radeon_atif atif ; <nl> struct radeon_atcs atcs ; <nl> + /* srbm instance registers */ <nl> + struct mutex srbm_mutex ; <nl> }; <nl>  <nl> int radeon_device_init ( struct radeon_device * rdev ,
mmm drivers / staging / wilc1000 / wilc_wfi_cfgoperations . c <nl> ppp drivers / staging / wilc1000 / wilc_wfi_cfgoperations . c <nl> int WILC_WFI_mgmt_tx ( struct wiphy * wiphy , <nl> mgmt_tx -> buff = WILC_MALLOC ( buf_len ); <nl> if ( mgmt_tx -> buff == NULL ) { <nl> PRINT_ER (" Failed to allocate memory for mgmt_tx buff \ n "); <nl> + kfree ( mgmt_tx ); <nl> return WILC_FAIL ; <nl> } <nl> memcpy ( mgmt_tx -> buff , buf , len );mmm drivers / staging / wilc1000 / linux_mon . c <nl> ppp drivers / staging / wilc1000 / linux_mon . c <nl> int WILC_WFI_mgmt_tx ( struct wiphy * wiphy , <nl> mgmt_tx -> buff = WILC_MALLOC ( buf_len ); <nl> if ( mgmt_tx -> buff == NULL ) { <nl> PRINT_ER (" Failed to allocate memory for mgmt_tx buff \ n "); <nl> + kfree ( mgmt_tx ); <nl> return WILC_FAIL ; <nl> } <nl> memcpy ( mgmt_tx -> buff , buf , len ); <nl> static int mon_mgmt_tx ( struct net_device * dev , const u8 * buf , size_t len ) <nl> mgmt_tx -> buff = kmalloc ( len , GFP_ATOMIC ); <nl> if ( mgmt_tx -> buff == NULL ) { <nl> PRINT_ER (" Failed to allocate memory for mgmt_tx buff \ n "); <nl> + kfree ( mgmt_tx ); <nl> return WILC_FAIL ; <nl>  <nl> }
mmm drivers / block / drbd / drbd_main . c <nl> ppp drivers / block / drbd / drbd_main . c <nl> static void after_state_ch ( struct drbd_conf * mdev , union drbd_state os , <nl> drbd_free_bc ( mdev -> ldev ); <nl> mdev -> ldev = NULL ;); <nl>  <nl> - if ( mdev -> md_io_tmpp ) <nl> + if ( mdev -> md_io_tmpp ) { <nl> __free_page ( mdev -> md_io_tmpp ); <nl> + mdev -> md_io_tmpp = NULL ; <nl> + } <nl> } <nl>  <nl> /* Disks got bigger while they were detached */
mmm fs / xfs / xfs_attr_inactive . c <nl> ppp fs / xfs / xfs_attr_inactive . c <nl> xfs_attr_inactive ( <nl> */ <nl> xfs_trans_ijoin ( trans , dp , 0 ); <nl>  <nl> - /* invalidate and truncate the attribute fork extents */ <nl> - if ( dp -> i_d . di_aformat != XFS_DINODE_FMT_LOCAL ) { <nl> + /* <nl> + * Invalidate and truncate the attribute fork extents . Make sure the <nl> + * fork actually has attributes as otherwise the invalidation has no <nl> + * blocks to read and returns an error . In this case , just do the fork <nl> + * removal below . <nl> + */ <nl> + if ( xfs_inode_hasattr ( dp ) && <nl> + dp -> i_d . di_aformat != XFS_DINODE_FMT_LOCAL ) { <nl> error = xfs_attr3_root_inactive (& trans , dp ); <nl> if ( error ) <nl> goto out_cancel ;
mmm net / batman - adv / bat_debugfs . c <nl> ppp net / batman - adv / bat_debugfs . c <nl> int debug_log ( struct bat_priv * bat_priv , char * fmt , ...) <nl>  <nl> va_start ( args , fmt ); <nl> vscnprintf ( tmp_log_buf , sizeof ( tmp_log_buf ), fmt , args ); <nl> - fdebug_log ( bat_priv -> debug_log , "[% 10u ] % s ", <nl> + fdebug_log ( bat_priv -> debug_log , "[% 10lu ] % s ", <nl> ( jiffies / HZ ), tmp_log_buf ); <nl> va_end ( args ); <nl> 
mmm mm / pagewalk . c <nl> ppp mm / pagewalk . c <nl> int walk_page_range ( unsigned long start , unsigned long end , <nl> vma = vma -> vm_next ; <nl>  <nl> err = walk_page_test ( start , next , walk ); <nl> - if ( err > 0 ) <nl> + if ( err > 0 ) { <nl> + /* <nl> + * positive return values are purely for <nl> + * controlling the pagewalk , so should never <nl> + * be passed to the callers . <nl> + */ <nl> + err = 0 ; <nl> continue ; <nl> + } <nl> if ( err < 0 ) <nl> break ; <nl> }
mmm arch / x86 / kernel / topology . c <nl> ppp arch / x86 / kernel / topology . c <nl> int __ref _debug_hotplug_cpu ( int cpu , int action ) <nl> ret = cpu_down ( cpu ); <nl> if (! ret ) { <nl> pr_info (" CPU % u is now offline \ n ", cpu ); <nl> + dev -> offline = true ; <nl> kobject_uevent (& dev -> kobj , KOBJ_OFFLINE ); <nl> } else <nl> pr_debug (" Can ' t offline CPU % d .\ n ", cpu ); <nl> break ; <nl> case 1 : <nl> ret = cpu_up ( cpu ); <nl> - if (! ret ) <nl> + if (! ret ) { <nl> + dev -> offline = false ; <nl> kobject_uevent (& dev -> kobj , KOBJ_ONLINE ); <nl> - else <nl> + } else { <nl> pr_debug (" Can ' t online CPU % d .\ n ", cpu ); <nl> + } <nl> break ; <nl> default : <nl> ret = - EINVAL ;
mmm drivers / gpu / drm / i915 / intel_guc . c <nl> ppp drivers / gpu / drm / i915 / intel_guc . c <nl> int intel_guc_send_mmio ( struct intel_guc * guc , const u32 * action , u32 len , <nl> " ret =% d status = 0x % 08X response = 0x % 08X \ n ", <nl> action [ 0 ], ret , status , <nl> I915_READ ( SOFT_SCRATCH ( 15 ))); <nl> - } else { <nl> - /* Use data from the GuC response as our return value */ <nl> - ret = INTEL_GUC_MSG_TO_DATA ( status ); <nl> + goto out ; <nl> } <nl>  <nl> + if ( response_buf ) { <nl> + int count = min ( response_buf_size , guc -> send_regs . count - 1 ); <nl> + <nl> + for ( i = 0 ; i < count ; i ++) <nl> + response_buf [ i ] = I915_READ ( guc_send_reg ( guc , i + 1 )); <nl> + } <nl> + <nl> + /* Use data from the GuC response as our return value */ <nl> + ret = INTEL_GUC_MSG_TO_DATA ( status ); <nl> + <nl> + out : <nl> intel_uncore_forcewake_put ( dev_priv , guc -> send_regs . fw_domains ); <nl> mutex_unlock (& guc -> send_mutex ); <nl> 
mmm drivers / tty / serial / amba - pl011 . c <nl> ppp drivers / tty / serial / amba - pl011 . c <nl> static void pl011_dma_probe_initcall ( struct device * dev , struct uart_amba_port * <nl> dmaengine_slave_config ( chan , & rx_conf ); <nl> uap -> dmarx . chan = chan ; <nl>  <nl> - if ( plat -> dma_rx_poll_enable ) { <nl> + if ( plat && plat -> dma_rx_poll_enable ) { <nl> /* Set poll rate if specified . */ <nl> if ( plat -> dma_rx_poll_rate ) { <nl> uap -> dmarx . auto_poll_rate = false ;
mmm drivers / platform / x86 / peaq - wmi . c <nl> ppp drivers / platform / x86 / peaq - wmi . c <nl> static void peaq_wmi_poll ( struct input_polled_dev * dev ) <nl> } <nl>  <nl> /* Some other devices ( Shuttle XS35 ) use the same WMI GUID for other purposes */ <nl> - static const struct dmi_system_id peaq_dmi_table [] = { <nl> + static const struct dmi_system_id peaq_dmi_table [] __initconst = { <nl> { <nl> . matches = { <nl> DMI_MATCH ( DMI_SYS_VENDOR , " PEAQ "), <nl> static int __init peaq_wmi_init ( void ) <nl>  <nl> static void __exit peaq_wmi_exit ( void ) <nl> { <nl> - if (! dmi_check_system ( peaq_dmi_table )) <nl> - return ; <nl> - <nl> - if (! wmi_has_guid ( PEAQ_DOLBY_BUTTON_GUID )) <nl> - return ; <nl> - <nl> input_unregister_polled_device ( peaq_poll_dev ); <nl> } <nl> 
mmm drivers / isdn / i4l / isdn_ppp . c <nl> ppp drivers / isdn / i4l / isdn_ppp . c <nl> static struct sk_buff * isdn_ppp_decompress ( struct sk_buff * skb , struct ippp_struc <nl> rsparm . maxdlen = IPPP_RESET_MAXDATABYTES ; <nl>  <nl> skb_out = dev_alloc_skb ( is -> mru + PPP_HDRLEN ); <nl> + if (! skb_out ) { <nl> + kfree_skb ( skb ); <nl> + printk ( KERN_ERR " ippp : decomp memory allocation failure \ n "); <nl> + return NULL ; <nl> + } <nl> len = ipc -> decompress ( stat , skb , skb_out , & rsparm ); <nl> kfree_skb ( skb ); <nl> if ( len <= 0 ) {
mmm drivers / media / i2c / tvp5150 . c <nl> ppp drivers / media / i2c / tvp5150 . c <nl> static int tvp5150_fill_fmt ( struct v4l2_subdev * sd , <nl> struct v4l2_mbus_framefmt * f ; <nl> struct tvp5150 * decoder = to_tvp5150 ( sd ); <nl>  <nl> - if (! format || format -> pad ) <nl> + if (! format || ( format -> pad != DEMOD_PAD_VID_OUT )) <nl> return - EINVAL ; <nl>  <nl> f = & format -> format ;
mmm sound / soc / codecs / wm5102 . c <nl> ppp sound / soc / codecs / wm5102 . c <nl> static int wm5102_sysclk_ev ( struct snd_soc_dapm_widget * w , <nl> struct snd_kcontrol * kcontrol , int event ) <nl> { <nl> struct snd_soc_codec * codec = w -> codec ; <nl> - struct arizona * arizona = dev_get_drvdata ( codec -> dev ); <nl> + struct arizona * arizona = dev_get_drvdata ( codec -> dev -> parent ); <nl> struct regmap * regmap = codec -> control_data ; <nl> const struct reg_default * patch = NULL ; <nl> int i , patch_size ;
mmm Documentation / lguest / lguest . c <nl> ppp Documentation / lguest / lguest . c <nl> struct virtqueue <nl> /* Remember the arguments to the program so we can " reboot " */ <nl> static char ** main_args ; <nl>  <nl> -/* Since guest is UP and we don ' t run at the same time , we don ' t need barriers . <nl> - * But I include them in the code in case others copy it . */ <nl> -# define wmb () <nl> +/* We have to be careful with barriers : our devices are all run in separate <nl> + * threads and so we need to make sure that changes visible to the Guest happen <nl> + * in precise order . */ <nl> +# define wmb () __asm__ __volatile__ ("" : : : " memory ") <nl>  <nl> /* Convert an iovec element to the given type . <nl> *
mmm drivers / scsi / sr_ioctl . c <nl> ppp drivers / scsi / sr_ioctl . c <nl> int sr_do_ioctl ( Scsi_CD * cd , struct packet_command * cgc ) <nl> struct scsi_device * SDev ; <nl> struct scsi_sense_hdr sshdr ; <nl> int result , err = 0 , retries = 0 ; <nl> + unsigned char sense_buffer [ SCSI_SENSE_BUFFERSIZE ], * senseptr = NULL ; <nl>  <nl> SDev = cd -> device ; <nl>  <nl> + if ( cgc -> sense ) <nl> + senseptr = sense_buffer ; <nl> + <nl> retry : <nl> if (! scsi_block_when_processing_errors ( SDev )) { <nl> err = - ENODEV ; <nl> int sr_do_ioctl ( Scsi_CD * cd , struct packet_command * cgc ) <nl> } <nl>  <nl> result = scsi_execute ( SDev , cgc -> cmd , cgc -> data_direction , <nl> - cgc -> buffer , cgc -> buflen , <nl> - ( unsigned char *) cgc -> sense , & sshdr , <nl> + cgc -> buffer , cgc -> buflen , senseptr , & sshdr , <nl> cgc -> timeout , IOCTL_RETRIES , 0 , 0 , NULL ); <nl>  <nl> + if ( cgc -> sense ) <nl> + memcpy ( cgc -> sense , sense_buffer , sizeof (* cgc -> sense )); <nl> + <nl> /* Minimal error checking . Ignore cases we know about , and report the rest . */ <nl> if ( driver_byte ( result ) != 0 ) { <nl> switch ( sshdr . sense_key ) {
mmm net / sched / act_police . c <nl> ppp net / sched / act_police . c <nl> static int tcf_act_police ( struct sk_buff * skb , const struct tc_action * a , <nl> police -> tcfp_t_c = now ; <nl> police -> tcfp_toks = toks ; <nl> police -> tcfp_ptoks = ptoks ; <nl> + if ( police -> tcfp_result == TC_ACT_SHOT ) <nl> + police -> tcf_qstats . drops ++; <nl> spin_unlock (& police -> tcf_lock ); <nl> return police -> tcfp_result ; <nl> }
mmm drivers / iommu / intel - iommu . c <nl> ppp drivers / iommu / intel - iommu . c <nl> static int domain_context_mapping_one ( struct dmar_domain * domain , <nl> if ( context_copied ( context )) { <nl> u16 did_old = context_domain_id ( context ); <nl>  <nl> - if ( did_old >= 0 && did_old < cap_ndoms ( iommu -> cap )) <nl> + if ( did_old >= 0 && did_old < cap_ndoms ( iommu -> cap )) { <nl> iommu -> flush . flush_context ( iommu , did_old , <nl> ((( u16 ) bus ) << 8 ) | devfn , <nl> DMA_CCMD_MASK_NOBIT , <nl> DMA_CCMD_DEVICE_INVL ); <nl> + iommu -> flush . flush_iotlb ( iommu , did_old , 0 , 0 , <nl> + DMA_TLB_DSI_FLUSH ); <nl> + } <nl> } <nl>  <nl> pgd = domain -> pgd ;
mmm net / dccp / options . c <nl> ppp net / dccp / options . c <nl> int dccp_parse_options ( struct sock * sk , struct sk_buff * skb ) <nl> opt_recv -> dccpor_timestamp_echo = ntohl (*( __be32 *) value ); <nl>  <nl> dccp_pr_debug ("% s rx opt : TIMESTAMP_ECHO =% u , len =% d , " <nl> - " ackno =% llu , ", dccp_role ( sk ), <nl> + " ackno =% llu ", dccp_role ( sk ), <nl> opt_recv -> dccpor_timestamp_echo , <nl> len + 2 , <nl> ( unsigned long long ) <nl> DCCP_SKB_CB ( skb )-> dccpd_ack_seq ); <nl>  <nl>  <nl> - if ( len == 4 ) <nl> + if ( len == 4 ) { <nl> + dccp_pr_debug_cat ("\ n "); <nl> break ; <nl> + } <nl>  <nl> if ( len == 6 ) <nl> elapsed_time = ntohs (*( __be16 *)( value + 4 )); <nl> else <nl> elapsed_time = ntohl (*( __be32 *)( value + 4 )); <nl>  <nl> + dccp_pr_debug_cat (", ELAPSED_TIME =% d \ n ", elapsed_time ); <nl> + <nl> /* Give precedence to the biggest ELAPSED_TIME */ <nl> if ( elapsed_time > opt_recv -> dccpor_elapsed_time ) <nl> opt_recv -> dccpor_elapsed_time = elapsed_time ;
mmm drivers / gpu / drm / i915 / intel_lrc . c <nl> ppp drivers / gpu / drm / i915 / intel_lrc . c <nl> void intel_lrc_irq_handler ( struct intel_engine_cs * ring ) <nl>  <nl> spin_unlock (& ring -> execlist_lock ); <nl>  <nl> - WARN ( submit_contexts > 2 , " More than two context complete events ?\ n "); <nl> + if ( unlikely ( submit_contexts > 2 )) <nl> + DRM_ERROR (" More than two context complete events ?\ n "); <nl> + <nl> ring -> next_context_status_buffer = write_pointer % GEN8_CSB_ENTRIES ; <nl>  <nl> /* Update the read pointer to the old write pointer . Manual ringbuffer
mmm net / xfrm / xfrm_user . c <nl> ppp net / xfrm / xfrm_user . c <nl> static int xfrm_del_sa ( struct sk_buff * skb , struct nlmsghdr * nlh , <nl>  <nl> static void copy_to_user_state ( struct xfrm_state * x , struct xfrm_usersa_info * p ) <nl> { <nl> + memset ( p , 0 , sizeof (* p )); <nl> memcpy (& p -> id , & x -> id , sizeof ( p -> id )); <nl> memcpy (& p -> sel , & x -> sel , sizeof ( p -> sel )); <nl> memcpy (& p -> lft , & x -> lft , sizeof ( p -> lft ));
mmm arch / powerpc / mm / pgtable - radix . c <nl> ppp arch / powerpc / mm / pgtable - radix . c <nl> void radix__mark_rodata_ro ( void ) <nl> { <nl> unsigned long start , end ; <nl>  <nl> + /* <nl> + * mark_rodata_ro () will mark itself as ! writable at some point . <nl> + * Due to DD1 workaround in radix__pte_update (), we ' ll end up with <nl> + * an invalid pte and the system will crash quite severly . <nl> + */ <nl> + if ( cpu_has_feature ( CPU_FTR_POWER9_DD1 )) { <nl> + pr_warn (" Warning : Unable to mark rodata read only on P9 DD1 \ n "); <nl> + return ; <nl> + } <nl> + <nl> start = ( unsigned long ) _stext ; <nl> end = ( unsigned long ) __init_begin ; <nl> 
mmm drivers / net / ethernet / xilinx / ll_temac_main . c <nl> ppp drivers / net / ethernet / xilinx / ll_temac_main . c <nl> void temac_indirect_out32 ( struct temac_local * lp , int reg , u32 value ) <nl> return ; <nl> temac_iow ( lp , XTE_LSW0_OFFSET , value ); <nl> temac_iow ( lp , XTE_CTL0_OFFSET , CNTLREG_WRITE_ENABLE_MASK | reg ); <nl> + temac_indirect_busywait ( lp ); <nl> } <nl>  <nl> /**
mmm fs / ext2 / ialloc . c <nl> ppp fs / ext2 / ialloc . c <nl> struct inode * ext2_new_inode ( struct inode * dir , umode_t mode , <nl>  <nl> for ( i = 0 ; i < sbi -> s_groups_count ; i ++) { <nl> gdp = ext2_get_group_desc ( sb , group , & bh2 ); <nl> + if (! gdp ) { <nl> + if (++ group == sbi -> s_groups_count ) <nl> + group = 0 ; <nl> + continue ; <nl> + } <nl> brelse ( bitmap_bh ); <nl> bitmap_bh = read_inode_bitmap ( sb , group ); <nl> if (! bitmap_bh ) {
mmm drivers / uwb / drp . c <nl> ppp drivers / uwb / drp . c <nl> static void uwb_drp_handle_alien_drp ( struct uwb_rc * rc , struct uwb_ie_drp * drp_i <nl>  <nl> /* alloc and initialize new uwb_cnflt_alien */ <nl> cnflt = kzalloc ( sizeof ( struct uwb_cnflt_alien ), GFP_KERNEL ); <nl> - if (! cnflt ) <nl> + if (! cnflt ) { <nl> dev_err ( dev , " failed to alloc uwb_cnflt_alien struct \ n "); <nl> + return ; <nl> + } <nl> + <nl> INIT_LIST_HEAD (& cnflt -> rc_node ); <nl> init_timer (& cnflt -> timer ); <nl> cnflt -> timer . function = uwb_cnflt_timer ;
mmm arch / x86 / kvm / x86 . c <nl> ppp arch / x86 / kvm / x86 . c <nl> int kvm_set_msr_common ( struct kvm_vcpu * vcpu , u32 msr , u64 data ) <nl> return 1 ; <nl> } <nl> break ; <nl> + case MSR_FAM10H_MMIO_CONF_BASE : <nl> + if ( data != 0 ) { <nl> + pr_unimpl ( vcpu , " unimplemented MMIO_CONF_BASE wrmsr : " <nl> + " 0x % llx \ n ", data ); <nl> + return 1 ; <nl> + } <nl> + break ; <nl> case MSR_AMD64_NB_CFG : <nl> break ; <nl> case MSR_IA32_DEBUGCTLMSR : <nl> int kvm_get_msr_common ( struct kvm_vcpu * vcpu , u32 msr , u64 * pdata ) <nl> case MSR_K7_EVNTSEL0 : <nl> case MSR_K8_INT_PENDING_MSG : <nl> case MSR_AMD64_NB_CFG : <nl> + case MSR_FAM10H_MMIO_CONF_BASE : <nl> data = 0 ; <nl> break ; <nl> case MSR_MTRRcap :
mmm drivers / scsi / qla2xxx / qla_isr . c <nl> ppp drivers / scsi / qla2xxx / qla_isr . c <nl> qla25xx_process_bidir_status_iocb ( scsi_qla_host_t * vha , void * pkt , <nl> bsg_job -> reply_len = sizeof ( struct fc_bsg_reply ); <nl> /* Always return DID_OK , bsg will send the vendor specific response <nl> * in this case only */ <nl> - sp -> done ( sp , DID_OK << 6 ); <nl> + sp -> done ( sp , DID_OK << 16 ); <nl>  <nl> } <nl> 
mmm drivers / pnp / pnpacpi / rsparser . c <nl> ppp drivers / pnp / pnpacpi / rsparser . c <nl> static void pnpacpi_encode_ext_irq ( struct acpi_resource * resource , <nl> resource -> data . extended_irq . triggering = triggering ; <nl> resource -> data . extended_irq . polarity = polarity ; <nl> if ( triggering == ACPI_EDGE_SENSITIVE ) <nl> - resource -> data . irq . sharable = ACPI_EXCLUSIVE ; <nl> + resource -> data . extended_irq . sharable = ACPI_EXCLUSIVE ; <nl> else <nl> - resource -> data . irq . sharable = ACPI_SHARED ; <nl> + resource -> data . extended_irq . sharable = ACPI_SHARED ; <nl> resource -> data . extended_irq . interrupt_count = 1 ; <nl> resource -> data . extended_irq . interrupts [ 0 ] = p -> start ; <nl> }
mmm drivers / net / ethernet / calxeda / xgmac . c <nl> ppp drivers / net / ethernet / calxeda / xgmac . c <nl> static int xgmac_hw_init ( struct net_device * dev ) <nl> DMA_BUS_MODE_FB | DMA_BUS_MODE_ATDS | DMA_BUS_MODE_AAL ; <nl> writel ( value , ioaddr + XGMAC_DMA_BUS_MODE ); <nl>  <nl> - /* Enable interrupts */ <nl> - writel ( DMA_INTR_DEFAULT_MASK , ioaddr + XGMAC_DMA_STATUS ); <nl> - writel ( DMA_INTR_DEFAULT_MASK , ioaddr + XGMAC_DMA_INTR_ENA ); <nl> + writel ( 0 , ioaddr + XGMAC_DMA_INTR_ENA ); <nl>  <nl> /* Mask power mgt interrupt */ <nl> writel ( XGMAC_INT_STAT_PMTIM , ioaddr + XGMAC_INT_STAT ); <nl> static int xgmac_open ( struct net_device * dev ) <nl> napi_enable (& priv -> napi ); <nl> netif_start_queue ( dev ); <nl>  <nl> + /* Enable interrupts */ <nl> + writel ( DMA_INTR_DEFAULT_MASK , ioaddr + XGMAC_DMA_STATUS ); <nl> + writel ( DMA_INTR_DEFAULT_MASK , ioaddr + XGMAC_DMA_INTR_ENA ); <nl> + <nl> return 0 ; <nl> } <nl> 
mmm sound / soc / intel / skylake / skl - sst . c <nl> ppp sound / soc / intel / skylake / skl - sst . c <nl> static int skl_unload_module ( struct sst_dsp * ctx , u16 mod_id ) <nl> dev_err ( ctx -> dev , " Module bad usage cnt !:% d \ n ", usage_cnt ); <nl> return - EIO ; <nl> } <nl> + <nl> + /* if module is used by others return , no need to unload */ <nl> + if ( usage_cnt > 0 ) <nl> + return 0 ; <nl> + <nl> ret = skl_ipc_unload_modules (& skl -> ipc , <nl> SKL_NUM_MODULES , & mod_id ); <nl> if ( ret < 0 ) {
mmm drivers / mtd / ubi / vtbl . c <nl> ppp drivers / mtd / ubi / vtbl . c <nl> static int init_volumes ( struct ubi_device * ubi , const struct ubi_scan_info * si , <nl> if ( ubi -> autoresize_vol_id != - 1 ) { <nl> ubi_err (" more then one auto - resize volume (% d " <nl> " and % d )", ubi -> autoresize_vol_id , i ); <nl> + kfree ( vol ); <nl> return - EINVAL ; <nl> } <nl> 
mmm drivers / scsi / aic7xxx / aic7xxx_osm . c <nl> ppp drivers / scsi / aic7xxx / aic7xxx_osm . c <nl> ahc_send_async ( struct ahc_softc * ahc , char channel , <nl> spi_period ( starget ) = tinfo -> curr . period ; <nl> spi_width ( starget ) = tinfo -> curr . width ; <nl> spi_offset ( starget ) = tinfo -> curr . offset ; <nl> - spi_dt ( starget ) = tinfo -> curr . ppr_options & MSG_EXT_PPR_DT_REQ ; <nl> - spi_qas ( starget ) = tinfo -> curr . ppr_options & MSG_EXT_PPR_QAS_REQ ; <nl> - spi_iu ( starget ) = tinfo -> curr . ppr_options & MSG_EXT_PPR_IU_REQ ; <nl> + spi_dt ( starget ) = tinfo -> curr . ppr_options & MSG_EXT_PPR_DT_REQ ? 1 : 0 ; <nl> + spi_qas ( starget ) = tinfo -> curr . ppr_options & MSG_EXT_PPR_QAS_REQ ? 1 : 0 ; <nl> + spi_iu ( starget ) = tinfo -> curr . ppr_options & MSG_EXT_PPR_IU_REQ ? 1 : 0 ; <nl> spi_display_xfer_agreement ( starget ); <nl> break ; <nl> } <nl> static void ahc_linux_set_dt ( struct scsi_target * starget , int dt ) <nl> if ( dt ) { <nl> period = 9 ; /* 12 . 5ns is the only period valid for DT */ <nl> ppr_options |= MSG_EXT_PPR_DT_REQ ; <nl> - } else if ( period == 9 ) <nl> + } else if ( period == 9 ) { <nl> period = 10 ; /* if resetting DT , period must be >= 25ns */ <nl> + ppr_options &= ~ MSG_EXT_PPR_DT_REQ ; <nl> + } <nl>  <nl> ahc_compile_devinfo (& devinfo , shost -> this_id , starget -> id , 0 , <nl> starget -> channel + ' A ', ROLE_INITIATOR );
mmm fs / btrfs / ioctl . c <nl> ppp fs / btrfs / ioctl . c <nl> static noinline long btrfs_ioctl_clone ( struct file * file , unsigned long srcfd , <nl> btrfs_wait_ordered_range ( src , off , len ); <nl> } <nl>  <nl> + /* truncate page cache pages from target inode range */ <nl> + truncate_inode_pages_range (& inode -> i_data , off , <nl> + ALIGN ( off + len , PAGE_CACHE_SIZE ) - 1 ); <nl> + <nl> /* clone data */ <nl> key . objectid = btrfs_ino ( src ); <nl> key . type = BTRFS_EXTENT_DATA_KEY ;
mmm kernel / relay . c <nl> ppp kernel / relay . c <nl> static ssize_t subbuf_splice_actor ( struct file * in , <nl> . nr_pages = 0 , <nl> . nr_pages_max = PIPE_DEF_BUFFERS , <nl> . partial = partial , <nl> - . flags = flags , <nl> . ops = & relay_pipe_buf_ops , <nl> . spd_release = relay_page_release , <nl> };mmm net / core / skbuff . c <nl> ppp net / core / skbuff . c <nl> static ssize_t subbuf_splice_actor ( struct file * in , <nl> . nr_pages = 0 , <nl> . nr_pages_max = PIPE_DEF_BUFFERS , <nl> . partial = partial , <nl> - . flags = flags , <nl> . ops = & relay_pipe_buf_ops , <nl> . spd_release = relay_page_release , <nl> }; <nl> int skb_splice_bits ( struct sk_buff * skb , struct sock * sk , unsigned int offset , <nl> . pages = pages , <nl> . partial = partial , <nl> . nr_pages_max = MAX_SKB_FRAGS , <nl> - . flags = flags , <nl> . ops = & nosteal_pipe_buf_ops , <nl> . spd_release = sock_spd_release , <nl> };mmm include / linux / splice . h <nl> ppp include / linux / splice . h <nl> static ssize_t subbuf_splice_actor ( struct file * in , <nl> . nr_pages = 0 , <nl> . nr_pages_max = PIPE_DEF_BUFFERS , <nl> . partial = partial , <nl> - . flags = flags , <nl> . ops = & relay_pipe_buf_ops , <nl> . spd_release = relay_page_release , <nl> }; <nl> int skb_splice_bits ( struct sk_buff * skb , struct sock * sk , unsigned int offset , <nl> . pages = pages , <nl> . partial = partial , <nl> . nr_pages_max = MAX_SKB_FRAGS , <nl> - . flags = flags , <nl> . ops = & nosteal_pipe_buf_ops , <nl> . spd_release = sock_spd_release , <nl> }; <nl> struct splice_pipe_desc { <nl> struct partial_page * partial ; /* pages [] may not be contig */ <nl> int nr_pages ; /* number of populated pages in map */ <nl> unsigned int nr_pages_max ; /* pages [] & partial [] arrays size */ <nl> - unsigned int flags ; /* splice flags */ <nl> const struct pipe_buf_operations * ops ;/* ops associated with output pipe */ <nl> void (* spd_release )( struct splice_pipe_desc *, unsigned int ); <nl> };mmm kernel / trace / trace . c <nl> ppp kernel / trace / trace . c <nl> static ssize_t subbuf_splice_actor ( struct file * in , <nl> . nr_pages = 0 , <nl> . nr_pages_max = PIPE_DEF_BUFFERS , <nl> . partial = partial , <nl> - . flags = flags , <nl> . ops = & relay_pipe_buf_ops , <nl> . spd_release = relay_page_release , <nl> }; <nl> int skb_splice_bits ( struct sk_buff * skb , struct sock * sk , unsigned int offset , <nl> . pages = pages , <nl> . partial = partial , <nl> . nr_pages_max = MAX_SKB_FRAGS , <nl> - . flags = flags , <nl> . ops = & nosteal_pipe_buf_ops , <nl> . spd_release = sock_spd_release , <nl> }; <nl> struct splice_pipe_desc { <nl> struct partial_page * partial ; /* pages [] may not be contig */ <nl> int nr_pages ; /* number of populated pages in map */ <nl> unsigned int nr_pages_max ; /* pages [] & partial [] arrays size */ <nl> - unsigned int flags ; /* splice flags */ <nl> const struct pipe_buf_operations * ops ;/* ops associated with output pipe */ <nl> void (* spd_release )( struct splice_pipe_desc *, unsigned int ); <nl> }; <nl> static ssize_t tracing_splice_read_pipe ( struct file * filp , <nl> . partial = partial_def , <nl> . nr_pages = 0 , /* This gets updated below . */ <nl> . nr_pages_max = PIPE_DEF_BUFFERS , <nl> - . flags = flags , <nl> . ops = & tracing_pipe_buf_ops , <nl> . spd_release = tracing_spd_release_pipe , <nl> }; <nl> tracing_buffers_splice_read ( struct file * file , loff_t * ppos , <nl> . pages = pages_def , <nl> . partial = partial_def , <nl> . nr_pages_max = PIPE_DEF_BUFFERS , <nl> - . flags = flags , <nl> . ops = & buffer_pipe_buf_ops , <nl> . spd_release = buffer_spd_release , <nl> };
mmm drivers / net / wireless / wl12xx / wl1271_spi . c <nl> ppp drivers / net / wireless / wl12xx / wl1271_spi . c <nl> static void wl1271_spi_reset ( struct wl1271 * wl ) <nl> spi_message_add_tail (& t , & m ); <nl>  <nl> spi_sync ( wl_to_spi ( wl ), & m ); <nl> + kfree ( cmd ); <nl>  <nl> wl1271_dump ( DEBUG_SPI , " spi reset -> ", cmd , WSPI_INIT_CMD_LEN ); <nl> } <nl> static void wl1271_spi_init ( struct wl1271 * wl ) <nl> spi_message_add_tail (& t , & m ); <nl>  <nl> spi_sync ( wl_to_spi ( wl ), & m ); <nl> + kfree ( cmd ); <nl>  <nl> wl1271_dump ( DEBUG_SPI , " spi init -> ", cmd , WSPI_INIT_CMD_LEN ); <nl> }
mmm net / xfrm / xfrm_user . c <nl> ppp net / xfrm / xfrm_user . c <nl> static inline int xfrm_replay_verify_len ( struct xfrm_replay_state_esn * replay_es <nl> up = nla_data ( rp ); <nl> ulen = xfrm_replay_state_esn_len ( up ); <nl>  <nl> - if ( nla_len ( rp ) < ulen || xfrm_replay_state_esn_len ( replay_esn ) != ulen ) <nl> + /* Check the overall length and the internal bitmap length to avoid <nl> + * potential overflow . */ <nl> + if ( nla_len ( rp ) < ulen || <nl> + xfrm_replay_state_esn_len ( replay_esn ) != ulen || <nl> + replay_esn -> bmp_len != up -> bmp_len ) <nl> return - EINVAL ; <nl>  <nl> if ( up -> replay_window > up -> bmp_len * sizeof ( __u32 ) * 8 )
mmm arch / x86 / kernel / cpu / microcode / intel_early . c <nl> ppp arch / x86 / kernel / cpu / microcode / intel_early . c <nl> get_matching_model_microcode ( int cpu , unsigned long start , <nl> unsigned int mc_saved_count = mc_saved_data -> mc_saved_count ; <nl> int i ; <nl>  <nl> - while ( leftover ) { <nl> + while ( leftover && mc_saved_count < ARRAY_SIZE ( mc_saved_tmp )) { <nl> mc_header = ( struct microcode_header_intel *) ucode_ptr ; <nl>  <nl> mc_size = get_totalsize ( mc_header );
mmm arch / powerpc / sysdev / qe_lib / ucc . c <nl> ppp arch / powerpc / sysdev / qe_lib / ucc . c <nl> int ucc_set_qe_mux_rxtx ( int ucc_num , enum qe_clock clock , enum comm_dir mode ) <nl> case QE_CLK18 : source = 8 ; break ; <nl> case QE_CLK7 : source = 9 ; break ; <nl> case QE_CLK8 : source = 10 ; break ; <nl> + case QE_CLK16 : source = 11 ; break ; <nl> default : source = - 1 ; break ; <nl> } <nl> break ; <nl> int ucc_set_qe_mux_rxtx ( int ucc_num , enum qe_clock clock , enum comm_dir mode ) <nl> case QE_CLK22 : source = 8 ; break ; <nl> case QE_CLK7 : source = 9 ; break ; <nl> case QE_CLK8 : source = 10 ; break ; <nl> + case QE_CLK16 : source = 11 ; break ; <nl> default : source = - 1 ; break ; <nl> } <nl> break ;
mmm drivers / staging / rtl8192u / r819xU_firmware . c <nl> ppp drivers / staging / rtl8192u / r819xU_firmware . c <nl> bool fw_download_code ( struct net_device * dev , u8 * code_virtual_address , u32 buff <nl> * add 4 to avoid packet appending overflow . <nl> * */ <nl> skb = dev_alloc_skb ( USB_HWDESC_HEADER_LEN + frag_length + 4 ); <nl> + if (! skb ) <nl> + return false ; <nl> memcpy (( unsigned char *)( skb -> cb ),& dev , sizeof ( dev )); <nl> tcb_desc = ( cb_desc *)( skb -> cb + MAX_DEV_ADDR_SIZE ); <nl> tcb_desc -> queue_index = TXCMD_QUEUE ;
mmm drivers / scsi / aacraid / linit . c <nl> ppp drivers / scsi / aacraid / linit . c <nl> static long aac_compat_do_ioctl ( struct aac_dev * dev , unsigned cmd , unsigned long <nl> static int aac_compat_ioctl ( struct scsi_device * sdev , int cmd , void __user * arg ) <nl> { <nl> struct aac_dev * dev = ( struct aac_dev *) sdev -> host -> hostdata ; <nl> + if (! capable ( CAP_SYS_RAWIO )) <nl> + return - EPERM ; <nl> return aac_compat_do_ioctl ( dev , cmd , ( unsigned long ) arg ); <nl> } <nl> 
mmm drivers / net / wireless / intel / iwlwifi / pcie / tx - gen2 . c <nl> ppp drivers / net / wireless / intel / iwlwifi / pcie / tx - gen2 . c <nl> int iwl_trans_pcie_dyn_txq_alloc ( struct iwl_trans * trans , <nl> rsp = ( void *) hcmd . resp_pkt -> data ; <nl> qid = le16_to_cpu ( rsp -> queue_number ); <nl>  <nl> - if ( qid > ARRAY_SIZE ( trans_pcie -> txq )) { <nl> + if ( qid >= ARRAY_SIZE ( trans_pcie -> txq )) { <nl> WARN_ONCE ( 1 , " queue index % d unsupported ", qid ); <nl> ret = - EIO ; <nl> goto error_free_resp ;
mmm net / xfrm / xfrm_policy . c <nl> ppp net / xfrm / xfrm_policy . c <nl> static int xfrm_expand_policies ( const struct flowi * fl , u16 family , <nl> * num_xfrms = 0 ; <nl> return 0 ; <nl> } <nl> - if ( IS_ERR ( pols [ 0 ])) <nl> + if ( IS_ERR ( pols [ 0 ])) { <nl> + * num_pols = 0 ; <nl> return PTR_ERR ( pols [ 0 ]); <nl> + } <nl>  <nl> * num_xfrms = pols [ 0 ]-> xfrm_nr ; <nl>  <nl> static int xfrm_expand_policies ( const struct flowi * fl , u16 family , <nl> if ( pols [ 1 ]) { <nl> if ( IS_ERR ( pols [ 1 ])) { <nl> xfrm_pols_put ( pols , * num_pols ); <nl> + * num_pols = 0 ; <nl> return PTR_ERR ( pols [ 1 ]); <nl> } <nl> (* num_pols )++;
mmm net / netfilter / nft_hash . c <nl> ppp net / netfilter / nft_hash . c <nl> static int nft_hash_init ( const struct nft_ctx * ctx , <nl> if (! tb [ NFTA_HASH_SREG ] || <nl> ! tb [ NFTA_HASH_DREG ] || <nl> ! tb [ NFTA_HASH_LEN ] || <nl> - ! tb [ NFTA_HASH_SEED ] || <nl> ! tb [ NFTA_HASH_MODULUS ]) <nl> return - EINVAL ; <nl>  <nl> static int nft_hash_init ( const struct nft_ctx * ctx , <nl> if ( priv -> offset + priv -> modulus - 1 < priv -> offset ) <nl> return - EOVERFLOW ; <nl>  <nl> - priv -> seed = ntohl ( nla_get_be32 ( tb [ NFTA_HASH_SEED ])); <nl> + if ( tb [ NFTA_HASH_SEED ]) <nl> + priv -> seed = ntohl ( nla_get_be32 ( tb [ NFTA_HASH_SEED ])); <nl> + else <nl> + get_random_bytes (& priv -> seed , sizeof ( priv -> seed )); <nl>  <nl> return nft_validate_register_load ( priv -> sreg , len ) && <nl> nft_validate_register_store ( ctx , priv -> dreg , NULL ,
mmm drivers / md / bcache / super . c <nl> ppp drivers / md / bcache / super . c <nl> static void cache_set_flush ( struct closure * cl ) <nl> struct btree * b ; <nl> unsigned i ; <nl>  <nl> + if (! c ) <nl> + closure_return ( cl ); <nl> + <nl> bch_cache_accounting_destroy (& c -> accounting ); <nl>  <nl> kobject_put (& c -> internal );
mmm drivers / mfd / stm32 - timers . c <nl> ppp drivers / mfd / stm32 - timers . c <nl> static int stm32_timers_probe ( struct platform_device * pdev ) <nl> return of_platform_populate ( pdev -> dev . of_node , NULL , NULL , & pdev -> dev ); <nl> } <nl>  <nl> + static int stm32_timers_remove ( struct platform_device * pdev ) <nl> +{ <nl> + of_platform_depopulate (& pdev -> dev ); <nl> + <nl> + return 0 ; <nl> +} <nl> + <nl> static const struct of_device_id stm32_timers_of_match [] = { <nl> { . compatible = " st , stm32 - timers ", }, <nl> { /* end node */ }, <nl> MODULE_DEVICE_TABLE ( of , stm32_timers_of_match ); <nl>  <nl> static struct platform_driver stm32_timers_driver = { <nl> . probe = stm32_timers_probe , <nl> + . remove = stm32_timers_remove , <nl> . driver = { <nl> . name = " stm32 - timers ", <nl> . of_match_table = stm32_timers_of_match ,
mmm drivers / gpu / drm / drm_crtc . c <nl> ppp drivers / gpu / drm / drm_crtc . c <nl> void drm_mode_config_reset ( struct drm_device * dev ) <nl> if ( encoder -> funcs -> reset ) <nl> encoder -> funcs -> reset ( encoder ); <nl>  <nl> + mutex_lock (& dev -> mode_config . mutex ); <nl> drm_for_each_connector ( connector , dev ) { <nl> connector -> status = connector_status_unknown ; <nl>  <nl> if ( connector -> funcs -> reset ) <nl> connector -> funcs -> reset ( connector ); <nl> } <nl> + mutex_unlock (& dev -> mode_config . mutex ); <nl> } <nl> EXPORT_SYMBOL ( drm_mode_config_reset ); <nl> 
mmm drivers / net / wireless / iwlwifi / dvm / power . c <nl> ppp drivers / net / wireless / iwlwifi / dvm / power . c <nl> # include " commands . h " <nl> # include " power . h " <nl>  <nl> - static bool force_cam ; <nl> + static bool force_cam = true ; <nl> module_param ( force_cam , bool , 0644 ); <nl> MODULE_PARM_DESC ( force_cam , " force continuously aware mode ( no power saving at all )"); <nl> 
mmm drivers / net / netxen / netxen_nic_init . c <nl> ppp drivers / net / netxen / netxen_nic_init . c <nl> static inline int do_rom_fast_write_words ( struct netxen_adapter * adapter , <nl> while ( 1 ) { <nl> int data1 ; <nl>  <nl> - do_rom_fast_read ( adapter , addridx , & data1 ); <nl> + ret = do_rom_fast_read ( adapter , addridx , & data1 ); <nl> + if ( ret < 0 ) <nl> + return ret ; <nl> + <nl> if ( data1 == data ) <nl> break ; <nl> 
mmm net / bridge / br_netfilter . c <nl> ppp net / bridge / br_netfilter . c <nl> static int br_parse_ip_options ( struct sk_buff * skb ) <nl> goto drop ; <nl> } <nl>  <nl> - /* Zero out the CB buffer if no options present */ <nl> - if ( iph -> ihl == 5 ) { <nl> - memset ( IPCB ( skb ), 0 , sizeof ( struct inet_skb_parm )); <nl> + memset ( IPCB ( skb ), 0 , sizeof ( struct inet_skb_parm )); <nl> + if ( iph -> ihl == 5 ) <nl> return 0 ; <nl> - } <nl>  <nl> opt -> optlen = iph -> ihl * 4 - sizeof ( struct iphdr ); <nl> if ( ip_options_compile ( dev_net ( dev ), opt , skb ))
mmm drivers / spi / spi - imx . c <nl> ppp drivers / spi / spi - imx . c <nl> static int spi_imx_dma_transfer ( struct spi_imx_data * spi_imx , <nl> tx -> sgl , tx -> nents , DMA_MEM_TO_DEV , <nl> DMA_PREP_INTERRUPT | DMA_CTRL_ACK ); <nl> if (! desc_tx ) <nl> - goto no_dma ; <nl> + goto tx_nodma ; <nl>  <nl> desc_tx -> callback = spi_imx_dma_tx_callback ; <nl> desc_tx -> callback_param = ( void *) spi_imx ; <nl> static int spi_imx_dma_transfer ( struct spi_imx_data * spi_imx , <nl> rx -> sgl , rx -> nents , DMA_DEV_TO_MEM , <nl> DMA_PREP_INTERRUPT | DMA_CTRL_ACK ); <nl> if (! desc_rx ) <nl> - goto no_dma ; <nl> + goto rx_nodma ; <nl>  <nl> desc_rx -> callback = spi_imx_dma_rx_callback ; <nl> desc_rx -> callback_param = ( void *) spi_imx ; <nl> static int spi_imx_dma_transfer ( struct spi_imx_data * spi_imx , <nl>  <nl> return ret ; <nl>  <nl> - no_dma : <nl> + rx_nodma : <nl> + dmaengine_terminate_all ( master -> dma_tx ); <nl> + tx_nodma : <nl> pr_warn_once ("% s % s : DMA not available , falling back to PIO \ n ", <nl> dev_driver_string (& master -> dev ), <nl> dev_name (& master -> dev ));
mmm drivers / scsi / libfc / fc_exch . c <nl> ppp drivers / scsi / libfc / fc_exch . c <nl> static struct fc_exch * fc_exch_em_alloc ( struct fc_lport * lport , <nl> * EM is selected when a NULL match function pointer is encountered <nl> * or when a call to a match function returns true . <nl> */ <nl> - static inline struct fc_exch * fc_exch_alloc ( struct fc_lport * lport , <nl> - struct fc_frame * fp ) <nl> + static struct fc_exch * fc_exch_alloc ( struct fc_lport * lport , <nl> + struct fc_frame * fp ) <nl> { <nl> struct fc_exch_mgr_anchor * ema ; <nl> + struct fc_exch * ep ; <nl>  <nl> - list_for_each_entry ( ema , & lport -> ema_list , ema_list ) <nl> - if (! ema -> match || ema -> match ( fp )) <nl> - return fc_exch_em_alloc ( lport , ema -> mp ); <nl> + list_for_each_entry ( ema , & lport -> ema_list , ema_list ) { <nl> + if (! ema -> match || ema -> match ( fp )) { <nl> + ep = fc_exch_em_alloc ( lport , ema -> mp ); <nl> + if ( ep ) <nl> + return ep ; <nl> + } <nl> + } <nl> return NULL ; <nl> } <nl> 
mmm fs / autofs4 / expire . c <nl> ppp fs / autofs4 / expire . c <nl> static int autofs4_tree_busy ( struct vfsmount * mnt , <nl> struct autofs_info * ino = autofs4_dentry_ino ( p ); <nl> unsigned int ino_count = atomic_read (& ino -> count ); <nl>  <nl> + /* <nl> + * Clean stale dentries below that have not been <nl> + * invalidated after a mount fail during lookup <nl> + */ <nl> + d_invalidate ( p ); <nl> + <nl> /* allow for dget above and top is already dgot */ <nl> if ( p == top ) <nl> ino_count += 2 ;
mmm net / netfilter / ipvs / ip_vs_core . c <nl> ppp net / netfilter / ipvs / ip_vs_core . c <nl> static inline bool is_new_conn_expected ( const struct ip_vs_conn * cp , <nl> switch ( cp -> protocol ) { <nl> case IPPROTO_TCP : <nl> return ( cp -> state == IP_VS_TCP_S_TIME_WAIT ) || <nl> + ( cp -> state == IP_VS_TCP_S_CLOSE ) || <nl> (( conn_reuse_mode & 2 ) && <nl> ( cp -> state == IP_VS_TCP_S_FIN_WAIT ) && <nl> ( cp -> flags & IP_VS_CONN_F_NOOUTPUT ));
mmm drivers / hwmon / applesmc . c <nl> ppp drivers / hwmon / applesmc . c <nl> static const char * temperature_sensors_sets [][ 36 ] = { <nl> /* Set 5 : iMac */ <nl> { " TC0D ", " TA0P ", " TG0P ", " TG0D ", " TG0H ", " TH0P ", " Tm0P ", " TO0P ", <nl> " Tp0C ", NULL }, <nl> +/* Set 6 : Macbook3 set */ <nl> + { " TB0T ", " TC0D ", " TC0P ", " TM0P ", " TN0P ", " TTF0 ", " TW0P ", " Th0H ", <nl> + " Th0S ", " Th1H ", NULL }, <nl> }; <nl>  <nl> /* List of keys used to read / write fan speeds */ <nl> static __initdata struct dmi_match_data applesmc_dmi_data [] = { <nl> { . accelerometer = 0 , . light = 0 , . temperature_set = 4 }, <nl> /* iMac : temperature set 5 */ <nl> { . accelerometer = 0 , . light = 0 , . temperature_set = 5 }, <nl> +/* MacBook3 : accelerometer and temperature set 6 */ <nl> + { . accelerometer = 1 , . light = 0 , . temperature_set = 6 }, <nl> }; <nl>  <nl> /* Note that DMI_MATCH (...," MacBook ") will match " MacBookPro1 , 1 ". <nl> static __initdata struct dmi_system_id applesmc_whitelist [] = { <nl> DMI_MATCH ( DMI_BOARD_VENDOR ," Apple "), <nl> DMI_MATCH ( DMI_PRODUCT_NAME ," MacBookPro ") }, <nl> ( void *)& applesmc_dmi_data [ 0 ]}, <nl> - { applesmc_dmi_match , " Apple MacBook ", { <nl> + { applesmc_dmi_match , " Apple MacBook ( v2 )", { <nl> DMI_MATCH ( DMI_BOARD_VENDOR ," Apple "), <nl> DMI_MATCH ( DMI_PRODUCT_NAME ," MacBook2 ") }, <nl> ( void *)& applesmc_dmi_data [ 1 ]}, <nl> + { applesmc_dmi_match , " Apple MacBook ( v3 )", { <nl> + DMI_MATCH ( DMI_BOARD_VENDOR ," Apple "), <nl> + DMI_MATCH ( DMI_PRODUCT_NAME ," MacBook3 ") }, <nl> + ( void *)& applesmc_dmi_data [ 6 ]}, <nl> { applesmc_dmi_match , " Apple MacBook ", { <nl> DMI_MATCH ( DMI_BOARD_VENDOR ," Apple "), <nl> DMI_MATCH ( DMI_PRODUCT_NAME ," MacBook ") },
mmm drivers / video / atmel_lcdfb . c <nl> ppp drivers / video / atmel_lcdfb . c <nl> static int atmel_lcdfb_check_var ( struct fb_var_screeninfo * var , <nl> var -> transp . offset = var -> transp . length = 0 ; <nl> var -> xoffset = var -> yoffset = 0 ; <nl>  <nl> + if ( info -> fix . smem_len ) { <nl> + unsigned int smem_len = ( var -> xres_virtual * var -> yres_virtual <nl> + * (( var -> bits_per_pixel + 7 ) / 8 )); <nl> + if ( smem_len > info -> fix . smem_len ) <nl> + return - EINVAL ; <nl> + } <nl> + <nl> /* Saturate vertical and horizontal timings at maximum values */ <nl> var -> vsync_len = min_t ( u32 , var -> vsync_len , <nl> ( ATMEL_LCDC_VPW >> ATMEL_LCDC_VPW_OFFSET ) + 1 );
mmm drivers / infiniband / hw / ocrdma / ocrdma_verbs . c <nl> ppp drivers / infiniband / hw / ocrdma / ocrdma_verbs . c <nl> int ocrdma_arm_cq ( struct ib_cq * ibcq , enum ib_cq_notify_flags cq_flags ) <nl> if ( cq -> first_arm ) { <nl> ocrdma_ring_cq_db ( dev , cq_id , arm_needed , sol_needed , 0 ); <nl> cq -> first_arm = false ; <nl> - goto skip_defer ; <nl> } <nl> - cq -> deferred_arm = true ; <nl>  <nl> - skip_defer : <nl> + cq -> deferred_arm = true ; <nl> cq -> deferred_sol = sol_needed ; <nl> spin_unlock_irqrestore (& cq -> cq_lock , flags ); <nl> 
mmm arch / x86 / xen / grant - table . c <nl> ppp arch / x86 / xen / grant - table . c <nl> static int __init xlated_setup_gnttab_pages ( void ) <nl> rc = arch_gnttab_map_shared ( pfns , nr_grant_frames , nr_grant_frames , <nl> & xen_auto_xlat_grant_frames . vaddr ); <nl>  <nl> - kfree ( pages ); <nl> if ( rc ) { <nl> pr_warn ("% s Couldn ' t map % ld pfns rc :% d \ n ", __func__ , <nl> nr_grant_frames , rc ); <nl> free_xenballooned_pages ( nr_grant_frames , pages ); <nl> + kfree ( pages ); <nl> kfree ( pfns ); <nl> return rc ; <nl> } <nl> + kfree ( pages ); <nl>  <nl> xen_auto_xlat_grant_frames . pfn = pfns ; <nl> xen_auto_xlat_grant_frames . count = nr_grant_frames ;
mmm net / netlink / af_netlink . c <nl> ppp net / netlink / af_netlink . c <nl> netlink_kernel_create ( struct net * net , int unit , unsigned int groups , <nl> nl_table [ unit ]. cb_mutex = cb_mutex ; <nl> nl_table [ unit ]. module = module ; <nl> nl_table [ unit ]. registered = 1 ; <nl> + } else { <nl> + kfree ( listeners ); <nl> } <nl> netlink_table_ungrab (); <nl> 
mmm net / bluetooth / rfcomm / tty . c <nl> ppp net / bluetooth / rfcomm / tty . c <nl> static int rfcomm_get_dev_list ( void __user * arg ) <nl>  <nl> size = sizeof (* dl ) + dev_num * sizeof (* di ); <nl>  <nl> - dl = kmalloc ( size , GFP_KERNEL ); <nl> + dl = kzalloc ( size , GFP_KERNEL ); <nl> if (! dl ) <nl> return - ENOMEM ; <nl> 
mmm drivers / ide / ide - atapi . c <nl> ppp drivers / ide / ide - atapi . c <nl> ide_startstop_t ide_issue_pc ( ide_drive_t * drive , unsigned int timeout , <nl> { <nl> struct ide_atapi_pc * pc = drive -> pc ; <nl> ide_hwif_t * hwif = drive -> hwif ; <nl> + u32 tf_flags ; <nl> u16 bcount ; <nl> u8 scsi = !!( drive -> dev_flags & IDE_DFLAG_SCSI ); <nl>  <nl> ide_startstop_t ide_issue_pc ( ide_drive_t * drive , unsigned int timeout , <nl> if (! drive -> dma ) <nl> pc -> flags &= ~ PC_FLAG_DMA_OK ; <nl>  <nl> - ide_pktcmd_tf_load ( drive , scsi ? 0 : IDE_TFLAG_OUT_DEVICE , bcount , <nl> - drive -> dma ); <nl> + if ( scsi ) <nl> + tf_flags = 0 ; <nl> + else if ( drive -> media == ide_cdrom || drive -> media == ide_optical ) <nl> + tf_flags = IDE_TFLAG_OUT_NSECT | IDE_TFLAG_OUT_LBAL ; <nl> + else <nl> + tf_flags = IDE_TFLAG_OUT_DEVICE ; <nl> + <nl> + ide_pktcmd_tf_load ( drive , tf_flags , bcount , drive -> dma ); <nl>  <nl> /* Issue the packet command */ <nl> if ( drive -> atapi_flags & IDE_AFLAG_DRQ_INTERRUPT ) {
mmm drivers / gpu / drm / ttm / ttm_tt . c <nl> ppp drivers / gpu / drm / ttm / ttm_tt . c <nl> struct ttm_tt * ttm_tt_create ( struct ttm_bo_device * bdev , unsigned long size , <nl> ttm -> dummy_read_page = dummy_read_page ; <nl>  <nl> ttm_tt_alloc_page_directory ( ttm ); <nl> - if (! ttm -> pages ) { <nl> + if (! ttm -> pages || ! ttm -> dma_address ) { <nl> ttm_tt_destroy ( ttm ); <nl> printk ( KERN_ERR TTM_PFX " Failed allocating page table \ n "); <nl> return NULL ;
mmm include / linux / mm . h <nl> ppp include / linux / mm . h <nl> static inline bool is_pci_p2pdma_page ( const struct page * page ) <nl> } <nl> # endif /* CONFIG_DEV_PAGEMAP_OPS */ <nl>  <nl> +/* 127 : arbitrary random number , small enough to assemble well */ <nl> +# define page_ref_zero_or_close_to_overflow ( page ) \ <nl> + (( unsigned int ) page_ref_count ( page ) + 127u <= 127u ) <nl> + <nl> static inline void get_page ( struct page * page ) <nl> { <nl> page = compound_head ( page ); <nl> static inline void get_page ( struct page * page ) <nl> * Getting a normal page or the head of a compound page <nl> * requires to already have an elevated page -> _refcount . <nl> */ <nl> - VM_BUG_ON_PAGE ( page_ref_count ( page ) <= 0 , page ); <nl> + VM_BUG_ON_PAGE ( page_ref_zero_or_close_to_overflow ( page ), page ); <nl> page_ref_inc ( page ); <nl> } <nl> 
mmm fs / nfsd / nfs4xdr . c <nl> ppp fs / nfsd / nfs4xdr . c <nl> nfsd4_encode_getdeviceinfo ( struct nfsd4_compoundres * resp , __be32 nfserr , <nl> struct nfsd4_getdeviceinfo * gdev ) <nl> { <nl> struct xdr_stream * xdr = & resp -> xdr ; <nl> - const struct nfsd4_layout_ops * ops = <nl> - nfsd4_layout_ops [ gdev -> gd_layout_type ]; <nl> + const struct nfsd4_layout_ops * ops ; <nl> u32 starting_len = xdr -> buf -> len , needed_len ; <nl> __be32 * p ; <nl>  <nl> nfsd4_encode_getdeviceinfo ( struct nfsd4_compoundres * resp , __be32 nfserr , <nl>  <nl> /* If maxcount is 0 then just update notifications */ <nl> if ( gdev -> gd_maxcount != 0 ) { <nl> + ops = nfsd4_layout_ops [ gdev -> gd_layout_type ]; <nl> nfserr = ops -> encode_getdeviceinfo ( xdr , gdev ); <nl> if ( nfserr ) { <nl> /* <nl> nfsd4_encode_layoutget ( struct nfsd4_compoundres * resp , __be32 nfserr , <nl> struct nfsd4_layoutget * lgp ) <nl> { <nl> struct xdr_stream * xdr = & resp -> xdr ; <nl> - const struct nfsd4_layout_ops * ops = <nl> - nfsd4_layout_ops [ lgp -> lg_layout_type ]; <nl> + const struct nfsd4_layout_ops * ops ; <nl> __be32 * p ; <nl>  <nl> dprintk ("% s : err % d \ n ", __func__ , nfserr ); <nl> nfsd4_encode_layoutget ( struct nfsd4_compoundres * resp , __be32 nfserr , <nl> * p ++ = cpu_to_be32 ( lgp -> lg_seg . iomode ); <nl> * p ++ = cpu_to_be32 ( lgp -> lg_layout_type ); <nl>  <nl> + ops = nfsd4_layout_ops [ lgp -> lg_layout_type ]; <nl> nfserr = ops -> encode_layoutget ( xdr , lgp ); <nl> out : <nl> kfree ( lgp -> lg_content );
mmm scripts / kconfig / nconf . c <nl> ppp scripts / kconfig / nconf . c <nl> static void conf_message_callback ( const char * fmt , va_list ap ) <nl>  <nl> static void show_help ( struct menu * menu ) <nl> { <nl> - struct gstr help = str_new (); <nl> + struct gstr help ; <nl> + <nl> + if (! menu ) <nl> + return ; <nl> + <nl> + help = str_new (); <nl> menu_get_ext_help ( menu , & help ); <nl> show_scroll_win ( main_window , _ ( menu_get_prompt ( menu )), str_get (& help )); <nl> str_free (& help );
mmm drivers / net / ixgb / ixgb_main . c <nl> ppp drivers / net / ixgb / ixgb_main . c <nl> ixgb_restore_vlan ( struct ixgb_adapter * adapter ) <nl>  <nl> static void ixgb_netpoll ( struct net_device * dev ) <nl> { <nl> - struct ixgb_adapter * adapter = dev -> priv ; <nl> + struct ixgb_adapter * adapter = netdev_priv ( dev ); <nl>  <nl> disable_irq ( adapter -> pdev -> irq ); <nl> ixgb_intr ( adapter -> pdev -> irq , dev , NULL );
mmm ipc / mqueue . c <nl> ppp ipc / mqueue . c <nl> static int do_mq_notify ( mqd_t mqdes , const struct sigevent * notification ) <nl>  <nl> timeo = MAX_SCHEDULE_TIMEOUT ; <nl> ret = netlink_attachskb ( sock , nc , & timeo , NULL ); <nl> - if ( ret == 1 ) <nl> + if ( ret == 1 ) { <nl> + sock = NULL ; <nl> goto retry ; <nl> + } <nl> if ( ret ) { <nl> sock = NULL ; <nl> nc = NULL ;
mmm sound / soc / codecs / wm8903 . c <nl> ppp sound / soc / codecs / wm8903 . c <nl> static int wm8903_probe ( struct snd_soc_codec * codec ) <nl> /* power down chip */ <nl> static int wm8903_remove ( struct snd_soc_codec * codec ) <nl> { <nl> + struct wm8903_priv * wm8903 = snd_soc_codec_get_drvdata ( codec ); <nl> + <nl> wm8903_free_gpio ( codec ); <nl> wm8903_set_bias_level ( codec , SND_SOC_BIAS_OFF ); <nl> + if ( wm8903 -> irq ) <nl> + free_irq ( wm8903 -> irq , codec ); <nl> + <nl> return 0 ; <nl> } <nl> 
mmm net / netfilter / ipset / ip_set_core . c <nl> ppp net / netfilter / ipset / ip_set_core . c <nl> ip_set_net_exit ( struct net * net ) <nl>  <nl> inst -> is_deleted = true ; /* flag for ip_set_nfnl_put */ <nl>  <nl> + nfnl_lock ( NFNL_SUBSYS_IPSET ); <nl> for ( i = 0 ; i < inst -> ip_set_max ; i ++) { <nl> set = ip_set ( inst , i ); <nl> if ( set ) { <nl> ip_set_net_exit ( struct net * net ) <nl> ip_set_destroy_set ( set ); <nl> } <nl> } <nl> + nfnl_unlock ( NFNL_SUBSYS_IPSET ); <nl> kfree ( rcu_dereference_protected ( inst -> ip_set_list , 1 )); <nl> } <nl> 
mmm arch / mips / kernel / csrc - sb1250 . c <nl> ppp arch / mips / kernel / csrc - sb1250 . c <nl> static cycle_t sb1250_hpt_read ( void ) <nl> } <nl>  <nl> struct clocksource bcm1250_clocksource = { <nl> - . name = " MIPS ", <nl> + . name = " bcm1250 - counter - 3 ", <nl> . rating = 200 , <nl> . read = sb1250_hpt_read , <nl> . mask = CLOCKSOURCE_MASK ( 23 ),
mmm drivers / net / wireless / microchip / wilc1000 / cfg80211 . c <nl> ppp drivers / net / wireless / microchip / wilc1000 / cfg80211 . c <nl> static inline void wilc_wfi_cfg_parse_ch_attr ( u8 * buf , u32 len , u8 sta_ch ) <nl> if ( index + sizeof (* e ) + attr_size > len ) <nl> return ; <nl>  <nl> - if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST ) <nl> + if ( e -> attr_type == IEEE80211_P2P_ATTR_CHANNEL_LIST && <nl> + attr_size >= ( sizeof ( struct wilc_attr_ch_list ) - sizeof (* e ))) <nl> ch_list_idx = index ; <nl> else if ( e -> attr_type == IEEE80211_P2P_ATTR_OPER_CHANNEL && <nl> attr_size == ( sizeof ( struct wilc_attr_oper_ch ) - sizeof (* e )))
mmm net / openvswitch / flow . c <nl> ppp net / openvswitch / flow . c <nl> static void stats_read ( struct flow_stats * stats , <nl> unsigned long * used , __be16 * tcp_flags ) <nl> { <nl> spin_lock (& stats -> lock ); <nl> - if ( time_after ( stats -> used , * used )) <nl> + if (!* used || time_after ( stats -> used , * used )) <nl> * used = stats -> used ; <nl> * tcp_flags |= stats -> tcp_flags ; <nl> ovs_stats -> n_packets += stats -> packet_count ;
mmm drivers / infiniband / hw / mlx4 / qp . c <nl> ppp drivers / infiniband / hw / mlx4 / qp . c <nl> static struct ib_qp * _mlx4_ib_create_qp_rss ( struct ib_pd * pd , <nl> return ERR_PTR (- EFAULT ); <nl> } <nl>  <nl> + if ( memchr_inv ( ucmd . reserved , 0 , sizeof ( ucmd . reserved ))) <nl> + return ERR_PTR (- EOPNOTSUPP ); <nl> + <nl> if ( ucmd . comp_mask || ucmd . reserved1 ) <nl> return ERR_PTR (- EOPNOTSUPP ); <nl> 
mmm kernel / ucount . c <nl> ppp kernel / ucount . c <nl> struct ucounts * alloc_ucounts ( struct user_namespace * ns , kuid_t uid ) <nl> kfree ( new ); <nl> } else { <nl> hlist_add_head (& new -> node , hashent ); <nl> + get_user_ns ( new -> ns ); <nl> spin_unlock_irq (& ucounts_lock ); <nl> return new ; <nl> } <nl> void put_ucounts ( struct ucounts * ucounts ) <nl> if ( atomic_dec_and_lock_irqsave (& ucounts -> count , & ucounts_lock , flags )) { <nl> hlist_del_init (& ucounts -> node ); <nl> spin_unlock_irqrestore (& ucounts_lock , flags ); <nl> + put_user_ns ( ucounts -> ns ); <nl> kfree ( ucounts ); <nl> } <nl> }
mmm net / packet / af_packet . c <nl> ppp net / packet / af_packet . c <nl> static void * packet_lookup_frame ( struct packet_sock * po , unsigned int position , <nl> h . raw = po -> pg_vec [ pg_vec_pos ] + ( frame_offset * po -> frame_size ); <nl> switch ( po -> tp_version ) { <nl> case TPACKET_V1 : <nl> - if ( status != h . h1 -> tp_status ? TP_STATUS_USER : <nl> - TP_STATUS_KERNEL ) <nl> + if ( status != ( h . h1 -> tp_status ? TP_STATUS_USER : <nl> + TP_STATUS_KERNEL )) <nl> return NULL ; <nl> break ; <nl> case TPACKET_V2 : <nl> - if ( status != h . h2 -> tp_status ? TP_STATUS_USER : <nl> - TP_STATUS_KERNEL ) <nl> + if ( status != ( h . h2 -> tp_status ? TP_STATUS_USER : <nl> + TP_STATUS_KERNEL )) <nl> return NULL ; <nl> break ; <nl> }
mmm fs / btrfs / inode . c <nl> ppp fs / btrfs / inode . c <nl> int btrfs_check_free_space ( struct btrfs_root * root , u64 num_required , <nl> int ret = 0 ; <nl>  <nl> if ( for_del ) <nl> - thresh = ( total * 90 ) / 100 ; <nl> + thresh = total * 90 ; <nl> else <nl> - thresh = ( total * 85 ) / 100 ; <nl> + thresh = total * 85 ; <nl> + <nl> + do_div ( thresh , 100 ); <nl>  <nl> spin_lock (& root -> fs_info -> delalloc_lock ); <nl> if ( used + root -> fs_info -> delalloc_bytes + num_required > thresh ) <nl> static int btrfs_ioctl_resize ( struct btrfs_root * root , void __user * arg ) <nl> ret = - EFBIG ; <nl> goto out_unlock ; <nl> } <nl> - new_size = ( new_size / root -> sectorsize ) * root -> sectorsize ; <nl> + <nl> + do_div ( new_size , root -> sectorsize ); <nl> + new_size *= root -> sectorsize ; <nl>  <nl> printk (" new size is % Lu \ n ", new_size ); <nl> if ( new_size > old_size ) {mmm fs / btrfs / extent - tree . c <nl> ppp fs / btrfs / extent - tree . c <nl> int btrfs_check_free_space ( struct btrfs_root * root , u64 num_required , <nl> int ret = 0 ; <nl>  <nl> if ( for_del ) <nl> - thresh = ( total * 90 ) / 100 ; <nl> + thresh = total * 90 ; <nl> else <nl> - thresh = ( total * 85 ) / 100 ; <nl> + thresh = total * 85 ; <nl> + <nl> + do_div ( thresh , 100 ); <nl>  <nl> spin_lock (& root -> fs_info -> delalloc_lock ); <nl> if ( used + root -> fs_info -> delalloc_bytes + num_required > thresh ) <nl> static int btrfs_ioctl_resize ( struct btrfs_root * root , void __user * arg ) <nl> ret = - EFBIG ; <nl> goto out_unlock ; <nl> } <nl> - new_size = ( new_size / root -> sectorsize ) * root -> sectorsize ; <nl> + <nl> + do_div ( new_size , root -> sectorsize ); <nl> + new_size *= root -> sectorsize ; <nl>  <nl> printk (" new size is % Lu \ n ", new_size ); <nl> if ( new_size > old_size ) { <nl> int btrfs_grow_extent_tree ( struct btrfs_trans_handle * trans , <nl> u64 nr = 0 ; <nl> u64 cur_byte ; <nl> u64 old_size ; <nl> + unsigned long rem ; <nl> struct btrfs_block_group_cache * cache ; <nl> struct btrfs_block_group_item * item ; <nl> struct btrfs_fs_info * info = root -> fs_info ; <nl> int btrfs_grow_extent_tree ( struct btrfs_trans_handle * trans , <nl> struct btrfs_block_group_item ); <nl>  <nl> btrfs_set_disk_block_group_used ( leaf , item , 0 ); <nl> - if ( nr % 3 ) { <nl> + div_long_long_rem ( nr , 3 , & rem ); <nl> + if ( rem ) { <nl> btrfs_set_disk_block_group_flags ( leaf , item , <nl> BTRFS_BLOCK_GROUP_DATA ); <nl> } else {
mmm drivers / tty / serial / arc_uart . c <nl> ppp drivers / tty / serial / arc_uart . c <nl> static int arc_serial_probe ( struct platform_device * pdev ) <nl> if ( dev_id < 0 ) <nl> dev_id = 0 ; <nl>  <nl> + if ( dev_id >= ARRAY_SIZE ( arc_uart_ports )) { <nl> + dev_err (& pdev -> dev , " serial % d out of range \ n ", dev_id ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> uart = & arc_uart_ports [ dev_id ]; <nl> port = & uart -> port ; <nl> 
mmm drivers / net / wireless / wl12xx / main . c <nl> ppp drivers / net / wireless / wl12xx / main . c <nl> static struct platform_device wl1271_device = { <nl> }, <nl> }; <nl>  <nl> + static DEFINE_MUTEX ( wl_list_mutex ); <nl> static LIST_HEAD ( wl_list ); <nl>  <nl> static int wl1271_dev_notify ( struct notifier_block * me , unsigned long what , <nl> static int wl1271_dev_notify ( struct notifier_block * me , unsigned long what , <nl> return NOTIFY_DONE ; <nl>  <nl> wl_temp = hw -> priv ; <nl> + mutex_lock (& wl_list_mutex ); <nl> list_for_each_entry ( wl , & wl_list , list ) { <nl> if ( wl == wl_temp ) <nl> break ; <nl> } <nl> + mutex_unlock (& wl_list_mutex ); <nl> if ( wl != wl_temp ) <nl> return NOTIFY_DONE ; <nl>  <nl> static int wl1271_op_add_interface ( struct ieee80211_hw * hw , <nl> out : <nl> mutex_unlock (& wl -> mutex ); <nl>  <nl> + mutex_lock (& wl_list_mutex ); <nl> if (! ret ) <nl> list_add (& wl -> list , & wl_list ); <nl> + mutex_unlock (& wl_list_mutex ); <nl>  <nl> return ret ; <nl> } <nl> static void __wl1271_op_remove_interface ( struct wl1271 * wl ) <nl>  <nl> wl1271_info (" down "); <nl>  <nl> + mutex_lock (& wl_list_mutex ); <nl> list_del (& wl -> list ); <nl> + mutex_unlock (& wl_list_mutex ); <nl>  <nl> WARN_ON ( wl -> state != WL1271_STATE_ON ); <nl> 
mmm drivers / pinctrl / pinctrl - amd . c <nl> ppp drivers / pinctrl / pinctrl - amd . c <nl> static struct irq_chip amd_gpio_irqchip = { <nl> . irq_set_type = amd_gpio_irq_set_type , <nl> }; <nl>  <nl> - static void amd_gpio_irq_handler ( unsigned int irq , struct irq_desc * desc ) <nl> + static void amd_gpio_irq_handler ( unsigned int __irq , struct irq_desc * desc ) <nl> { <nl> + unsigned int irq = irq_desc_get_irq ( desc ); <nl> u32 i ; <nl> u32 off ; <nl> u32 reg ;
mmm fs / partitions / efi . c <nl> ppp fs / partitions / efi . c <nl> static int is_gpt_valid ( struct parsed_partitions * state , u64 lba , <nl> goto fail ; <nl> } <nl>  <nl> + /* Check that sizeof_partition_entry has the correct value */ <nl> + if ( le32_to_cpu ((* gpt )-> sizeof_partition_entry ) != sizeof ( gpt_entry )) { <nl> + pr_debug (" GUID Partitition Entry Size check failed .\ n "); <nl> + goto fail ; <nl> + } <nl> + <nl> if (!(* ptes = alloc_read_gpt_entries ( state , * gpt ))) <nl> goto fail ; <nl> 
mmm drivers / usb / dwc3 / dwc3 - qcom . c <nl> ppp drivers / usb / dwc3 / dwc3 - qcom . c <nl> static int dwc3_qcom_acpi_register_core ( struct platform_device * pdev ) <nl> qcom -> dwc3 -> dev . coherent_dma_mask = dev -> coherent_dma_mask ; <nl>  <nl> child_res = kcalloc ( 2 , sizeof (* child_res ), GFP_KERNEL ); <nl> - if (! child_res ) <nl> + if (! child_res ) { <nl> + platform_device_put ( qcom -> dwc3 ); <nl> return - ENOMEM ; <nl> + } <nl>  <nl> res = platform_get_resource ( pdev , IORESOURCE_MEM , 0 ); <nl> if (! res ) { <nl> static int dwc3_qcom_acpi_register_core ( struct platform_device * pdev ) <nl> if ( ret ) { <nl> dev_err (& pdev -> dev , " failed to add device \ n "); <nl> device_remove_software_node (& qcom -> dwc3 -> dev ); <nl> + goto out ; <nl> } <nl> + kfree ( child_res ); <nl> + return 0 ; <nl>  <nl> out : <nl> + platform_device_put ( qcom -> dwc3 ); <nl> kfree ( child_res ); <nl> return ret ; <nl> }
mmm drivers / char / misc . c <nl> ppp drivers / char / misc . c <nl> static int misc_open ( struct inode * inode , struct file * file ) <nl> old_fops = file -> f_op ; <nl> file -> f_op = new_fops ; <nl> if ( file -> f_op -> open ) { <nl> + file -> private_data = c ; <nl> err = file -> f_op -> open ( inode , file ); <nl> if ( err ) { <nl> fops_put ( file -> f_op );
mmm include / linux / init_task . h <nl> ppp include / linux / init_task . h <nl> extern struct cred init_cred ; <nl> [ PIDTYPE_PGID ] = INIT_PID_LINK ( PIDTYPE_PGID ), \ <nl> [ PIDTYPE_SID ] = INIT_PID_LINK ( PIDTYPE_SID ), \ <nl> }, \ <nl> + . thread_group = LIST_HEAD_INIT ( tsk . thread_group ), \ <nl> . dirties = INIT_PROP_LOCAL_SINGLE ( dirties ), \ <nl> INIT_IDS \ <nl> INIT_PERF_EVENTS ( tsk ) \
mmm drivers / usb / dwc2 / core . h <nl> ppp drivers / usb / dwc2 / core . h <nl> struct dwc2_hregs_backup { <nl> * @ ctrl_req : Request for EP0 control packets . <nl> * @ ep0_state : EP0 control transfers state <nl> * @ test_mode : USB test mode requested by the host <nl> + * @ remote_wakeup_allowed : True if device is allowed to wake - up host by <nl> + * remote - wakeup signalling <nl> * @ setup_desc_dma : EP0 setup stage desc chain DMA address <nl> * @ setup_desc : EP0 setup stage desc chain pointer <nl> * @ ctrl_in_desc_dma : EP0 IN data phase desc chain DMA address <nl> struct dwc2_hsotg { <nl> struct usb_gadget gadget ; <nl> unsigned int enabled : 1 ; <nl> unsigned int connected : 1 ; <nl> + unsigned int remote_wakeup_allowed : 1 ; <nl> struct dwc2_hsotg_ep * eps_in [ MAX_EPS_CHANNELS ]; <nl> struct dwc2_hsotg_ep * eps_out [ MAX_EPS_CHANNELS ]; <nl> # endif /* CONFIG_USB_DWC2_PERIPHERAL || CONFIG_USB_DWC2_DUAL_ROLE */mmm drivers / usb / dwc2 / gadget . c <nl> ppp drivers / usb / dwc2 / gadget . c <nl> struct dwc2_hregs_backup { <nl> * @ ctrl_req : Request for EP0 control packets . <nl> * @ ep0_state : EP0 control transfers state <nl> * @ test_mode : USB test mode requested by the host <nl> + * @ remote_wakeup_allowed : True if device is allowed to wake - up host by <nl> + * remote - wakeup signalling <nl> * @ setup_desc_dma : EP0 setup stage desc chain DMA address <nl> * @ setup_desc : EP0 setup stage desc chain pointer <nl> * @ ctrl_in_desc_dma : EP0 IN data phase desc chain DMA address <nl> struct dwc2_hsotg { <nl> struct usb_gadget gadget ; <nl> unsigned int enabled : 1 ; <nl> unsigned int connected : 1 ; <nl> + unsigned int remote_wakeup_allowed : 1 ; <nl> struct dwc2_hsotg_ep * eps_in [ MAX_EPS_CHANNELS ]; <nl> struct dwc2_hsotg_ep * eps_out [ MAX_EPS_CHANNELS ]; <nl> # endif /* CONFIG_USB_DWC2_PERIPHERAL || CONFIG_USB_DWC2_DUAL_ROLE */ <nl> static int dwc2_hsotg_process_req_feature ( struct dwc2_hsotg * hsotg , <nl> switch ( recip ) { <nl> case USB_RECIP_DEVICE : <nl> switch ( wValue ) { <nl> + case USB_DEVICE_REMOTE_WAKEUP : <nl> + hsotg -> remote_wakeup_allowed = 1 ; <nl> + break ; <nl> + <nl> case USB_DEVICE_TEST_MODE : <nl> if (( wIndex & 0xff ) != 0 ) <nl> return - EINVAL ; <nl> int dwc2_gadget_init ( struct dwc2_hsotg * hsotg ) <nl> hsotg -> gadget . max_speed = USB_SPEED_HIGH ; <nl> hsotg -> gadget . ops = & dwc2_hsotg_gadget_ops ; <nl> hsotg -> gadget . name = dev_name ( dev ); <nl> + hsotg -> remote_wakeup_allowed = 0 ; <nl>  <nl> if ( hsotg -> params . lpm ) <nl> hsotg -> gadget . lpm_capable = true ;
mmm arch / x86 / kvm / paging_tmpl . h <nl> ppp arch / x86 / kvm / paging_tmpl . h <nl> static int FNAME ( walk_addr_generic )( struct guest_walker * walker , <nl> } <nl>  <nl> ptep_user = ( pt_element_t __user *)(( void *) host_addr + offset ); <nl> - if ( unlikely ( copy_from_user (& pte , ptep_user , sizeof ( pte )))) { <nl> + if ( unlikely ( __copy_from_user (& pte , ptep_user , sizeof ( pte )))) { <nl> present = false ; <nl> break ; <nl> }mmm virt / kvm / kvm_main . c <nl> ppp virt / kvm / kvm_main . c <nl> static int FNAME ( walk_addr_generic )( struct guest_walker * walker , <nl> } <nl>  <nl> ptep_user = ( pt_element_t __user *)(( void *) host_addr + offset ); <nl> - if ( unlikely ( copy_from_user (& pte , ptep_user , sizeof ( pte )))) { <nl> + if ( unlikely ( __copy_from_user (& pte , ptep_user , sizeof ( pte )))) { <nl> present = false ; <nl> break ; <nl> } <nl> int __kvm_set_memory_region ( struct kvm * kvm , <nl> goto out ; <nl> if ( mem -> guest_phys_addr & ( PAGE_SIZE - 1 )) <nl> goto out ; <nl> - if ( user_alloc && ( mem -> userspace_addr & ( PAGE_SIZE - 1 ))) <nl> + /* We can read the guest memory with __xxx_user () later on . */ <nl> + if ( user_alloc && <nl> + (( mem -> userspace_addr & ( PAGE_SIZE - 1 )) || <nl> + ! access_ok ( VERIFY_WRITE , mem -> userspace_addr , mem -> memory_size ))) <nl> goto out ; <nl> if ( mem -> slot >= KVM_MEMORY_SLOTS + KVM_PRIVATE_MEM_SLOTS ) <nl> goto out ; <nl> int kvm_read_guest_page ( struct kvm * kvm , gfn_t gfn , void * data , int offset , <nl> addr = gfn_to_hva ( kvm , gfn ); <nl> if ( kvm_is_error_hva ( addr )) <nl> return - EFAULT ; <nl> - r = copy_from_user ( data , ( void __user *) addr + offset , len ); <nl> + r = __copy_from_user ( data , ( void __user *) addr + offset , len ); <nl> if ( r ) <nl> return - EFAULT ; <nl> return 0 ;
mmm drivers / base / regmap / regmap . c <nl> ppp drivers / base / regmap / regmap . c <nl> int regmap_raw_read ( struct regmap * map , unsigned int reg , void * val , <nl> return - EINVAL ; <nl> if ( reg % map -> reg_stride ) <nl> return - EINVAL ; <nl> + if ( val_count == 0 ) <nl> + return - EINVAL ; <nl>  <nl> map -> lock ( map -> lock_arg ); <nl> 
mmm drivers / media / video / gspca / gspca . c <nl> ppp drivers / media / video / gspca / gspca . c <nl> void gspca_frame_add ( struct gspca_dev * gspca_dev , <nl> } else { <nl> switch ( gspca_dev -> last_packet_type ) { <nl> case DISCARD_PACKET : <nl> - if ( packet_type == LAST_PACKET ) <nl> + if ( packet_type == LAST_PACKET ) { <nl> gspca_dev -> last_packet_type = packet_type ; <nl> + gspca_dev -> image = NULL ; <nl> + gspca_dev -> image_len = 0 ; <nl> + } <nl> return ; <nl> case LAST_PACKET : <nl> return ;
mmm arch / arm / mach - pxa / magician . c <nl> ppp arch / arm / mach - pxa / magician . c <nl> static void samsung_lcd_power ( int on , struct fb_var_screeninfo * si ) <nl> gpio_set_value ( GPIO75_MAGICIAN_SAMSUNG_POWER , 1 ); <nl> else <nl> gpio_set_value ( EGPIO_MAGICIAN_LCD_POWER , 1 ); <nl> - mdelay ( 10 ); <nl> + mdelay ( 6 ); <nl> gpio_set_value ( GPIO106_MAGICIAN_LCD_DCDC_NRESET , 1 ); <nl> - mdelay ( 10 ); <nl> + mdelay ( 6 ); /* Avdd -> Voff > 5ms */ <nl> gpio_set_value ( GPIO104_MAGICIAN_LCD_VOFF_EN , 1 ); <nl> - mdelay ( 30 ); <nl> + mdelay ( 16 ); /* Voff -> Von >( 5 + 10 ) ms */ <nl> gpio_set_value ( GPIO105_MAGICIAN_LCD_VON_EN , 1 ); <nl> - mdelay ( 10 ); <nl> } else { <nl> - mdelay ( 10 ); <nl> gpio_set_value ( GPIO105_MAGICIAN_LCD_VON_EN , 0 ); <nl> - mdelay ( 30 ); <nl> + mdelay ( 16 ); <nl> gpio_set_value ( GPIO104_MAGICIAN_LCD_VOFF_EN , 0 ); <nl> - mdelay ( 10 ); <nl> + mdelay ( 6 ); <nl> gpio_set_value ( GPIO106_MAGICIAN_LCD_DCDC_NRESET , 0 ); <nl> - mdelay ( 10 ); <nl> + mdelay ( 6 ); <nl> if ( system_rev < 3 ) <nl> gpio_set_value ( GPIO75_MAGICIAN_SAMSUNG_POWER , 0 ); <nl> else
mmm sound / soc / codecs / sgtl5000 . c <nl> ppp sound / soc / codecs / sgtl5000 . c <nl> static int sgtl5000_set_clock ( struct snd_soc_codec * codec , int frame_rate ) <nl> } else { <nl> dev_err ( codec -> dev , <nl> " PLL not supported in slave mode \ n "); <nl> + dev_err ( codec -> dev , "% d ratio is not supported . " <nl> + " SYS_MCLK needs to be 256 , 384 or 512 * fs \ n ", <nl> + sgtl5000 -> sysclk / sys_fs ); <nl> return - EINVAL ; <nl> } <nl> }
mmm sound / soc / soc - cache . c <nl> ppp sound / soc / soc - cache . c <nl> static int snd_soc_8_16_write ( struct snd_soc_codec * codec , unsigned int reg , <nl> data [ 1 ] = ( value >> 8 ) & 0xff ; <nl> data [ 2 ] = value & 0xff ; <nl>  <nl> - if (! snd_soc_codec_volatile_register ( codec , reg )) <nl> - reg_cache [ reg ] = value ; <nl> + if (! snd_soc_codec_volatile_register ( codec , reg ) <nl> + && reg < codec -> driver -> reg_cache_size ) <nl> + reg_cache [ reg ] = value ; <nl>  <nl> if ( codec -> cache_only ) { <nl> codec -> cache_sync = 1 ;
mmm drivers / net / wireless / rtlwifi / pci . c <nl> ppp drivers / net / wireless / rtlwifi / pci . c <nl> static bool _rtl_pci_find_adapter ( struct pci_dev * pdev , <nl> pci_read_config_byte ( pdev , 0x8 , & revisionid ); <nl> pci_read_config_word ( pdev , 0x3C , & irqline ); <nl>  <nl> + /* PCI ID 0x10ec : 0x8192 occurs for both RTL8192E , which uses <nl> + * r8192e_pci , and RTL8192SE , which uses this driver . If the <nl> + * revision ID is RTL_PCI_REVISION_ID_8192PCIE ( 0x01 ), then <nl> + * the correct driver is r8192e_pci , thus this routine should <nl> + * return false . <nl> + */ <nl> + if ( deviceid == RTL_PCI_8192SE_DID && <nl> + revisionid == RTL_PCI_REVISION_ID_8192PCIE ) <nl> + return false ; <nl> + <nl> if ( deviceid == RTL_PCI_8192_DID || <nl> deviceid == RTL_PCI_0044_DID || <nl> deviceid == RTL_PCI_0047_DID || <nl> int __devinit rtl_pci_probe ( struct pci_dev * pdev , <nl> pci_write_config_byte ( pdev , 0x04 , 0x07 ); <nl>  <nl> /* find adapter */ <nl> - _rtl_pci_find_adapter ( pdev , hw ); <nl> + if (! _rtl_pci_find_adapter ( pdev , hw )) <nl> + goto fail3 ; <nl>  <nl> /* Init IO handler */ <nl> _rtl_pci_io_handler_init (& pdev -> dev , hw );
mmm drivers / char / ipmi / ipmi_devintf . c <nl> ppp drivers / char / ipmi / ipmi_devintf . c <nl> static long compat_ipmi_ioctl ( struct file * filep , unsigned int cmd , <nl> struct ipmi_recv __user * precv64 ; <nl> struct ipmi_recv recv64 ; <nl>  <nl> + memset (& recv64 , 0 , sizeof ( recv64 )); <nl> if ( get_compat_ipmi_recv (& recv64 , compat_ptr ( arg ))) <nl> return - EFAULT ; <nl> 
mmm kernel / cgroup . c <nl> ppp kernel / cgroup . c <nl> int cgroup_add_legacy_cftypes ( struct cgroup_subsys * ss , struct cftype * cfts ) <nl> { <nl> struct cftype * cft ; <nl>  <nl> - for ( cft = cfts ; cft && cft -> name [ 0 ] != '\ 0 '; cft ++) <nl> - cft -> flags |= __CFTYPE_NOT_ON_DFL ; <nl> + /* <nl> + * If legacy_flies_on_dfl , we want to show the legacy files on the <nl> + * dfl hierarchy but iff the target subsystem hasn ' t been updated <nl> + * for the dfl hierarchy yet . <nl> + */ <nl> + if (! cgroup_legacy_files_on_dfl || <nl> + ss -> dfl_cftypes != ss -> legacy_cftypes ) { <nl> + for ( cft = cfts ; cft && cft -> name [ 0 ] != '\ 0 '; cft ++) <nl> + cft -> flags |= __CFTYPE_NOT_ON_DFL ; <nl> + } <nl> + <nl> return cgroup_add_cftypes ( ss , cfts ); <nl> } <nl> 
mmm fs / xfs / xfs_acl . c <nl> ppp fs / xfs / xfs_acl . c <nl> xfs_acl_from_disk ( struct xfs_acl * aclp ) <nl> int count , i ; <nl>  <nl> count = be32_to_cpu ( aclp -> acl_cnt ); <nl> + if ( count > XFS_ACL_MAX_ENTRIES ) <nl> + return ERR_PTR (- EFSCORRUPTED ); <nl>  <nl> acl = posix_acl_alloc ( count , GFP_KERNEL ); <nl> if (! acl )
mmm drivers / block / nbd . c <nl> ppp drivers / block / nbd . c <nl> static void nbd_config_put ( struct nbd_device * nbd ) <nl> } <nl> kfree ( config -> socks ); <nl> } <nl> + kfree ( nbd -> config ); <nl> nbd -> config = NULL ; <nl>  <nl> nbd -> tag_set . timeout = 0 ;
mmm drivers / nvme / host / core . c <nl> ppp drivers / nvme / host / core . c <nl> static void nvme_init_integrity ( struct nvme_ns * ns ) <nl> { <nl> struct blk_integrity integrity ; <nl>  <nl> + memset (& integrity , 0 , sizeof ( integrity )); <nl> switch ( ns -> pi_type ) { <nl> case NVME_NS_DPS_PI_TYPE3 : <nl> integrity . profile = & t10_pi_type3_crc ;
mmm drivers / net / phy / dp83640 . c <nl> ppp drivers / net / phy / dp83640 . c <nl> static int ptp_dp83640_enable ( struct ptp_clock_info * ptp , <nl> event_num = EXT_EVENT + index ; <nl> evnt = EVNT_WR | ( event_num & EVNT_SEL_MASK ) << EVNT_SEL_SHIFT ; <nl> if ( on ) { <nl> - gpio_num = gpio_tab [ EXTTS0_GPIO + index ]; <nl> + gpio_num = 1 + ptp_find_pin ( clock -> ptp_clock , <nl> + PTP_PF_EXTTS , index ); <nl> + if ( gpio_num < 1 ) <nl> + return - EINVAL ; <nl> evnt |= ( gpio_num & EVNT_GPIO_MASK ) << EVNT_GPIO_SHIFT ; <nl> if ( rq -> extts . flags & PTP_FALLING_EDGE ) <nl> evnt |= EVNT_FALL ;
mmm drivers / gpu / drm / i915 / gvt / kvmgt . c <nl> ppp drivers / gpu / drm / i915 / gvt / kvmgt . c <nl> static int kvmgt_write_protect_add ( unsigned long handle , u64 gfn ) <nl>  <nl> idx = srcu_read_lock (& kvm -> srcu ); <nl> slot = gfn_to_memslot ( kvm , gfn ); <nl> + if (! slot ) { <nl> + srcu_read_unlock (& kvm -> srcu , idx ); <nl> + return - EINVAL ; <nl> + } <nl>  <nl> spin_lock (& kvm -> mmu_lock ); <nl>  <nl> static int kvmgt_write_protect_remove ( unsigned long handle , u64 gfn ) <nl>  <nl> idx = srcu_read_lock (& kvm -> srcu ); <nl> slot = gfn_to_memslot ( kvm , gfn ); <nl> + if (! slot ) { <nl> + srcu_read_unlock (& kvm -> srcu , idx ); <nl> + return - EINVAL ; <nl> + } <nl>  <nl> spin_lock (& kvm -> mmu_lock ); <nl> 
mmm net / netlink / af_netlink . c <nl> ppp net / netlink / af_netlink . c <nl> static struct pernet_operations __net_initdata netlink_net_ops = { <nl>  <nl> static int __init netlink_proto_init ( void ) <nl> { <nl> - struct sk_buff * dummy_skb ; <nl> int i ; <nl> unsigned long limit ; <nl> unsigned int order ; <nl> static int __init netlink_proto_init ( void ) <nl> if ( err != 0 ) <nl> goto out ; <nl>  <nl> - BUILD_BUG_ON ( sizeof ( struct netlink_skb_parms ) > sizeof ( dummy_skb -> cb )); <nl> + BUILD_BUG_ON ( sizeof ( struct netlink_skb_parms ) > FIELD_SIZEOF ( struct sk_buff , cb )); <nl>  <nl> nl_table = kcalloc ( MAX_LINKS , sizeof (* nl_table ), GFP_KERNEL ); <nl> if (! nl_table )
mmm drivers / staging / wilc1000 / host_interface . c <nl> ppp drivers / staging / wilc1000 / host_interface . c <nl> int wilc_deinit ( struct wilc_vif * vif ) <nl>  <nl> if ( hif_drv -> usr_scan_req . scan_result ) { <nl> hif_drv -> usr_scan_req . scan_result ( SCAN_EVENT_ABORTED , NULL , <nl> - hif_drv -> usr_scan_req . arg , NULL ); <nl> + hif_drv -> usr_scan_req . arg , <nl> + NULL ); <nl> hif_drv -> usr_scan_req . scan_result = NULL ; <nl> } <nl>  <nl> int wilc_del_allstation ( struct wilc_vif * vif , u8 mac_addr [][ ETH_ALEN ]) <nl>  <nl> for ( i = 0 ; i < MAX_NUM_STA ; i ++) { <nl> if ( memcmp ( mac_addr [ i ], zero_addr , ETH_ALEN )) { <nl> - memcpy ( del_all_sta_info -> del_all_sta [ i ], mac_addr [ i ], ETH_ALEN ); <nl> + memcpy ( del_all_sta_info -> del_all_sta [ i ], mac_addr [ i ], <nl> + ETH_ALEN ); <nl> assoc_sta ++; <nl> } <nl> }
mmm kernel / torture . c <nl> ppp kernel / torture . c <nl> static int torture_shutdown_notify ( struct notifier_block * unused1 , <nl> unsigned long unused2 , void * unused3 ) <nl> { <nl> mutex_lock (& fullstop_mutex ); <nl> - if ( fullstop == FULLSTOP_DONTSTOP ) <nl> + if ( fullstop == FULLSTOP_DONTSTOP ) { <nl> + VERBOSE_TOROUT_STRING (" Unscheduled system shutdown detected "); <nl> fullstop = FULLSTOP_SHUTDOWN ; <nl> - else <nl> + } else { <nl> pr_warn (" Concurrent rmmod and shutdown illegal !\ n "); <nl> + } <nl> mutex_unlock (& fullstop_mutex ); <nl> return NOTIFY_DONE ; <nl> }
mmm drivers / net / wireless / brcm80211 / brcmfmac / wl_cfg80211 . c <nl> ppp drivers / net / wireless / brcm80211 / brcmfmac / wl_cfg80211 . c <nl> static int brcmf_enable_bw40_2g ( struct brcmf_cfg80211_info * cfg ) <nl>  <nl> ch . band = BRCMU_CHAN_BAND_2G ; <nl> ch . bw = BRCMU_CHAN_BW_40 ; <nl> + ch . sb = BRCMU_CHAN_SB_NONE ; <nl> ch . chnum = 0 ; <nl> cfg -> d11inf . encchspec (& ch ); <nl>  <nl> static int brcmf_enable_bw40_2g ( struct brcmf_cfg80211_info * cfg ) <nl>  <nl> brcmf_update_bw40_channel_flag (& band -> channels [ j ], & ch ); <nl> } <nl> + kfree ( pbuf ); <nl> } <nl> return err ; <nl> }
mmm drivers / gpu / drm / amd / amdgpu / amdgpu . h <nl> ppp drivers / gpu / drm / amd / amdgpu / amdgpu . h <nl> int amdgpu_cs_parser_init ( struct amdgpu_cs_parser * p , void * data ); <nl> int amdgpu_cs_get_ring ( struct amdgpu_device * adev , u32 ip_type , <nl> u32 ip_instance , u32 ring , <nl> struct amdgpu_ring ** out_ring ); <nl> + void amdgpu_cs_report_moved_bytes ( struct amdgpu_device * adev , u64 num_bytes ); <nl> void amdgpu_ttm_placement_from_domain ( struct amdgpu_bo * abo , u32 domain ); <nl> bool amdgpu_ttm_bo_is_amdgpu_bo ( struct ttm_buffer_object * bo ); <nl> int amdgpu_ttm_tt_get_user_pages ( struct ttm_tt * ttm , struct page ** pages );mmm drivers / gpu / drm / amd / amdgpu / amdgpu_object . c <nl> ppp drivers / gpu / drm / amd / amdgpu / amdgpu_object . c <nl> int amdgpu_cs_parser_init ( struct amdgpu_cs_parser * p , void * data ); <nl> int amdgpu_cs_get_ring ( struct amdgpu_device * adev , u32 ip_type , <nl> u32 ip_instance , u32 ring , <nl> struct amdgpu_ring ** out_ring ); <nl> + void amdgpu_cs_report_moved_bytes ( struct amdgpu_device * adev , u64 num_bytes ); <nl> void amdgpu_ttm_placement_from_domain ( struct amdgpu_bo * abo , u32 domain ); <nl> bool amdgpu_ttm_bo_is_amdgpu_bo ( struct ttm_buffer_object * bo ); <nl> int amdgpu_ttm_tt_get_user_pages ( struct ttm_tt * ttm , struct page ** pages ); <nl> int amdgpu_bo_create_restricted ( struct amdgpu_device * adev , <nl> struct amdgpu_bo * bo ; <nl> enum ttm_bo_type type ; <nl> unsigned long page_align ; <nl> + u64 initial_bytes_moved ; <nl> size_t acc_size ; <nl> int r ; <nl>  <nl> int amdgpu_bo_create_restricted ( struct amdgpu_device * adev , <nl> locked = ww_mutex_trylock (& bo -> tbo . ttm_resv . lock ); <nl> WARN_ON (! locked ); <nl> } <nl> + <nl> + initial_bytes_moved = atomic64_read (& adev -> num_bytes_moved ); <nl> r = ttm_bo_init (& adev -> mman . bdev , & bo -> tbo , size , type , <nl> & bo -> placement , page_align , ! kernel , NULL , <nl> acc_size , sg , resv ? resv : & bo -> tbo . ttm_resv , <nl> & amdgpu_ttm_bo_destroy ); <nl> + amdgpu_cs_report_moved_bytes ( adev , <nl> + atomic64_read (& adev -> num_bytes_moved ) - initial_bytes_moved ); <nl> + <nl> if ( unlikely ( r != 0 )) { <nl> if (! resv ) <nl> ww_mutex_unlock (& bo -> tbo . resv -> lock );mmm drivers / gpu / drm / amd / amdgpu / amdgpu_cs . c <nl> ppp drivers / gpu / drm / amd / amdgpu / amdgpu_cs . c <nl> int amdgpu_cs_parser_init ( struct amdgpu_cs_parser * p , void * data ); <nl> int amdgpu_cs_get_ring ( struct amdgpu_device * adev , u32 ip_type , <nl> u32 ip_instance , u32 ring , <nl> struct amdgpu_ring ** out_ring ); <nl> + void amdgpu_cs_report_moved_bytes ( struct amdgpu_device * adev , u64 num_bytes ); <nl> void amdgpu_ttm_placement_from_domain ( struct amdgpu_bo * abo , u32 domain ); <nl> bool amdgpu_ttm_bo_is_amdgpu_bo ( struct ttm_buffer_object * bo ); <nl> int amdgpu_ttm_tt_get_user_pages ( struct ttm_tt * ttm , struct page ** pages ); <nl> int amdgpu_bo_create_restricted ( struct amdgpu_device * adev , <nl> struct amdgpu_bo * bo ; <nl> enum ttm_bo_type type ; <nl> unsigned long page_align ; <nl> + u64 initial_bytes_moved ; <nl> size_t acc_size ; <nl> int r ; <nl>  <nl> int amdgpu_bo_create_restricted ( struct amdgpu_device * adev , <nl> locked = ww_mutex_trylock (& bo -> tbo . ttm_resv . lock ); <nl> WARN_ON (! locked ); <nl> } <nl> + <nl> + initial_bytes_moved = atomic64_read (& adev -> num_bytes_moved ); <nl> r = ttm_bo_init (& adev -> mman . bdev , & bo -> tbo , size , type , <nl> & bo -> placement , page_align , ! kernel , NULL , <nl> acc_size , sg , resv ? resv : & bo -> tbo . ttm_resv , <nl> & amdgpu_ttm_bo_destroy ); <nl> + amdgpu_cs_report_moved_bytes ( adev , <nl> + atomic64_read (& adev -> num_bytes_moved ) - initial_bytes_moved ); <nl> + <nl> if ( unlikely ( r != 0 )) { <nl> if (! resv ) <nl> ww_mutex_unlock (& bo -> tbo . resv -> lock ); <nl> static u64 amdgpu_cs_get_threshold_for_moves ( struct amdgpu_device * adev ) <nl> * submission . This can result in a debt that can stop buffer migrations <nl> * temporarily . <nl> */ <nl> - static void amdgpu_cs_report_moved_bytes ( struct amdgpu_device * adev , <nl> - u64 num_bytes ) <nl> + void amdgpu_cs_report_moved_bytes ( struct amdgpu_device * adev , u64 num_bytes ) <nl> { <nl> spin_lock (& adev -> mm_stats . lock ); <nl> adev -> mm_stats . accum_us -= bytes_to_us ( adev , num_bytes );
mmm sound / soc / omap / omap - pcm . c <nl> ppp sound / soc / omap / omap - pcm . c <nl> static int omap_pcm_new ( struct snd_soc_pcm_runtime * rtd ) <nl> } <nl>  <nl> out : <nl> + /* free preallocated buffers in case of error */ <nl> + if ( ret ) <nl> + omap_pcm_free_dma_buffers ( pcm ); <nl> + <nl> return ret ; <nl> } <nl> 
mmm drivers / net / pcmcia / 3c589_cs . c <nl> ppp drivers / net / pcmcia / 3c589_cs . c <nl> struct el3_private { <nl> spinlock_t lock ; <nl> }; <nl>  <nl> - static const char * if_names [] = { " auto ", " 10baseT ", " 10base2 ", " AUI " }; <nl> + static const char * if_names [] = { " auto ", " 10base2 ", " 10baseT ", " AUI " }; <nl>  <nl> /*====================================================================*/ <nl> 
mmm drivers / input / keyboard / tegra - kbc . c <nl> ppp drivers / input / keyboard / tegra - kbc . c <nl> static int tegra_kbc_start ( struct tegra_kbc * kbc ) <nl> /* Reset the KBC controller to clear all previous status .*/ <nl> reset_control_assert ( kbc -> rst ); <nl> udelay ( 100 ); <nl> - reset_control_assert ( kbc -> rst ); <nl> + reset_control_deassert ( kbc -> rst ); <nl> udelay ( 100 ); <nl>  <nl> tegra_kbc_config_pins ( kbc );
mmm drivers / usb / serial / cp210x . c <nl> ppp drivers / usb / serial / cp210x . c <nl> static const struct usb_device_id id_table [] = { <nl> { USB_DEVICE ( 0x10C4 , 0x8341 ) }, /* Siemens MC35PU GPRS Modem */ <nl> { USB_DEVICE ( 0x10C4 , 0x8382 ) }, /* Cygnal Integrated Products , Inc . */ <nl> { USB_DEVICE ( 0x10C4 , 0x83A8 ) }, /* Amber Wireless AMB2560 */ <nl> + { USB_DEVICE ( 0x10C4 , 0x83D8 ) }, /* DekTec DTA Plus VHF / UHF Booster / Attenuator */ <nl> { USB_DEVICE ( 0x10C4 , 0x8411 ) }, /* Kyocera GPS Module */ <nl> + { USB_DEVICE ( 0x10C4 , 0x8418 ) }, /* IRZ Automation Teleport SG - 10 GSM / GPRS Modem */ <nl> { USB_DEVICE ( 0x10C4 , 0x846E ) }, /* BEI USB Sensor Interface ( VCP ) */ <nl> { USB_DEVICE ( 0x10C4 , 0x8477 ) }, /* Balluff RFID */ <nl> { USB_DEVICE ( 0x10C4 , 0xEA60 ) }, /* Silicon Labs factory default */
mmm drivers / tty / serial / pch_uart . c <nl> ppp drivers / tty / serial / pch_uart . c <nl> static struct eg20t_port * pch_uart_init_port ( struct pci_dev * pdev , <nl> int fifosize , base_baud ; <nl> int port_type ; <nl> struct pch_uart_driver_data * board ; <nl> + const char * board_name ; <nl>  <nl> board = & drv_dat [ id -> driver_data ]; <nl> port_type = board -> port_type ; <nl> static struct eg20t_port * pch_uart_init_port ( struct pci_dev * pdev , <nl> base_baud = 1843200 ; /* 1 . 8432MHz */ <nl>  <nl> /* quirk for CM - iTC board */ <nl> - if ( strstr ( dmi_get_system_info ( DMI_BOARD_NAME ), " CM - iTC ")) <nl> + board_name = dmi_get_system_info ( DMI_BOARD_NAME ); <nl> + if ( board_name && strstr ( board_name , " CM - iTC ")) <nl> base_baud = 192000000 ; /* 192 . 0MHz */ <nl>  <nl> switch ( port_type ) {
mmm drivers / media / video / saa7134 / saa7134 - dvb . c <nl> ppp drivers / media / video / saa7134 / saa7134 - dvb . c <nl> static int dvb_init ( struct saa7134_dev * dev ) <nl> dev -> dvb . frontend = dvb_attach ( mt352_attach , & avermedia_777 , <nl> & dev -> i2c_adap ); <nl> if ( dev -> dvb . frontend ) { <nl> - dvb_attach ( dvb_pll_attach , dev -> dvb . frontend , 0x61 , <nl> - NULL , DVB_PLL_PHILIPS_TD1316 ); <nl> + dvb_attach ( simple_tuner_attach , dev -> dvb . frontend , <nl> + & dev -> i2c_adap , 0x61 , <nl> + TUNER_PHILIPS_TD1316 ); <nl> } <nl> break ; <nl> case SAA7134_BOARD_MD7134 :
mmm drivers / staging / iio / meter / ade7854 . c <nl> ppp drivers / staging / iio / meter / ade7854 . c <nl> static int ade7854_set_irq ( struct device * dev , bool enable ) <nl> else <nl> irqen &= ~ BIT ( 17 ); <nl>  <nl> - ret = st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> - <nl> - return ret ; <nl> + return st -> write_reg_32 ( dev , ADE7854_MASK0 , irqen ); <nl> } <nl>  <nl> static int ade7854_initial_setup ( struct iio_dev * indio_dev )
mmm drivers / net / can / usb / gs_usb . c <nl> ppp drivers / net / can / usb / gs_usb . c <nl> static int gs_can_open ( struct net_device * netdev ) <nl> rc ); <nl>  <nl> usb_unanchor_urb ( urb ); <nl> + usb_free_urb ( urb ); <nl> break ; <nl> } <nl> 
mmm drivers / staging / rtl8192e / r8192E_cmdpkt . h <nl> ppp drivers / staging / rtl8192e / r8192E_cmdpkt . h <nl> # ifndef R819XUSB_CMDPKT_H <nl> # define R819XUSB_CMDPKT_H <nl> # define CMPK_RX_TX_FB_SIZE sizeof ( struct cmpk_txfb ) <nl> -# define CMPK_TX_SET_CONFIG_SIZE sizeof ( cmpk_set_cfg_t ) <nl> -# define CMPK_BOTH_QUERY_CONFIG_SIZE sizeof ( cmpk_set_cfg_t ) <nl> +# define CMPK_TX_SET_CONFIG_SIZE sizeof ( struct cmpk_set_cfg ) <nl> +# define CMPK_BOTH_QUERY_CONFIG_SIZE sizeof ( struct cmpk_set_cfg ) <nl> # define CMPK_RX_TX_STS_SIZE sizeof ( cmpk_tx_status_t ) <nl> # define CMPK_RX_DBG_MSG_SIZE sizeof ( cmpk_rx_dbginfo_t ) <nl> # define CMPK_TX_RAHIS_SIZE sizeof ( cmpk_tx_rahis_t ) <nl> struct cmpk_intr_sta { <nl> };//; <nl>  <nl>  <nl> - typedef struct tag_cmd_pkt_set_configuration <nl> -{ <nl> + struct cmpk_set_cfg { <nl> u8 element_id ; <nl> u8 length ; <nl> u16 reserve1 ; <nl> typedef struct tag_cmd_pkt_set_configuration <nl> u8 cfg_offset ; <nl> u32 value ; <nl> u32 mask ; <nl> -} cmpk_set_cfg_t ; <nl> +};//; <nl>  <nl> -# define cmpk_query_cfg_t cmpk_set_cfg_t <nl> +# define cmpk_query_cfg_t struct cmpk_set_cfg <nl>  <nl> typedef struct tag_tx_stats_feedback <nl> {
mmm kernel / sysctl . c <nl> ppp kernel / sysctl . c <nl> static int do_proc_douintvec_minmax_conv ( unsigned long * lvalp , <nl> if ( write ) { <nl> unsigned int val = * lvalp ; <nl>  <nl> + if (* lvalp > UINT_MAX ) <nl> + return - EINVAL ; <nl> + <nl> if (( param -> min && * param -> min > val ) || <nl> ( param -> max && * param -> max < val )) <nl> return - ERANGE ; <nl>  <nl> - if (* lvalp > UINT_MAX ) <nl> - return - EINVAL ; <nl> * valp = val ; <nl> } else { <nl> unsigned int val = * valp ; <nl> static int do_proc_dopipe_max_size_conv ( unsigned long * lvalp , <nl> struct do_proc_dopipe_max_size_conv_param * param = data ; <nl>  <nl> if ( write ) { <nl> - unsigned int val = round_pipe_size (* lvalp ); <nl> + unsigned int val ; <nl>  <nl> + if (* lvalp > UINT_MAX ) <nl> + return - EINVAL ; <nl> + <nl> + val = round_pipe_size (* lvalp ); <nl> if ( val == 0 ) <nl> return - EINVAL ; <nl>  <nl> if ( param -> min && * param -> min > val ) <nl> return - ERANGE ; <nl>  <nl> - if (* lvalp > UINT_MAX ) <nl> - return - EINVAL ; <nl> - <nl> * valp = val ; <nl> } else { <nl> unsigned int val = * valp ;
mmm drivers / scsi / ibmvscsi / ibmvscsi . c <nl> ppp drivers / scsi / ibmvscsi / ibmvscsi . c <nl> static int ibmvscsi_probe ( struct vio_dev * vdev , const struct vio_device_id * id ) <nl> host -> max_lun = 8 ; <nl> host -> max_id = max_id ; <nl> host -> max_channel = max_channel ; <nl> + host -> max_cmd_len = 16 ; <nl>  <nl> if ( scsi_add_host ( hostdata -> host , hostdata -> dev )) <nl> goto add_host_failed ;
mmm drivers / scsi / qla2xxx / qla_tmpl . c <nl> ppp drivers / scsi / qla2xxx / qla_tmpl . c <nl> qla27xx_fwdt_entry_t270 ( struct scsi_qla_host * vha , <nl> qla27xx_write_reg ( reg , 0xc0 , addr | 0x80000000 , buf ); <nl> qla27xx_insert32 ( addr , buf , len ); <nl> qla27xx_read_off ( reg , 0xc4 , buf , len ); <nl> - addr ++; <nl> + addr += sizeof ( uint32_t ); <nl> } <nl>  <nl> return false ;
mmm kernel / futex . c <nl> ppp kernel / futex . c <nl> static int futex_requeue ( u32 __user * uaddr1 , unsigned int flags , <nl> struct futex_q * this , * next ; <nl> DEFINE_WAKE_Q ( wake_q ); <nl>  <nl> + if ( nr_wake < 0 || nr_requeue < 0 ) <nl> + return - EINVAL ; <nl> + <nl> /* <nl> * When PI not supported : return - ENOSYS if requeue_pi is true , <nl> * consequently the compiler knows requeue_pi is always false past
mmm drivers / media / v4l2 - core / v4l2 - async . c <nl> ppp drivers / media / v4l2 - core / v4l2 - async . c <nl> int v4l2_async_notifier_register ( struct v4l2_device * v4l2_dev , <nl> struct v4l2_async_subdev * asd ; <nl> int i ; <nl>  <nl> - if (! notifier -> num_subdevs || notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> + if (! v4l2_dev || ! notifier -> num_subdevs || <nl> + notifier -> num_subdevs > V4L2_MAX_SUBDEVS ) <nl> return - EINVAL ; <nl>  <nl> notifier -> v4l2_dev = v4l2_dev ;
mmm fs / nfs / super . c <nl> ppp fs / nfs / super . c <nl> static int nfs_statfs ( struct dentry * dentry , struct kstatfs * buf ) <nl> goto out_err ; <nl>  <nl> error = server -> nfs_client -> rpc_ops -> statfs ( server , fh , & res ); <nl> + if ( unlikely ( error == - ESTALE )) { <nl> + struct dentry * pd_dentry ; <nl>  <nl> + pd_dentry = dget_parent ( dentry ); <nl> + if ( pd_dentry != NULL ) { <nl> + nfs_zap_caches ( pd_dentry -> d_inode ); <nl> + dput ( pd_dentry ); <nl> + } <nl> + } <nl> nfs_free_fattr ( res . fattr ); <nl> if ( error < 0 ) <nl> goto out_err ;
mmm fs / cifs / cifsacl . c <nl> ppp fs / cifs / cifsacl . c <nl> static int parse_sid ( struct cifs_sid * psid , char * end_of_acl ) <nl> return - EINVAL ; <nl> } <nl>  <nl> - if ( psid -> num_subauth ) { <nl> # ifdef CONFIG_CIFS_DEBUG2 <nl> + if ( psid -> num_subauth ) { <nl> int i ; <nl> cFYI ( 1 , " SID revision % d num_auth % d ", <nl> psid -> revision , psid -> num_subauth ); <nl> static int parse_sid ( struct cifs_sid * psid , char * end_of_acl ) <nl> num auths and therefore go off the end */ <nl> cFYI ( 1 , " RID 0x % x ", <nl> le32_to_cpu ( psid -> sub_auth [ psid -> num_subauth - 1 ])); <nl> -# endif <nl> } <nl> +# endif <nl>  <nl> return 0 ; <nl> }
mmm drivers / usb / gadget / fsl_qe_udc . c <nl> ppp drivers / usb / gadget / fsl_qe_udc . c <nl> static int qe_ep_enable ( struct usb_ep * _ep , <nl> ep = container_of ( _ep , struct qe_ep , ep ); <nl>  <nl> /* catch various bogus parameters */ <nl> - if (! _ep || ! desc || ep -> ep . desc || _ep -> name == ep_name [ 0 ] || <nl> + if (! _ep || ! desc || _ep -> name == ep_name [ 0 ] || <nl> ( desc -> bDescriptorType != USB_DT_ENDPOINT )) <nl> return - EINVAL ; <nl> 
mmm drivers / net / sis190 . c <nl> ppp drivers / net / sis190 . c <nl> static int __mdio_read ( struct net_device * dev , int phy_id , int reg ) <nl> return mdio_read ( tp -> mmio_addr , phy_id , reg ); <nl> } <nl>  <nl> + static u16 mdio_read_latched ( void __iomem * ioaddr , int phy_id , int reg ) <nl> +{ <nl> + mdio_read ( ioaddr , phy_id , reg ); <nl> + return mdio_read ( ioaddr , phy_id , reg ); <nl> +} <nl> + <nl> static u16 __devinit sis190_read_eeprom ( void __iomem * ioaddr , u32 reg ) <nl> { <nl> u16 data = 0xffff ; <nl> static void sis190_phy_task ( void * data ) <nl> if ( val & BMCR_RESET ) { <nl> // FIXME : needlessly high ? -- FR 02 / 07 / 2005 <nl> mod_timer (& tp -> timer , jiffies + HZ / 10 ); <nl> - } else if (!( mdio_read ( ioaddr , phy_id , MII_BMSR ) & BMSR_ANEGCOMPLETE )) { <nl> + } else if (!( mdio_read_latched ( ioaddr , phy_id , MII_BMSR ) & <nl> + BMSR_ANEGCOMPLETE )) { <nl> net_link ( tp , KERN_WARNING "% s : PHY reset until link up .\ n ", <nl> dev -> name ); <nl> mdio_write ( ioaddr , phy_id , MII_BMCR , val | BMCR_RESET );
mmm drivers / pci / probe . c <nl> ppp drivers / pci / probe . c <nl> int pci_scan_bridge ( struct pci_bus * bus , struct pci_dev * dev , int max , int pass ) <nl> goto out ; <nl> } <nl>  <nl> + if ( max >= bus -> busn_res . end ) { <nl> + dev_warn (& dev -> dev , " can ' t allocate child bus % 02x from % pR \ n ", <nl> + max , & bus -> busn_res ); <nl> + goto out ; <nl> + } <nl> + <nl> /* Clear errors */ <nl> pci_write_config_word ( dev , PCI_STATUS , 0xffff ); <nl>  <nl> - /* Prevent assigning a bus number that already exists . <nl> - * This can happen when a bridge is hot - plugged , so in <nl> - * this case we only re - scan this bus . */ <nl> + /* The bus will already exist if we are rescanning */ <nl> child = pci_find_bus ( pci_domain_nr ( bus ), max + 1 ); <nl> if (! child ) { <nl> child = pci_add_new_bus ( bus , dev , max + 1 );
mmm drivers / net / wireless / brcm80211 / brcmfmac / firmware . c <nl> ppp drivers / net / wireless / brcm80211 / brcmfmac / firmware . c <nl> struct nvram_parser { <nl> bool multi_dev_v2 ; <nl> }; <nl>  <nl> +/** <nl> + * is_nvram_char () - check if char is a valid one for NVRAM entry <nl> + * <nl> + * It accepts all printable ASCII chars except for '#' which opens a comment . <nl> + * Please note that ' ' ( space ) while accepted is not a valid key name char . <nl> + */ <nl> static bool is_nvram_char ( char c ) <nl> { <nl> /* comment marker excluded */ <nl> static bool is_nvram_char ( char c ) <nl> return false ; <nl>  <nl> /* key and value may have any other readable character */ <nl> - return ( c > 0x20 && c < 0x7f ); <nl> + return ( c >= 0x20 && c < 0x7f ); <nl> } <nl>  <nl> static bool is_whitespace ( char c ) <nl> static enum nvram_parser_state brcmf_nvram_handle_key ( struct nvram_parser * nvp ) <nl> nvp -> multi_dev_v1 = true ; <nl> if ( strncmp (& nvp -> fwnv -> data [ nvp -> entry ], " pcie /", 5 ) == 0 ) <nl> nvp -> multi_dev_v2 = true ; <nl> - } else if (! is_nvram_char ( c )) { <nl> + } else if (! is_nvram_char ( c ) || c == ' ') { <nl> brcmf_dbg ( INFO , " warning : ln =% d : col =% d : '=' expected , skip invalid key entry \ n ", <nl> nvp -> line , nvp -> column ); <nl> return COMMENT ;
mmm sound / core / seq_device . c <nl> ppp sound / core / seq_device . c <nl> void snd_seq_device_load_drivers ( void ) <nl> flush_work (& autoload_work ); <nl> } <nl> EXPORT_SYMBOL ( snd_seq_device_load_drivers ); <nl> +# define cancel_autoload_drivers () cancel_work_sync (& autoload_work ) <nl> # else <nl> # define queue_autoload_drivers () /* NOP */ <nl> +# define cancel_autoload_drivers () /* NOP */ <nl> # endif <nl>  <nl> /* <nl> static int snd_seq_device_dev_free ( struct snd_device * device ) <nl> { <nl> struct snd_seq_device * dev = device -> device_data ; <nl>  <nl> + cancel_autoload_drivers (); <nl> put_device (& dev -> dev ); <nl> return 0 ; <nl> }
mmm arch / x86 / kvm / x86 . c <nl> ppp arch / x86 / kvm / x86 . c <nl> EXPORT_SYMBOL_GPL ( kvm_inject_realmode_interrupt ); <nl>  <nl> static int handle_emulation_failure ( struct kvm_vcpu * vcpu ) <nl> { <nl> + int r = EMULATE_DONE ; <nl> + <nl> ++ vcpu -> stat . insn_emulation_fail ; <nl> trace_kvm_emulate_insn_failed ( vcpu ); <nl> - vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; <nl> - vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_EMULATION ; <nl> - vcpu -> run -> internal . ndata = 0 ; <nl> + if (! is_guest_mode ( vcpu )) { <nl> + vcpu -> run -> exit_reason = KVM_EXIT_INTERNAL_ERROR ; <nl> + vcpu -> run -> internal . suberror = KVM_INTERNAL_ERROR_EMULATION ; <nl> + vcpu -> run -> internal . ndata = 0 ; <nl> + r = EMULATE_FAIL ; <nl> + } <nl> kvm_queue_exception ( vcpu , UD_VECTOR ); <nl> - return EMULATE_FAIL ; <nl> + <nl> + return r ; <nl> } <nl>  <nl> static bool reexecute_instruction ( struct kvm_vcpu * vcpu , gva_t gva )
mmm fs / pstore / ram . c <nl> ppp fs / pstore / ram . c <nl> static int ramoops_probe ( struct platform_device * pdev ) <nl> goto fail_out ; <nl> } <nl>  <nl> - /* Only a single ramoops area allowed at a time , so fail extra <nl> + /* <nl> + * Only a single ramoops area allowed at a time , so fail extra <nl> * probes . <nl> */ <nl> - if ( cxt -> max_dump_cnt ) <nl> + if ( cxt -> max_dump_cnt ) { <nl> + pr_err (" already initialized \ n "); <nl> goto fail_out ; <nl> + } <nl> + <nl> + /* Make sure we didn ' t get bogus platform data pointer . */ <nl> + if (! pdata ) { <nl> + pr_err (" NULL platform data \ n "); <nl> + goto fail_out ; <nl> + } <nl>  <nl> if (! pdata -> mem_size || (! pdata -> record_size && ! pdata -> console_size && <nl> ! pdata -> ftrace_size && ! pdata -> pmsg_size )) {
mmm drivers / gpu / drm / drm_edid . c <nl> ppp drivers / gpu / drm / drm_edid . c <nl> drm_mode_std ( struct drm_connector * connector , struct edid * edid , <nl> * secondary GTF curve . Please don ' t do that . <nl> */ <nl> mode = drm_gtf_mode ( dev , hsize , vsize , vrefresh_rate , 0 , 0 ); <nl> + if (! mode ) <nl> + return NULL ; <nl> if ( drm_mode_hsync ( mode ) > drm_gtf2_hbreak ( edid )) { <nl> drm_mode_destroy ( dev , mode ); <nl> mode = drm_gtf_mode_complex ( dev , hsize , vsize , <nl> drm_gtf_modes_for_range ( struct drm_connector * connector , struct edid * edid , <nl> for ( i = 0 ; i < num_extra_modes ; i ++) { <nl> const struct minimode * m = & extra_modes [ i ]; <nl> newmode = drm_gtf_mode ( dev , m -> w , m -> h , m -> r , 0 , 0 ); <nl> + if (! newmode ) <nl> + return modes ; <nl>  <nl> if (! mode_in_range ( newmode , edid , timing )) { <nl> drm_mode_destroy ( dev , newmode ); <nl> drm_cvt_modes_for_range ( struct drm_connector * connector , struct edid * edid , <nl> for ( i = 0 ; i < num_extra_modes ; i ++) { <nl> const struct minimode * m = & extra_modes [ i ]; <nl> newmode = drm_cvt_mode ( dev , m -> w , m -> h , m -> r , rb , 0 , 0 ); <nl> + if (! newmode ) <nl> + return modes ; <nl>  <nl> if (! mode_in_range ( newmode , edid , timing )) { <nl> drm_mode_destroy ( dev , newmode );
mmm drivers / net / ethernet / qlogic / qlcnic / qlcnic_main . c <nl> ppp drivers / net / ethernet / qlogic / qlcnic / qlcnic_main . c <nl> static int qlcnic_82xx_setup_intr ( struct qlcnic_adapter * adapter ) <nl> qlcnic_disable_multi_tx ( adapter ); <nl>  <nl> err = qlcnic_enable_msi_legacy ( adapter ); <nl> - if (! err ) <nl> + if ( err ) <nl> return err ; <nl> } <nl> }
mmm fs / nfs / super . c <nl> ppp fs / nfs / super . c <nl> static int nfs_parse_mount_options ( char * raw , <nl> string = match_strdup ( args ); <nl> if ( string == NULL ) <nl> goto out_nomem ; <nl> + kfree ( mnt -> client_address ); <nl> mnt -> client_address = string ; <nl> break ; <nl> case Opt_mounthost : <nl> string = match_strdup ( args ); <nl> if ( string == NULL ) <nl> goto out_nomem ; <nl> + kfree ( mnt -> mount_server . hostname ); <nl> mnt -> mount_server . hostname = string ; <nl> break ; <nl> case Opt_mountaddr :
mmm drivers / mtd / chips / cfi_util . c <nl> ppp drivers / mtd / chips / cfi_util . c <nl> int __xipram cfi_qry_mode_on ( uint32_t base , struct map_info * map , <nl> cfi_send_gen_cmd ( 0xAA , 0x5555 , base , map , cfi , cfi -> device_type , NULL ); <nl> cfi_send_gen_cmd ( 0x55 , 0x2AAA , base , map , cfi , cfi -> device_type , NULL ); <nl> cfi_send_gen_cmd ( 0x98 , 0x5555 , base , map , cfi , cfi -> device_type , NULL ); <nl> + if ( cfi_qry_present ( map , base , cfi )) <nl> + return 1 ; <nl> + /* SST 39VF640xB */ <nl> + cfi_send_gen_cmd ( 0xF0 , 0 , base , map , cfi , cfi -> device_type , NULL ); <nl> + cfi_send_gen_cmd ( 0xAA , 0x555 , base , map , cfi , cfi -> device_type , NULL ); <nl> + cfi_send_gen_cmd ( 0x55 , 0x2AA , base , map , cfi , cfi -> device_type , NULL ); <nl> + cfi_send_gen_cmd ( 0x98 , 0x555 , base , map , cfi , cfi -> device_type , NULL ); <nl> if ( cfi_qry_present ( map , base , cfi )) <nl> return 1 ; <nl> /* QRY not found */
mmm arch / x86 / kernel / cpu / perf_event . c <nl> ppp arch / x86 / kernel / cpu / perf_event . c <nl> static inline void x86_assign_hw_event ( struct perf_event * event , <nl> hwc -> event_base = 0 ; <nl> } else if ( hwc -> idx >= X86_PMC_IDX_FIXED ) { <nl> hwc -> config_base = MSR_ARCH_PERFMON_FIXED_CTR_CTRL ; <nl> - hwc -> event_base = MSR_ARCH_PERFMON_FIXED_CTR0 ; <nl> + hwc -> event_base = MSR_ARCH_PERFMON_FIXED_CTR0 + ( hwc -> idx - X86_PMC_IDX_FIXED ); <nl> } else { <nl> hwc -> config_base = x86_pmu_config_addr ( hwc -> idx ); <nl> hwc -> event_base = x86_pmu_event_addr ( hwc -> idx );
mmm drivers / usb / gadget / fsl_udc_core . c <nl> ppp drivers / usb / gadget / fsl_udc_core . c <nl> static int __exit fsl_udc_remove ( struct platform_device * pdev ) <nl> if (! udc_controller ) <nl> return - ENODEV ; <nl>  <nl> - usb_del_gadget_udc (& udc_controller -> gadget ); <nl> udc_controller -> done = & done ; <nl> + usb_del_gadget_udc (& udc_controller -> gadget ); <nl>  <nl> fsl_udc_clk_release (); <nl> 
mmm io_uring / msg_ring . c <nl> ppp io_uring / msg_ring . c <nl> int io_msg_ring ( struct io_kiocb * req , unsigned int issue_flags ) <nl> req_set_fail ( req ); <nl> io_req_set_res ( req , ret , 0 ); <nl> /* put file to avoid an attempt to IOPOLL the req */ <nl> - io_put_file ( req -> file ); <nl> + if (!( req -> flags & REQ_F_FIXED_FILE )) <nl> + io_put_file ( req -> file ); <nl> req -> file = NULL ; <nl> return IOU_OK ; <nl> }
mmm drivers / input / evdev . c <nl> ppp drivers / input / evdev . c <nl> static long evdev_do_ioctl ( struct file * file , unsigned int cmd , <nl> return - EFAULT ; <nl>  <nl> error = input_ff_upload ( dev , & effect , file ); <nl> + if ( error ) <nl> + return error ; <nl>  <nl> if ( put_user ( effect . id , &((( struct ff_effect __user *) p )-> id ))) <nl> return - EFAULT ; <nl>  <nl> - return error ; <nl> + return 0 ; <nl> } <nl>  <nl> /* Multi - number variable - length handlers */
mmm drivers / misc / fastrpc . c <nl> ppp drivers / misc / fastrpc . c <nl> static int fastrpc_dma_buf_attach ( struct dma_buf * dmabuf , <nl> FASTRPC_PHYS ( buffer -> phys ), buffer -> size ); <nl> if ( ret < 0 ) { <nl> dev_err ( buffer -> dev , " failed to get scatterlist from DMA API \ n "); <nl> + kfree ( a ); <nl> return - EINVAL ; <nl> } <nl> 
mmm drivers / net / bonding / bond_main . c <nl> ppp drivers / net / bonding / bond_main . c <nl> int bond_enslave ( struct net_device * bond_dev , struct net_device * slave_dev ) <nl> write_unlock_bh (& bond -> curr_slave_lock ); <nl> read_unlock (& bond -> lock ); <nl> } <nl> + slave_disable_netpoll ( new_slave ); <nl>  <nl> err_close : <nl> slave_dev -> priv_flags &= ~ IFF_BONDING ;
mmm drivers / block / drbd / drbd_nl . c <nl> ppp drivers / block / drbd / drbd_nl . c <nl> drbd_determine_dev_size ( struct drbd_device * device , enum dds_flags flags , struct <nl> if ( la_size_changed || md_moved || rs ) { <nl> u32 prev_flags ; <nl>  <nl> + /* We do some synchronous IO below , which may take some time . <nl> + * Clear the timer , to avoid scary " timer expired !" messages , <nl> + * " Superblock " is written out at least twice below , anyways . */ <nl> + del_timer (& device -> md_sync_timer ); <nl> drbd_al_shrink ( device ); /* All extents inactive . */ <nl>  <nl> prev_flags = md -> flags ;
mmm kernel / sched . c <nl> ppp kernel / sched . c <nl> struct runqueue { <nl> unsigned long ttwu_cnt ; <nl> unsigned long ttwu_local ; <nl> # endif <nl> + struct lock_class_key rq_lock_key ; <nl> }; <nl>  <nl> static DEFINE_PER_CPU ( struct runqueue , runqueues ); <nl> void __init sched_init ( void ) <nl>  <nl> rq = cpu_rq ( i ); <nl> spin_lock_init (& rq -> lock ); <nl> + lockdep_set_class (& rq -> lock , & rq -> rq_lock_key ); <nl> rq -> nr_running = 0 ; <nl> rq -> active = rq -> arrays ; <nl> rq -> expired = rq -> arrays + 1 ;
mmm drivers / base / component . c <nl> ppp drivers / base / component . c <nl> static void component_detach_master ( struct master * master , struct component * c ) <nl> c -> master = NULL ; <nl> } <nl>  <nl> +/* <nl> + * Add a component to a master , finding the component via the compare <nl> + * function and compare data . This is safe to call for duplicate matches <nl> + * and will not result in the same component being added multiple times . <nl> + */ <nl> int component_master_add_child ( struct master * master , <nl> int (* compare )( struct device *, void *), void * compare_data ) <nl> { <nl> int component_master_add_child ( struct master * master , <nl> int ret = - ENXIO ; <nl>  <nl> list_for_each_entry ( c , & component_list , node ) { <nl> - if ( c -> master ) <nl> + if ( c -> master && c -> master != master ) <nl> continue ; <nl>  <nl> if ( compare ( c -> dev , compare_data )) { <nl> - component_attach_master ( master , c ); <nl> + if (! c -> master ) <nl> + component_attach_master ( master , c ); <nl> ret = 0 ; <nl> break ; <nl> }
mmm drivers / kvm / mmu . c <nl> ppp drivers / kvm / mmu . c <nl> void kvm_mmu_pte_write ( struct kvm_vcpu * vcpu , gpa_t gpa , <nl> unsigned pte_size ; <nl> unsigned page_offset ; <nl> unsigned misaligned ; <nl> + unsigned quadrant ; <nl> int level ; <nl> int flooded = 0 ; <nl> int npte ; <nl> void kvm_mmu_pte_write ( struct kvm_vcpu * vcpu , gpa_t gpa , <nl> page_offset <<= 1 ; <nl> npte = 2 ; <nl> } <nl> + quadrant = page_offset >> PAGE_SHIFT ; <nl> page_offset &= ~ PAGE_MASK ; <nl> + if ( quadrant != page -> role . quadrant ) <nl> + continue ; <nl> } <nl> spte = __va ( page -> page_hpa ); <nl> spte += page_offset / sizeof (* spte );
mmm drivers / net / wireless / iwlwifi / mvm / power . c <nl> ppp drivers / net / wireless / iwlwifi / mvm / power . c <nl> static void iwl_mvm_power_build_cmd ( struct iwl_mvm * mvm , <nl> cmd -> flags |= cpu_to_le16 ( POWER_FLAGS_POWER_SAVE_ENA_MSK ); <nl>  <nl> if (! vif -> bss_conf . ps || iwl_mvm_vif_low_latency ( mvmvif ) || <nl> - ! mvmvif -> pm_enabled || iwl_mvm_tdls_sta_count ( mvm , vif )) <nl> + ! mvmvif -> pm_enabled ) <nl> return ; <nl>  <nl> cmd -> flags |= cpu_to_le16 ( POWER_FLAGS_POWER_MANAGEMENT_ENA_MSK ); <nl> static void iwl_mvm_power_set_pm ( struct iwl_mvm * mvm , <nl> if ( vifs -> ap_vif ) <nl> ap_mvmvif = iwl_mvm_vif_from_mac80211 ( vifs -> ap_vif ); <nl>  <nl> + /* don ' t allow PM if any TDLS stations exist */ <nl> + if ( iwl_mvm_tdls_sta_count ( mvm , NULL )) <nl> + return ; <nl> + <nl> /* enable PM on bss if bss stand alone */ <nl> if ( vifs -> bss_active && ! vifs -> p2p_active && ! vifs -> ap_active ) { <nl> bss_mvmvif -> pm_enabled = true ;
mmm drivers / media / video / tlg2300 / pd - video . c <nl> ppp drivers / media / video / tlg2300 / pd - video . c <nl> int alloc_bulk_urbs_generic ( struct urb ** urb_array , int num , <nl> int buf_size , gfp_t gfp_flags , <nl> usb_complete_t complete_fn , void * context ) <nl> { <nl> - struct urb * urb ; <nl> - void * mem ; <nl> - int i ; <nl> + int i = 0 ; <nl>  <nl> - for ( i = 0 ; i < num ; i ++) { <nl> - urb = usb_alloc_urb ( 0 , gfp_flags ); <nl> + for (; i < num ; i ++) { <nl> + void * mem ; <nl> + struct urb * urb = usb_alloc_urb ( 0 , gfp_flags ); <nl> if ( urb == NULL ) <nl> return i ; <nl>  <nl> mem = usb_alloc_coherent ( udev , buf_size , gfp_flags , <nl> & urb -> transfer_dma ); <nl> - if ( mem == NULL ) <nl> + if ( mem == NULL ) { <nl> + usb_free_urb ( urb ); <nl> return i ; <nl> + } <nl>  <nl> usb_fill_bulk_urb ( urb , udev , usb_rcvbulkpipe ( udev , ep_addr ), <nl> mem , buf_size , complete_fn , context );
mmm sound / pci / hda / patch_analog . c <nl> ppp sound / pci / hda / patch_analog . c <nl> static struct hda_verb ad1988_spdif_init_verbs [] = { <nl> { } <nl> }; <nl>  <nl> + static struct hda_verb ad1988_spdif_in_init_verbs [] = { <nl> + /* unmute SPDIF input pin */ <nl> + { 0x1c , AC_VERB_SET_AMP_GAIN_MUTE , AMP_IN_UNMUTE ( 0 )}, <nl> + { } <nl> +}; <nl> + <nl> /* AD1989 has no ADC -> SPDIF route */ <nl> static struct hda_verb ad1989_spdif_init_verbs [] = { <nl> /* SPDIF - 1 out pin */ <nl> static int patch_ad1988 ( struct hda_codec * codec ) <nl> ad1988_spdif_init_verbs ; <nl> } <nl> } <nl> - if ( spec -> dig_in_nid && codec -> vendor_id < 0x11d4989a ) <nl> + if ( spec -> dig_in_nid && codec -> vendor_id < 0x11d4989a ) { <nl> spec -> mixers [ spec -> num_mixers ++] = ad1988_spdif_in_mixers ; <nl> + spec -> init_verbs [ spec -> num_init_verbs ++] = <nl> + ad1988_spdif_in_init_verbs ; <nl> + } <nl>  <nl> codec -> patch_ops = ad198x_patch_ops ; <nl> switch ( board_config ) {
mmm drivers / net / bonding / bond_main . c <nl> ppp drivers / net / bonding / bond_main . c <nl> static u16 bond_select_queue ( struct net_device * dev , struct sk_buff * skb ) <nl> { <nl> /* <nl> * This helper function exists to help dev_pick_tx get the correct <nl> - * destination queue . Using a helper function skips the a call to <nl> + * destination queue . Using a helper function skips a call to <nl> * skb_tx_hash and will put the skbs in the queue we expect on their <nl> * way down to the bonding driver . <nl> */ <nl> - return skb -> queue_mapping ; <nl> + u16 txq = skb_rx_queue_recorded ( skb ) ? skb_get_rx_queue ( skb ) : 0 ; <nl> + <nl> + if ( unlikely ( txq >= dev -> real_num_tx_queues )) { <nl> + do <nl> + txq -= dev -> real_num_tx_queues ; <nl> + while ( txq >= dev -> real_num_tx_queues ); <nl> + } <nl> + return txq ; <nl> } <nl>  <nl> static netdev_tx_t bond_start_xmit ( struct sk_buff * skb , struct net_device * dev )
mmm sound / soc / soc - cache . c <nl> ppp sound / soc / soc - cache . c <nl> static bool snd_soc_set_cache_val ( void * base , unsigned int idx , <nl> static unsigned int snd_soc_get_cache_val ( const void * base , unsigned int idx , <nl> unsigned int word_size ) <nl> { <nl> + if (! base ) <nl> + return - 1 ; <nl> + <nl> switch ( word_size ) { <nl> case 1 : { <nl> const u8 * cache = base ;
mmm drivers / staging / xgifb / XGI_main_26 . c <nl> ppp drivers / staging / xgifb / XGI_main_26 . c <nl> static int xgifb_probe ( struct pci_dev * pdev , <nl>  <nl> if ( xgifb_info -> mode_idx < 0 ) { <nl> dev_err (& pdev -> dev , " No supported video mode found \ n "); <nl> + ret = - EINVAL ; <nl> goto error_1 ; <nl> } <nl> 
mmm drivers / staging / speakup / kobjects . c <nl> ppp drivers / staging / speakup / kobjects . c <nl> static ssize_t synth_store ( struct kobject * kobj , struct kobj_attribute * attr , <nl> len = strlen ( buf ); <nl> if ( len < 2 || len > 9 ) <nl> return - EINVAL ; <nl> - strncpy ( new_synth_name , buf , len ); <nl> + memcpy ( new_synth_name , buf , len ); <nl> if ( new_synth_name [ len - 1 ] == '\ n ') <nl> len --; <nl> new_synth_name [ len ] = '\ 0 '; <nl> static ssize_t punc_store ( struct kobject * kobj , struct kobj_attribute * attr , <nl> return - EINVAL ; <nl> } <nl>  <nl> - strncpy ( punc_buf , buf , x ); <nl> + memcpy ( punc_buf , buf , x ); <nl>  <nl> while ( x && punc_buf [ x - 1 ] == '\ n ') <nl> x --;
mmm drivers / regulator / pwm - regulator . c <nl> ppp drivers / regulator / pwm - regulator . c <nl> static int pwm_regulator_probe ( struct platform_device * pdev ) <nl> return ret ; <nl> } <nl>  <nl> - /* <nl> - * FIXME : pwm_apply_args () should be removed when switching to the <nl> - * atomic PWM API . <nl> - */ <nl> - pwm_apply_args ( drvdata -> pwm ); <nl> + ret = pwm_adjust_config ( drvdata -> pwm ); <nl> + if ( ret ) <nl> + return ret ; <nl>  <nl> regulator = devm_regulator_register (& pdev -> dev , <nl> & drvdata -> desc , & config );
mmm drivers / staging / lustre / lustre / obdclass / obd_config . c <nl> ppp drivers / staging / lustre / lustre / obdclass / obd_config . c <nl> int class_process_proc_param ( char * prefix , struct lprocfs_vars * lvars , <nl>  <nl> oldfs = get_fs (); <nl> set_fs ( KERNEL_DS ); <nl> - rc = var -> fops -> write (& fakefile , sval , <nl> + rc = var -> fops -> write (& fakefile , <nl> + ( const char __user *) sval , <nl> vallen , NULL ); <nl> set_fs ( oldfs ); <nl> }
mmm drivers / staging / imx - drm / imx - drm - core . c <nl> ppp drivers / staging / imx - drm / imx - drm - core . c <nl> int imx_drm_add_crtc ( struct drm_crtc * crtc , <nl>  <nl> mutex_lock (& imxdrm -> mutex ); <nl>  <nl> + /* <nl> + * The vblank arrays are dimensioned by MAX_CRTC - we can ' t <nl> + * pass IDs greater than this to those functions . <nl> + */ <nl> + if ( imxdrm -> pipes >= MAX_CRTC ) { <nl> + ret = - EINVAL ; <nl> + goto err_busy ; <nl> + } <nl> + <nl> if ( imxdrm -> drm -> open_count ) { <nl> ret = - EBUSY ; <nl> goto err_busy ;
mmm drivers / media / video / pvrusb2 / pvrusb2 - v4l2 . c <nl> ppp drivers / media / video / pvrusb2 / pvrusb2 - v4l2 . c <nl> static int pvr2_v4l2_do_ioctl ( struct inode * inode , struct file * file , <nl> ret = 0 ; <nl> switch ( vf -> type ) { <nl> case V4L2_BUF_TYPE_VIDEO_CAPTURE : { <nl> - int lmin , lmax ; <nl> + int lmin , lmax , ldef ; <nl> struct pvr2_ctrl * hcp ,* vcp ; <nl> int h = vf -> fmt . pix . height ; <nl> int w = vf -> fmt . pix . width ; <nl> static int pvr2_v4l2_do_ioctl ( struct inode * inode , struct file * file , <nl>  <nl> lmin = pvr2_ctrl_get_min ( hcp ); <nl> lmax = pvr2_ctrl_get_max ( hcp ); <nl> - if ( w < lmin ) { <nl> + ldef = pvr2_ctrl_get_def ( hcp ); <nl> + if ( w == - 1 ) { <nl> + w = ldef ; <nl> + } else if ( w < lmin ) { <nl> w = lmin ; <nl> } else if ( w > lmax ) { <nl> w = lmax ; <nl> } <nl> lmin = pvr2_ctrl_get_min ( vcp ); <nl> lmax = pvr2_ctrl_get_max ( vcp ); <nl> - if ( h < lmin ) { <nl> + ldef = pvr2_ctrl_get_def ( vcp ); <nl> + if ( h == - 1 ) { <nl> + h = ldef ; <nl> + } else if ( h < lmin ) { <nl> h = lmin ; <nl> } else if ( h > lmax ) { <nl> h = lmax ;
mmm include / net / udp . h <nl> ppp include / net / udp . h <nl> static inline int copy_linear_skb ( struct sk_buff * skb , int len , int off , <nl> { <nl> int n , copy = len - off ; <nl>  <nl> + if ( copy < 0 ) <nl> + return - EINVAL ; <nl> n = copy_to_iter ( skb -> data + off , copy , to ); <nl> if ( n == copy ) <nl> return 0 ;
mmm drivers / gpio / gpio - mb86s7x . c <nl> ppp drivers / gpio / gpio - mb86s7x . c <nl> static int mb86s70_gpio_request ( struct gpio_chip * gc , unsigned gpio ) <nl> spin_lock_irqsave (& gchip -> lock , flags ); <nl>  <nl> val = readl ( gchip -> base + PFR ( gpio )); <nl> + if (!( val & OFFSET ( gpio ))) { <nl> + spin_unlock_irqrestore (& gchip -> lock , flags ); <nl> + return - EINVAL ; <nl> + } <nl> + <nl> val &= ~ OFFSET ( gpio ); <nl> writel ( val , gchip -> base + PFR ( gpio )); <nl> 
mmm fs / fuse / inode . c <nl> ppp fs / fuse / inode . c <nl> static int fuse_fill_super ( struct super_block * sb , void * data , int silent ) <nl> err_put_root : <nl> dput ( root_dentry ); <nl> err_put_conn : <nl> + bdi_destroy (& fc -> bdi ); <nl> fuse_conn_put ( fc ); <nl> err_fput : <nl> fput ( file );
mmm sound / core / control . c <nl> ppp sound / core / control . c <nl> int snd_ctl_add ( struct snd_card * card , struct snd_kcontrol * kcontrol ) <nl> { <nl> struct snd_ctl_elem_id id ; <nl> unsigned int idx ; <nl> + unsigned int count ; <nl> int err = - EINVAL ; <nl>  <nl> if (! kcontrol ) <nl> int snd_ctl_add ( struct snd_card * card , struct snd_kcontrol * kcontrol ) <nl> card -> controls_count += kcontrol -> count ; <nl> kcontrol -> id . numid = card -> last_numid + 1 ; <nl> card -> last_numid += kcontrol -> count ; <nl> + count = kcontrol -> count ; <nl> up_write (& card -> controls_rwsem ); <nl> - for ( idx = 0 ; idx < kcontrol -> count ; idx ++, id . index ++, id . numid ++) <nl> + for ( idx = 0 ; idx < count ; idx ++, id . index ++, id . numid ++) <nl> snd_ctl_notify ( card , SNDRV_CTL_EVENT_MASK_ADD , & id ); <nl> return 0 ; <nl>  <nl> int snd_ctl_replace ( struct snd_card * card , struct snd_kcontrol * kcontrol , <nl> bool add_on_replace ) <nl> { <nl> struct snd_ctl_elem_id id ; <nl> + unsigned int count ; <nl> unsigned int idx ; <nl> struct snd_kcontrol * old ; <nl> int ret ; <nl> int snd_ctl_replace ( struct snd_card * card , struct snd_kcontrol * kcontrol , <nl> card -> controls_count += kcontrol -> count ; <nl> kcontrol -> id . numid = card -> last_numid + 1 ; <nl> card -> last_numid += kcontrol -> count ; <nl> + count = kcontrol -> count ; <nl> up_write (& card -> controls_rwsem ); <nl> - for ( idx = 0 ; idx < kcontrol -> count ; idx ++, id . index ++, id . numid ++) <nl> + for ( idx = 0 ; idx < count ; idx ++, id . index ++, id . numid ++) <nl> snd_ctl_notify ( card , SNDRV_CTL_EVENT_MASK_ADD , & id ); <nl> return 0 ; <nl>  <nl> static int snd_ctl_elem_write ( struct snd_card * card , struct snd_ctl_file * file , <nl> result = kctl -> put ( kctl , control ); <nl> } <nl> if ( result > 0 ) { <nl> + struct snd_ctl_elem_id id = control -> id ; <nl> up_read (& card -> controls_rwsem ); <nl> - snd_ctl_notify ( card , SNDRV_CTL_EVENT_MASK_VALUE , <nl> - & control -> id ); <nl> + snd_ctl_notify ( card , SNDRV_CTL_EVENT_MASK_VALUE , & id ); <nl> return 0 ; <nl> } <nl> } <nl> static int snd_ctl_tlv_ioctl ( struct snd_ctl_file * file , <nl> } <nl> err = kctl -> tlv . c ( kctl , op_flag , tlv . length , _tlv -> tlv ); <nl> if ( err > 0 ) { <nl> + struct snd_ctl_elem_id id = kctl -> id ; <nl> up_read (& card -> controls_rwsem ); <nl> - snd_ctl_notify ( card , SNDRV_CTL_EVENT_MASK_TLV , & kctl -> id ); <nl> + snd_ctl_notify ( card , SNDRV_CTL_EVENT_MASK_TLV , & id ); <nl> return 0 ; <nl> } <nl> } else {
mmm net / mpls / af_mpls . c <nl> ppp net / mpls / af_mpls . c <nl> static int mpls_dev_sysctl_register ( struct net_device * dev , <nl> free : <nl> kfree ( table ); <nl> out : <nl> + mdev -> sysctl = NULL ; <nl> return - ENOBUFS ; <nl> } <nl>  <nl> static void mpls_dev_sysctl_unregister ( struct net_device * dev , <nl> struct net * net = dev_net ( dev ); <nl> struct ctl_table * table ; <nl>  <nl> + if (! mdev -> sysctl ) <nl> + return ; <nl> + <nl> table = mdev -> sysctl -> ctl_table_arg ; <nl> unregister_net_sysctl_table ( mdev -> sysctl ); <nl> kfree ( table );
mmm net / netfilter / nf_tables_core . c <nl> ppp net / netfilter / nf_tables_core . c <nl> unsigned int <nl> nft_do_chain ( struct nft_pktinfo * pkt , const struct nf_hook_ops * ops ) <nl> { <nl> const struct nft_chain * chain = ops -> priv , * basechain = chain ; <nl> - const struct net * net = read_pnet (& nft_base_chain ( basechain )-> pnet ); <nl> + const struct net * chain_net = read_pnet (& nft_base_chain ( basechain )-> pnet ); <nl> + const struct net * net = dev_net ( pkt -> in ? pkt -> in : pkt -> out ); <nl> const struct nft_rule * rule ; <nl> const struct nft_expr * expr , * last ; <nl> struct nft_regs regs ; <nl> nft_do_chain ( struct nft_pktinfo * pkt , const struct nf_hook_ops * ops ) <nl> int rulenum ; <nl> unsigned int gencursor = nft_genmask_cur ( net ); <nl>  <nl> + /* Ignore chains that are not for the current network namespace */ <nl> + if (! net_eq ( net , chain_net )) <nl> + return NF_ACCEPT ; <nl> + <nl> do_chain : <nl> rulenum = 0 ; <nl> rule = list_entry (& chain -> rules , struct nft_rule , list );
mmm net / rxrpc / conn_service . c <nl> ppp net / rxrpc / conn_service . c <nl> struct rxrpc_connection * rxrpc_find_service_conn_rcu ( struct rxrpc_peer * peer , <nl> else if ( conn -> proto . index_key > k . index_key ) <nl> p = rcu_dereference_raw ( p -> rb_right ); <nl> else <nl> - goto done ; <nl> + break ; <nl> conn = NULL ; <nl> } <nl> } while ( need_seqretry (& peer -> service_conn_lock , seq )); <nl>  <nl> - done : <nl> done_seqretry (& peer -> service_conn_lock , seq ); <nl> _leave (" = % d ", conn ? conn -> debug_id : - 1 ); <nl> return conn ;
mmm drivers / net / wireless / ath / ath6kl / sdio . c <nl> ppp drivers / net / wireless / ath / ath6kl / sdio . c <nl> struct ath6kl_sdio { <nl> # define CMD53_ARG_FIXED_ADDRESS 0 <nl> # define CMD53_ARG_INCR_ADDRESS 1 <nl>  <nl> + static int ath6kl_sdio_config ( struct ath6kl * ar ); <nl> + <nl> static inline struct ath6kl_sdio * ath6kl_sdio_priv ( struct ath6kl * ar ) <nl> { <nl> return ar -> hif_priv ; <nl> static int ath6kl_sdio_power_on ( struct ath6kl * ar ) <nl> */ <nl> msleep ( 10 ); <nl>  <nl> + ret = ath6kl_sdio_config ( ar ); <nl> + if ( ret ) { <nl> + ath6kl_err (" Failed to config sdio : % d \ n ", ret ); <nl> + goto out ; <nl> + } <nl> + <nl> ar_sdio -> is_disabled = false ; <nl>  <nl> + out : <nl> return ret ; <nl> } <nl> 
mmm net / sctp / ipv6 . c <nl> ppp net / sctp / ipv6 . c <nl> static struct sock * sctp_v6_create_accept_sk ( struct sock * sk , <nl> newnp = inet6_sk ( newsk ); <nl>  <nl> memcpy ( newnp , np , sizeof ( struct ipv6_pinfo )); <nl> + newnp -> ipv6_mc_list = NULL ; <nl> + newnp -> ipv6_ac_list = NULL ; <nl> + newnp -> ipv6_fl_list = NULL ; <nl>  <nl> rcu_read_lock (); <nl> opt = rcu_dereference ( np -> opt );
mmm drivers / gpu / drm / radeon / radeon_cs . c <nl> ppp drivers / gpu / drm / radeon / radeon_cs . c <nl> int radeon_cs_parser_init ( struct radeon_cs_parser * p , void * data ) <nl> cdata = ( uint32_t *)( unsigned long ) user_chunk . chunk_data ; <nl>  <nl> size = p -> chunks [ i ]. length_dw * sizeof ( uint32_t ); <nl> - p -> chunks [ i ]. kdata = kzalloc ( size , GFP_KERNEL ); <nl> + p -> chunks [ i ]. kdata = kmalloc ( size , GFP_KERNEL ); <nl> if ( p -> chunks [ i ]. kdata == NULL ) { <nl> return - ENOMEM ; <nl> }
mmm drivers / net / r8169 . c <nl> ppp drivers / net / r8169 . c <nl> static const int multicast_filter_limit = 32 ; <nl> # define RX_DMA_BURST 6 /* Maximum PCI burst , ' 6 ' is 1024 */ <nl> # define TX_DMA_BURST 6 /* Maximum PCI burst , ' 6 ' is 1024 */ <nl> # define EarlyTxThld 0x3F /* 0x3F means NO early transmit */ <nl> -# define RxPacketMaxSize 0x3FE8 /* 16K - 1 - ETH_HLEN - VLAN - CRC ... */ <nl> # define SafeMtu 0x1c20 /* ... actually life sucks beyond ~ 7k */ <nl> # define InterFrameGap 0x03 /* 3 means InterFrameGap = the shortest one */ <nl>  <nl> static u16 rtl_rw_cpluscmd ( void __iomem * ioaddr ) <nl> return cmd ; <nl> } <nl>  <nl> - static void rtl_set_rx_max_size ( void __iomem * ioaddr ) <nl> + static void rtl_set_rx_max_size ( void __iomem * ioaddr , unsigned int rx_buf_sz ) <nl> { <nl> /* Low hurts . Let ' s disable the filtering . */ <nl> - RTL_W16 ( RxMaxSize , 16383 ); <nl> + RTL_W16 ( RxMaxSize , rx_buf_sz ); <nl> } <nl>  <nl> static void rtl8169_set_magic_reg ( void __iomem * ioaddr , unsigned mac_version ) <nl> static void rtl_hw_start_8169 ( struct net_device * dev ) <nl>  <nl> RTL_W8 ( EarlyTxThres , EarlyTxThld ); <nl>  <nl> - rtl_set_rx_max_size ( ioaddr ); <nl> + rtl_set_rx_max_size ( ioaddr , tp -> rx_buf_sz ); <nl>  <nl> if (( tp -> mac_version == RTL_GIGA_MAC_VER_01 ) || <nl> ( tp -> mac_version == RTL_GIGA_MAC_VER_02 ) || <nl> static void rtl_hw_start_8168 ( struct net_device * dev ) <nl>  <nl> RTL_W8 ( EarlyTxThres , EarlyTxThld ); <nl>  <nl> - rtl_set_rx_max_size ( ioaddr ); <nl> + rtl_set_rx_max_size ( ioaddr , tp -> rx_buf_sz ); <nl>  <nl> tp -> cp_cmd |= RTL_R16 ( CPlusCmd ) | PktCntrDisable | INTT_1 ; <nl>  <nl> static void rtl_hw_start_8101 ( struct net_device * dev ) <nl>  <nl> RTL_W8 ( EarlyTxThres , EarlyTxThld ); <nl>  <nl> - rtl_set_rx_max_size ( ioaddr ); <nl> + rtl_set_rx_max_size ( ioaddr , tp -> rx_buf_sz ); <nl>  <nl> tp -> cp_cmd |= rtl_rw_cpluscmd ( ioaddr ) | PCIMulRW ; <nl> 
mmm virt / kvm / arm / vgic . c <nl> ppp virt / kvm / arm / vgic . c <nl> bool kvm_vgic_map_is_active ( struct kvm_vcpu * vcpu , struct irq_phys_map * map ) <nl> return true ; <nl> } <nl>  <nl> - return dist_active_irq ( vcpu ); <nl> + return vgic_irq_is_active ( vcpu , map -> virt_irq ); <nl> } <nl>  <nl> /*
mmm net / ipv4 / tcp_input . c <nl> ppp net / ipv4 / tcp_input . c <nl> int tcp_rcv_state_process ( struct sock * sk , struct sk_buff * skb , <nl> goto discard ; <nl>  <nl> if ( th -> syn ) { <nl> + if ( th -> fin ) <nl> + goto discard ; <nl> if ( icsk -> icsk_af_ops -> conn_request ( sk , skb ) < 0 ) <nl> return 1 ; <nl> 
mmm drivers / acpi / events / evgpe . c <nl> ppp drivers / acpi / events / evgpe . c <nl> acpi_ev_gpe_dispatch ( struct acpi_gpe_event_info * gpe_event_info , u32 gpe_number ) <nl>  <nl> ACPI_FUNCTION_TRACE ( ev_gpe_dispatch ); <nl>  <nl> + acpi_gpe_count ++; <nl> + <nl> /* <nl> * If edge - triggered , clear the GPE status bit now . Note that <nl> * level - triggered events are cleared after the GPE is serviced .mmm include / acpi / acglobal . h <nl> ppp include / acpi / acglobal . h <nl> acpi_ev_gpe_dispatch ( struct acpi_gpe_event_info * gpe_event_info , u32 gpe_number ) <nl>  <nl> ACPI_FUNCTION_TRACE ( ev_gpe_dispatch ); <nl>  <nl> + acpi_gpe_count ++; <nl> + <nl> /* <nl> * If edge - triggered , clear the GPE status bit now . Note that <nl> * level - triggered events are cleared after the GPE is serviced . <nl> extern u32 acpi_dbg_layer ; <nl>  <nl> extern u32 acpi_gbl_nesting_level ; <nl>  <nl> +/* Event counters */ <nl> + <nl> + ACPI_EXTERN u32 acpi_gpe_count ; <nl> + <nl> /* Support for dynamic control method tracing mechanism */ <nl>  <nl> ACPI_EXTERN u32 acpi_gbl_original_dbg_level ;mmm drivers / acpi / utilities / utglobal . c <nl> ppp drivers / acpi / utilities / utglobal . c <nl> acpi_ev_gpe_dispatch ( struct acpi_gpe_event_info * gpe_event_info , u32 gpe_number ) <nl>  <nl> ACPI_FUNCTION_TRACE ( ev_gpe_dispatch ); <nl>  <nl> + acpi_gpe_count ++; <nl> + <nl> /* <nl> * If edge - triggered , clear the GPE status bit now . Note that <nl> * level - triggered events are cleared after the GPE is serviced . <nl> extern u32 acpi_dbg_layer ; <nl>  <nl> extern u32 acpi_gbl_nesting_level ; <nl>  <nl> +/* Event counters */ <nl> + <nl> + ACPI_EXTERN u32 acpi_gpe_count ; <nl> + <nl> /* Support for dynamic control method tracing mechanism */ <nl>  <nl> ACPI_EXTERN u32 acpi_gbl_original_dbg_level ; <nl> void acpi_ut_init_globals ( void ) <nl>  <nl> /* GPE support */ <nl>  <nl> + acpi_gpe_count = 0 ; <nl> acpi_gbl_gpe_xrupt_list_head = NULL ; <nl> acpi_gbl_gpe_fadt_blocks [ 0 ] = NULL ; <nl> acpi_gbl_gpe_fadt_blocks [ 1 ] = NULL ; <nl> void acpi_ut_init_globals ( void ) <nl>  <nl> ACPI_EXPORT_SYMBOL ( acpi_dbg_level ) <nl> ACPI_EXPORT_SYMBOL ( acpi_dbg_layer ) <nl> + ACPI_EXPORT_SYMBOL ( acpi_gpe_count )
mmm net / netfilter / nf_tables_netdev . c <nl> ppp net / netfilter / nf_tables_netdev . c <nl> nft_do_chain_netdev ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_netdev __read_mostly = { <nl> . family = NFPROTO_NETDEV , <nl> - . nhooks = NF_NETDEV_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> . flags = NFT_AF_NEEDS_DEV , <nl> };mmm net / netfilter / nf_tables_inet . c <nl> ppp net / netfilter / nf_tables_inet . c <nl> nft_do_chain_netdev ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_netdev __read_mostly = { <nl> . family = NFPROTO_NETDEV , <nl> - . nhooks = NF_NETDEV_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> . flags = NFT_AF_NEEDS_DEV , <nl> }; <nl> static unsigned int nft_do_chain_inet ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_inet __read_mostly = { <nl> . family = NFPROTO_INET , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl> mmm net / ipv6 / netfilter / nf_tables_ipv6 . c <nl> ppp net / ipv6 / netfilter / nf_tables_ipv6 . c <nl> nft_do_chain_netdev ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_netdev __read_mostly = { <nl> . family = NFPROTO_NETDEV , <nl> - . nhooks = NF_NETDEV_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> . flags = NFT_AF_NEEDS_DEV , <nl> }; <nl> static unsigned int nft_do_chain_inet ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_inet __read_mostly = { <nl> . family = NFPROTO_INET , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv6 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv6 __read_mostly = { <nl> . family = NFPROTO_IPV6 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl> mmm net / ipv4 / netfilter / nf_tables_ipv4 . c <nl> ppp net / ipv4 / netfilter / nf_tables_ipv4 . c <nl> nft_do_chain_netdev ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_netdev __read_mostly = { <nl> . family = NFPROTO_NETDEV , <nl> - . nhooks = NF_NETDEV_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> . flags = NFT_AF_NEEDS_DEV , <nl> }; <nl> static unsigned int nft_do_chain_inet ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_inet __read_mostly = { <nl> . family = NFPROTO_INET , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv6 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv6 __read_mostly = { <nl> . family = NFPROTO_IPV6 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv4 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv4 __read_mostly = { <nl> . family = NFPROTO_IPV4 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl> mmm net / ipv4 / netfilter / nf_tables_arp . c <nl> ppp net / ipv4 / netfilter / nf_tables_arp . c <nl> nft_do_chain_netdev ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_netdev __read_mostly = { <nl> . family = NFPROTO_NETDEV , <nl> - . nhooks = NF_NETDEV_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> . flags = NFT_AF_NEEDS_DEV , <nl> }; <nl> static unsigned int nft_do_chain_inet ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_inet __read_mostly = { <nl> . family = NFPROTO_INET , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv6 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv6 __read_mostly = { <nl> . family = NFPROTO_IPV6 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv4 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv4 __read_mostly = { <nl> . family = NFPROTO_IPV4 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> nft_do_chain_arp ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_arp __read_mostly = { <nl> . family = NFPROTO_ARP , <nl> - . nhooks = NF_ARP_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl> mmm net / bridge / netfilter / nf_tables_bridge . c <nl> ppp net / bridge / netfilter / nf_tables_bridge . c <nl> nft_do_chain_netdev ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_netdev __read_mostly = { <nl> . family = NFPROTO_NETDEV , <nl> - . nhooks = NF_NETDEV_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> . flags = NFT_AF_NEEDS_DEV , <nl> }; <nl> static unsigned int nft_do_chain_inet ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_inet __read_mostly = { <nl> . family = NFPROTO_INET , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv6 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv6 __read_mostly = { <nl> . family = NFPROTO_IPV6 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv4 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv4 __read_mostly = { <nl> . family = NFPROTO_IPV4 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> nft_do_chain_arp ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_arp __read_mostly = { <nl> . family = NFPROTO_ARP , <nl> - . nhooks = NF_ARP_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> nft_do_chain_bridge ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_bridge __read_mostly = { <nl> . family = NFPROTO_BRIDGE , <nl> - . nhooks = NF_BR_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl> mmm include / net / netfilter / nf_tables . h <nl> ppp include / net / netfilter / nf_tables . h <nl> nft_do_chain_netdev ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_netdev __read_mostly = { <nl> . family = NFPROTO_NETDEV , <nl> - . nhooks = NF_NETDEV_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> . flags = NFT_AF_NEEDS_DEV , <nl> }; <nl> static unsigned int nft_do_chain_inet ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_inet __read_mostly = { <nl> . family = NFPROTO_INET , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv6 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv6 __read_mostly = { <nl> . family = NFPROTO_IPV6 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv4 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv4 __read_mostly = { <nl> . family = NFPROTO_IPV4 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> nft_do_chain_arp ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_arp __read_mostly = { <nl> . family = NFPROTO_ARP , <nl> - . nhooks = NF_ARP_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> nft_do_chain_bridge ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_bridge __read_mostly = { <nl> . family = NFPROTO_BRIDGE , <nl> - . nhooks = NF_BR_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> enum nft_af_flags { <nl> * <nl> * @ list : used internally <nl> * @ family : address family <nl> - * @ nhooks : number of hooks in this family <nl> * @ owner : module owner <nl> * @ tables : used internally <nl> * @ flags : family flags <nl> enum nft_af_flags { <nl> struct nft_af_info { <nl> struct list_head list ; <nl> int family ; <nl> - unsigned int nhooks ; <nl> struct module * owner ; <nl> struct list_head tables ; <nl> u32 flags ;mmm net / netfilter / nf_tables_api . c <nl> ppp net / netfilter / nf_tables_api . c <nl> nft_do_chain_netdev ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_netdev __read_mostly = { <nl> . family = NFPROTO_NETDEV , <nl> - . nhooks = NF_NETDEV_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> . flags = NFT_AF_NEEDS_DEV , <nl> }; <nl> static unsigned int nft_do_chain_inet ( void * priv , struct sk_buff * skb , <nl>  <nl> static struct nft_af_info nft_af_inet __read_mostly = { <nl> . family = NFPROTO_INET , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv6 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv6 __read_mostly = { <nl> . family = NFPROTO_IPV6 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> static unsigned int nft_do_chain_ipv4 ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_ipv4 __read_mostly = { <nl> . family = NFPROTO_IPV4 , <nl> - . nhooks = NF_INET_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> nft_do_chain_arp ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_arp __read_mostly = { <nl> . family = NFPROTO_ARP , <nl> - . nhooks = NF_ARP_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> nft_do_chain_bridge ( void * priv , <nl>  <nl> static struct nft_af_info nft_af_bridge __read_mostly = { <nl> . family = NFPROTO_BRIDGE , <nl> - . nhooks = NF_BR_NUMHOOKS , <nl> . owner = THIS_MODULE , <nl> }; <nl>  <nl> enum nft_af_flags { <nl> * <nl> * @ list : used internally <nl> * @ family : address family <nl> - * @ nhooks : number of hooks in this family <nl> * @ owner : module owner <nl> * @ tables : used internally <nl> * @ flags : family flags <nl> enum nft_af_flags { <nl> struct nft_af_info { <nl> struct list_head list ; <nl> int family ; <nl> - unsigned int nhooks ; <nl> struct module * owner ; <nl> struct list_head tables ; <nl> u32 flags ; <nl> static int nft_chain_parse_hook ( struct net * net , <nl> return - EINVAL ; <nl>  <nl> hook -> num = ntohl ( nla_get_be32 ( ha [ NFTA_HOOK_HOOKNUM ])); <nl> - if ( hook -> num >= afi -> nhooks ) <nl> - return - EINVAL ; <nl> - <nl> hook -> priority = ntohl ( nla_get_be32 ( ha [ NFTA_HOOK_PRIORITY ])); <nl>  <nl> type = chain_type [ afi -> family ][ NFT_CHAIN_T_DEFAULT ]; <nl> static int nf_tables_flowtable_parse_hook ( const struct nft_ctx * ctx , <nl> return - EINVAL ; <nl>  <nl> hooknum = ntohl ( nla_get_be32 ( tb [ NFTA_FLOWTABLE_HOOK_NUM ])); <nl> - if ( hooknum >= ctx -> afi -> nhooks ) <nl> + if ( hooknum != NF_NETDEV_INGRESS ) <nl> return - EINVAL ; <nl>  <nl> priority = ntohl ( nla_get_be32 ( tb [ NFTA_FLOWTABLE_HOOK_PRIORITY ]));
mmm kernel / printk . c <nl> ppp kernel / printk . c <nl> asmlinkage int printk ( const char * fmt , ...) <nl> return r ; <nl> } <nl>  <nl> +/* cpu currently holding logbuf_lock */ <nl> + static volatile unsigned int printk_cpu = UINT_MAX ; <nl> + <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> { <nl> unsigned long flags ; <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> static char printk_buf [ 1024 ]; <nl> static int log_level_unknown = 1 ; <nl>  <nl> - if ( unlikely ( oops_in_progress )) <nl> + preempt_disable (); <nl> + if ( unlikely ( oops_in_progress ) && printk_cpu == smp_processor_id ()) <nl> + /* If a crash is occurring during printk () on this CPU , <nl> + * make sure we can ' t deadlock */ <nl> zap_locks (); <nl>  <nl> /* This stops the holder of console_sem just where we want him */ <nl> spin_lock_irqsave (& logbuf_lock , flags ); <nl> + printk_cpu = smp_processor_id (); <nl>  <nl> /* Emit the output into the temporary buffer */ <nl> printed_len = vscnprintf ( printk_buf , sizeof ( printk_buf ), fmt , args ); <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> * CPU until it is officially up . We shouldn ' t be calling into <nl> * random console drivers on a CPU which doesn ' t exist yet .. <nl> */ <nl> + printk_cpu = UINT_MAX ; <nl> spin_unlock_irqrestore (& logbuf_lock , flags ); <nl> goto out ; <nl> } <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> * We own the drivers . We can drop the spinlock and let <nl> * release_console_sem () print the text <nl> */ <nl> + printk_cpu = UINT_MAX ; <nl> spin_unlock_irqrestore (& logbuf_lock , flags ); <nl> console_may_schedule = 0 ; <nl> release_console_sem (); <nl> asmlinkage int vprintk ( const char * fmt , va_list args ) <nl> * allows the semaphore holder to proceed and to call the <nl> * console drivers with the output which we just produced . <nl> */ <nl> + printk_cpu = UINT_MAX ; <nl> spin_unlock_irqrestore (& logbuf_lock , flags ); <nl> } <nl> out : <nl> + preempt_enable (); <nl> return printed_len ; <nl> } <nl> EXPORT_SYMBOL ( printk );
mmm tools / objtool / check . c <nl> ppp tools / objtool / check . c <nl> static bool ignore_unreachable_insn ( struct instruction * insn ) <nl> if ( is_kasan_insn ( insn ) || is_ubsan_insn ( insn )) <nl> return true ; <nl>  <nl> - if ( insn -> type == INSN_JUMP_UNCONDITIONAL && insn -> jump_dest ) { <nl> - insn = insn -> jump_dest ; <nl> - continue ; <nl> + if ( insn -> type == INSN_JUMP_UNCONDITIONAL ) { <nl> + if ( insn -> jump_dest && <nl> + insn -> jump_dest -> func == insn -> func ) { <nl> + insn = insn -> jump_dest ; <nl> + continue ; <nl> + } <nl> + <nl> + break ; <nl> } <nl>  <nl> if ( insn -> offset + insn -> len >= insn -> func -> offset + insn -> func -> len ) <nl> break ; <nl> + <nl> insn = list_next_entry ( insn , list ); <nl> } <nl> 
mmm net / netfilter / nf_conntrack_netlink . c <nl> ppp net / netfilter / nf_conntrack_netlink . c <nl> ctnetlink_setup_nat ( struct nf_conn * ct , const struct nlattr * const cda []) <nl> # ifdef CONFIG_NF_NAT_NEEDED <nl> int ret ; <nl>  <nl> + if (! cda [ CTA_NAT_DST ] && ! cda [ CTA_NAT_SRC ]) <nl> + return 0 ; <nl> + <nl> ret = ctnetlink_parse_nat_setup ( ct , NF_NAT_MANIP_DST , <nl> cda [ CTA_NAT_DST ]); <nl> if ( ret < 0 )
mmm net / nfc / llcp / sock . c <nl> ppp net / nfc / llcp / sock . c <nl> static int llcp_sock_getname ( struct socket * sock , struct sockaddr * uaddr , <nl> struct nfc_llcp_sock * llcp_sock = nfc_llcp_sock ( sk ); <nl> DECLARE_SOCKADDR ( struct sockaddr_nfc_llcp *, llcp_addr , uaddr ); <nl>  <nl> + if ( llcp_sock == NULL || llcp_sock -> dev == NULL ) <nl> + return - EBADFD ; <nl> + <nl> pr_debug ("% p % d % d % d \ n ", sk , llcp_sock -> target_idx , <nl> llcp_sock -> dsap , llcp_sock -> ssap ); <nl> 
mmm drivers / gpu / drm / amd / display / amdgpu_dm / amdgpu_dm . c <nl> ppp drivers / gpu / drm / amd / display / amdgpu_dm / amdgpu_dm . c <nl> fill_stream_properties_from_drm_display_mode ( struct dc_stream_state * stream , <nl> const struct drm_connector * connector ) <nl> { <nl> struct dc_crtc_timing * timing_out = & stream -> timing ; <nl> + const struct drm_display_info * info = & connector -> display_info ; <nl>  <nl> memset ( timing_out , 0 , sizeof ( struct dc_crtc_timing )); <nl>  <nl> fill_stream_properties_from_drm_display_mode ( struct dc_stream_state * stream , <nl> timing_out -> v_border_top = 0 ; <nl> timing_out -> v_border_bottom = 0 ; <nl> /* TODO : un - hardcode */ <nl> - <nl> - if (( connector -> display_info . color_formats & DRM_COLOR_FORMAT_YCRCB444 ) <nl> + if ( drm_mode_is_420_only ( info , mode_in ) <nl> + && stream -> sink -> sink_signal == SIGNAL_TYPE_HDMI_TYPE_A ) <nl> + timing_out -> pixel_encoding = PIXEL_ENCODING_YCBCR420 ; <nl> + else if (( connector -> display_info . color_formats & DRM_COLOR_FORMAT_YCRCB444 ) <nl> && stream -> sink -> sink_signal == SIGNAL_TYPE_HDMI_TYPE_A ) <nl> timing_out -> pixel_encoding = PIXEL_ENCODING_YCBCR444 ; <nl> else
mmm fs / isofs / export . c <nl> ppp fs / isofs / export . c <nl> isofs_export_encode_fh ( struct inode * inode , <nl> len = 3 ; <nl> fh32 [ 0 ] = ei -> i_iget5_block ; <nl> fh16 [ 2 ] = ( __u16 ) ei -> i_iget5_offset ; /* fh16 [ sic ] */ <nl> + fh16 [ 3 ] = 0 ; /* avoid leaking uninitialized data */ <nl> fh32 [ 2 ] = inode -> i_generation ; <nl> if ( parent ) { <nl> struct iso_inode_info * eparent ;
mmm arch / mips / math - emu / cp1emu . c <nl> ppp arch / mips / math - emu / cp1emu . c <nl> int mm_isBranchInstr ( struct pt_regs * regs , struct mm_decoded_insn dec_insn , <nl> unsigned int fcr31 ; <nl> unsigned int bit ; <nl>  <nl> + if (! cpu_has_mmips ) <nl> + return 0 ; <nl> + <nl> switch ( insn . mm_i_format . opcode ) { <nl> case mm_pool32a_op : <nl> if (( insn . mm_i_format . simmediate & MM_POOL32A_MINOR_MASK ) ==
mmm drivers / macintosh / via - cuda . c <nl> ppp drivers / macintosh / via - cuda . c <nl> cuda_poll ( void ) <nl> } <nl> EXPORT_SYMBOL ( cuda_poll ); <nl>  <nl> +# define ARRAY_FULL ( a , p ) (( p ) - ( a ) == ARRAY_SIZE ( a )) <nl> + <nl> static irqreturn_t <nl> cuda_interrupt ( int irq , void * arg ) <nl> { <nl> cuda_interrupt ( int irq , void * arg ) <nl> break ; <nl>  <nl> case reading : <nl> - * reply_ptr ++ = in_8 (& via [ SR ]); <nl> + if ( reading_reply ? ARRAY_FULL ( current_req -> reply , reply_ptr ) <nl> + : ARRAY_FULL ( cuda_rbuf , reply_ptr )) <nl> + ( void ) in_8 (& via [ SR ]); <nl> + else <nl> + * reply_ptr ++ = in_8 (& via [ SR ]); <nl> if (! TREQ_asserted ( status )) { <nl> /* that ' s all folks */ <nl> negate_TIP_and_TACK ();
mmm drivers / base / dma - contiguous . c <nl> ppp drivers / base / dma - contiguous . c <nl> static int __init cma_activate_area ( struct cma * cma ) <nl> base_pfn = pfn ; <nl> for ( j = pageblock_nr_pages ; j ; -- j , pfn ++) { <nl> WARN_ON_ONCE (! pfn_valid ( pfn )); <nl> + /* <nl> + * alloc_contig_range requires the pfn range <nl> + * specified to be in the same zone . Make this <nl> + * simple by forcing the entire CMA resv range <nl> + * to be in the same zone . <nl> + */ <nl> if ( page_zone ( pfn_to_page ( pfn )) != zone ) <nl> - return - EINVAL ; <nl> + goto err ; <nl> } <nl> init_cma_reserved_pageblock ( pfn_to_page ( base_pfn )); <nl> } while (-- i ); <nl>  <nl> mutex_init (& cma -> lock ); <nl> return 0 ; <nl> + <nl> + err : <nl> + kfree ( cma -> bitmap ); <nl> + return - EINVAL ; <nl> } <nl>  <nl> static struct cma cma_areas [ MAX_CMA_AREAS ];
mmm mm / memory . c <nl> ppp mm / memory . c <nl> static int __access_remote_vm ( struct task_struct * tsk , struct mm_struct * mm , <nl> */ <nl> # ifdef CONFIG_HAVE_IOREMAP_PROT <nl> vma = find_vma ( mm , addr ); <nl> - if (! vma ) <nl> + if (! vma || vma -> vm_start > addr ) <nl> break ; <nl> if ( vma -> vm_ops && vma -> vm_ops -> access ) <nl> ret = vma -> vm_ops -> access ( vma , addr , buf ,
mmm kernel / trace / trace_events . c <nl> ppp kernel / trace / trace_events . c <nl> int trace_define_field ( struct ftrace_event_call * call , char * type , <nl> { <nl> struct ftrace_event_field * field ; <nl>  <nl> - field = kmalloc ( sizeof (* field ), GFP_KERNEL ); <nl> + field = kzalloc ( sizeof (* field ), GFP_KERNEL ); <nl> if (! field ) <nl> goto err ; <nl> + <nl> field -> name = kstrdup ( name , GFP_KERNEL ); <nl> if (! field -> name ) <nl> goto err ; <nl> + <nl> field -> type = kstrdup ( type , GFP_KERNEL ); <nl> if (! field -> type ) <nl> goto err ; <nl> + <nl> field -> offset = offset ; <nl> field -> size = size ; <nl> list_add (& field -> link , & call -> fields ); <nl>  <nl> return 0 ; <nl> + <nl> err : <nl> if ( field ) { <nl> kfree ( field -> name ); <nl> kfree ( field -> type ); <nl> } <nl> kfree ( field ); <nl> + <nl> return - ENOMEM ; <nl> } <nl> 
mmm drivers / media / video / saa7134 / saa7134 - input . c <nl> ppp drivers / media / video / saa7134 / saa7134 - input . c <nl> int saa7134_input_init1 ( struct saa7134_dev * dev ) <nl> mask_keyup = 0x400000 ; <nl> polling = 50 ; // ms <nl> break ; <nl> + case SAA7134_BOARD_VIDEOMATE_DVBT_300 : <nl> + ir_codes = videomate_tv_pvr_codes ; <nl> + mask_keycode = 0x003F00 ; <nl> + mask_keyup = 0x040000 ; <nl> + break ; <nl> } <nl> if ( NULL == ir_codes ) { <nl> printk ("% s : Oops : IR config error [ card =% d ]\ n ",mmm drivers / media / video / saa7134 / saa7134 - cards . c <nl> ppp drivers / media / video / saa7134 / saa7134 - cards . c <nl> int saa7134_input_init1 ( struct saa7134_dev * dev ) <nl> mask_keyup = 0x400000 ; <nl> polling = 50 ; // ms <nl> break ; <nl> + case SAA7134_BOARD_VIDEOMATE_DVBT_300 : <nl> + ir_codes = videomate_tv_pvr_codes ; <nl> + mask_keycode = 0x003F00 ; <nl> + mask_keyup = 0x040000 ; <nl> + break ; <nl> } <nl> if ( NULL == ir_codes ) { <nl> printk ("% s : Oops : IR config error [ card =% d ]\ n ", <nl> int saa7134_board_init1 ( struct saa7134_dev * dev ) <nl> /* case SAA7134_BOARD_SABRENT_SBTTVFM : */ /* not finished yet */ <nl> case SAA7134_BOARD_VIDEOMATE_TV_PVR : <nl> case SAA7134_BOARD_VIDEOMATE_TV_GOLD_PLUSII : <nl> + case SAA7134_BOARD_VIDEOMATE_DVBT_300 : <nl> case SAA7134_BOARD_MANLI_MTV001 : <nl> case SAA7134_BOARD_MANLI_MTV002 : <nl> case SAA7134_BOARD_BEHOLD_409FM :
mmm security / keys / key . c <nl> ppp security / keys / key . c <nl> static inline void key_alloc_serial ( struct key * key ) <nl> key -> serial = 2 ; <nl> key_serial_next = key -> serial + 1 ; <nl>  <nl> - if (! parent -> rb_parent ) <nl> + if (! rb_parent ( parent )) <nl> p = & key_serial_tree . rb_node ; <nl> - else if ( parent -> rb_parent -> rb_left == parent ) <nl> - p = & parent -> rb_parent -> rb_left ; <nl> + else if ( rb_parent ( parent )-> rb_left == parent ) <nl> + p = &( rb_parent ( parent )-> rb_left ); <nl> else <nl> - p = & parent -> rb_parent -> rb_right ; <nl> + p = &( rb_parent ( parent )-> rb_right ); <nl>  <nl> parent = rb_next ( parent ); <nl> if (! parent )
mmm drivers / net / ethernet / intel / i40e / i40e_main . c <nl> ppp drivers / net / ethernet / intel / i40e / i40e_main . c <nl> static void i40e_link_event ( struct i40e_pf * pf ) <nl> { <nl> bool new_link , old_link ; <nl> struct i40e_vsi * vsi = pf -> vsi [ pf -> lan_vsi ]; <nl> + u8 new_link_speed , old_link_speed ; <nl>  <nl> /* set this to force the get_link_status call to refresh state */ <nl> pf -> hw . phy . get_link_info = true ; <nl>  <nl> old_link = ( pf -> hw . phy . link_info_old . link_info & I40E_AQ_LINK_UP ); <nl> new_link = i40e_get_link_status (& pf -> hw ); <nl> + old_link_speed = pf -> hw . phy . link_info_old . link_speed ; <nl> + new_link_speed = pf -> hw . phy . link_info . link_speed ; <nl>  <nl> if ( new_link == old_link && <nl> + new_link_speed == old_link_speed && <nl> ( test_bit ( __I40E_DOWN , & vsi -> state ) || <nl> new_link == netif_carrier_ok ( vsi -> netdev ))) <nl> return ;
mmm drivers / scsi / storvsc_drv . c <nl> ppp drivers / scsi / storvsc_drv . c <nl> static void storvsc_handle_error ( struct vmscsi_request * vm_srb , <nl> do_work = true ; <nl> process_err_fn = storvsc_remove_lun ; <nl> break ; <nl> - case ( SRB_STATUS_ABORTED | SRB_STATUS_AUTOSENSE_VALID ): <nl> - if (( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> + case SRB_STATUS_ABORTED : <nl> + if ( vm_srb -> srb_status & SRB_STATUS_AUTOSENSE_VALID && <nl> + ( asc == 0x2a ) && ( ascq == 0x9 )) { <nl> do_work = true ; <nl> process_err_fn = storvsc_device_scan ; <nl> /*
mmm drivers / acpi / bus . c <nl> ppp drivers / acpi / bus . c <nl> int acpi_device_set_power ( struct acpi_device * device , int state ) <nl> int result = 0 ; <nl> acpi_status status = AE_OK ; <nl> char object_name [ 5 ] = { ' _ ', ' P ', ' S ', ' 0 ' + state , '\ 0 ' }; <nl> + bool cut_power = false ; <nl>  <nl> if (! device || ( state < ACPI_STATE_D0 ) || ( state > ACPI_STATE_D3_COLD )) <nl> return - EINVAL ; <nl> int acpi_device_set_power ( struct acpi_device * device , int state ) <nl> return - ENODEV ; <nl> } <nl>  <nl> - /* For D3cold we should execute _PS3 , not _PS4 . */ <nl> - if ( state == ACPI_STATE_D3_COLD ) <nl> + /* For D3cold we should first transition into D3hot . */ <nl> + if ( state == ACPI_STATE_D3_COLD <nl> + && device -> power . states [ ACPI_STATE_D3_COLD ]. flags . os_accessible ) { <nl> + state = ACPI_STATE_D3_HOT ; <nl> object_name [ 3 ] = ' 3 '; <nl> + cut_power = true ; <nl> + } <nl>  <nl> /* <nl> * Transition Power <nl> int acpi_device_set_power ( struct acpi_device * device , int state ) <nl> } <nl> } <nl>  <nl> + if ( cut_power ) <nl> + result = acpi_power_transition ( device , ACPI_STATE_D3_COLD ); <nl> + <nl> end : <nl> if ( result ) <nl> printk ( KERN_WARNING PREFIX
mmm arch / powerpc / kernel / pci_64 . c <nl> ppp arch / powerpc / kernel / pci_64 . c <nl> long sys_pciconfig_iobase ( long which , unsigned long in_bus , <nl> unsigned long in_devfn ) <nl> { <nl> struct pci_controller * hose ; <nl> - struct pci_bus * bus = NULL ; <nl> + struct pci_bus * tmp_bus , * bus = NULL ; <nl> struct device_node * hose_node ; <nl>  <nl> /* Argh ! Please forgive me for that hack , but that ' s the <nl> long sys_pciconfig_iobase ( long which , unsigned long in_bus , <nl> * used on pre - domains setup . We return the first match <nl> */ <nl>  <nl> - list_for_each_entry ( bus , & pci_root_buses , node ) { <nl> - if ( in_bus >= bus -> number && in_bus <= bus -> busn_res . end ) <nl> + list_for_each_entry ( tmp_bus , & pci_root_buses , node ) { <nl> + if ( in_bus >= tmp_bus -> number && <nl> + in_bus <= tmp_bus -> busn_res . end ) { <nl> + bus = tmp_bus ; <nl> break ; <nl> - bus = NULL ; <nl> + } <nl> } <nl> if ( bus == NULL || bus -> dev . of_node == NULL ) <nl> return - ENODEV ;
mmm drivers / i2c / busses / i2c - omap . c <nl> ppp drivers / i2c / busses / i2c - omap . c <nl> static int omap_i2c_init ( struct omap_i2c_dev * dev ) <nl> * to get longer filter period for better noise suppression . <nl> * The filter is iclk ( fclk for HS ) period . <nl> */ <nl> - if ( dev -> speed > 400 || cpu_is_omap_2430 ()) <nl> + if ( dev -> speed > 400 || cpu_is_omap2430 ()) <nl> internal_clk = 19200 ; <nl> else if ( dev -> speed > 100 ) <nl> internal_clk = 9600 ;
mmm sound / usb / usbaudio . c <nl> ppp sound / usb / usbaudio . c <nl> static int check_hw_params_convention ( struct snd_usb_substream * subs ) <nl>  <nl> channels = kcalloc ( MAX_MASK , sizeof ( u32 ), GFP_KERNEL ); <nl> rates = kcalloc ( MAX_MASK , sizeof ( u32 ), GFP_KERNEL ); <nl> + if (! channels || ! rates ) <nl> + goto __out ; <nl>  <nl> list_for_each ( p , & subs -> fmt_list ) { <nl> struct audioformat * f ;
mmm drivers / staging / comedi / drivers / ni_660x . c <nl> ppp drivers / staging / comedi / drivers / ni_660x . c <nl> static int ni_660x_request_mite_channel ( struct comedi_device * dev , <nl> struct mite_channel * mite_chan ; <nl>  <nl> spin_lock_irqsave (& devpriv -> mite_channel_lock , flags ); <nl> - BUG_ON ( counter -> mite_chan ); <nl> mite_chan = mite_request_channel ( devpriv -> mite , <nl> mite_ring ( devpriv , counter )); <nl> if (! mite_chan ) {
mmm drivers / net / tun . c <nl> ppp drivers / net / tun . c <nl> static int tun_set_iff ( struct net * net , struct file * file , struct ifreq * ifr ) <nl>  <nl> err_detach : <nl> tun_detach_all ( dev ); <nl> + /* register_netdevice () already called tun_free_netdev () */ <nl> + goto err_free_dev ; <nl> + <nl> err_free_flow : <nl> tun_flow_uninit ( tun ); <nl> security_tun_dev_free_security ( tun -> security );
mmm drivers / staging / unisys / visorchannel / visorchannel_funcs . c <nl> ppp drivers / staging / unisys / visorchannel / visorchannel_funcs . c <nl> visorchannel_read ( VISORCHANNEL * channel , ulong offset , <nl> int rc = visor_memregion_read ( channel -> memregion , offset , <nl> local , nbytes ); <nl> if (( rc >= 0 ) && ( offset == 0 ) && <nl> - ( nbytes >= sizeof ( struct channel_header ))) { <nl> + ( nbytes >= sizeof ( struct channel_header ))) { <nl> memcpy (& channel -> chan_hdr , local , <nl> sizeof ( struct channel_header )); <nl> } <nl> safe_sig_queue_validate ( struct signal_queue_header * psafe_sqh , <nl> punsafe_sqh -> tail = * ptail ; <nl>  <nl> ERRDRV (" safe_sig_queue_validate : head = 0x % x , tail = 0x % x , MaxSlots = 0x % x ", <nl> - * phead , * ptail , psafe_sqh -> max_slots ); <nl> + * phead , * ptail , psafe_sqh -> max_slots ); <nl> return 0 ; <nl> } <nl> return 1 ; <nl> visorchannel_debug ( VISORCHANNEL * channel , int nQueues , <nl> struct signal_queue_header q ; <nl>  <nl> errcode = visorchannel_read ( channel , <nl> - off + phdr -> ch_space_offset + <nl> - ( i * sizeof ( q )), <nl> - & q , sizeof ( q )); <nl> + off + <nl> + phdr -> ch_space_offset + <nl> + ( i * sizeof ( q )), <nl> + & q , sizeof ( q )); <nl> if ( errcode < 0 ) { <nl> seq_printf ( seq , <nl> " failed to read signal queue #% d from channel @ 0x %- 16 . 16Lx errcode =% d \ n ",
mmm net / wireless / nl80211 . c <nl> ppp net / wireless / nl80211 . c <nl> static int nl80211_start_radar_detection ( struct sk_buff * skb , <nl> if ( err ) <nl> return err ; <nl>  <nl> + if ( netif_carrier_ok ( dev )) <nl> + return - EBUSY ; <nl> + <nl> if ( wdev -> cac_started ) <nl> return - EBUSY ; <nl> 
mmm drivers / gpu / drm / radeon / radeon_display . c <nl> ppp drivers / gpu / drm / radeon / radeon_display . c <nl> bool radeon_crtc_scaling_mode_fixup ( struct drm_crtc * crtc , <nl> radeon_crtc -> rmx_type = radeon_encoder -> rmx_type ; <nl> else <nl> radeon_crtc -> rmx_type = RMX_OFF ; <nl> - src_v = crtc -> mode . vdisplay ; <nl> - dst_v = radeon_crtc -> native_mode . vdisplay ; <nl> - src_h = crtc -> mode . hdisplay ; <nl> - dst_h = radeon_crtc -> native_mode . vdisplay ; <nl> /* copy native mode */ <nl> memcpy (& radeon_crtc -> native_mode , <nl> & radeon_encoder -> native_mode , <nl> sizeof ( struct drm_display_mode )); <nl> + src_v = crtc -> mode . vdisplay ; <nl> + dst_v = radeon_crtc -> native_mode . vdisplay ; <nl> + src_h = crtc -> mode . hdisplay ; <nl> + dst_h = radeon_crtc -> native_mode . hdisplay ; <nl>  <nl> /* fix up for overscan on hdmi */ <nl> if ( ASIC_IS_AVIVO ( rdev ) &&
mmm fs / cifs / smb2ops . c <nl> ppp fs / cifs / smb2ops . c <nl> smb2_query_symlink ( const unsigned int xid , struct cifs_tcon * tcon , <nl> & resp_buftype ); <nl> if (! rc || ! err_iov . iov_base ) { <nl> rc = - ENOENT ; <nl> - goto querty_exit ; <nl> + goto free_path ; <nl> } <nl>  <nl> err_buf = err_iov . iov_base ; <nl> smb2_query_symlink ( const unsigned int xid , struct cifs_tcon * tcon , <nl>  <nl> querty_exit : <nl> free_rsp_buf ( resp_buftype , err_buf ); <nl> + free_path : <nl> kfree ( utf16_path ); <nl> return rc ; <nl> }
mmm mm / compaction . c <nl> ppp mm / compaction . c <nl> static isolate_migrate_t isolate_migratepages ( struct zone * zone , <nl> low_pfn = isolate_migratepages_block ( cc , low_pfn , end_pfn , <nl> isolate_mode ); <nl>  <nl> - if (! low_pfn || cc -> contended ) <nl> + if (! low_pfn || cc -> contended ) { <nl> + acct_isolated ( zone , cc ); <nl> return ISOLATE_ABORT ; <nl> + } <nl>  <nl> /* <nl> * Either we isolated something and proceed with migration . Or
mmm drivers / video / amba - clcd . c <nl> ppp drivers / video / amba - clcd . c <nl> static int clcdfb_register ( struct clcd_fb * fb ) <nl>  <nl> fb_set_var (& fb -> fb , & fb -> fb . var ); <nl>  <nl> - printk ( KERN_INFO " CLCD : % s hardware , % s display \ n ", <nl> - fb -> board -> name , fb -> panel -> mode . name ); <nl> + dev_info (& fb -> dev -> dev , "% s hardware , % s display \ n ", <nl> + fb -> board -> name , fb -> panel -> mode . name ); <nl>  <nl> ret = register_framebuffer (& fb -> fb ); <nl> if ( ret == 0 ) <nl> static int clcdfb_probe ( struct amba_device * dev , struct amba_id * id ) <nl> fb -> dev = dev ; <nl> fb -> board = board ; <nl>  <nl> + dev_info (& fb -> dev -> dev , " PL % 03x rev % u at 0x % 08llx \ n ", <nl> + amba_part ( dev ), amba_rev ( dev ), <nl> + ( unsigned long long ) dev -> res . start ); <nl> + <nl> ret = fb -> board -> setup ( fb ); <nl> if ( ret ) <nl> goto free_fb ;
mmm arch / s390 / kvm / kvm - s390 . h <nl> ppp arch / s390 / kvm / kvm - s390 . h <nl> static inline void kvm_s390_get_base_disp_sse ( struct kvm_vcpu * vcpu , <nl>  <nl> static inline void kvm_s390_get_regs_rre ( struct kvm_vcpu * vcpu , int * r1 , int * r2 ) <nl> { <nl> - * r1 = ( vcpu -> arch . sie_block -> ipb & 0x00f00000 ) >> 20 ; <nl> - * r2 = ( vcpu -> arch . sie_block -> ipb & 0x000f0000 ) >> 16 ; <nl> + if ( r1 ) <nl> + * r1 = ( vcpu -> arch . sie_block -> ipb & 0x00f00000 ) >> 20 ; <nl> + if ( r2 ) <nl> + * r2 = ( vcpu -> arch . sie_block -> ipb & 0x000f0000 ) >> 16 ; <nl> } <nl>  <nl> static inline u64 kvm_s390_get_base_disp_rsy ( struct kvm_vcpu * vcpu )
mmm drivers / infiniband / ulp / ipoib / ipoib_main . c <nl> ppp drivers / infiniband / ulp / ipoib / ipoib_main . c <nl> static void unicast_arp_send ( struct sk_buff * skb , struct net_device * dev , <nl> skb_push ( skb , sizeof * phdr ); <nl> __skb_queue_tail (& path -> queue , skb ); <nl>  <nl> - if ( path_rec_start ( dev , path )) { <nl> + if (! path -> query && path_rec_start ( dev , path )) { <nl> spin_unlock_irqrestore (& priv -> lock , flags ); <nl> path_free ( dev , path ); <nl> return ;
mmm drivers / gpu / drm / i915 / intel_ringbuffer . c <nl> ppp drivers / gpu / drm / i915 / intel_ringbuffer . c <nl> intel_ring_alloc_request ( struct intel_engine_cs * ring ) <nl> return - ENOMEM ; <nl>  <nl> kref_init (& request -> ref ); <nl> + request -> ring = ring ; <nl>  <nl> ret = i915_gem_get_seqno ( ring -> dev , & request -> seqno ); <nl> if ( ret ) {mmm drivers / gpu / drm / i915 / intel_lrc . c <nl> ppp drivers / gpu / drm / i915 / intel_lrc . c <nl> intel_ring_alloc_request ( struct intel_engine_cs * ring ) <nl> return - ENOMEM ; <nl>  <nl> kref_init (& request -> ref ); <nl> + request -> ring = ring ; <nl>  <nl> ret = i915_gem_get_seqno ( ring -> dev , & request -> seqno ); <nl> if ( ret ) { <nl> static int logical_ring_alloc_request ( struct intel_engine_cs * ring , <nl> } <nl>  <nl> kref_init (& request -> ref ); <nl> + request -> ring = ring ; <nl>  <nl> ret = i915_gem_get_seqno ( ring -> dev , & request -> seqno ); <nl> if ( ret ) {mmm drivers / gpu / drm / i915 / i915_gem . c <nl> ppp drivers / gpu / drm / i915 / i915_gem . c <nl> intel_ring_alloc_request ( struct intel_engine_cs * ring ) <nl> return - ENOMEM ; <nl>  <nl> kref_init (& request -> ref ); <nl> + request -> ring = ring ; <nl>  <nl> ret = i915_gem_get_seqno ( ring -> dev , & request -> seqno ); <nl> if ( ret ) { <nl> static int logical_ring_alloc_request ( struct intel_engine_cs * ring , <nl> } <nl>  <nl> kref_init (& request -> ref ); <nl> + request -> ring = ring ; <nl>  <nl> ret = i915_gem_get_seqno ( ring -> dev , & request -> seqno ); <nl> if ( ret ) { <nl> int __i915_add_request ( struct intel_engine_cs * ring , <nl> return ret ; <nl> } <nl>  <nl> - request -> ring = ring ; <nl> request -> head = request_start ; <nl> request -> tail = request_ring_position ; <nl> 
mmm drivers / input / touchscreen / sur40 . c <nl> ppp drivers / input / touchscreen / sur40 . c <nl> static int sur40_probe ( struct usb_interface * interface , <nl> sur40 -> alloc_ctx = vb2_dma_sg_init_ctx ( sur40 -> dev ); <nl> if ( IS_ERR ( sur40 -> alloc_ctx )) { <nl> dev_err ( sur40 -> dev , " Can ' t allocate buffer context "); <nl> + error = PTR_ERR ( sur40 -> alloc_ctx ); <nl> goto err_unreg_v4l2 ; <nl> } <nl> 
mmm drivers / staging / goldfish / goldfish_audio . c <nl> ppp drivers / staging / goldfish / goldfish_audio . c <nl> static int goldfish_audio_probe ( struct platform_device * pdev ) <nl> return 0 ; <nl>  <nl> err_misc_register_failed : <nl> + free_irq ( data -> irq , data ); <nl> err_request_irq_failed : <nl> dma_free_coherent (& pdev -> dev , COMBINED_BUFFER_SIZE , <nl> data -> buffer_virt , data -> buffer_phys );
mmm kernel / module . c <nl> ppp kernel / module . c <nl> static noinline struct module * load_module ( void __user * umod , <nl> free_unload : <nl> module_unload_free ( mod ); <nl> # if defined ( CONFIG_MODULE_UNLOAD ) && defined ( CONFIG_SMP ) <nl> - free_init : <nl> percpu_modfree ( mod -> refptr ); <nl> + free_init : <nl> # endif <nl> module_free ( mod , mod -> module_init ); <nl> free_core :
mmm arch / arm / kernel / sched_clock . c <nl> ppp arch / arm / kernel / sched_clock . c <nl> static unsigned long long notrace cyc_to_sched_clock ( u32 cyc , u32 mask ) <nl> u64 epoch_ns ; <nl> u32 epoch_cyc ; <nl>  <nl> - if ( cd . suspended ) <nl> - return cd . epoch_ns ; <nl> - <nl> /* <nl> * Load the epoch_cyc and epoch_ns atomically . We do this by <nl> * ensuring that we always write epoch_cyc , epoch_ns and <nl> unsigned long long __read_mostly (* sched_clock_func )( void ) = sched_clock_32 ; <nl>  <nl> unsigned long long notrace sched_clock ( void ) <nl> { <nl> + if ( cd . suspended ) <nl> + return cd . epoch_ns ; <nl> + <nl> return sched_clock_func (); <nl> } <nl> 
mmm drivers / lightnvm / pblk - init . c <nl> ppp drivers / lightnvm / pblk - init . c <nl> static int pblk_rwb_init ( struct pblk * pblk ) <nl> struct pblk_rb_entry * entries ; <nl> unsigned long nr_entries , buffer_size ; <nl> unsigned int power_size , power_seg_sz ; <nl> + int pgs_in_buffer ; <nl>  <nl> - if ( write_buffer_size && ( write_buffer_size > pblk -> pgs_in_buffer )) <nl> + pgs_in_buffer = max ( geo -> mw_cunits , geo -> ws_opt ) * geo -> all_luns ; <nl> + <nl> + if ( write_buffer_size && ( write_buffer_size > pgs_in_buffer )) <nl> buffer_size = write_buffer_size ; <nl> else <nl> - buffer_size = pblk -> pgs_in_buffer ; <nl> + buffer_size = pgs_in_buffer ; <nl>  <nl> nr_entries = pblk_rb_calculate_size ( buffer_size ); <nl>  <nl> static int pblk_core_init ( struct pblk * pblk ) <nl> atomic64_set (& pblk -> nr_flush , 0 ); <nl> pblk -> nr_flush_rst = 0 ; <nl>  <nl> - pblk -> pgs_in_buffer = geo -> mw_cunits * geo -> all_luns ; <nl> - <nl> pblk -> min_write_pgs = geo -> ws_opt * ( geo -> csecs / PAGE_SIZE ); <nl> max_write_ppas = pblk -> min_write_pgs * geo -> all_luns ; <nl> pblk -> max_write_pgs = min_t ( int , max_write_ppas , NVM_MAX_VLBA );mmm drivers / lightnvm / pblk . h <nl> ppp drivers / lightnvm / pblk . h <nl> static int pblk_rwb_init ( struct pblk * pblk ) <nl> struct pblk_rb_entry * entries ; <nl> unsigned long nr_entries , buffer_size ; <nl> unsigned int power_size , power_seg_sz ; <nl> + int pgs_in_buffer ; <nl>  <nl> - if ( write_buffer_size && ( write_buffer_size > pblk -> pgs_in_buffer )) <nl> + pgs_in_buffer = max ( geo -> mw_cunits , geo -> ws_opt ) * geo -> all_luns ; <nl> + <nl> + if ( write_buffer_size && ( write_buffer_size > pgs_in_buffer )) <nl> buffer_size = write_buffer_size ; <nl> else <nl> - buffer_size = pblk -> pgs_in_buffer ; <nl> + buffer_size = pgs_in_buffer ; <nl>  <nl> nr_entries = pblk_rb_calculate_size ( buffer_size ); <nl>  <nl> static int pblk_core_init ( struct pblk * pblk ) <nl> atomic64_set (& pblk -> nr_flush , 0 ); <nl> pblk -> nr_flush_rst = 0 ; <nl>  <nl> - pblk -> pgs_in_buffer = geo -> mw_cunits * geo -> all_luns ; <nl> - <nl> pblk -> min_write_pgs = geo -> ws_opt * ( geo -> csecs / PAGE_SIZE ); <nl> max_write_ppas = pblk -> min_write_pgs * geo -> all_luns ; <nl> pblk -> max_write_pgs = min_t ( int , max_write_ppas , NVM_MAX_VLBA ); <nl> struct pblk { <nl>  <nl> int min_write_pgs ; /* Minimum amount of pages required by controller */ <nl> int max_write_pgs ; /* Maximum amount of pages supported by controller */ <nl> - int pgs_in_buffer ; /* Number of pages that need to be held in buffer to <nl> - * guarantee successful reads . <nl> - */ <nl>  <nl> sector_t capacity ; /* Device capacity when bad blocks are subtracted */ <nl> 
mmm sound / soc / davinci / davinci - mcasp . c <nl> ppp sound / soc / davinci / davinci - mcasp . c <nl> static int davinci_mcasp_set_dai_fmt ( struct snd_soc_dai * cpu_dai , <nl> int ret = 0 ; <nl> u32 data_delay ; <nl> bool fs_pol_rising ; <nl> + bool inv_fs = false ; <nl>  <nl> pm_runtime_get_sync ( mcasp -> dev ); <nl> switch ( fmt & SND_SOC_DAIFMT_FORMAT_MASK ) { <nl> static int davinci_mcasp_set_dai_fmt ( struct snd_soc_dai * cpu_dai , <nl> /* No delay after FS */ <nl> data_delay = 0 ; <nl> break ; <nl> - default : <nl> + case SND_SOC_DAIFMT_I2S : <nl> /* configure a full - word SYNC pulse ( LRCLK ) */ <nl> mcasp_set_bits ( mcasp , DAVINCI_MCASP_TXFMCTL_REG , FSXDUR ); <nl> mcasp_set_bits ( mcasp , DAVINCI_MCASP_RXFMCTL_REG , FSRDUR ); <nl>  <nl> /* 1st data bit occur one ACLK cycle after the frame sync */ <nl> data_delay = 1 ; <nl> + /* FS need to be inverted */ <nl> + inv_fs = true ; <nl> break ; <nl> + default : <nl> + ret = - EINVAL ; <nl> + goto out ; <nl> } <nl>  <nl> mcasp_mod_bits ( mcasp , DAVINCI_MCASP_TXFMT_REG , FSXDLY ( data_delay ), <nl> static int davinci_mcasp_set_dai_fmt ( struct snd_soc_dai * cpu_dai , <nl> goto out ; <nl> } <nl>  <nl> + if ( inv_fs ) <nl> + fs_pol_rising = ! fs_pol_rising ; <nl> + <nl> if ( fs_pol_rising ) { <nl> mcasp_clr_bits ( mcasp , DAVINCI_MCASP_TXFMCTL_REG , FSXPOL ); <nl> mcasp_clr_bits ( mcasp , DAVINCI_MCASP_RXFMCTL_REG , FSRPOL );
mmm crypto / crypto_user_base . c <nl> ppp crypto / crypto_user_base . c <nl> static int crypto_report ( struct sk_buff * in_skb , struct nlmsghdr * in_nlh , <nl> drop_alg : <nl> crypto_mod_put ( alg ); <nl>  <nl> - if ( err ) <nl> + if ( err ) { <nl> + kfree_skb ( skb ); <nl> return err ; <nl> + } <nl>  <nl> return nlmsg_unicast ( net -> crypto_nlsk , skb , NETLINK_CB ( in_skb ). portid ); <nl> }
mmm drivers / infiniband / hw / mthca / mthca_qp . c <nl> ppp drivers / infiniband / hw / mthca / mthca_qp . c <nl> int mthca_alloc_qp ( struct mthca_dev * dev , <nl> } <nl>  <nl> static void mthca_lock_cqs ( struct mthca_cq * send_cq , struct mthca_cq * recv_cq ) <nl> + __acquires (& send_cq -> lock ) __acquires (& recv_cq -> lock ) <nl> { <nl> - if ( send_cq == recv_cq ) <nl> + if ( send_cq == recv_cq ) { <nl> spin_lock_irq (& send_cq -> lock ); <nl> - else if ( send_cq -> cqn < recv_cq -> cqn ) { <nl> + __acquire (& recv_cq -> lock ); <nl> + } else if ( send_cq -> cqn < recv_cq -> cqn ) { <nl> spin_lock_irq (& send_cq -> lock ); <nl> spin_lock_nested (& recv_cq -> lock , SINGLE_DEPTH_NESTING ); <nl> } else { <nl> static void mthca_lock_cqs ( struct mthca_cq * send_cq , struct mthca_cq * recv_cq ) <nl> } <nl>  <nl> static void mthca_unlock_cqs ( struct mthca_cq * send_cq , struct mthca_cq * recv_cq ) <nl> + __releases (& send_cq -> lock ) __releases (& recv_cq -> lock ) <nl> { <nl> - if ( send_cq == recv_cq ) <nl> + if ( send_cq == recv_cq ) { <nl> + __release (& recv_cq -> lock ); <nl> spin_unlock_irq (& send_cq -> lock ); <nl> - else if ( send_cq -> cqn < recv_cq -> cqn ) { <nl> + } else if ( send_cq -> cqn < recv_cq -> cqn ) { <nl> spin_unlock (& recv_cq -> lock ); <nl> spin_unlock_irq (& send_cq -> lock ); <nl> } else {
mmm tools / perf / util / sort . c <nl> ppp tools / perf / util / sort . c <nl> static int hist_entry__srcline_snprintf ( struct hist_entry * self , char * bf , <nl> if ( path != NULL ) <nl> goto out_path ; <nl>  <nl> + if (! self -> ms . map ) <nl> + goto out_ip ; <nl> + <nl> snprintf ( cmd , sizeof ( cmd ), " addr2line - e % s % 016 " PRIx64 , <nl> self -> ms . map -> dso -> long_name , self -> ip ); <nl> fp = popen ( cmd , " r ");
mmm sound / pci / hda / patch_sigmatel . c <nl> ppp sound / pci / hda / patch_sigmatel . c <nl> static const struct snd_pci_quirk stac92hd73xx_cfg_tbl [] = { <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x02bd , <nl> " Dell Studio 1557 ", STAC_DELL_M6_DMIC ), <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x02fe , <nl> - " Dell Studio XPS 1645 ", STAC_DELL_M6_BOTH ), <nl> + " Dell Studio XPS 1645 ", STAC_DELL_M6_DMIC ), <nl> SND_PCI_QUIRK ( PCI_VENDOR_ID_DELL , 0x0413 , <nl> " Dell Studio 1558 ", STAC_DELL_M6_DMIC ), <nl> {} /* terminator */
mmm src / src_parser . c <nl> ppp src / src_parser . c <nl> static int src_parser_trans_stage_1_2_3 ( const int tmp_fd , const char * src , const <nl> ( PBUF_TMP_PREV_CHAR ( pbuf ) == ' ' || PBUF_TMP_PREV_CHAR ( pbuf ) == '\ t ' || <nl> PBUF_TMP_PREV_CHAR ( pbuf ) == '\ n ')) { <nl> pbuf . f_indx ++; <nl> - } else if ( pbuf . tmp_indx && <nl> + } else if ( pbuf . tmp_indx && <nl> ( PBUF_TMP_PREV_CHAR ( pbuf ) == '\\')) { <nl> pbuf . tmp_indx --; <nl> pbuf . f_indx ++; <nl> static int src_parser_trans_stage_1_2_3 ( const int tmp_fd , const char * src , const <nl> continue ; <nl>  <nl> case '\\': <nl> + p_buf_write_tmp (& pbuf , tmp_fd ); <nl> p_buf_push_tmp_char (& pbuf , '\\'); <nl> continue ; <nl>  <nl> case '/': <nl> + p_buf_write_tmp (& pbuf , tmp_fd ); <nl> p_buf_push_tmp_char (& pbuf , '/'); <nl> continue ; <nl> 
mmm ssdpd . c <nl> ppp ssdpd . c <nl> static void ssdp_recv ( int sd ) <nl> ssize_t len ; <nl> struct sockaddr sa ; <nl> socklen_t salen ; <nl> - char buf [ MAX_PKT_SIZE ]; <nl> + char buf [ MAX_PKT_SIZE + 1 ]; <nl>  <nl> memset ( buf , 0 , sizeof ( buf )); <nl> - len = recvfrom ( sd , buf , sizeof ( buf ), MSG_DONTWAIT , & sa , & salen ); <nl> + len = recvfrom ( sd , buf , sizeof ( buf ) - 1 , MSG_DONTWAIT , & sa , & salen ); <nl> if ( len > 0 ) { <nl> - buf [ len ] = 0 ; <nl> - <nl> if ( sa . sa_family != AF_INET ) <nl> return ; <nl> 
mmm src / ftpcmd . c <nl> ppp src / ftpcmd . c <nl> static void handle_PORT ( ctrl_t * ctrl , char * str ) <nl>  <nl> /* Convert PORT command ' s argument to IP address + port */ <nl> sscanf ( str , "% d ,% d ,% d ,% d ,% d ,% d ", & a , & b , & c , & d , & e , & f ); <nl> - sprintf ( addr , "% d .% d .% d .% d ", a , b , c , d ); <nl> + snprintf ( addr , sizeof ( addr ), "% d .% d .% d .% d ", a , b , c , d ); <nl>  <nl> /* Check IPv4 address using inet_aton (), throw away converted result */ <nl> if (! inet_aton ( addr , &( sin . sin_addr ))) {
mmm src / common . c <nl> ppp src / common . c <nl> check : <nl> strlcat ( rpath , name , sizeof ( rpath )); <nl> } <nl>  <nl> - if (! chrooted && strncmp ( dir , home , strlen ( home ))) { <nl> + if (! chrooted && strncmp ( rpath , home , strlen ( home ))) { <nl> DBG (" Failed non - chroot dir :% s vs home :% s ", dir , home ); <nl> return NULL ; <nl> }
mmm src / statement . cc <nl> ppp src / statement . cc <nl> template < class T > Values :: Field * <nl> return new Values :: Float ( pos , source . ToNumber (). DoubleValue ()); <nl> } <nl> else if ( source . IsObject ()) { <nl> - std :: string val = source . ToString (). Utf8Value (); <nl> + Napi :: String napiVal = source . ToString (); <nl> + // Check whether toString returned a value that is not undefined . <nl> + if ( napiVal . Type () == 0 ) { <nl> + return NULL ; <nl> + } <nl> + <nl> + std :: string val = napiVal . Utf8Value (); <nl> return new Values :: Text ( pos , val . length (), val . c_str ()); <nl> } <nl> else {
mmm unix / Xvnc / programs / Xserver / hw / vnc / auth . c <nl> ppp unix / Xvnc / programs / Xserver / hw / vnc / auth . c <nl> Bool rfbOptPamAuth ( void ) <nl>  <nl> for ( s = secTypes ; s -> name != NULL ; s ++) { <nl> if ((! strcmp ( s -> name , " unixlogin ") || <nl> - ! strcmp (& s -> name [ strlen ( s -> name ) - 5 ], " plain ")) && s -> enabled ) <nl> + strstr ( s -> name , " plain ")) && s -> enabled ) <nl> return TRUE ; <nl> } <nl> mmm unix / Xvnc / programs / Xserver / hw / vnc / rfbserver . c <nl> ppp unix / Xvnc / programs / Xserver / hw / vnc / rfbserver . c <nl> Bool rfbOptPamAuth ( void ) <nl>  <nl> for ( s = secTypes ; s -> name != NULL ; s ++) { <nl> if ((! strcmp ( s -> name , " unixlogin ") || <nl> - ! strcmp (& s -> name [ strlen ( s -> name ) - 5 ], " plain ")) && s -> enabled ) <nl> + strstr ( s -> name , " plain ")) && s -> enabled ) <nl> return TRUE ; <nl> } <nl>  <nl> static void rfbProcessClientNormalMessage ( rfbClientPtr cl ) <nl>  <nl> flags = Swap32IfLE ( msg . f . flags ); <nl>  <nl> - READ ( data , msg . f . length ) <nl> - <nl> - if ( msg . f . length > sizeof ( data )) <nl> + if ( msg . f . length > sizeof ( data )) { <nl> rfbLog (" Ignoring fence . Payload of % d bytes is too large .\ n ", <nl> msg . f . length ); <nl> - else <nl> + SKIP ( msg . f . length ) <nl> + } else { <nl> + READ ( data , msg . f . length ) <nl> HandleFence ( cl , flags , msg . f . length , data ); <nl> + } <nl> + <nl> return ; <nl> } <nl> 
mmm net / nfs . c <nl> ppp net / nfs . c <nl> static int nfs_lookup_reply ( uchar * pkt , unsigned len ) <nl> } <nl>  <nl> if ( supported_nfs_versions & NFSV2_FLAG ) { <nl> + if ((( uchar *)&( rpc_pkt . u . reply . data [ 0 ]) - ( uchar *)(& rpc_pkt ) + NFS_FHSIZE ) > len ) <nl> + return - NFS_RPC_DROP ; <nl> memcpy ( filefh , rpc_pkt . u . reply . data + 1 , NFS_FHSIZE ); <nl> } else { /* NFSV3_FLAG */ <nl> filefh3_length = ntohl ( rpc_pkt . u . reply . data [ 1 ]); <nl> if ( filefh3_length > NFS3_FHSIZE ) <nl> filefh3_length = NFS3_FHSIZE ; <nl> + if ((( uchar *)&( rpc_pkt . u . reply . data [ 0 ]) - ( uchar *)(& rpc_pkt ) + filefh3_length ) > len ) <nl> + return - NFS_RPC_DROP ; <nl> memcpy ( filefh , rpc_pkt . u . reply . data + 2 , filefh3_length ); <nl> } <nl> 
mmm common / fdt_region . c <nl> ppp common / fdt_region . c <nl> int fdt_find_regions ( const void * fdt , char * const inc [], int inc_count , <nl> int depth = - 1 ; <nl> int want = 0 ; <nl> int base = fdt_off_dt_struct ( fdt ); <nl> + bool expect_end = false ; <nl>  <nl> end = path ; <nl> * end = '\ 0 '; <nl> int fdt_find_regions ( const void * fdt , char * const inc [], int inc_count , <nl> tag = fdt_next_tag ( fdt , offset , & nextoffset ); <nl> stop_at = nextoffset ; <nl>  <nl> + /* If we see two root nodes , something is wrong */ <nl> + if ( expect_end && tag != FDT_END ) <nl> + return - FDT_ERR_BADLAYOUT ; <nl> + <nl> switch ( tag ) { <nl> case FDT_PROP : <nl> include = want >= 2 ; <nl> int fdt_find_regions ( const void * fdt , char * const inc [], int inc_count , <nl> if ( depth == FDT_MAX_DEPTH ) <nl> return - FDT_ERR_BADSTRUCTURE ; <nl> name = fdt_get_name ( fdt , offset , & len ); <nl> + <nl> + /* The root node must have an empty name */ <nl> + if (! depth && * name ) <nl> + return - FDT_ERR_BADLAYOUT ; <nl> if ( end - path + 2 + len >= path_len ) <nl> return - FDT_ERR_NOSPACE ; <nl> if ( end != path + 1 ) <nl> int fdt_find_regions ( const void * fdt , char * const inc [], int inc_count , <nl> while ( end > path && *-- end != '/') <nl> ; <nl> * end = '\ 0 '; <nl> + if ( depth == - 1 ) <nl> + expect_end = true ; <nl> break ; <nl>  <nl> case FDT_END :
mmm src / bin / common / color . c <nl> ppp src / bin / common / color . c <nl> void color_cmyk_to_rgb ( opj_image_t * image ) <nl> w = image -> comps [ 0 ]. w ; <nl> h = image -> comps [ 0 ]. h ; <nl>  <nl> - if ( image -> numcomps < 4 ) return ; <nl> + if ( <nl> + ( image -> numcomps < 4 ) <nl> + || ( image -> comps [ 0 ]. dx != image -> comps [ 1 ]. dx ) || ( image -> comps [ 0 ]. dx != image -> comps [ 2 ]. dx ) || ( image -> comps [ 0 ]. dx != image -> comps [ 3 ]. dx ) <nl> + || ( image -> comps [ 0 ]. dy != image -> comps [ 1 ]. dy ) || ( image -> comps [ 0 ]. dy != image -> comps [ 2 ]. dy ) || ( image -> comps [ 0 ]. dy != image -> comps [ 3 ]. dy ) <nl> + ) { <nl> + fprintf ( stderr ,"% s :% d : color_cmyk_to_rgb \ n \ tCAN NOT CONVERT \ n ", __FILE__ , __LINE__ ); <nl> + return ; <nl> + } <nl>  <nl> max = w * h ; <nl> 
mmm src / lib / openjp2 / image . c <nl> ppp src / lib / openjp2 / image . c <nl> opj_image_t * OPJ_CALLCONV opj_image_create ( OPJ_UINT32 numcmpts , <nl> image -> color_space = clrspc ; <nl> image -> numcomps = numcmpts ; <nl> /* allocate memory for the per - component information */ <nl> - image -> comps = ( opj_image_comp_t *) opj_calloc ( 1 , <nl> - image -> numcomps * sizeof ( opj_image_comp_t )); <nl> + image -> comps = ( opj_image_comp_t *) opj_calloc ( image -> numcomps , <nl> + sizeof ( opj_image_comp_t )); <nl> if (! image -> comps ) { <nl> /* TODO replace with event manager , breaks API */ <nl> /* fprintf ( stderr ," Unable to allocate memory for image .\ n "); */
mmm src / bin / common / opj_getopt . c <nl> ppp src / bin / common / opj_getopt . c <nl> again : <nl> } <nl>  <nl> if ( argv [ opj_optind ][ 0 ] == '-') { /* long option */ <nl> - char * arg = argv [ opj_optind ] + 1 ; <nl> + char * arg ; <nl> const opj_option_t * o ; <nl> o = longopts ; <nl> len = sizeof ( longopts [ 0 ]); <nl>  <nl> if ( param > 1 ) { <nl> + if ( opj_optind + 1 >= argc ) { <nl> + return - 1 ; <nl> + } <nl> arg = argv [ opj_optind + 1 ]; <nl> opj_optind ++; <nl> } else {
mmm src / bin / jp2 / opj_decompress . c <nl> ppp src / bin / jp2 / opj_decompress . c <nl> int get_num_images ( char * imgdirpath ){ <nl> continue ; <nl> num_images ++; <nl> } <nl> + closedir ( dir ); <nl> return num_images ; <nl> } <nl>  <nl> int load_images ( dircnt_t * dirptr , char * imgdirpath ){ <nl> strcpy ( dirptr -> filename [ i ], content -> d_name ); <nl> i ++; <nl> } <nl> + closedir ( dir ); <nl> return 0 ; <nl> } <nl> 
mmm codec / image_to_j2k . c <nl> ppp codec / image_to_j2k . c <nl> int main ( int argc , char ** argv ) <nl>  <nl> /* Remove the temporary files */ <nl> /* -------------------------- */ <nl> - if ( cp . decod_format != PGX_CFMT ) { /* PNM PGM PPM or BMP */ <nl> + if ( cp . decod_format != PGX_DFMT ) { /* PNM PGM PPM or BMP */ <nl> for ( i = 0 ; i < img . numcomps ; i ++) { <nl> char tmp ; <nl> sprintf (& tmp , " Compo % d ", i );
mmm src / bin / jp2 / convert . c <nl> ppp src / bin / jp2 / convert . c <nl> void scale_component ( opj_image_comp_t * component , OPJ_UINT32 precision ) <nl> return ; <nl> } <nl> if ( component -> prec < precision ) { <nl> - shift = precision - component -> prec ; <nl> + shift = ( int )( precision - component -> prec ); <nl> } else { <nl> - shift = component -> prec - precision ; <nl> + shift = ( int )( component -> prec - precision ); <nl> } <nl> len = ( OPJ_SIZE_T ) component -> w * ( OPJ_SIZE_T ) component -> h ; <nl>  <nl> static opj_image_t * rawtoimage_common ( const char * filename , opj_cparameters_t * p <nl> } <nl> w = raw_cp -> rawWidth ; <nl> h = raw_cp -> rawHeight ; <nl> - cmptparm = ( opj_image_cmptparm_t *) calloc ( numcomps , sizeof ( opj_image_cmptparm_t )); <nl> + cmptparm = ( opj_image_cmptparm_t *) calloc (( OPJ_UINT32 ) numcomps , sizeof ( opj_image_cmptparm_t )); <nl> if (! cmptparm ) { <nl> fprintf ( stderr , " Failed to allocate image components parameters !!\ n "); <nl> fprintf ( stderr ," Aborting \ n ");
mmm src / lib / openjp2 / dwt . c <nl> ppp src / lib / openjp2 / dwt . c <nl> static INLINE OPJ_BOOL opj_dwt_encode_procedure ( opj_tcd_tilecomp_t * tilec , void <nl>  <nl> l_data_size = opj_dwt_max_resolution ( tilec -> resolutions , tilec -> numresolutions ) * ( OPJ_UINT32 ) sizeof ( OPJ_INT32 ); <nl> bj = ( OPJ_INT32 *) opj_malloc (( size_t ) l_data_size ); <nl> - if (! bj ) { <nl> + if ( l_data_size != 0 && ! bj ) { <nl> return OPJ_FALSE ; <nl> } <nl> i = l ;
mmm src / bin / jp2 / convertbmp . c <nl> ppp src / bin / jp2 / convertbmp . c <nl> static OPJ_BOOL bmp_read_raw_data ( FILE * IN , OPJ_UINT8 * pData , OPJ_UINT32 stride , <nl> static OPJ_BOOL bmp_read_rle8_data ( FILE * IN , OPJ_UINT8 * pData , <nl> OPJ_UINT32 stride , OPJ_UINT32 width , OPJ_UINT32 height ) <nl> { <nl> - OPJ_UINT32 x , y ; <nl> + OPJ_UINT32 x , y , written ; <nl> OPJ_UINT8 * pix ; <nl> const OPJ_UINT8 * beyond ; <nl>  <nl> beyond = pData + stride * height ; <nl> pix = pData ; <nl>  <nl> - x = y = 0U ; <nl> + x = y = written = 0U ; <nl> while ( y < height ) { <nl> int c = getc ( IN ); <nl> if ( c == EOF ) { <nl> static OPJ_BOOL bmp_read_rle8_data ( FILE * IN , OPJ_UINT8 * pData , <nl> for ( j = 0 ; ( j < c ) && ( x < width ) && <nl> (( OPJ_SIZE_T ) pix < ( OPJ_SIZE_T ) beyond ); j ++, x ++, pix ++) { <nl> * pix = c1 ; <nl> + written ++; <nl> } <nl> } else { <nl> c = getc ( IN ); <nl> static OPJ_BOOL bmp_read_rle8_data ( FILE * IN , OPJ_UINT8 * pData , <nl> } <nl> c1 = ( OPJ_UINT8 ) c1_int ; <nl> * pix = c1 ; <nl> + written ++; <nl> } <nl> if (( OPJ_UINT32 ) c & 1U ) { /* skip padding byte */ <nl> c = getc ( IN ); <nl> static OPJ_BOOL bmp_read_rle8_data ( FILE * IN , OPJ_UINT8 * pData , <nl> } <nl> } <nl> }/* while () */ <nl> + <nl> + if ( written != width * height ) { <nl> + fprintf ( stderr , " warning , image ' s actual size does not match advertized one \ n "); <nl> + return OPJ_FALSE ; <nl> + } <nl> + <nl> return OPJ_TRUE ; <nl> } <nl> 
mmm src / lib / openjp2 / tcd . c <nl> ppp src / lib / openjp2 / tcd . c <nl> static INLINE OPJ_BOOL opj_tcd_init_tile ( opj_tcd_t * p_tcd , OPJ_UINT32 p_tile_no , <nl>  <nl> /* compute l_data_size with overflow check */ <nl> l_data_size = ( OPJ_UINT32 )( l_tilec -> x1 - l_tilec -> x0 ); <nl> - if (((( OPJ_UINT32 )- 1 ) / l_data_size ) < ( OPJ_UINT32 )( l_tilec -> y1 - l_tilec -> y0 )) { <nl> + /* issue 733 , l_data_size == 0U , probably something wrong should be checked before getting here */ <nl> + if (( l_data_size > 0U ) && (((( OPJ_UINT32 )- 1 ) / l_data_size ) < ( OPJ_UINT32 )( l_tilec -> y1 - l_tilec -> y0 ))) { <nl> opj_event_msg ( manager , EVT_ERROR , " Not enough memory for tile data \ n "); <nl> return OPJ_FALSE ; <nl> }
mmm src / lib / openjp2 / j2k . c <nl> ppp src / lib / openjp2 / j2k . c <nl> static OPJ_BOOL opj_j2k_write_mco ( opj_j2k_t * p_j2k , <nl> assert ( p_stream != 00 ); <nl>  <nl> l_tcp =&( p_j2k -> m_cp . tcps [ p_j2k -> m_current_tile_number ]); <nl> - l_current_data = p_j2k -> m_specific_param . m_encoder . m_header_tile_data ; <nl> - <nl> + <nl> l_mco_size = 5 + l_tcp -> m_nb_mcc_records ; <nl> if ( l_mco_size > p_j2k -> m_specific_param . m_encoder . m_header_tile_data_size ) { <nl>  <nl> static OPJ_BOOL opj_j2k_write_mco ( opj_j2k_t * p_j2k , <nl> p_j2k -> m_specific_param . m_encoder . m_header_tile_data = new_header_tile_data ; <nl> p_j2k -> m_specific_param . m_encoder . m_header_tile_data_size = l_mco_size ; <nl> } <nl> + l_current_data = p_j2k -> m_specific_param . m_encoder . m_header_tile_data ; <nl> + <nl>  <nl> opj_write_bytes ( l_current_data , J2K_MS_MCO , 2 ); /* MCO */ <nl> l_current_data += 2 ; <nl> static OPJ_BOOL opj_j2k_write_mco ( opj_j2k_t * p_j2k , <nl> ++ l_current_data ; <nl>  <nl> l_mcc_record = l_tcp -> m_mcc_records ; <nl> - for ( i = 0 ; i < l_tcp -> m_nb_mcc_records ;++ i ) { <nl> + for ( i = 0 ; i < l_tcp -> m_nb_mcc_records ;++ i ) { <nl> opj_write_bytes ( l_current_data , l_mcc_record -> m_index , 1 );/* Imco -> use the mcc indicated by 1 */ <nl> ++ l_current_data ; <nl> - <nl> ++ l_mcc_record ; <nl> } <nl> 
mmm src / bin / common / color . c <nl> ppp src / bin / common / color . c <nl> void color_esycc_to_rgb ( opj_image_t * image ) <nl> int flip_value = ( 1 << ( image -> comps [ 0 ]. prec - 1 )); <nl> int max_value = ( 1 << image -> comps [ 0 ]. prec ) - 1 ; <nl>  <nl> - if ( image -> numcomps < 3 ) return ; <nl> + if ( <nl> + ( image -> numcomps < 3 ) <nl> + || ( image -> comps [ 0 ]. dx != image -> comps [ 1 ]. dx ) || ( image -> comps [ 0 ]. dx != image -> comps [ 2 ]. dx ) <nl> + || ( image -> comps [ 0 ]. dy != image -> comps [ 1 ]. dy ) || ( image -> comps [ 0 ]. dy != image -> comps [ 2 ]. dy ) <nl> + ) { <nl> + fprintf ( stderr ,"% s :% d : color_esycc_to_rgb \ n \ tCAN NOT CONVERT \ n ", __FILE__ , __LINE__ ); <nl> + return ; <nl> + } <nl>  <nl> w = image -> comps [ 0 ]. w ; <nl> h = image -> comps [ 0 ]. h ;
mmm src / lib / openjp2 / tcd . c <nl> ppp src / lib / openjp2 / tcd . c <nl> static OPJ_BOOL opj_tcd_code_block_enc_allocate_data ( opj_tcd_cblk_enc_t * <nl> { <nl> OPJ_UINT32 l_data_size ; <nl>  <nl> - /* The + 1 is needed for https :// github . com / uclouvain / openjpeg / issues / 835 */ <nl> - l_data_size = 1 + ( OPJ_UINT32 )(( p_code_block -> x1 - p_code_block -> x0 ) * <nl> + /* + 1 is needed for https :// github . com / uclouvain / openjpeg / issues / 835 */ <nl> + /* and actually + 2 required for https :// github . com / uclouvain / openjpeg / issues / 982 */ <nl> + /* TODO : is there a theoretical upper - bound for the compressed code */ <nl> + /* block size ? */ <nl> + l_data_size = 2 + ( OPJ_UINT32 )(( p_code_block -> x1 - p_code_block -> x0 ) * <nl> ( p_code_block -> y1 - p_code_block -> y0 ) * ( OPJ_INT32 ) sizeof ( OPJ_UINT32 )); <nl>  <nl> if ( l_data_size > p_code_block -> data_size ) {
mmm src / lib / openjp2 / tcd . c <nl> ppp src / lib / openjp2 / tcd . c <nl> void opj_tcd_makelayer ( opj_tcd_t * tcd , <nl> n = passno + 1 ; <nl> continue ; <nl> } <nl> - if ( thresh - ( dd / dr ) <= DBL_EPSILON ) /* do not rely on float equality , check with DBL_EPSILON margin */ <nl> + if ( thresh - ( dd / dr ) < DBL_EPSILON ) /* do not rely on float equality , check with DBL_EPSILON margin */ <nl> n = passno + 1 ; <nl> } <nl> 
mmm src / bin / jp2 / convertbmp . c <nl> ppp src / bin / jp2 / convertbmp . c <nl> static OPJ_BOOL bmp_read_info_header ( FILE * IN , OPJ_BITMAPINFOHEADER * header ) <nl>  <nl> header -> biBitCount = ( OPJ_UINT16 ) getc ( IN ); <nl> header -> biBitCount |= ( OPJ_UINT16 )(( OPJ_UINT32 ) getc ( IN ) << 8 ); <nl> + if ( header -> biBitCount == 0 ) { <nl> + fprintf ( stderr , " Error , invalid biBitCount % d \ n ", 0 ); <nl> + return OPJ_FALSE ; <nl> + } <nl>  <nl> if ( header -> biSize >= 40U ) { <nl> header -> biCompression = ( OPJ_UINT32 ) getc ( IN );
mmm src / lib / openjp2 / openjpeg . h <nl> ppp src / lib / openjp2 / openjpeg . h <nl> # ifdef __GNUC__ <nl> # define DEPRECATED ( func ) func __attribute__ (( deprecated )) <nl> # elif defined ( _MSC_VER ) <nl> - # define DEPRECATED ( func ) __declspec ( deprecated ) func <nl> + # define OPJ_DEPRECATED ( func ) __declspec ( deprecated ) func <nl> # else <nl> # pragma message (" WARNING : You need to implement DEPRECATED for this compiler ") <nl> - # define DEPRECATED ( func ) func <nl> + # define OPJ_DEPRECATED ( func ) func <nl> # endif <nl>  <nl> # if defined ( OPJ_STATIC ) || ! defined ( _WIN32 ) <nl> OPJ_API opj_stream_t * OPJ_CALLCONV opj_stream_create ( OPJ_SIZE_T p_buffer_size , O <nl> * <nl> * @ param p_stream the stream to destroy . <nl> */ <nl> - OPJ_API void OPJ_CALLCONV opj_stream_destroy ( opj_stream_t * p_stream ); <nl> + OPJ_DEPRECATED ( OPJ_API void OPJ_CALLCONV opj_stream_destroy ( opj_stream_t * p_stream )); <nl> OPJ_API void OPJ_CALLCONV opj_stream_destroy_v3 ( opj_stream_t * p_stream ); <nl>  <nl> /**
mmm src / lib / openjp2 / pi . c <nl> ppp src / lib / openjp2 / pi . c <nl> opj_pi_iterator_t * opj_pi_create_decode ( opj_image_t * p_image , <nl> l_current_pi = l_pi ; <nl>  <nl> /* memory allocation for include */ <nl> - l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( l_tcp -> numlayers + 1 ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> + /* prevent an integer overflow issue */ <nl> + l_current_pi -> include = 00 ; <nl> + if ( l_step_l <= ( SIZE_MAX / ( l_tcp -> numlayers + 1U ))) <nl> + { <nl> + l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( l_tcp -> numlayers + 1 ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> + } <nl> + <nl> if <nl> (! l_current_pi -> include ) <nl> {
mmm src / bin / mj2 / opj_mj2_extract . c <nl> ppp src / bin / mj2 / opj_mj2_extract . c <nl> int main ( int argc , char * argv []) <nl> fread ( frame_codestream , sample -> sample_size - 8 , 1 , <nl> file ); /* Assuming that jp and ftyp markers size do */ <nl>  <nl> - sprintf ( outfilename , "% s_ % 05d . j2k ", argv [ 2 ], snum ); <nl> + { <nl> + int num = snprintf ( outfilename , sizeof ( outfilename ), <nl> + "% s_ % 05d . j2k ", argv [ 2 ], <nl> + snum ); <nl> + if ( num >= sizeof ( outfilename )) { <nl> + fprintf ( stderr , " maximum length of output prefix exceeded \ n "); <nl> + free ( frame_codestream ); <nl> + return 1 ; <nl> + } <nl> + } <nl> + <nl> outfile = fopen ( outfilename , " wb "); <nl> if (! outfile ) { <nl> fprintf ( stderr , " failed to open % s for writing \ n ", outfilename ); <nl> + free ( frame_codestream ); <nl> return 1 ; <nl> } <nl> fwrite ( frame_codestream , sample -> sample_size - 8 , 1 , outfile );
mmm src / bin / jp2 / convert . c <nl> ppp src / bin / jp2 / convert . c <nl> opj_image_t * tiftoimage ( const char * filename , opj_cparameters_t * parameters ) <nl> */ <nl> memset (& cmptparm [ 0 ], 0 , 4 * sizeof ( opj_image_cmptparm_t )); <nl>  <nl> + if (( tiPhoto == PHOTOMETRIC_RGB ) && ( parameters -> cp_cinema )) { <nl> + fprintf ( stdout ," WARNING :\ n " <nl> + " Input image bitdepth is % d bits \ n " <nl> + " TIF conversion has automatically rescaled to 12 - bits \ n " <nl> + " to comply with cinema profiles .\ n ", <nl> + tiBps ); <nl> + } <nl> + <nl> if ( tiPhoto == PHOTOMETRIC_RGB ) /* RGB ( A ) */ <nl> { <nl> numcomps = 3 + has_alpha ;
mmm src / bin / jp2 / convert . c <nl> ppp src / bin / jp2 / convert . c <nl> opj_image_t * pgxtoimage ( const char * filename , opj_cparameters_t * parameters ) <nl> } <nl>  <nl> fseek ( f , 0 , SEEK_SET ); <nl> - if ( fscanf ( f , " PG %[ \ t ]% c % c %[ \ t +-]% d %[ \ t ]% d %[ \ t ]% d ", temp , & endian1 , <nl> + if ( fscanf ( f , " PG % 31 [ \ t ]% c % c % 31 [ \ t +-]% d % 31 [ \ t ]% d % 31 [ \ t ]% d ", temp , & endian1 , <nl> & endian2 , signtmp , & prec , temp , & w , temp , & h ) != 9 ) { <nl> fclose ( f ); <nl> fprintf ( stderr ,
mmm src / lib / openjp2 / pi . c <nl> ppp src / lib / openjp2 / pi . c <nl> opj_pi_iterator_t * opj_pi_create_decode ( opj_image_t * p_image , <nl> l_current_pi -> include = 00 ; <nl> if ( l_step_l <= ( SIZE_MAX / ( l_tcp -> numlayers + 1U ))) <nl> { <nl> - l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( l_tcp -> numlayers + 1 ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> + l_current_pi -> include = ( OPJ_INT16 *) opj_calloc (( size_t )( l_tcp -> numlayers + 1U ) * l_step_l , sizeof ( OPJ_INT16 )); <nl> } <nl>  <nl> if
mmm src / lib / openjp2 / j2k . c <nl> ppp src / lib / openjp2 / j2k . c <nl> static OPJ_BOOL opj_j2k_read_rgn ( opj_j2k_t * p_j2k , <nl> }; <nl> # endif /* USE_JPWL */ <nl>  <nl> + /* testcase 3635 . pdf . asan . 77 . 2930 */ <nl> + if ( l_comp_no >= l_nb_comp ) { <nl> + opj_event_msg ( p_manager , EVT_ERROR , <nl> + " bad component number in RGN (% d when there are only % d )\ n ", <nl> + l_comp_no , l_nb_comp ); <nl> + return OPJ_FALSE ; <nl> + } <nl> + <nl> opj_read_bytes ( p_header_data ,( OPJ_UINT32 *) (&( l_tcp -> tccps [ l_comp_no ]. roishift )), 1 ); /* SPrgn */ <nl> ++ p_header_data ; <nl> 
mmm v4l2loopback . c <nl> ppp v4l2loopback . c <nl> static int vidioc_querycap ( struct file * file , void * priv , <nl> __u32 capabilities = V4L2_CAP_STREAMING | V4L2_CAP_READWRITE ; <nl>  <nl> strlcpy ( cap -> driver , " v4l2 loopback ", sizeof ( cap -> driver )); <nl> - snprintf ( cap -> card , labellen , dev -> card_label ); <nl> + snprintf ( cap -> card , labellen , "% s ", dev -> card_label ); <nl> snprintf ( cap -> bus_info , sizeof ( cap -> bus_info ), <nl> " platform : v4l2loopback -% 03d ", device_nr ); <nl>  <nl> static int v4l2_loopback_add ( struct v4l2_loopback_config * conf , int * ret_nr ) <nl> } <nl>  <nl> MARK (); <nl> - snprintf ( dev -> vdev -> name , sizeof ( dev -> vdev -> name ), dev -> card_label ); <nl> + snprintf ( dev -> vdev -> name , sizeof ( dev -> vdev -> name ), "% s ", dev -> card_label ); <nl>  <nl> vdev_priv -> device_nr = nr ; <nl> 
mmm core / master . c <nl> ppp core / master . c <nl> static void master_check_listen_queue () { <nl> if ( uwsgi_sock -> queue > load ) { <nl> load = uwsgi_sock -> queue ; <nl> } <nl> - if ( uwsgi_sock -> queue >= uwsgi_sock -> max_queue ) { <nl> + if ( uwsgi_sock -> queue > 0 && uwsgi_sock -> queue >= uwsgi_sock -> max_queue ) { <nl> uwsgi_log_verbose ("*** uWSGI listen queue of socket \"% s \" ( fd : % d ) full !!! (% llu /% llu ) ***\ n ", uwsgi_sock -> name , uwsgi_sock -> fd , ( unsigned long long ) uwsgi_sock -> queue , ( unsigned long long ) uwsgi_sock -> max_queue ); <nl> uwsgi . shared -> options [ UWSGI_OPTION_BACKLOG_ERRORS ]++; <nl> }
mmm plugins / python / tracebacker . c <nl> ppp plugins / python / tracebacker . c <nl> void * uwsgi_python_tracebacker_thread ( void * foobar ) { <nl> uwsgi . no_defer_accept = current_defer_accept ; <nl>  <nl> PyObject * traceback_module = PyImport_ImportModule (" traceback "); <nl> - if (! traceback_module ) return NULL ; <nl> + if (! traceback_module ) { <nl> + free ( str_wid ); <nl> + free ( sock_path ); <nl> + close ( fd ); <nl> + return NULL ; <nl> + } <nl> PyObject * traceback_dict = PyModule_GetDict ( traceback_module ); <nl> PyObject * extract_stack = PyDict_GetItemString ( traceback_dict , " extract_stack "); <nl> 
mmm core / alarm . c <nl> ppp core / alarm . c <nl> static void uwsgi_alarm_thread_loop ( struct uwsgi_thread * ut ) { <nl> long ptr = 0 ; <nl> memcpy (& ptr , buf , sizeof ( long )); <nl> struct uwsgi_alarm_instance * uai = ( struct uwsgi_alarm_instance *) ptr ; <nl> - if (! uai ) return ; <nl> + if (! uai ) <nl> + break ; <nl> uwsgi_alarm_run ( uai , msg , msg_size ); <nl> } <nl> } <nl> } <nl> + free ( buf ); <nl> } <nl>  <nl> // initialize alarms , instances and log regexps
mmm plugins / rawrouter / rawrouter . c <nl> ppp plugins / rawrouter / rawrouter . c <nl> static struct uwsgi_option rawrouter_options [] = { <nl> {" rawrouter - ss ", required_argument , 0 , " run the rawrouter stats server ", uwsgi_opt_set_str , & urr . cr . stats_server , 0 }, <nl> {" rawrouter - harakiri ", required_argument , 0 , " enable rawrouter harakiri ", uwsgi_opt_set_int , & urr . cr . harakiri , 0 }, <nl>  <nl> - {" rawrouter - xclient ", no_argument , 0 , " use the xclient protocol to pass the client addres ", uwsgi_opt_true , & urr . xclient , 0 }, <nl> + {" rawrouter - xclient ", no_argument , 0 , " use the xclient protocol to pass the client address ", uwsgi_opt_true , & urr . xclient , 0 }, <nl>  <nl> {" rawrouter - buffer - size ", required_argument , 0 , " set internal buffer size ( default : page size )", uwsgi_opt_set_64bit , & urr . cr . buffer_size , 0 }, <nl> 
mmm plugins / stats_pusher_statsd / plugin . c <nl> ppp plugins / stats_pusher_statsd / plugin . c <nl> # include < uwsgi . h > <nl>  <nl> +/* <nl> + <nl> + this is a stats pusher plugin for the statsd server : <nl> + <nl> +-- stats - push statsd : address [, prefix ] <nl> + <nl> + example : <nl> + <nl> +-- stats - push statsd : 127 . 0 . 0 . 1 : 8125 , myinstance <nl> + <nl> + it is pretty minimal , but will be extended after the 2 . 0 metric subsystem will be released <nl> + <nl> +*/ <nl> + <nl> extern struct uwsgi_server uwsgi ; <nl>  <nl> +// configuration of a statsd node <nl> struct statsd_node { <nl> int fd ; <nl> union uwsgi_sockaddr addr ;
mmm uwsgi . c <nl> ppp uwsgi . c <nl> int uwsgi_start ( void * v_argv ) { <nl> } <nl>  <nl> uwsgi_add_sockets_to_queue ( uwsgi . async_queue ); <nl> - } <nl>  <nl> - uwsgi . rb_async_timeouts = uwsgi_init_rb_timer (); <nl> + uwsgi . rb_async_timeouts = uwsgi_init_rb_timer (); <nl>  <nl> - uwsgi . async_queue_unused = uwsgi_malloc ( sizeof ( struct wsgi_request *) * uwsgi . async ); <nl> + uwsgi . async_queue_unused = uwsgi_malloc ( sizeof ( struct wsgi_request *) * uwsgi . async ); <nl>  <nl> - for ( i = 0 ; i < uwsgi . async ; i ++) { <nl> - uwsgi . async_queue_unused [ i ] = uwsgi . wsgi_requests [ i ]; <nl> - } <nl> + for ( i = 0 ; i < uwsgi . async ; i ++) { <nl> + uwsgi . async_queue_unused [ i ] = uwsgi . wsgi_requests [ i ]; <nl> + } <nl>  <nl> - uwsgi . async_queue_unused_ptr = uwsgi . async - 1 ; <nl> + uwsgi . async_queue_unused_ptr = uwsgi . async - 1 ; <nl> + } <nl> # endif <nl>  <nl> 
mmm core / utils . c <nl> ppp core / utils . c <nl> void uwsgi_uuid ( char * buf ) { <nl> int uwsgi_uuid_cmp ( char * x , char * y ) { <nl> int i ; <nl> for ( i = 0 ; i < 36 ; i ++) { <nl> - if ( x [ i ] > y [ i ]) { <nl> - return 1 ; <nl> + if ( x [ i ] != y [ i ]) { <nl> + if ( x [ i ] > y [ i ]) { <nl> + return 1 ; <nl> + } <nl> + return 0 ; <nl> } <nl> } <nl> return 0 ;
mmm plugins / gevent / gevent . c <nl> ppp plugins / gevent / gevent . c <nl> PyObject * py_uwsgi_gevent_graceful ( PyObject * self , PyObject * args ) { <nl>  <nl> void uwsgi_gevent_gbcw () { <nl>  <nl> - uwsgi_log ("... The work of process % d is done . Seeya !\ n ", getpid ()); <nl> - <nl> py_uwsgi_gevent_graceful ( NULL , NULL ); <nl> + <nl> + uwsgi_log ("... The work of process % d is done . Seeya !\ n ", getpid ()); <nl> + exit ( 0 ); <nl> } <nl>  <nl> struct wsgi_request * uwsgi_gevent_current_wsgi_req ( void ) {
mmm plugins / pypy / pypy_plugin . c <nl> ppp plugins / pypy / pypy_plugin . c <nl> static void uwsgi_pypy_onload () { <nl> # ifdef UWSGI_PYPY_HOME <nl> upypy . home = UWSGI_PYPY_HOME ; <nl> # endif <nl> + uwsgi . has_threads = 1 ; <nl> } <nl>  <nl> static int uwsgi_pypy_mule ( char * opt ) {
mmm plugins / python / pyloader . c <nl> ppp plugins / python / pyloader . c <nl> PyObject * uwsgi_file_loader ( void * arg1 ) { <nl> Py_DECREF ( wsgi_file_dict ); <nl> Py_DECREF ( wsgi_file_module ); <nl> free ( py_filename ); <nl> - uwsgi_log ( " unable to find \" application \" callable in file % s \ n ", filename ); <nl> + uwsgi_log ( " unable to find \"% s \" callable in file % s \ n ", callable , filename ); <nl> return NULL ; <nl> } <nl>  <nl> if (! PyFunction_Check ( wsgi_file_callable ) && ! PyCallable_Check ( wsgi_file_callable )) { <nl> - uwsgi_log ( "\" application \" must be a callable object in file % s \ n ", filename ); <nl> + uwsgi_log ( "\"% s \" must be a callable object in file % s \ n ", callable , filename ); <nl> Py_DECREF ( wsgi_file_callable ); <nl> Py_DECREF ( wsgi_file_dict ); <nl> Py_DECREF ( wsgi_file_module );
mmm plugins / stats_pusher_statsd / plugin . c <nl> ppp plugins / stats_pusher_statsd / plugin . c <nl> struct uwsgi_stats_pusher_statsd { <nl>  <nl> static struct uwsgi_option stats_pusher_statsd_options [] = { <nl> {" statsd - no - workers ", no_argument , 0 , " disable generation of single worker metrics ", uwsgi_opt_true , & u_stats_pusher_statsd . no_workers , 0 }, <nl> - {" statsd - all - gauges ", no_argument , 0 , " push all metrics to statsd as gauges ", uwsgi_opt_true , & u_stats_pusher_statsd . all_gauges , 0 } <nl> + {" statsd - all - gauges ", no_argument , 0 , " push all metrics to statsd as gauges ", uwsgi_opt_true , & u_stats_pusher_statsd . all_gauges , 0 }, <nl> + UWSGI_END_OF_OPTIONS <nl> }; <nl>  <nl> // configuration of a statsd node
mmm core / logging . c <nl> ppp core / logging . c <nl> void logto ( char * logfile ) { <nl> uwsgi . logfile = logfile ; <nl>  <nl> if ( uwsgi . chmod_logfile_value ) { <nl> - if ( chmod ( uwsgi . logfile , uwsgi . chmod_logfile_value )) { <nl> - uwsgi_error (" chmod ()"); <nl> + if ( fchmod ( fd , uwsgi . chmod_logfile_value )) { <nl> + uwsgi_error (" fchmod ()"); <nl> } <nl> } <nl> }
mmm core / io . c <nl> ppp core / io . c <nl> static char * uwsgi_scheme_section ( char * url , size_t * size , int add_zero ) { <nl> } <nl>  <nl> struct uwsgi_string_list * uwsgi_register_scheme ( char * name , char * (* func )( char *, size_t *, int )) { <nl> - struct uwsgi_string_list * usl = uwsgi_string_new_list (& uwsgi . schemes , name ); <nl> + struct uwsgi_string_list * usl = NULL ; <nl> + uwsgi_foreach ( usl , uwsgi . schemes ) { <nl> + if (! strcmp ( usl -> value , name )) goto found ; <nl> + } <nl> + <nl> + usl = uwsgi_string_new_list (& uwsgi . schemes , name ); <nl> + <nl> + found : <nl> usl -> custom_ptr = func ; <nl> return usl ; <nl> }
mmm core / legion . c <nl> ppp core / legion . c <nl> static void legions_check_nodes_step2 () { <nl> memcpy ( best_uuid , node -> uuid , 36 ); <nl> } <nl> // go on if i am not an arbiter <nl> - else if ( ul -> valor > 0 ) { <nl> - // no potential Lord is available , i will propose myself <nl> - // but only if i am not suspended ... <nl> - if ( uwsgi_now () > ul -> suspended_til ) { <nl> - best_valor = ul -> valor ; <nl> - memcpy ( best_uuid , ul -> uuid , 36 ); <nl> - i_am_the_best = 1 ; <nl> - } <nl> + // no potential Lord is available , i will propose myself <nl> + // but only if i am not suspended ... <nl> + else if ( ul -> valor > 0 && uwsgi_now () > ul -> suspended_til ) { <nl> + best_valor = ul -> valor ; <nl> + memcpy ( best_uuid , ul -> uuid , 36 ); <nl> + i_am_the_best = 1 ; <nl> } <nl> else { <nl> // empty lord
mmm plugins / http / http . c <nl> ppp plugins / http / http . c <nl> int http_parse ( struct http_session * h_session ) { <nl> hv = hv -> next ; <nl> } <nl>  <nl> + // security check <nl> + if ( c >= MAX_HTTP_VEC - 4 ) { <nl> + uwsgi_log (" too much headers in request . skipping it .\ n "); <nl> + return 0 ; <nl> + } <nl> + <nl> return c ; <nl>  <nl> }
mmm master . c <nl> ppp master . c <nl> int master_loop ( char ** argv , char ** environ ) { <nl>  <nl> # endif <nl> } <nl> - kill ( uwsgi . workers [ i ]. pid , SIGUSR2 ); <nl> - // allow SIGUSR2 to be delivered <nl> - sleep ( 1 ); <nl> - kill ( uwsgi . workers [ i ]. pid , SIGKILL ); <nl> + <nl> + if ( uwsgi . workers [ i ]. pid > 0 ) { <nl> + kill ( uwsgi . workers [ i ]. pid , SIGUSR2 ); <nl> + // allow SIGUSR2 to be delivered <nl> + sleep ( 1 ); <nl> + kill ( uwsgi . workers [ i ]. pid , SIGKILL ); <nl> + } <nl> // to avoid races <nl> uwsgi . workers [ i ]. harakiri = 0 ; <nl> } <nl> int master_loop ( char ** argv , char ** environ ) { <nl> if ( uwsgi . workers [ uwsgi . mywid ]. cheaped == 1 ) { <nl> uwsgi . workers [ uwsgi . mywid ]. pid = 0 ; <nl> uwsgi_log (" uWSGI worker % d cheaped .\ n ", uwsgi . mywid ); <nl> + uwsgi . workers [ uwsgi . mywid ]. harakiri = 0 ; <nl> continue ; <nl> } <nl> gettimeofday (& last_respawn , NULL );
mmm master . c <nl> ppp master . c <nl> void master_loop ( char ** argv , char ** environ ) { <nl> // checking logsize <nl> if ( uwsgi . logfile ) { <nl> uwsgi . shared -> logsize = lseek ( 2 , 0 , SEEK_CUR ); <nl> - if ( uwsgi . shared -> logsize > 4096 ) { <nl> + if ( uwsgi . shared -> logsize > 8192 ) { <nl> uwsgi_log (" logsize : % d \ n ", uwsgi . shared -> logsize ); <nl> + char * new_logfile = uwsgi_malloc ( strlen ( uwsgi . logfile ) + 14 + 1 ); <nl> + memset ( new_logfile , 0 , strlen ( uwsgi . logfile ) + 14 + 1 ); <nl> + if (! rename ( uwsgi . logfile , new_logfile )) { <nl> + // close 2 , reopen logfile dup ' it and gracefully reload workers ; <nl> + } <nl> + free ( new_logfile ); <nl> } <nl> } <nl> 
mmm core / master_utils . c <nl> ppp core / master_utils . c <nl> int uwsgi_calc_cheaper ( void ) { <nl> ignore_algo = 1 ; <nl> } <nl> uwsgi . cheaper_fifo_delta = 0 ; <nl> + goto safe ; <nl> } <nl>  <nl> // if cheaper limits wants to change worker count , then skip cheaper algo <nl> int uwsgi_calc_cheaper ( void ) { <nl> needed_workers = 0 ; <nl> } <nl>  <nl> + safe : <nl> if ( needed_workers > 0 ) { <nl> for ( i = 1 ; i <= uwsgi . numproc ; i ++) { <nl> if ( uwsgi . workers [ i ]. cheaped == 1 && uwsgi . workers [ i ]. pid == 0 ) {
mmm plugins / v8 / v8_commonjs . cc <nl> ppp plugins / v8 / v8_commonjs . cc <nl> static v8 :: Handle < v8 :: Value > uwsgi_v8_commonjs_require ( const v8 :: Arguments & <nl> free ( tmp_filename ); <nl> return ret ; <nl> } <nl> + free ( tmp_filename ); <nl> } <nl> - free ( tmp_filename ); <nl> usl = usl -> next ; <nl> } <nl> }
mmm uwsgi . c <nl> ppp uwsgi . c <nl> int uwsgi_start ( void * v_argv ) { <nl> # ifndef __OpenBSD__ <nl>  <nl> if ( uwsgi . rl . rlim_max > 0 ) { <nl> + uwsgi . rl . rlim_cur = uwsgi . rl . rlim_max ; <nl> uwsgi_log (" limiting address space of processes ...\ n "); <nl> if ( setrlimit ( RLIMIT_AS , & uwsgi . rl )) { <nl> uwsgi_error (" setrlimit ()");
mmm core / cache . c <nl> ppp core / cache . c <nl> extern struct uwsgi_server uwsgi ; <nl>  <nl> static void cache_full ( struct uwsgi_cache * uc ) { <nl> uint64_t i ; <nl> + int force_clear = 0 ; <nl>  <nl> if (! uc -> ignore_full ) { <nl> if ( uc -> purge_lru ) <nl> static void cache_full ( struct uwsgi_cache * uc ) { <nl>  <nl> // we do not need locking here ! <nl> if ( uc -> sweep_on_full ) { <nl> + uint64_t removed = 0 ; <nl> uint64_t now = ( uint64_t ) uwsgi_now (); <nl> if ( uc -> next_scan <= now ) { <nl> uc -> next_scan = now + uc -> sweep_on_full ; <nl> for ( i = 1 ; i < uc -> max_items ; i ++) { <nl> struct uwsgi_cache_item * uci = cache_item ( i ); <nl> if ( uci -> expires > 0 && uci -> expires <= now ) { <nl> - uwsgi_cache_del2 ( uc , NULL , 0 , i , 0 ); <nl> + if (! uwsgi_cache_del2 ( uc , NULL , 0 , i , 0 )) { <nl> + removed ++; <nl> + } <nl> } <nl> } <nl> + if ( removed == 0 ) { <nl> + force_clear = 1 ; <nl> + } <nl> } <nl> } <nl>  <nl> - if ( uc -> clear_on_full ) { <nl> + if ( uc -> clear_on_full || force_clear ) { <nl> for ( i = 1 ; i < uc -> max_items ; i ++) { <nl> uwsgi_cache_del2 ( uc , NULL , 0 , i , 0 ); <nl> }
mmm plugins / alarm_xmpp / gloox . cc <nl> ppp plugins / alarm_xmpp / gloox . cc <nl> class Jabbo : public ConnectionListener { <nl> p = strtok_r ( NULL , ",", & ctx ); <nl> } <nl>  <nl> + full_jid = jab_username ; <nl> + <nl> JID jid ( jab_username ); <nl> client = new Client ( jid , jab_password ); <nl> client -> registerConnectionListener ( this ); <nl> class Jabbo : public ConnectionListener { <nl> event_queue_add_fd_read ( u_thread -> queue , fd ); <nl> event_queue_add_fd_read ( u_thread -> queue , u_thread -> pipe [ 1 ]); <nl> u_connected = 1 ; <nl> - uwsgi_log ("[ uwsgi - alarm - xmpp ] connected to the XMPP server \ n "); <nl> + uwsgi_log_alarm ("- xmpp ] (% s ) connected to the XMPP server \ n ", full_jid ); <nl> } <nl>  <nl> virtual void onDisconnect ( ConnectionError e ) { <nl> - uwsgi_log ("[ uwsgi - alarm - xmpp ] trying reconnect to the XMPP server ...\ n "); <nl> + uwsgi_log_alarm ("- xmpp ] (% s ) trying reconnect to the XMPP server ...\ n ", full_jid ); <nl> if ( u_connected ) { <nl> // no need to remove it as it is already closed ... <nl> // event_queue_del_fd ( u_thread -> queue , fd , event_queue_read ()); <nl> class Jabbo : public ConnectionListener { <nl> } <nl>  <nl> virtual void onResourceBindError ( const Error * error ) { <nl> - uwsgi_log ("[ uwsgi - alarm - xmpp ] onResourceBindError (): % s \ n ", error -> text (). c_str ()); <nl> + uwsgi_log_alarm ("- xmpp ] (% s ) onResourceBindError (): % s \ n ", full_jid , error -> text (). c_str ()); <nl> client -> disconnect (); <nl> } <nl>  <nl> virtual void onSessionCreateError ( const Error * error ) { <nl> - uwsgi_log ("[ uwsgi - alarm - xmpp ] onSessionCreateError (): % s \ n ", error -> text (). c_str ()); <nl> + uwsgi_log_alarm ("- xmpp ] (% s ) onSessionCreateError (): % s \ n ", full_jid , error -> text (). c_str ()); <nl> client -> disconnect (); <nl> } <nl>  <nl> class Jabbo : public ConnectionListener { <nl> } <nl>  <nl> Client * client ; <nl> + char * full_jid ; <nl> int fd ; <nl> int u_connected ; <nl> struct uwsgi_thread * u_thread ;
mmm core / emperor . c <nl> ppp core / emperor . c <nl> static int on_demand_bind ( char * socket_name ) { <nl> goto error ; <nl> } <nl>  <nl> + if (! is_tcp ) { <nl> + if ( chmod ( socket_name , 0666 )) { <nl> + goto error ; <nl> + } <nl> + } <nl> + <nl> if ( listen ( fd , uwsgi . listen_queue ) != 0 ) { <nl> goto error ; <nl> }
mmm plugins / alarm_curl / alarm_curl_plugin . c <nl> ppp plugins / alarm_curl / alarm_curl_plugin . c <nl> static void uwsgi_alarm_curl_to ( CURL * curl , CURLoption option , char * arg , struct <nl> p = strtok_r ( NULL , ",", & ctx ); <nl> } <nl> curl_easy_setopt ( curl , option , list ); <nl> + free ( items ); <nl> } <nl> # endif <nl>  <nl> static void uwsgi_alarm_curl_loop ( struct uwsgi_thread * ut ) { <nl> exit ( 1 ); <nl> } <nl>  <nl> + free ( opts ); <nl> + <nl> for (;;) { <nl> int ret = event_queue_wait ( ut -> queue , - 1 , & interesting_fd ); <nl> if ( ret < 0 ) return ;
mmm core / legion . c <nl> ppp core / legion . c <nl> next : <nl> } <nl>  <nl> // this must be called only by the master !!! <nl> - if ( uwsgi . mywid > 0 ) return ; <nl> + if (! uwsgi . workers ) return ; <nl> + if ( uwsgi . workers [ 0 ]. pid != getpid ()) return ; <nl> uwsgi_legion_announce_death (); <nl> } <nl> 
mmm uwsgi . c <nl> ppp uwsgi . c <nl> void uwsgi_opt_set_placeholder ( char * opt , char * value , void * none ) { <nl>  <nl> p [ 0 ] = 0 ; <nl> add_exported_option ( uwsgi_str ( value ), p + 1 , 1 ); <nl> - p [ 1 ] = '='; <nl> + p [ 0 ] = '='; <nl>  <nl> } <nl> 
mmm plugins / mongrel2 / mongrel2 . c <nl> ppp plugins / mongrel2 / mongrel2 . c <nl> static void mongrel2_connect () { <nl> } <nl> char * responder = strchr ( uwsgi_sock -> name , ','); <nl> if (! responder ) { <nl> - uwsgi_log (" invalid zeromq address \ n "); <nl> + uwsgi_log (" invalid zeromq address : % s \ n ", uwsgi_sock -> name ); <nl> exit ( 1 ); <nl> } <nl> uwsgi_sock -> receiver = uwsgi_concat2n ( uwsgi_sock -> name , responder - uwsgi_sock -> name , "", 0 );
mmm uwsgi . c <nl> ppp uwsgi . c <nl> void reap_them_all ( int signum ) { <nl> } <nl>  <nl> for ( i = 0 ; i < uwsgi . mules_cnt ; i ++) { <nl> + if (! uwsgi . mules ) break ; <nl> if ( uwsgi . mules [ i ]. pid > 0 ) <nl> kill ( uwsgi . mules [ i ]. pid , SIGKILL ); <nl> }
mmm uwsgi . c <nl> ppp uwsgi . c <nl> int main ( int argc , char * argv [], char * envp []) { <nl>  <nl> plugins_requested = getenv (" UWSGI_PLUGINS "); <nl> if ( plugins_requested ) { <nl> + plugins_requested = uwsgi_concat2 ( plugins_requested , ""); <nl> char * p = strtok ( plugins_requested , ","); <nl> while ( p != NULL ) { <nl> uwsgi_load_plugin (- 1 , p , NULL , 0 ); <nl> static int manage_base_opt ( int i , char * optarg ) { <nl> uwsgi . allowed_modifiers = optarg ; <nl> return 1 ; <nl> case LONG_ARGS_PLUGINS : <nl> - p = strtok ( optarg , ","); <nl> + p = strtok ( uwsgi_concat2 ( optarg , ""), ","); <nl> while ( p != NULL ) { <nl> # ifdef UWSGI_DEBUG <nl> uwsgi_debug (" loading plugin % s \ n ", p );
mmm plugins / python / uwsgi_pymodule . c <nl> ppp plugins / python / uwsgi_pymodule . c <nl> PyObject * py_uwsgi_unlock ( PyObject * self , PyObject * args ) { <nl> return Py_None ; <nl> } <nl>  <nl> + PyObject * py_uwsgi_connection_fd ( PyObject * self , PyObject * args ) { <nl> + struct wsgi_request * wsgi_req = current_wsgi_req (); <nl> + return PyInt_FromLong ( wsgi_req -> poll . fd ); <nl> +} <nl> + <nl> PyObject * py_uwsgi_embedded_data ( PyObject * self , PyObject * args ) { <nl>  <nl> char * name ; <nl> static PyMethodDef uwsgi_advanced_methods [] = { <nl> # endif <nl>  <nl> {" connect ", py_uwsgi_connect , METH_VARARGS , ""}, <nl> + {" connection_fd ", py_uwsgi_connection_fd , METH_VARARGS , ""}, <nl> {" is_connected ", py_uwsgi_is_connected , METH_VARARGS , ""}, <nl> {" send ", py_uwsgi_send , METH_VARARGS , ""}, <nl> {" recv ", py_uwsgi_recv , METH_VARARGS , ""},mmm plugins / http / http . c <nl> ppp plugins / http / http . c <nl> PyObject * py_uwsgi_unlock ( PyObject * self , PyObject * args ) { <nl> return Py_None ; <nl> } <nl>  <nl> + PyObject * py_uwsgi_connection_fd ( PyObject * self , PyObject * args ) { <nl> + struct wsgi_request * wsgi_req = current_wsgi_req (); <nl> + return PyInt_FromLong ( wsgi_req -> poll . fd ); <nl> +} <nl> + <nl> PyObject * py_uwsgi_embedded_data ( PyObject * self , PyObject * args ) { <nl>  <nl> char * name ; <nl> static PyMethodDef uwsgi_advanced_methods [] = { <nl> # endif <nl>  <nl> {" connect ", py_uwsgi_connect , METH_VARARGS , ""}, <nl> + {" connection_fd ", py_uwsgi_connection_fd , METH_VARARGS , ""}, <nl> {" is_connected ", py_uwsgi_is_connected , METH_VARARGS , ""}, <nl> {" send ", py_uwsgi_send , METH_VARARGS , ""}, <nl> {" recv ", py_uwsgi_recv , METH_VARARGS , ""}, <nl> struct uwsgi_http { <nl> uint8_t modifier1 ; <nl> struct uwsgi_string_list * http_vars ; <nl> int manage_expect ; <nl> + <nl> + int raw_body ; <nl> + <nl> int keepalive ; <nl> # ifdef UWSGI_SSL <nl> int https_export_cert ; <nl> struct uwsgi_option http_options [] = { <nl> {" http - manage - expect ", no_argument , 0 , " manage the Expect HTTP request header ", uwsgi_opt_true , & uhttp . manage_expect , 0 }, <nl> {" http - keepalive ", no_argument , 0 , " support HTTP keepalive ( non - pipelined ) requests ( requires backend support )", uwsgi_opt_true , & uhttp . keepalive , 0 }, <nl>  <nl> + {" http - raw - body ", no_argument , 0 , " blindly send HTTP body to backends ( required for WebSockets and Icecast support )", uwsgi_opt_true , & uhttp . raw_body , 0 }, <nl> + <nl> {" http - use - code - string ", required_argument , 0 , " use code string as hostname -> server mapper for the http router ", uwsgi_opt_corerouter_cs , & uhttp , 0 }, <nl> {" http - use - socket ", optional_argument , 0 , " forward request to the specified uwsgi socket ", uwsgi_opt_corerouter_use_socket , & uhttp , 0 }, <nl> {" http - gracetime ", required_argument , 0 , " retry connections to dead static nodes after the specified amount of seconds ", uwsgi_opt_set_int , & uhttp . cr . static_node_gracetime , 0 }, <nl> To have a reliable implementation , we need to reset a bunch of values <nl> break ; <nl> } <nl>  <nl> + if ( cs -> post_cl == 0 && uhttp . raw_body ) goto raw ; <nl> + <nl> // avoid pipelined input <nl> if ( hs -> received_body >= cs -> post_cl ) { <nl> break ; <nl> To have a reliable implementation , we need to reset a bunch of values <nl> len = cs -> post_cl - hs -> received_body ; <nl> } <nl>  <nl> + raw : <nl> len = send ( cs -> instance_fd , bbuf , len , 0 ); <nl>  <nl> if ( len <= 0 ) {
mmm core / mount . c <nl> ppp core / mount . c <nl> int uwsgi_mount ( char * fs , char * what , char * where , char * flags , char * data ) { <nl> char * mflags = uwsgi_str ( flags ); <nl> char * p , * ctx = NULL ; <nl> uwsgi_foreach_token ( mflags , ",", p , ctx ) { <nl> + if ( strcmp ( p , " defaults ") == 0 ) <nl> + continue ; <nl> unsigned long flag = ( unsigned long ) uwsgi_mount_flag ( p ); <nl> if (! flag ) { <nl> uwsgi_log (" unknown mount flag \"% s \"\ n ", p ); <nl> int uwsgi_mount ( char * fs , char * what , char * where , char * flags , char * data ) { <nl> } <nl> mountflags |= flag ; <nl> } <nl> + if (!* fs ) fs = NULL ; <nl> free ( mflags ); <nl> parsed : <nl> # ifdef __linux__
mmm proto / http . c <nl> ppp proto / http . c <nl> static uint16_t http_add_uwsgi_header ( struct wsgi_request * wsgi_req , char * hh , i <nl> wsgi_req -> if_modified_since = val ; <nl> wsgi_req -> if_modified_since_len = vallen ; <nl> } <nl> + else if (! uwsgi_strncmp (" X_FORWARDED_SSL ", 15 , hh , keylen )) { <nl> + if ( vallen == 2 && val [ 0 ] == ' o ' && val [ 1 ] == ' n ') { <nl> + wsgi_req -> scheme = " https "; <nl> + wsgi_req -> scheme_len = 5 ; <nl> + } <nl> + } <nl> else if ( uwsgi . vhost_host && ! uwsgi_strncmp (" HOST ", 4 , hh , keylen )) { <nl> wsgi_req -> host = val ; <nl> wsgi_req -> host_len = vallen ;
mmm plugins / python / pyutils . c <nl> ppp plugins / python / pyutils . c <nl> void init_pyargv () { <nl> # endif <nl>  <nl> up . argc = 1 ; <nl> - char * tmp_ptr = uwsgi_str ( up . argv ); <nl> + if ( up . argv ) { <nl> + char * tmp_ptr = uwsgi_str ( up . argv ); <nl> # ifdef __sun__ <nl> // FIX THIS !!! <nl> ap = strtok ( tmp_ptr , " "); <nl> void init_pyargv () { <nl> } <nl> } <nl>  <nl> - free ( tmp_ptr ); <nl> + free ( tmp_ptr ); <nl> + } <nl>  <nl> # ifdef PYTHREE <nl> up . py_argv = uwsgi_calloc ( sizeof ( wchar_t *) * up . argc + 1 );
mmm apache2 / mod_uwsgi . c <nl> ppp apache2 / mod_uwsgi . c <nl> static int uwsgi_handler ( request_rec * r ) { <nl> } <nl> else { <nl> if ( r -> path_info ) { <nl> - ap_log_rerror ( APLOG_MARK , APLOG_ERR , 0 , r , " uwsgi : PATH_INFO : % s ", r -> path_info ); <nl> - vecptr = uwsgi_add_var ( uwsgi_vars , vecptr , " SCRIPT_NAME ", apr_pstrndup ( r -> pool , r -> uri , ( strlen ( r -> uri ) - strlen ( r -> path_info ) )) , & pkt_size ) ; <nl> - vecptr = uwsgi_add_var ( uwsgi_vars , vecptr , " PATH_INFO ", r -> path_info , & pkt_size ) ; <nl> + if ( strlen ( r -> path_info ) <= 0 ) { <nl> + vecptr = uwsgi_add_var ( uwsgi_vars , vecptr , " SCRIPT_NAME ", "", & pkt_size ) ; <nl> + vecptr = uwsgi_add_var ( uwsgi_vars , vecptr , " PATH_INFO ", r -> uri , & pkt_size ) ; <nl> + } <nl> + else { <nl> + vecptr = uwsgi_add_var ( uwsgi_vars , vecptr , " SCRIPT_NAME ", apr_pstrndup ( r -> pool , r -> uri , ( strlen ( r -> uri ) - strlen ( r -> path_info ) )) , & pkt_size ) ; <nl> + vecptr = uwsgi_add_var ( uwsgi_vars , vecptr , " PATH_INFO ", r -> path_info , & pkt_size ) ; <nl> + } <nl> } <nl> else { <nl> vecptr = uwsgi_add_var ( uwsgi_vars , vecptr , " SCRIPT_NAME ", "", & pkt_size ) ;
mmm core / uwsgi . c <nl> ppp core / uwsgi . c <nl> void * mem_collector ( void * foobar ) { <nl> uwsgi_log_verbose (" mem - collector thread started for worker % d \ n ", uwsgi . mywid ); <nl> for (;;) { <nl> sleep ( uwsgi . mem_collector_freq ); <nl> - uint64_t rss , vsz ; <nl> + uint64_t rss = 0 , vsz = 0 ; <nl> get_memusage (& rss , & vsz ); <nl> uwsgi . workers [ uwsgi . mywid ]. rss_size = rss ; <nl> uwsgi . workers [ uwsgi . mywid ]. vsz_size = vsz ;
mmm plugins / router_rewrite / router_rewrite . c <nl> ppp plugins / router_rewrite / router_rewrite . c <nl> static int uwsgi_routing_func_rewrite ( struct wsgi_request * wsgi_req , struct uwsg <nl> char * ptr = uwsgi_req_append ( wsgi_req , " PATH_INFO ", 9 , path_info , path_info_len ); <nl> if (! ptr ) goto clear ; <nl>  <nl> + free ( path_info ); <nl> + <nl> // set new path_info <nl> wsgi_req -> path_info = ptr ; <nl> wsgi_req -> path_info_len = path_info_len ;
mmm utils . c <nl> ppp utils . c <nl> void uwsgi_set_processname ( char * name ) { <nl> strncat ( uwsgi . orig_argv [ 0 ], uwsgi . procname_append , uwsgi . max_procname -( amount + 1 )); <nl> } <nl>  <nl> - memset ( uwsgi . orig_argv [ 0 ]+ amount + 1 , ' ', uwsgi . max_procname -( amount - 1 )); <nl> + // fill with spaces ... <nl> + memset ( uwsgi . orig_argv [ 0 ]+ amount + 1 , ' ', uwsgi . max_procname -( amount )); <nl> + // end with \ 0 <nl> + memset ( uwsgi . orig_argv [ 0 ]+ amount + 1 +( uwsgi . max_procname -( amount )), '\ 0 ', 1 ); <nl> + <nl> # elif defined ( __FreeBSD__ ) <nl> if ( uwsgi . procname_prefix ) { <nl> if (! uwsgi . procname_append ) {
mmm plugins / router_cache / router_cache . c <nl> ppp plugins / router_cache / router_cache . c <nl> error : <nl> if ( urcc -> key ) free ( urcc -> key ); <nl> if ( urcc -> name ) free ( urcc -> name ); <nl> if ( urcc -> expires_str ) free ( urcc -> expires_str ); <nl> + free ( urcc ); <nl> return - 1 ; <nl> } <nl> 
mmm plugins / cgi / cgi_plugin . c <nl> ppp plugins / cgi / cgi_plugin . c <nl> char * uwsgi_cgi_get_docroot ( char * path_info , uint16_t path_info_len , int * need_f <nl> } <nl>  <nl> if ( choosen_udd -> status == 0 ) { <nl> - char * tmp_udd = realpath ( path , NULL ); <nl> - if (! tmp_udd ) { <nl> + char * tmp_udd = uwsgi_malloc ( PATH_MAX + 1 ); <nl> + if (! realpath ( path , tmp_udd )) { <nl> return NULL ; <nl> } <nl> 
mmm core / utils . c <nl> ppp core / utils . c <nl> void uwsgi_write_pidfile_explicit ( char * pidfile_name , pid_t pid ) { <nl> } <nl>  <nl> char * uwsgi_expand_path ( char * dir , int dir_len , char * ptr ) { <nl> - char src [ PATH_MAX + 1 ]; <nl> - memcpy ( src , dir , dir_len ); <nl> - src [ dir_len ] = 0 ; <nl> + if ( dir_len > PATH_MAX ) <nl> + { <nl> + uwsgi_log (" invalid path size : % d ( max % d )\ n ", dir_len , PATH_MAX ); <nl> + return NULL ; <nl> + } <nl> + char * src = uwsgi_concat2n ( dir , dir_len , "", 0 ); <nl> char * dst = ptr ; <nl> if (! dst ) <nl> dst = uwsgi_malloc ( PATH_MAX + 1 ); <nl> char * uwsgi_expand_path ( char * dir , int dir_len , char * ptr ) { <nl> uwsgi_error_realpath ( src ); <nl> if (! ptr ) <nl> free ( dst ); <nl> + free ( src ); <nl> return NULL ; <nl> } <nl> + free ( src ); <nl> return dst ; <nl> } <nl> 
mmm core / cache . c <nl> ppp core / cache . c <nl> static uint64_t uwsgi_cache_find_free_blocks ( struct uwsgi_cache * uc , uint64_t ne <nl>  <nl> // ok we now have the start position , let ' s search for contiguous blocks <nl> uint8_t * bitmap = uc -> blocks_bitmap ; <nl> - uint64_t base = 0xffffffffffffffff ; <nl> + uint64_t base = 0xffffffffffffffffLLU ; <nl> uint8_t base_bit = 0 ; <nl> uint64_t j ; <nl> uint64_t found = 0 ; <nl> static uint64_t uwsgi_cache_find_free_blocks ( struct uwsgi_cache * uc , uint64_t ne <nl> // used block <nl> if ( num & i ) { <nl> found = 0 ; <nl> - base = 0xffffffffffffffff ; <nl> + base = 0xffffffffffffffffLLU ; <nl> base_bit = 0 ; <nl> } <nl> // free block <nl> else { <nl> - if ( base == 0xffffffffffffffff ) { <nl> + if ( base == 0xffffffffffffffffLLU ) { <nl> base = j ; <nl> base_bit = bit_pos ; <nl> } <nl> static uint64_t uwsgi_cache_find_free_blocks ( struct uwsgi_cache * uc , uint64_t ne <nl> if ( j >= need_to_scan ) { <nl> j = 0 ; <nl> found = 0 ; <nl> - base = 0xffffffffffffffff ; <nl> + base = 0xffffffffffffffffLLU ; <nl> base_bit = 0 ; <nl> } <nl> } <nl>  <nl>  <nl> // no more free blocks <nl> - return 0xffffffffffffffff ; <nl> + return 0xffffffffffffffffLLU ; <nl> } <nl>  <nl> static uint64_t cache_mark_blocks ( struct uwsgi_cache * uc , uint64_t index , uint64_t len ) { <nl> int uwsgi_cache_set2 ( struct uwsgi_cache * uc , char * key , uint16_t keylen , char * v <nl> else { <nl> uci -> first_block = uwsgi_cache_find_free_blocks ( uc , vallen ); <nl> // uwsgi_log (" first block = % llu \ n ", uci -> first_block ); <nl> - if ( uci -> first_block == 0xffffffffffffffff ) { <nl> + if ( uci -> first_block == 0xffffffffffffffffLLU ) { <nl> uwsgi_log ("*** DANGER cache \"% s \" is FULL !!! ***\ n ", uc -> name ); <nl> uc -> full ++; <nl> if ( rollback_mode == 0 ) { <nl> int uwsgi_cache_set2 ( struct uwsgi_cache * uc , char * key , uint16_t keylen , char * v <nl> // we have a special case here , as we need to find a new series of free blocks <nl> uint64_t old_first_block = uci -> first_block ; <nl> uci -> first_block = uwsgi_cache_find_free_blocks ( uc , vallen ); <nl> - if ( uci -> first_block == 0xffffffffffffffff ) { <nl> + if ( uci -> first_block == 0xffffffffffffffffLLU ) { <nl> uwsgi_log ("*** DANGER cache \"% s \" is FULL !!! ***\ n ", uc -> name ); <nl> uc -> full ++; <nl> uci -> first_block = old_first_block ;
mmm plugins / python / python_plugin . c <nl> ppp plugins / python / python_plugin . c <nl> void uwsgi_python_reset_random_seed () { <nl> void uwsgi_python_atexit () { <nl>  <nl> // if hijacked do not run atexit hooks <nl> + if ( uwsgi . workers [ uwsgi . mywid ]. hijacked ) <nl> + return ; <nl>  <nl> // this time we use this higher level function <nl> // as this code can be executed in a signal handler
mmm plugins / python / python_plugin . c <nl> ppp plugins / python / python_plugin . c <nl> void uwsgi_python_harakiri ( int wid ) { <nl> char * address = uwsgi_concat2 ( up . tracebacker , uwsgi_num2str ( wid )); <nl>  <nl> int fd = uwsgi_connect ( address , - 1 , 0 ); <nl> - for (;;) { <nl> + while ( fd >= 0 ) { <nl> int ret = uwsgi_waitfd ( fd , uwsgi . shared -> options [ UWSGI_OPTION_SOCKET_TIMEOUT ]); <nl> if ( ret <= 0 ) { <nl> break ;
mmm apache2 / mod_proxy_uwsgi . c <nl> ppp apache2 / mod_proxy_uwsgi . c <nl> static int uwsgi_response ( request_rec * r , proxy_conn_rec * backend , proxy_server_ <nl> ap_set_content_type ( r , apr_pstrdup ( r -> pool , buf )); <nl> } <nl>  <nl> - for (;;) { <nl> + int finish = 0 ; <nl> + while (! finish ) { <nl> rv = ap_get_brigade ( rp -> input_filters , bb , <nl> AP_MODE_READBYTES , mode , <nl> conf -> io_buffer_size ); <nl> static int uwsgi_response ( request_rec * r , proxy_conn_rec * backend , proxy_server_ <nl>  <nl> ap_proxy_buckets_lifetime_transform ( r , bb , pass_bb ); <nl>  <nl> - ap_pass_brigade ( r -> output_filters , pass_bb ); <nl> + // found the last brigade ? <nl> + if ( APR_BUCKET_IS_EOS ( APR_BRIGADE_LAST ( bb ))) finish = 1 ; <nl> + <nl> + if ( ap_pass_brigade ( r -> output_filters , pass_bb ) != APR_SUCCESS || c -> aborted ) { <nl> + finish = 1 ; <nl> + } <nl> + <nl> apr_brigade_cleanup ( bb ); <nl> apr_brigade_cleanup ( pass_bb ); <nl> }
mmm core / init . c <nl> ppp core / init . c <nl> void sanitize_args () { <nl>  <nl> /* here we try to choose if thunder lock is a good thing */ <nl> # ifdef UNBIT <nl> - if ( uwsgi . numproc > 1 ) { <nl> + if ( uwsgi . numproc > 1 && ! uwsgi . map_socket ) { <nl> uwsgi . use_thunder_lock = 1 ; <nl> } <nl> # endif
mmm core / emperor . c <nl> ppp core / emperor . c <nl> next : <nl> } <nl>  <nl> void uwsgi_emperor_simple_do_with_attrs ( struct uwsgi_emperor_scanner * ues , char * name , char * config , time_t ts , uid_t uid , gid_t gid , char * socket_name , struct uwsgi_dyn_dict * attrs ) { <nl> - if (! uwsgi_emperor_is_valid ( name )) <nl> + if (! uwsgi_emperor_is_valid ( name )) { <nl> + if ( attrs ) <nl> + uwsgi_dyn_dict_free (& attrs ); <nl> return ; <nl> + } <nl>  <nl> struct uwsgi_instance * ui_current = emperor_get ( name ); <nl>  <nl> if ( ui_current ) { <nl> + if ( ui_current -> attrs ) { <nl> + uwsgi_dyn_dict_free (& ui_current -> attrs ); <nl> + } <nl> + ui_current -> attrs = attrs ; <nl>  <nl> // skip in case the instance is going down ... <nl> if ( ui_current -> status > 0 )
mmm core / master_utils . c <nl> ppp core / master_utils . c <nl> struct uwsgi_stats * uwsgi_master_generate_stats () { <nl> uc = uc -> next ; <nl> } <nl>  <nl> + if ( uwsgi_stats_list_close ( us )) <nl> + goto end ; <nl> + <nl> if ( uwsgi_stats_comma ( us )) <nl> goto end ; <nl> }
mmm core / utils . c <nl> ppp core / utils . c <nl> char * uwsgi_chomp2 ( char * str ) { <nl>  <nl>  <nl> int uwsgi_tmpfd () { <nl> + int fd = - 1 ; <nl> char * tmpdir = getenv (" TMPDIR "); <nl> if (! tmpdir ) { <nl> tmpdir = "/ tmp "; <nl> } <nl> +# ifdef O_TMPFILE <nl> + fd = open ( tmpdir , O_TMPFILE | O_RDWR ); <nl> + if ( fd >= 0 ) { <nl> + return fd ; <nl> + } <nl> + // fallback to old style <nl> +# endif <nl> char * template = uwsgi_concat2 ( tmpdir , "/ uwsgiXXXXXX "); <nl> - int fd = mkstemp ( template ); <nl> + fd = mkstemp ( template ); <nl> unlink ( template ); <nl> free ( template ); <nl> return fd ;
mmm core / master_utils . c <nl> ppp core / master_utils . c <nl> int uwsgi_respawn_worker ( int wid ) { <nl> for ( i = 0 ; i < uwsgi . cores ; i ++) { <nl> uwsgi . workers [ uwsgi . mywid ]. cores [ i ]. in_request = 0 ; <nl> memset (& uwsgi . workers [ uwsgi . mywid ]. cores [ i ]. req , 0 , sizeof ( struct wsgi_request )); <nl> + memset ( uwsgi . workers [ uwsgi . mywid ]. cores [ i ]. buffer , 0 , sizeof ( struct uwsgi_header )); <nl> } <nl>  <nl> uwsgi_fixup_fds ( wid , 0 , NULL );
mmm uwsgi . h <nl> ppp uwsgi . h <nl> struct uwsgi_route_var { <nl> char * name ; <nl> uint16_t name_len ; <nl> char *(* func )( struct wsgi_request *, char *, uint16_t , uint16_t *); <nl> + int need_free ; <nl> struct uwsgi_route_var * next ; <nl> }; <nl> mmm core / routing . c <nl> ppp core / routing . c <nl> struct uwsgi_route_var { <nl> char * name ; <nl> uint16_t name_len ; <nl> char *(* func )( struct wsgi_request *, char *, uint16_t , uint16_t *); <nl> + int need_free ; <nl> struct uwsgi_route_var * next ; <nl> }; <nl>  <nl> struct uwsgi_buffer * uwsgi_routing_translate ( struct wsgi_request * wsgi_req , stru <nl> case 2 : <nl> if ( pass1 [ i ] == '}') { <nl> uint16_t vallen = 0 ; <nl> + int need_free = 0 ; <nl> char * value = NULL ; <nl> char * bracket = memchr ( key , '[', keylen ); <nl> if ( bracket && keylen > 0 && key [ keylen - 1 ] == ']') { <nl> struct uwsgi_route_var * urv = uwsgi_get_route_var ( key , bracket - key ); <nl> if ( urv ) { <nl> + need_free = urv -> need_free ; <nl> value = urv -> func ( wsgi_req , bracket + 1 , keylen - ( urv -> name_len + 2 ), & vallen ); <nl> } <nl> else { <nl> struct uwsgi_buffer * uwsgi_routing_translate ( struct wsgi_request * wsgi_req , stru <nl> value = uwsgi_get_var ( wsgi_req , key , keylen , & vallen ); <nl> } <nl> if ( value ) { <nl> - if ( uwsgi_buffer_append ( ub , value , vallen )) goto error ; <nl> + if ( uwsgi_buffer_append ( ub , value , vallen )) { <nl> + if ( need_free ) { <nl> + free ( value ); <nl> + } <nl> + goto error ; <nl> + } <nl> + if ( need_free ) { <nl> + free ( value ); <nl> + } <nl> } <nl> status = 0 ; <nl> key = NULL ;
mmm core / fork_server . c <nl> ppp core / fork_server . c <nl>  <nl> extern struct uwsgi_server uwsgi ; <nl>  <nl> +/* <nl> + <nl> + on connection retrieve the uid , gid and pid of the connecting process , in addition to up to 3 <nl> + file descriptors ( emperor pipe , emperor pipe_config , on_demand socket dup ()' ed to 0 ) <nl> + <nl> + if authorized , double fork , get the pid of the second child and exit () <nl> + its parent ( this will force the Emperor to became its subreaper ). <nl> + <nl> + from now on , we can consider the new child as a full - featured vassal <nl> + <nl> +*/ <nl> + <nl> void uwsgi_fork_server ( char * socket ) { <nl> int fd = bind_to_unix ( socket , uwsgi . listen_queue , uwsgi . chmod_socket , uwsgi . abstract_socket ); <nl> if ( fd < 0 ) exit ( 1 );
mmm plugins / emperor_amqp / amqp . c <nl> ppp plugins / emperor_amqp / amqp . c <nl> static char * amqp_simple_get_frame ( int fd , struct amqp_frame_header * fh ) { <nl> while ( len < fh -> size + 1 ) { <nl> rlen = recv ( fd , ptr , ( fh -> size + 1 )- len , 0 ); <nl> if ( rlen <= 0 ) { <nl> - if ( rlen < 0 ) <nl> + if ( rlen < 0 ) { <nl> uwsgi_error (" recv ()"); <nl> + } <nl> + free ( frame ); <nl> return NULL ; <nl> } <nl> len += rlen ;
mmm src / TopicTree . h <nl> ppp src / TopicTree . h <nl> private : <nl>  <nl> /* Should be getData and commit ? */ <nl> void publish ( Topic * iterator , size_t start , size_t stop , std :: string_view topic , std :: pair < std :: string_view , std :: string_view > message ) { <nl> - /* If we already have 64 triggered topics make sure to drain it here */ <nl> - if ( numTriggeredTopics == 64 ) { <nl> - drain (); <nl> - } <nl>  <nl> /* Iterate over all segments in given topic */ <nl> for (; stop != std :: string :: npos ; start = stop + 1 ) { <nl> private : <nl>  <nl> /* Add this topic to triggered */ <nl> if (! iterator -> terminatingWildcardChild -> triggered ) { <nl> + /* If we already have 64 triggered topics make sure to drain it here */ <nl> + if ( numTriggeredTopics == 64 ) { <nl> + drain (); <nl> + } <nl> + <nl> triggeredTopics [ numTriggeredTopics ++] = iterator -> terminatingWildcardChild ; <nl> iterator -> terminatingWildcardChild -> triggered = true ; <nl> } <nl> private : <nl>  <nl> /* Add this topic to triggered */ <nl> if (! iterator -> triggered ) { <nl> + /* If we already have 64 triggered topics make sure to drain it here */ <nl> + if ( numTriggeredTopics == 64 ) { <nl> + drain (); <nl> + } <nl> + <nl> triggeredTopics [ numTriggeredTopics ++] = iterator ; <nl> iterator -> triggered = true ; <nl> }
mmm auth_mellon_util . c <nl> ppp auth_mellon_util . c <nl> int am_check_url ( request_rec * r , const char * url ) <nl> " Control character detected in URL ."); <nl> return HTTP_BAD_REQUEST ; <nl> } <nl> + if (* i == '\\') { <nl> + /* Reject backslash character , as it can be used to bypass <nl> + * redirect URL validation . */ <nl> + AM_LOG_RERROR ( APLOG_MARK , APLOG_ERR , HTTP_BAD_REQUEST , r , <nl> + " Backslash character detected in URL ."); <nl> + return HTTP_BAD_REQUEST ; <nl> + } <nl> } <nl>  <nl> return OK ;
mmm management / univention - directory - notifier / src / callback . c <nl> ppp management / univention - directory - notifier / src / callback . c <nl> int data_on_connection ( int fd , callback_remove_handler remove ) <nl> p += strlen ( network_line ); <nl>  <nl>  <nl> - } else if ( ! strncmp ( network_line , " GET_DN ", strlen (" GET_DN ")) && msg_id != UINT32_MAX && network_client_get_version ( fd ) > 0 ) { <nl> + } else if ( ! strncmp ( network_line , " GET_DN ", strlen (" GET_DN ")) && msg_id != UINT32_MAX && version > PROTOCOL_UNKNOWN && version < PROTOCOL_3 ) { <nl>  <nl> univention_debug ( UV_DEBUG_TRANSFILE , UV_DEBUG_ALL , " RECV : GET_DN "); <nl> 
mmm src / p_lx_elf . cpp <nl> ppp src / p_lx_elf . cpp <nl> bool PackLinuxElf32 :: canPack () <nl> if ( sec_strndx ) { <nl> unsigned const sh_name = get_te32 (& sec_strndx -> sh_name ); <nl> if ( Elf32_Shdr :: SHT_STRTAB != get_te32 (& sec_strndx -> sh_type ) <nl> - || ( u32_t ) file_size <= sh_name // FIXME : weak <nl> + || ( u32_t ) file_size <= ( sizeof (". shstrtab ") <nl> + + sh_name + ( shstrtab - ( const char *)& file_image [ 0 ])) <nl> || ( sh_name <nl> && 0 != strcmp (( char const *)". shstrtab ", & shstrtab [ sh_name ])) <nl> ) { <nl> - throwCantPack (" bad e_shstrndx "); <nl> + throwCantPack (" bad e_shstrtab "); <nl> } <nl> } <nl> } <nl> PackLinuxElf64 :: canPack () <nl> if ( sec_strndx ) { <nl> unsigned const sh_name = get_te32 (& sec_strndx -> sh_name ); <nl> if ( Elf64_Shdr :: SHT_STRTAB != get_te32 (& sec_strndx -> sh_type ) <nl> - || ( u32_t ) file_size <= sh_name // FIXME : weak <nl> + || ( u32_t ) file_size <= ( sizeof (". shstrtab ") <nl> + + sh_name + ( shstrtab - ( const char *)& file_image [ 0 ])) <nl> || ( sh_name <nl> && 0 != strcmp (( char const *)". shstrtab ", & shstrtab [ sh_name ])) <nl> ) { <nl> - throwCantPack (" bad e_shstrndx "); <nl> + throwCantPack (" bad e_shstrtab "); <nl> } <nl> } <nl> }
mmm src / p_tmt . cpp <nl> ppp src / p_tmt . cpp <nl> int PackTmt :: readFileHeader () { <nl> unsigned const imagesize = ih . imagesize ; <nl> unsigned const entry = ih . entry ; <nl> unsigned const relocsize = ih . relocsize ; <nl> - if (! imagesize || file_size <= imagesize || file_size <= entry || file_size <= relocsize ) { <nl> + if ( imagesize < sizeof ( ih ) || entry < sizeof ( ih ) || file_size <= imagesize || <nl> + file_size <= entry || file_size <= relocsize ) { <nl> printWarn ( getName (), " bad header ; imagesize =%# x entry =%# x relocsize =%# x ", imagesize , <nl> entry , relocsize ); <nl> return 0 ; <nl> void PackTmt :: pack ( OutputFile * fo ) { <nl> obuf . allocForCompression ( usize + rsize + 128 ); <nl>  <nl> MemBuffer mb_wrkmem ; <nl> - mb_wrkmem . alloc ( rsize + EXTRA_INFO ); // relocations <nl> + mb_wrkmem . alloc ( rsize + EXTRA_INFO + 4 ); // relocations + original entry point + relocsize <nl> SPAN_S_VAR ( upx_byte , wrkmem , mb_wrkmem ); <nl>  <nl> fi -> seek ( adam_offset + sizeof ( ih ), SEEK_SET ); <nl> void PackTmt :: pack ( OutputFile * fo ) { <nl> fi -> readx ( wrkmem + 4 , rsize ); <nl> const unsigned overlay = file_size - fi -> tell (); <nl>  <nl> - if ( find_le32 ( ibuf , 128 , get_le32 (" UPX ")) >= 0 ) <nl> + if ( find_le32 ( ibuf , UPX_MIN ( 128u , usize ), get_le32 (" UPX ")) >= 0 ) <nl> throwAlreadyPacked (); <nl> if ( rsize == 0 ) <nl> throwCantPack (" file is already compressed with another packer ");
mmm src / p_lx_elf . cpp <nl> ppp src / p_lx_elf . cpp <nl> PackLinuxElf32 :: PackLinuxElf32help1 ( InputFile * f ) <nl> e_phnum = get_te16 (& ehdri . e_phnum ); <nl> e_shnum = get_te16 (& ehdri . e_shnum ); <nl> unsigned const e_phentsize = get_te16 (& ehdri . e_phentsize ); <nl> - if ( ehdri . e_ident [ Elf32_Ehdr :: EI_CLASS ]!= Elf32_Ehdr :: ELFCLASS32 <nl> + if ( memcmp (( char const *)& ehdri , "\ x7f \ x45 \ x4c \ x46 ", 4 ) // "\ 177ELF " <nl> + || ehdri . e_ident [ Elf32_Ehdr :: EI_CLASS ]!= Elf32_Ehdr :: ELFCLASS32 <nl> || sizeof ( Elf32_Phdr ) != e_phentsize <nl> || ( Elf32_Ehdr :: ELFDATA2MSB == ehdri . e_ident [ Elf32_Ehdr :: EI_DATA ] <nl> && & N_BELE_RTP :: be_policy != bele ) <nl> PackLinuxElf64 :: PackLinuxElf64help1 ( InputFile * f ) <nl> e_phnum = get_te16 (& ehdri . e_phnum ); <nl> e_shnum = get_te16 (& ehdri . e_shnum ); <nl> unsigned const e_phentsize = get_te16 (& ehdri . e_phentsize ); <nl> - if ( ehdri . e_ident [ Elf64_Ehdr :: EI_CLASS ]!= Elf64_Ehdr :: ELFCLASS64 <nl> + if ( memcmp (( char const *)& ehdri , "\ x7f \ x45 \ x4c \ x46 ", 4 ) // "\ 177ELF " <nl> + || ehdri . e_ident [ Elf64_Ehdr :: EI_CLASS ]!= Elf64_Ehdr :: ELFCLASS64 <nl> || sizeof ( Elf64_Phdr ) != e_phentsize <nl> || ( Elf64_Ehdr :: ELFDATA2MSB == ehdri . e_ident [ Elf64_Ehdr :: EI_DATA ] <nl> && & N_BELE_RTP :: be_policy != bele ) <nl> PackLinuxElf64 :: invert_pt_dynamic ( Elf64_Dyn const * dynp , upx_uint64_t headway ) <nl> } <nl> if ( file_size <= dt_offsets [ n_off ]) { <nl> char msg [ 60 ]; snprintf ( msg , sizeof ( msg ), " bad DT_ {%# x } = %# x ( beyond EOF )", <nl> - dt_names [ k ], dt_offsets [ n_off ]); <nl> + k , dt_offsets [ n_off ]); <nl> throwCantPack ( msg ); <nl> } <nl> n_off += !! dt_offsets [ n_off ];
mmm src / UriCommon . c <nl> ppp src / UriCommon . c <nl>  <nl>  <nl> void URI_FUNC ( ResetUri )( URI_TYPE ( Uri ) * uri ) { <nl> + if ( uri == NULL ) { <nl> + return ; <nl> + } <nl> memset ( uri , 0 , sizeof ( URI_TYPE ( Uri ))); <nl> } <nl> 
mmm src / UriQuery . c <nl> ppp src / UriQuery . c <nl>  <nl>  <nl>  <nl> +# include < limits . h > <nl> + <nl> + <nl> + <nl> static int URI_FUNC ( ComposeQueryEngine )( URI_CHAR * dest , <nl> const URI_TYPE ( QueryList ) * queryList , <nl> int maxChars , int * charsWritten , int * charsRequired , <nl> int URI_FUNC ( ComposeQueryEngine )( URI_CHAR * dest , <nl> const URI_CHAR * const value = queryList -> value ; <nl> const int worstCase = ( normalizeBreaks == URI_TRUE ? 6 : 3 ); <nl> const int keyLen = ( key == NULL ) ? 0 : ( int ) URI_STRLEN ( key ); <nl> - const int keyRequiredChars = worstCase * keyLen ; <nl> + int keyRequiredChars ; <nl> const int valueLen = ( value == NULL ) ? 0 : ( int ) URI_STRLEN ( value ); <nl> - const int valueRequiredChars = worstCase * valueLen ; <nl> + int valueRequiredChars ; <nl> + <nl> + if (( keyLen >= INT_MAX / worstCase ) || ( valueLen >= INT_MAX / worstCase )) { <nl> + return URI_ERROR_OUTPUT_TOO_LARGE ; <nl> + } <nl> + keyRequiredChars = worstCase * keyLen ; <nl> + valueRequiredChars = worstCase * valueLen ; <nl>  <nl> if ( dest == NULL ) { <nl> (* charsRequired ) += ampersandLen + keyRequiredChars + (( value == NULL )
mmm src / HTTPSocket . h <nl> ppp src / HTTPSocket . h <nl> enum HTTPVerb { <nl> PUT , <nl> DELETE , <nl> PATCH , <nl> + OPTIONS , <nl> INVALID <nl> }; <nl>  <nl> struct HttpRequest { <nl> } <nl> break ; <nl> } <nl> + case 7 : <nl> + if (! strncmp ( headers -> key , " options ", 7 )) { <nl> + return OPTIONS ; <nl> + } <nl> + break ; <nl> + } <nl> return INVALID ; <nl> } <nl> 
mmm src / WebSocketImpl . cpp <nl> ppp src / WebSocketImpl . cpp <nl> bool WebSocketProtocol < isServer >:: handleFragment ( char * data , size_t length , unsi <nl> webSocketData -> compressionStatus = WebSocket < isServer >:: Data :: CompressionStatus :: ENABLED ; <nl> Hub * hub = (( Group < isServer > *) s . getSocketData ()-> nodeData )-> hub ; <nl> data = hub -> inflate ( data , length ); <nl> + if (! data ) { <nl> + forceClose ( user ); <nl> + return true ; <nl> + } <nl> } <nl>  <nl> if ( opCode == 1 && ! isValidUtf8 (( unsigned char *) data , length )) { <nl> bool WebSocketProtocol < isServer >:: handleFragment ( char * data , size_t length , unsi <nl> Hub * hub = (( Group < isServer > *) s . getSocketData ()-> nodeData )-> hub ; <nl> webSocketData -> fragmentBuffer . append ("...."); <nl> data = hub -> inflate (( char *) webSocketData -> fragmentBuffer . data (), length ); <nl> + if (! data ) { <nl> + forceClose ( user ); <nl> + return true ; <nl> + } <nl> } else { <nl> data = ( char *) webSocketData -> fragmentBuffer . data (); <nl> }mmm src / Hub . cpp <nl> ppp src / Hub . cpp <nl> bool WebSocketProtocol < isServer >:: handleFragment ( char * data , size_t length , unsi <nl> webSocketData -> compressionStatus = WebSocket < isServer >:: Data :: CompressionStatus :: ENABLED ; <nl> Hub * hub = (( Group < isServer > *) s . getSocketData ()-> nodeData )-> hub ; <nl> data = hub -> inflate ( data , length ); <nl> + if (! data ) { <nl> + forceClose ( user ); <nl> + return true ; <nl> + } <nl> } <nl>  <nl> if ( opCode == 1 && ! isValidUtf8 (( unsigned char *) data , length )) { <nl> bool WebSocketProtocol < isServer >:: handleFragment ( char * data , size_t length , unsi <nl> Hub * hub = (( Group < isServer > *) s . getSocketData ()-> nodeData )-> hub ; <nl> webSocketData -> fragmentBuffer . append ("...."); <nl> data = hub -> inflate (( char *) webSocketData -> fragmentBuffer . data (), length ); <nl> + if (! data ) { <nl> + forceClose ( user ); <nl> + return true ; <nl> + } <nl> } else { <nl> data = ( char *) webSocketData -> fragmentBuffer . data (); <nl> } <nl> # include " HTTPSocket . h " <nl> # include < openssl / sha . h > <nl>  <nl> + static const int INFLATE_LESS_THAN_ROUGHLY = 16777216 ; <nl> + <nl> namespace uWS { <nl>  <nl> char * Hub :: inflate ( char * data , size_t & length ) { <nl> char * Hub :: inflate ( char * data , size_t & length ) { <nl> if (! inflationStream . avail_in ) { <nl> break ; <nl> } <nl> + <nl> dynamicInflationBuffer . append ( inflationBuffer , LARGE_BUFFER_SIZE - inflationStream . avail_out ); <nl> - } while ( err == Z_BUF_ERROR ); <nl> + } while ( err == Z_BUF_ERROR && dynamicInflationBuffer . length () <= INFLATE_LESS_THAN_ROUGHLY ); <nl>  <nl> inflateReset (& inflationStream ); <nl>  <nl> - if ( err != Z_BUF_ERROR && err != Z_OK ) { <nl> + if (( err != Z_BUF_ERROR && err != Z_OK ) || dynamicInflationBuffer . length () > INFLATE_LESS_THAN_ROUGHLY ) { <nl> length = 0 ; <nl> return nullptr ; <nl> }
mmm src / Hub . cpp <nl> ppp src / Hub . cpp <nl> void Hub :: connect ( std :: string uri , void * user , std :: map < std :: string , std :: string <nl> secure = true ; <nl> } else if ( protocol != " ws ") { <nl> eh -> errorHandler ( user ); <nl> + return ; <nl> } <nl>  <nl> if ( portStr . length ()) {mmm tests / main . cpp <nl> ppp tests / main . cpp <nl> void Hub :: connect ( std :: string uri , void * user , std :: map < std :: string , std :: string <nl> secure = true ; <nl> } else if ( protocol != " ws ") { <nl> eh -> errorHandler ( user ); <nl> + return ; <nl> } <nl>  <nl> if ( portStr . length ()) { <nl> void testConnections () { <nl> case 10 : <nl> std :: cout << " Client emitted error on poll error " << std :: endl ; <nl> break ; <nl> + case 11 : <nl> + static int protocolErrorCount = 0 ; <nl> + protocolErrorCount ++; <nl> + std :: cout << " Client emitted error on invalid protocol " << std :: endl ; <nl> + if ( protocolErrorCount > 1 ) { <nl> + std :: cout << " FAILURE : " << protocolErrorCount << " errors emitted for one connection !" << std :: endl ; <nl> + exit (- 1 ); <nl> + } <nl> + break ; <nl> default : <nl> std :: cout << " FAILURE : " << user << " should not emit error !" << std :: endl ; <nl> exit (- 1 ); <nl> void testConnections () { <nl> }); <nl>  <nl> h . connect (" invalid URI ", ( void *) 1 ); <nl> + h . connect (" invalid :// validButUnknown . yolo ", ( void *) 11 ); <nl> h . connect (" ws :// validButUnknown . yolo ", ( void *) 2 ); <nl> h . connect (" ws :// echo . websocket . org ", ( void *) 3 , {}, 10 ); <nl> h . connect (" ws :// echo . websocket . org ", ( void *) 8 );
mmm src / Server . cpp <nl> ppp src / Server . cpp <nl> Server :: Server ( int port , bool master , int options , int maxPayload , SSLContext ss <nl> listenPoll = new uv_poll_t ; <nl> listenPoll -> data = this ; <nl>  <nl> - if ( bind ( listenFd , ( sockaddr *) & listenAddr , sizeof ( sockaddr_in )) | listen ( listenFd , 10 )) { <nl> + int on = 1 ; <nl> + setsockopt ( listenFd , SOL_SOCKET , SO_REUSEADDR , & on , sizeof ( on )); <nl> + <nl> + if ( bind ( listenFd , ( sockaddr *) & listenAddr , sizeof ( sockaddr_in )) || listen ( listenFd , 10 )) { <nl> throw ERR_LISTEN ; <nl> } <nl> 
mmm tools / tiff2ps . c <nl> ppp tools / tiff2ps . c <nl> TIFF2PS ( FILE * fd , TIFF * tif , <nl> if (! generateEPSF && ( level2 || level3 )) { <nl> fprintf ( fd , <nl> " 1 dict begin / PageSize [ % f % f ] def currentdict end setpagedevice \ n ", <nl> - pw ? pw : ( rotate ? prh : prw ), <nl> - ph ? ph : ( rotate ? prw : prh )); <nl> + pw ? pw * PS_UNIT_SIZE : ( rotate ? prh : prw ), <nl> + ph ? ph * PS_UNIT_SIZE : ( rotate ? prw : prh )); <nl> fputs ( <nl> "<<\ n / Policies <<\ n / PageSize 3 \ n >>\ n >> setpagedevice \ n ", <nl> fd );
mmm libtiff / tif_dir . c <nl> ppp libtiff / tif_dir . c <nl> _TIFFVGetField ( TIFF * tif , ttag_t tag , va_list ap ) <nl> * va_arg ( ap , void **) = tv -> value ; <nl> ret_val = 1 ; <nl> } else { <nl> - int i ; <nl> + int j ; <nl> char * val = ( char *) tv -> value ; <nl>  <nl> - for ( i = 0 ; i < tv -> count ; <nl> - i ++, val += _TIFFDataSize ( fip -> field_type )) { <nl> + for ( j = 0 ; j < tv -> count ; <nl> + j ++, val += _TIFFDataSize ( tv -> info -> field_type )) { <nl> switch ( fip -> field_type ) { <nl> case TIFF_BYTE : <nl> case TIFF_UNDEFINED :
mmm libtiff / tif_pixarlog . c <nl> ppp libtiff / tif_pixarlog . c <nl> horizontalAccumulate8abgr ( uint16 * wp , int n , int stride , unsigned char * op , <nl> typedef struct { <nl> TIFFPredictorState predict ; <nl> z_stream stream ; <nl> + tmsize_t tbuf_size ; /* only set / used on reading for now */ <nl> uint16 * tbuf ; <nl> uint16 stride ; <nl> int state ; <nl> PixarLogSetupDecode ( TIFF * tif ) <nl> sp -> tbuf = ( uint16 *) _TIFFmalloc ( tbuf_size ); <nl> if ( sp -> tbuf == NULL ) <nl> return ( 0 ); <nl> + sp -> tbuf_size = tbuf_size ; <nl> if ( sp -> user_datafmt == PIXARLOGDATAFMT_UNKNOWN ) <nl> sp -> user_datafmt = PixarLogGuessDataFmt ( td ); <nl> if ( sp -> user_datafmt == PIXARLOGDATAFMT_UNKNOWN ) { <nl> PixarLogDecode ( TIFF * tif , uint8 * op , tmsize_t occ , uint16 s ) <nl> TIFFErrorExt ( tif -> tif_clientdata , module , " ZLib cannot deal with buffers this size "); <nl> return ( 0 ); <nl> } <nl> + /* Check that we will not fill more than what was allocated */ <nl> + if ( sp -> stream . avail_out > sp -> tbuf_size ) <nl> + { <nl> + TIFFErrorExt ( tif -> tif_clientdata , module , " sp -> stream . avail_out > sp -> tbuf_size "); <nl> + return ( 0 ); <nl> + } <nl> do { <nl> int state = inflate (& sp -> stream , Z_PARTIAL_FLUSH ); <nl> if ( state == Z_STREAM_END ) {
mmm libtiff / tif_win32 . c <nl> ppp libtiff / tif_win32 . c <nl> TIFFOpenW ( const wchar_t * name , const char * mode ) <nl> void * <nl> _TIFFmalloc ( tmsize_t s ) <nl> { <nl> + if ( s == 0 ) <nl> + return (( void *) NULL ); <nl> + <nl> return ( malloc (( size_t ) s )); <nl> } <nl> mmm libtiff / tif_vms . c <nl> ppp libtiff / tif_vms . c <nl> TIFFOpenW ( const wchar_t * name , const char * mode ) <nl> void * <nl> _TIFFmalloc ( tmsize_t s ) <nl> { <nl> + if ( s == 0 ) <nl> + return (( void *) NULL ); <nl> + <nl> return ( malloc (( size_t ) s )); <nl> } <nl>  <nl> TIFFOpen ( const char * name , const char * mode ) <nl> tdata_t <nl> _TIFFmalloc ( tsize_t s ) <nl> { <nl> + if ( s == 0 ) <nl> + return (( void *) NULL ); <nl> + <nl> return ( malloc (( size_t ) s )); <nl> } <nl> mmm libtiff / tif_unix . c <nl> ppp libtiff / tif_unix . c <nl> TIFFOpenW ( const wchar_t * name , const char * mode ) <nl> void * <nl> _TIFFmalloc ( tmsize_t s ) <nl> { <nl> + if ( s == 0 ) <nl> + return (( void *) NULL ); <nl> + <nl> return ( malloc (( size_t ) s )); <nl> } <nl>  <nl> TIFFOpen ( const char * name , const char * mode ) <nl> tdata_t <nl> _TIFFmalloc ( tsize_t s ) <nl> { <nl> + if ( s == 0 ) <nl> + return (( void *) NULL ); <nl> + <nl> return ( malloc (( size_t ) s )); <nl> } <nl>  <nl> TIFFOpenW ( const wchar_t * name , const char * mode ) <nl> void * <nl> _TIFFmalloc ( tmsize_t s ) <nl> { <nl> + if ( s == 0 ) <nl> + return (( void *) NULL ); <nl> + <nl> return ( malloc (( size_t ) s )); <nl> } <nl> 
mmm libtiff / tif_read . c <nl> ppp libtiff / tif_read . c <nl> TIFFReadEncodedStrip ( TIFF * tif , uint32 strip , void * buf , tmsize_t size ) <nl> rowsperstrip = td -> td_rowsperstrip ; <nl> if ( rowsperstrip > td -> td_imagelength ) <nl> rowsperstrip = td -> td_imagelength ; <nl> - stripsperplane =(( td -> td_imagelength + rowsperstrip - 1 )/ rowsperstrip ); <nl> + stripsperplane = TIFFhowmany_32_maxuint_compat ( td -> td_imagelength , rowsperstrip ); <nl> stripinplane =( strip % stripsperplane ); <nl> plane =( uint16 )( strip / stripsperplane ); <nl> rows = td -> td_imagelength - stripinplane * rowsperstrip ;mmm libtiff / tiffiop . h <nl> ppp libtiff / tiffiop . h <nl> TIFFReadEncodedStrip ( TIFF * tif , uint32 strip , void * buf , tmsize_t size ) <nl> rowsperstrip = td -> td_rowsperstrip ; <nl> if ( rowsperstrip > td -> td_imagelength ) <nl> rowsperstrip = td -> td_imagelength ; <nl> - stripsperplane =(( td -> td_imagelength + rowsperstrip - 1 )/ rowsperstrip ); <nl> + stripsperplane = TIFFhowmany_32_maxuint_compat ( td -> td_imagelength , rowsperstrip ); <nl> stripinplane =( strip % stripsperplane ); <nl> plane =( uint16 )( strip / stripsperplane ); <nl> rows = td -> td_imagelength - stripinplane * rowsperstrip ; <nl> struct tiff { <nl> # define TIFFhowmany_32 ( x , y ) ((( uint32 ) x < ( 0xffffffff - ( uint32 )( y - 1 ))) ? \ <nl> (((( uint32 )( x ))+((( uint32 )( y ))- 1 ))/(( uint32 )( y ))) : \ <nl> 0U ) <nl> +/* Variant of TIFFhowmany_32 () that doesn ' t return 0 if x close to MAXUINT . */ <nl> +/* Caution : TIFFhowmany_32_maxuint_compat ( x , y )* y might overflow */ <nl> +# define TIFFhowmany_32_maxuint_compat ( x , y ) \ <nl> + ((( uint32 )( x ) / ( uint32 )( y )) + (((( uint32 )( x ) % ( uint32 )( y )) != 0 ) ? 1 : 0 )) <nl> # define TIFFhowmany8_32 ( x ) ((( x )& 0x07 )?(( uint32 )( x )>> 3 )+ 1 :( uint32 )( x )>> 3 ) <nl> # define TIFFroundup_32 ( x , y ) ( TIFFhowmany_32 ( x , y )*( y )) <nl> # define TIFFhowmany_64 ( x , y ) (((( uint64 )( x ))+((( uint64 )( y ))- 1 ))/(( uint64 )( y )))
mmm libtiff / tif_ojpeg . c <nl> ppp libtiff / tif_ojpeg . c <nl> typedef enum { <nl>  <nl> typedef struct { <nl> TIFF * tif ; <nl> + int decoder_ok ; <nl> # ifndef LIBJPEG_ENCAP_EXTERNAL <nl> JMP_BUF exit_jmpbuf ; <nl> # endif <nl> OJPEGPreDecode ( TIFF * tif , uint16 s ) <nl> } <nl> sp -> write_curstrile ++; <nl> } <nl> + sp -> decoder_ok = 1 ; <nl> return ( 1 ); <nl> } <nl>  <nl> OJPEGPreDecodeSkipScanlines ( TIFF * tif ) <nl> static int <nl> OJPEGDecode ( TIFF * tif , uint8 * buf , tmsize_t cc , uint16 s ) <nl> { <nl> + static const char module []=" OJPEGDecode "; <nl> OJPEGState * sp =( OJPEGState *) tif -> tif_data ; <nl> ( void ) s ; <nl> + if ( ! sp -> decoder_ok ) <nl> + { <nl> + TIFFErrorExt ( tif -> tif_clientdata , module ," Cannot decode : decoder not correctly initialized "); <nl> + return 0 ; <nl> + } <nl> if ( sp -> libjpeg_jpeg_query_style == 0 ) <nl> { <nl> if ( OJPEGDecodeRaw ( tif , buf , cc )== 0 )
mmm tools / tiffcrop . c <nl> ppp tools / tiffcrop . c <nl> static int readContigStripsIntoBuffer ( TIFF * in , uint8 * buf ) <nl> { <nl> uint8 * bufp = buf ; <nl> int32 bytes_read = 0 ; <nl> - uint16 strip , nstrips = TIFFNumberOfStrips ( in ); <nl> + uint32 strip , nstrips = TIFFNumberOfStrips ( in ); <nl> uint32 stripsize = TIFFStripSize ( in ); <nl> uint32 rows = 0 ; <nl> uint32 rps = TIFFGetFieldDefaulted ( in , TIFFTAG_ROWSPERSTRIP , & rps ); <nl> static int readSeparateStripsIntoBuffer ( TIFF * in , uint8 * obuf , uint32 length , <nl> uint32 width , uint16 spp , <nl> struct dump_opts * dump ) <nl> { <nl> - int i , j , bytes_per_sample , bytes_per_pixel , shift_width , result = 1 ; <nl> + int i , bytes_per_sample , bytes_per_pixel , shift_width , result = 1 ; <nl> + uint32 j ; <nl> int32 bytes_read = 0 ; <nl> - uint16 bps , nstrips , planar , strips_per_sample ; <nl> + uint16 bps , planar ; <nl> + uint32 nstrips ; <nl> + uint32 strips_per_sample ; <nl> uint32 src_rowsize , dst_rowsize , rows_processed , rps ; <nl> uint32 rows_this_strip = 0 ; <nl> tsample_t s ;mmm tools / tiffcp . c <nl> ppp tools / tiffcp . c <nl> static int readContigStripsIntoBuffer ( TIFF * in , uint8 * buf ) <nl> { <nl> uint8 * bufp = buf ; <nl> int32 bytes_read = 0 ; <nl> - uint16 strip , nstrips = TIFFNumberOfStrips ( in ); <nl> + uint32 strip , nstrips = TIFFNumberOfStrips ( in ); <nl> uint32 stripsize = TIFFStripSize ( in ); <nl> uint32 rows = 0 ; <nl> uint32 rps = TIFFGetFieldDefaulted ( in , TIFFTAG_ROWSPERSTRIP , & rps ); <nl> static int readSeparateStripsIntoBuffer ( TIFF * in , uint8 * obuf , uint32 length , <nl> uint32 width , uint16 spp , <nl> struct dump_opts * dump ) <nl> { <nl> - int i , j , bytes_per_sample , bytes_per_pixel , shift_width , result = 1 ; <nl> + int i , bytes_per_sample , bytes_per_pixel , shift_width , result = 1 ; <nl> + uint32 j ; <nl> int32 bytes_read = 0 ; <nl> - uint16 bps , nstrips , planar , strips_per_sample ; <nl> + uint16 bps , planar ; <nl> + uint32 nstrips ; <nl> + uint32 strips_per_sample ; <nl> uint32 src_rowsize , dst_rowsize , rows_processed , rps ; <nl> uint32 rows_this_strip = 0 ; <nl> tsample_t s ; <nl> static copyFunc pickCopyFunc ( TIFF *, TIFF *, uint16 , uint16 ); <nl> static int <nl> tiffcp ( TIFF * in , TIFF * out ) <nl> { <nl> - uint16 bitspersample , samplesperpixel ; <nl> - uint16 input_compression , input_photometric ; <nl> + uint16 bitspersample , samplesperpixel = 1 ; <nl> + uint16 input_compression , input_photometric = PHOTOMETRIC_MINISBLACK ; <nl> copyFunc cf ; <nl> uint32 width , length ; <nl> struct cpTag * p ;
mmm libtiff / tif_dirread . c <nl> ppp libtiff / tif_dirread . c <nl> EstimateStripByteCounts ( TIFF * tif , TIFFDirEntry * dir , uint16 dircount ) <nl> td -> td_stripbytecount = ( uint64 *) <nl> _TIFFCheckMalloc ( tif , td -> td_nstrips , sizeof ( uint64 ), <nl> " for \" StripByteCounts \" array "); <nl> + if ( td -> td_stripbytecount == NULL ) <nl> + return - 1 ; <nl> + <nl> if ( td -> td_compression != COMPRESSION_NONE ) { <nl> uint64 space ; <nl> uint64 filesize ;
mmm tools / tiffcp . c <nl> ppp tools / tiffcp . c <nl> DECLAREcpFunc ( cpDecodedStrips ) <nl> tstrip_t s , ns = TIFFNumberOfStrips ( in ); <nl> uint32 row = 0 ; <nl> _TIFFmemset ( buf , 0 , stripsize ); <nl> - for ( s = 0 ; s < ns ; s ++) { <nl> + for ( s = 0 ; s < ns && row < imagelength ; s ++) { <nl> tsize_t cc = ( row + rowsperstrip > imagelength ) ? <nl> TIFFVStripSize ( in , imagelength - row ) : stripsize ; <nl> if ( TIFFReadEncodedStrip ( in , s , buf , cc ) < 0
mmm tools / tiffcp . c <nl> ppp tools / tiffcp . c <nl> DECLAREreadFunc ( readContigTilesIntoBuffer ) <nl> uint32 colb = 0 ; <nl> uint32 col ; <nl>  <nl> - for ( col = 0 ; col < imagewidth ; col += tw ) { <nl> + for ( col = 0 ; col < imagewidth && colb < imagew ; col += tw ) { <nl> if ( TIFFReadTile ( in , tilebuf , col , row , 0 , 0 ) < 0 <nl> && ! ignore ) { <nl> TIFFError ( TIFFFileName ( in ), <nl> DECLAREwriteFunc ( writeBufferToContigTiles ) <nl> uint32 colb = 0 ; <nl> uint32 col ; <nl>  <nl> - for ( col = 0 ; col < imagewidth ; col += tw ) { <nl> + for ( col = 0 ; col < imagewidth && colb < imagew ; col += tw ) { <nl> /* <nl> * Tile is clipped horizontally . Calculate <nl> * visible portion and skewing factors .
mmm libtiff / tif_aux . c <nl> ppp libtiff / tif_aux . c <nl> tdata_t <nl> _TIFFCheckMalloc ( TIFF * tif , size_t nmemb , size_t elem_size , const char * what ) <nl> { <nl> - tdata_t * cp = NULL ; <nl> + tdata_t cp = NULL ; <nl> tsize_t bytes = nmemb * elem_size ; <nl>  <nl> /*
mmm libtiff / tif_predict . c <nl> ppp libtiff / tif_predict . c <nl> fpAcc ( TIFF * tif , uint8 * cp0 , tmsize_t cc ) <nl> tmsize_t wc = cc / bps ; <nl> tmsize_t count = cc ; <nl> uint8 * cp = ( uint8 *) cp0 ; <nl> - uint8 * tmp = ( uint8 *) _TIFFmalloc ( cc ); <nl> + uint8 * tmp ; <nl>  <nl> if ( cc %( bps * stride )!= 0 ) <nl> { <nl> fpAcc ( TIFF * tif , uint8 * cp0 , tmsize_t cc ) <nl> return 0 ; <nl> } <nl>  <nl> + tmp = ( uint8 *) _TIFFmalloc ( cc ); <nl> if (! tmp ) <nl> return 0 ; <nl>  <nl> fpDiff ( TIFF * tif , uint8 * cp0 , tmsize_t cc ) <nl> tmsize_t wc = cc / bps ; <nl> tmsize_t count ; <nl> uint8 * cp = ( uint8 *) cp0 ; <nl> - uint8 * tmp = ( uint8 *) _TIFFmalloc ( cc ); <nl> + uint8 * tmp ; <nl>  <nl> if (( cc %( bps * stride ))!= 0 ) <nl> { <nl> fpDiff ( TIFF * tif , uint8 * cp0 , tmsize_t cc ) <nl> "% s ", "( cc %( bps * stride ))!= 0 "); <nl> return 0 ; <nl> } <nl> + <nl> + tmp = ( uint8 *) _TIFFmalloc ( cc ); <nl> if (! tmp ) <nl> return 0 ; <nl>  <nl> PredictorEncodeTile ( TIFF * tif , uint8 * bp0 , tmsize_t cc0 , uint16 s ) <nl> { <nl> TIFFErrorExt ( tif -> tif_clientdata , " PredictorEncodeTile ", <nl> "% s ", "( cc0 % rowsize )!= 0 "); <nl> + _TIFFfree ( working_copy ); <nl> return 0 ; <nl> } <nl> while ( cc > 0 ) {
mmm tools / tiffcp . c <nl> ppp tools / tiffcp . c <nl> bad : <nl>  <nl> static void <nl> cpStripToTile ( uint8 * out , uint8 * in , <nl> - uint32 rows , uint32 cols , int outskew , int inskew ) <nl> + uint32 rows , uint32 cols , int outskew , int64 inskew ) <nl> { <nl> while ( rows -- > 0 ) { <nl> uint32 j = cols ; <nl> DECLAREreadFunc ( readContigTilesIntoBuffer ) <nl> tdata_t tilebuf ; <nl> uint32 imagew = TIFFScanlineSize ( in ); <nl> uint32 tilew = TIFFTileRowSize ( in ); <nl> - int iskew = imagew - tilew ; <nl> + int64 iskew = ( int64 ) imagew - ( int64 ) tilew ; <nl> uint8 * bufp = ( uint8 *) buf ; <nl> uint32 tw , tl ; <nl> uint32 row ; <nl> DECLAREreadFunc ( readContigTilesIntoBuffer ) <nl> status = 0 ; <nl> goto done ; <nl> } <nl> - if ( colb + tilew > imagew ) { <nl> + if ( colb > iskew ) { <nl> uint32 width = imagew - colb ; <nl> uint32 oskew = tilew - width ; <nl> cpStripToTile ( bufp + colb ,
mmm tools / tiffcrop . c <nl> ppp tools / tiffcrop . c <nl> static int readContigStripsIntoBuffer ( TIFF * in , uint8 * buf ) <nl> ( unsigned long ) strip , ( unsigned long ) rows ); <nl> return 0 ; <nl> } <nl> - bufp += bytes_read ; <nl> + bufp += stripsize ; <nl> } <nl>  <nl> return 1 ;
mmm tools / tiffcrop . c <nl> ppp tools / tiffcrop . c <nl> static int readContigTilesIntoBuffer ( TIFF * in , uint8 * buf , <nl> } <nl> } <nl>  <nl> - tilebuf = _TIFFmalloc ( tile_buffsize ); <nl> + /* Add 3 padding bytes for extractContigSamplesShifted32bits */ <nl> + if ( tile_buffsize > 0xFFFFFFFFU - 3 ) <nl> + { <nl> + TIFFError (" readContigTilesIntoBuffer ", " Integer overflow when calculating buffer size ."); <nl> + exit (- 1 ); <nl> + } <nl> + tilebuf = _TIFFmalloc ( tile_buffsize + 3 ); <nl> if ( tilebuf == 0 ) <nl> return 0 ; <nl> + tilebuf [ tile_buffsize ] = 0 ; <nl> + tilebuf [ tile_buffsize + 1 ] = 0 ; <nl> + tilebuf [ tile_buffsize + 2 ] = 0 ; <nl>  <nl> dst_rowsize = (( imagewidth * bps * spp ) + 7 ) / 8 ; <nl> for ( row = 0 ; row < imagelength ; row += tl )
mmm libtiff / tif_next . c <nl> ppp libtiff / tif_next . c <nl> case 0 : op [ 0 ] = ( unsigned char ) (( v ) << 6 ); break ; \ <nl> case 1 : op [ 0 ] |= ( v ) << 4 ; break ; \ <nl> case 2 : op [ 0 ] |= ( v ) << 2 ; break ; \ <nl> - case 3 : * op ++ |= ( v ); break ; \ <nl> + case 3 : * op ++ |= ( v ); op_offset ++; break ; \ <nl> } \ <nl> } <nl>  <nl> NeXTDecode ( TIFF * tif , uint8 * buf , tmsize_t occ , uint16 s ) <nl> uint32 imagewidth = tif -> tif_dir . td_imagewidth ; <nl> if ( isTiled ( tif ) ) <nl> imagewidth = tif -> tif_dir . td_tilewidth ; <nl> + tmsize_t op_offset = 0 ; <nl>  <nl> /* <nl> * The scanline is composed of a sequence of constant <nl> NeXTDecode ( TIFF * tif , uint8 * buf , tmsize_t occ , uint16 s ) <nl> * bounds , potentially resulting in a security <nl> * issue . <nl> */ <nl> - while ( n -- > 0 && npixels < imagewidth ) <nl> + while ( n -- > 0 && npixels < imagewidth && op_offset < scanline ) <nl> SETPIXEL ( op , grey ); <nl> if ( npixels >= imagewidth ) <nl> break ; <nl> + if ( op_offset >= scanline ) { <nl> + TIFFErrorExt ( tif -> tif_clientdata , module , " Invalid data for scanline % ld ", <nl> + ( long ) tif -> tif_row ); <nl> + return ( 0 ); <nl> + } <nl> if ( cc == 0 ) <nl> goto bad ; <nl> n = * bp ++, cc --;
mmm tools / tiffset . c <nl> ppp tools / tiffset . c <nl> ****************************************************************************** <nl> * <nl> * $ Log $ <nl> - * Revision 1 . 11 2005 - 09 - 13 14 : 13 : 42 dron <nl> + * Revision 1 . 12 2007 - 02 - 24 17 : 14 : 14 dron <nl> + * Properly handle tags with TIFF_VARIABLE writecount . As per bug <nl> + * http :// bugzilla . remotesensing . org / show_bug . cgi ? id = 1350 <nl> + * <nl> + * Revision 1 . 11 2005 / 09 / 13 14 : 13 : 42 dron <nl> * Avoid warnings . <nl> * <nl> * Revision 1 . 10 2005 / 02 / 24 14 : 47 : 11 fwarmerdam <nl> main ( int argc , char * argv []) <nl> if ( TIFFSetField ( tiff , fip -> field_tag , argv [ arg_index ]) != 1 ) <nl> fprintf ( stderr , " Failed to set % s =% s \ n ", <nl> fip -> field_name , argv [ arg_index ] ); <nl> - } else if ( fip -> field_writecount > 0 ) { <nl> + } else if ( fip -> field_writecount > 0 <nl> + || fip -> field_writecount == TIFF_VARIABLE ) { <nl> int ret = 1 ; <nl> short wc ; <nl> 
mmm tools / tiff2pdf . c <nl> ppp tools / tiff2pdf . c <nl> tsize_t t2p_readwrite_pdf_image_tile ( T2P * t2p , TIFF * input , TIFF * output , ttile_ <nl> return ( 0 ); <nl> } <nl> if ( TIFFGetField ( input , TIFFTAG_JPEGTABLES , & count , & jpt ) != 0 ) { <nl> - if ( count >= 4 ) { <nl> + if ( count > 4 ) { <nl> int retTIFFReadRawTile ; <nl> /* Ignore EOI marker of JpegTables */ <nl> _TIFFmemcpy ( buffer , jpt , count - 2 );
mmm libtiff / tif_jpeg . c <nl> ppp libtiff / tif_jpeg . c <nl> JPEGVSetField ( TIFF * tif , ttag_t tag , va_list ap ) <nl> case TIFFTAG_JPEGTABLESMODE : <nl> sp -> jpegtablesmode = va_arg ( ap , int ); <nl> return ( 1 ); /* pseudo tag */ <nl> + case TIFFTAG_YCBCRSUBSAMPLING : <nl> + /* mark the fact that we have a real ycbcrsubsampling ! */ <nl> + sp -> ycbcrsampling_fetched = 1 ; <nl> + return (* sp -> vsetparent )( tif , tag , ap ); <nl> default : <nl> return (* sp -> vsetparent )( tif , tag , ap ); <nl> } <nl> JPEGVSetField ( TIFF * tif , ttag_t tag , va_list ap ) <nl> * loaded just to get the tags right , even if the imagery is never read . <nl> * It would be more efficient to just load a bit of the header , and <nl> * initialize things from that . <nl> - * o This code doesn ' t know whether or not the tag actually did occur in <nl> - * the file . If it knew this it could skip the hack but this is hard to <nl> - * know since we have already set the " field set " bit for the subsampling <nl> - * TIFFInitJPEG (). <nl> * <nl> * See the bug in bugzilla for details : <nl> * <nl> JPEGFixupTestSubsampling ( TIFF * tif ) <nl> * jpeg data to get the sampling . <nl> */ <nl> if ( ! sp -> cinfo . comm . is_decompressor <nl> - || sp -> ycbcrsampling_fetched ) <nl> + || sp -> ycbcrsampling_fetched <nl> + || sp -> photometric != PHOTOMETRIC_YCBCR ) <nl> return ; <nl>  <nl> sp -> ycbcrsampling_fetched = 1 ;
mmm libtiff / tiffiop . h <nl> ppp libtiff / tiffiop . h <nl> struct tiff { <nl> */ <nl> # ifndef ReadOK <nl> # define ReadOK ( tif , buf , size ) \ <nl> - ( TIFFReadFile ( tif , ( tdata_t ) buf , ( tsize_t ) size ) == ( tsize_t ) size ) <nl> + ( TIFFReadFile ( tif , ( tdata_t ) buf , ( tsize_t )( size )) == ( tsize_t )( size )) <nl> # endif <nl> # ifndef SeekOK <nl> # define SeekOK ( tif , off ) \
mmm tools / gif2tiff . c <nl> ppp tools / gif2tiff . c <nl> process ( register int code , unsigned char ** fill ) <nl> } <nl>  <nl> if ( oldcode == - 1 ) { <nl> + if ( code >= clear ) { <nl> + fprintf ( stderr , " bad input : code =% d is larger than clear =% d \ n ", code , clear ); <nl> + return 0 ; <nl> + } <nl> *(* fill )++ = suffix [ code ]; <nl> firstchar = oldcode = code ; <nl> return 1 ;
mmm bin / varnishd / cache_waiter . h <nl> ppp bin / varnishd / cache_waiter . h <nl> struct sess ; <nl> typedef void * waiter_init_f ( void ); <nl> typedef void waiter_pass_f ( void * priv , const struct sess *); <nl>  <nl> - extern int vca_pipes [ 2 ]; <nl> - <nl> struct waiter { <nl> const char * name ; <nl> waiter_init_f * init ;mmm bin / varnishd / cache_acceptor . c <nl> ppp bin / varnishd / cache_acceptor . c <nl> struct sess ; <nl> typedef void * waiter_init_f ( void ); <nl> typedef void waiter_pass_f ( void * priv , const struct sess *); <nl>  <nl> - extern int vca_pipes [ 2 ]; <nl> - <nl> struct waiter { <nl> const char * name ; <nl> waiter_init_f * init ; <nl> static const struct linger linger = { <nl>  <nl> static unsigned char need_sndtimeo , need_rcvtimeo , need_linger , need_test ; <nl>  <nl> - int vca_pipes [ 2 ] = { - 1 , - 1 }; <nl> - <nl> static void <nl> sock_test ( int fd ) <nl> { <nl> ccf_start ( struct cli * cli , const char * const * av , void * priv ) <nl> AN ( vca_act -> init ); <nl> AN ( vca_act -> pass ); <nl>  <nl> - AZ ( pipe ( vca_pipes )); /* XXX */ <nl> waiter_priv = vca_act -> init (); <nl> AZ ( pthread_create (& VCA_thread , NULL , vca_acct , NULL )); <nl> VSL ( SLT_Debug , 0 , " Acceptor is % s ", vca_act -> name );
mmm bin / varnishd / cache_acceptor_poll . c <nl> ppp bin / varnishd / cache_acceptor_poll . c <nl>  <nl> static pthread_t vca_poll_thread ; <nl> static struct pollfd * pollfd ; <nl> - static unsigned npoll ; <nl> + static unsigned npoll , hpoll ; <nl>  <nl> static VTAILQ_HEAD (, sess ) sesshead = VTAILQ_HEAD_INITIALIZER ( sesshead ); <nl>  <nl> vca_poll ( int fd ) <nl>  <nl> assert ( fd >= 0 ); <nl> vca_pollspace (( unsigned ) fd ); <nl> + if ( hpoll < fd ) <nl> + hpoll = fd ; <nl> pollfd [ fd ]. fd = fd ; <nl> pollfd [ fd ]. events = POLLIN ; <nl> } <nl> vca_unpoll ( int fd ) <nl> vca_pollspace (( unsigned ) fd ); <nl> pollfd [ fd ]. fd = - 1 ; <nl> pollfd [ fd ]. events = 0 ; <nl> + if ( hpoll == fd ) { <nl> + while ( pollfd [-- hpoll ]. fd == - 1 ) <nl> + continue ; <nl> + } <nl> } <nl>  <nl> /*--------------------------------------------------------------------*/ <nl> vca_main ( void * arg ) <nl> vca_poll ( vca_pipes [ 0 ]); <nl>  <nl> while ( 1 ) { <nl> - v = poll ( pollfd , npoll , 100 ); <nl> + v = poll ( pollfd , hpoll + 1 , 100 ); <nl> if ( v && pollfd [ vca_pipes [ 0 ]]. revents ) { <nl> v --; <nl> i = read ( vca_pipes [ 0 ], & sp , sizeof sp );
mmm bin / varnishd / cache_center . c <nl> ppp bin / varnishd / cache_center . c <nl> cnt_recv ( struct sess * sp ) <nl> return ( 0 ); <nl> } <nl>  <nl> + if ( params -> http_gzip_support && <nl> + ( recv_handling != VCL_RET_PIPE ) && <nl> + ( recv_handling != VCL_RET_PASS )) { <nl> + if ( RFC2616_Req_Gzip ( sp )) { <nl> + http_Unset ( sp -> http , H_Accept_Encoding ); <nl> + http_PrintfHeader ( sp -> wrk , sp -> fd , sp -> http , <nl> + " Accept - Encoding : gzip "); <nl> + } else { <nl> + http_Unset ( sp -> http , H_Accept_Encoding ); <nl> + } <nl> + } <nl> + <nl> SHA256_Init ( sp -> wrk -> sha256ctx ); <nl> VCL_hash_method ( sp ); <nl> assert ( sp -> handling == VCL_RET_HASH );
mmm bin / varnishd / cache_center . c <nl> ppp bin / varnishd / cache_center . c <nl> cnt_miss ( struct sess * sp ) <nl> HSH_Unbusy ( sp -> obj ); <nl> HSH_Deref ( sp -> obj ); <nl> sp -> obj = NULL ; <nl> + vbe_free_bereq ( sp -> bereq ); <nl> + sp -> bereq = NULL ; <nl> sp -> step = STP_ERROR ; <nl> return ( 0 ); <nl> }
mmm lib / libvarnish / vcli_proto . c <nl> ppp lib / libvarnish / vcli_proto . c <nl> read_tmo ( int fd , char * ptr , unsigned len , double tmo ) <nl> pfd . events = POLLIN ; <nl> for ( j = 0 ; len > 0 ; ) { <nl> i = poll (& pfd , 1 , to ); <nl> + if ( i < 0 ) { <nl> + errno = EINTR ; <nl> + return (- 1 ); <nl> + } <nl> if ( i == 0 ) { <nl> errno = ETIMEDOUT ; <nl> return (- 1 );
mmm bin / varnishtop / varnishtop . c <nl> ppp bin / varnishtop / varnishtop . c <nl>  <nl> static const char progname [] = " varnishtop "; <nl> static float period = 60 ; /* seconds */ <nl> + static int end_of_file = 0 ; <nl>  <nl> struct top { <nl> uint8_t tag ; <nl> update ( int p ) <nl> if ( n < p ) <nl> n ++; <nl> AC ( erase ()); <nl> - AC ( mvprintw ( 0 , 0 , "%* s ", COLS - 1 , VUT . name )); <nl> + if ( end_of_file ) <nl> + AC ( mvprintw ( 0 , COLS - 1 - strlen ( VUT . name ) - 5 , "% s ( EOF )", <nl> + VUT . name )); <nl> + else <nl> + AC ( mvprintw ( 0 , COLS - 1 - strlen ( VUT . name ), "% s ", VUT . name )); <nl> AC ( mvprintw ( 0 , 0 , " list length % u ", ntop )); <nl> for ( tp = VRB_MIN ( top_tree , & top_tree_head ); tp != NULL ; tp = tp2 ) { <nl> tp2 = VRB_NEXT ( top_tree , & top_tree_head , tp ); <nl> main ( int argc , char ** argv ) <nl> VUT . dispatch_f = & accumulate ; <nl> VUT . dispatch_priv = NULL ; <nl> VUT_Main (); <nl> + end_of_file = 1 ; <nl> if ( once ) <nl> dump (); <nl> else
mmm bin / varnishd / cache_expire . c <nl> ppp bin / varnishd / cache_expire . c <nl> EXP_Touch ( const struct object * o , double now ) <nl>  <nl> CHECK_OBJ_NOTNULL ( o , OBJECT_MAGIC ); <nl> oe = o -> objexp ; <nl> + if ( oe == NULL ) <nl> + return ; <nl> CHECK_OBJ_NOTNULL ( oe , OBJEXP_MAGIC ); <nl> if ( oe -> lru_stamp + params -> lru_timeout > now ) <nl> return ;
mmm bin / varnishd / cache / cache_fetch . c <nl> ppp bin / varnishd / cache / cache_fetch . c <nl> vbf_stp_error ( struct worker * wrk , struct busyobj * bo ) <nl> l = ll ; <nl> if ( VFP_GetStorage ( bo -> vfc , & l , & ptr ) != VFP_OK ) <nl> break ; <nl> + if ( l > ll ) <nl> + l = ll ; <nl> memcpy ( ptr , VSB_data ( synth_body ) + o , l ); <nl> VFP_Extend ( bo -> vfc , l ); <nl> ll -= l ;
mmm bin / varnishd / cache_waiter_poll . c <nl> ppp bin / varnishd / cache_waiter_poll . c <nl> vca_main ( void * arg ) <nl> if ( pollfd [ fd ]. revents ) { <nl> v --; <nl> i = HTC_Rx ( sp -> htc ); <nl> + if ( pollfd [ fd ]. revents != POLLIN ) <nl> + VSL ( SLT_Debug , fd , " Poll : % x / % d ", <nl> + pollfd [ fd ]. revents , i ); <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> if ( i == 0 ) { <nl> /* Mov to front of list for speed */ <nl> vca_main ( void * arg ) <nl> SES_Delete ( sp ); <nl> } <nl> } <nl> + assert ( v == 0 ); <nl> } <nl> NEEDLESS_RETURN ( NULL ); <nl> }
mmm bin / varnishd / mgt_child . c <nl> ppp bin / varnishd / mgt_child . c <nl> mgt_run ( int dflag , const char * T_arg ) <nl>  <nl> setproctitle (" Varnish - Mgr % s ", heritage . name ); <nl>  <nl> + memset (& sac , 0 , sizeof sac ); <nl> sac . sa_handler = SIG_IGN ; <nl> sac . sa_flags = SA_RESTART ; <nl> 
mmm bin / varnishd / storage / storage_persistent . h <nl> ppp bin / varnishd / storage / storage_persistent . h <nl> struct smp_segptr { <nl> struct smp_object { <nl> uint8_t hash [ 32 ]; /* really : DIGEST_LEN */ <nl> struct exp exp ; <nl> + uint32_t __filler__ ; /* -> align / 8 on 32bit */ <nl> double ban ; <nl> uint64_t ptr ; /* rel to silo */ <nl> };mmm bin / varnishd / storage / storage_persistent_mgt . c <nl> ppp bin / varnishd / storage / storage_persistent_mgt . c <nl> struct smp_segptr { <nl> struct smp_object { <nl> uint8_t hash [ 32 ]; /* really : DIGEST_LEN */ <nl> struct exp exp ; <nl> + uint32_t __filler__ ; /* -> align / 8 on 32bit */ <nl> double ban ; <nl> uint64_t ptr ; /* rel to silo */ <nl> }; <nl> smp_mgt_init ( struct stevedore * parent , int ac , char * const * av ) <nl> ASSERT_MGT (); <nl>  <nl> AZ ( av [ ac ]); <nl> + <nl> + /* Necessary alignment . See also smp_object :: __filler__ */ <nl> + assert ( sizeof ( struct smp_object ) % 8 == 0 ); <nl> + <nl> # define SIZOF ( foo ) fprintf ( stderr , \ <nl> " sizeof (% s ) = % zu = 0x % zx \ n ", # foo , sizeof ( foo ), sizeof ( foo )); <nl> SIZOF ( struct smp_ident );
mmm lib / libvarnish / vrnd . c <nl> ppp lib / libvarnish / vrnd . c <nl> VRND_CryptoQuality ( void * ptr , size_t len ) <nl> ssize_t l ; <nl>  <nl> AN ( ptr ); <nl> - fd = open ("/ dev / random ", O_RDONLY ); <nl> + fd = open ("/ dev / urandom ", O_RDONLY ); <nl> if ( fd < 0 ) <nl> return (- 1 ); <nl> for ( p = ptr ; len > 0 ; len --, p ++) {
mmm bin / varnishd / cache_hash . c <nl> ppp bin / varnishd / cache_hash . c <nl> HSH_FindBan ( struct sess * sp , struct objcore ** oc ) <nl> CHECK_OBJ_NOTNULL ( oc1 , OBJCORE_MAGIC ); <nl> oh = oc1 -> objhead ; <nl> CHECK_OBJ_NOTNULL ( oh , OBJHEAD_MAGIC ); <nl> - Lck_Lock (& oh -> mtx ); <nl> + if ( Lck_Trylock (& oh -> mtx )) { <nl> + * oc = NULL ; <nl> + return ; <nl> + } <nl> VTAILQ_FOREACH ( oc2 , & oh -> objcs , list ) <nl> if ( oc1 == oc2 ) <nl> break ;
mmm lib / libvarnishapi / vsl_dispatch . c <nl> ppp lib / libvarnishapi / vsl_dispatch . c <nl> vtx_set_parent ( struct vtx * parent , struct vtx * child ) <nl>  <nl> CHECK_OBJ_NOTNULL ( parent , VTX_MAGIC ); <nl> CHECK_OBJ_NOTNULL ( child , VTX_MAGIC ); <nl> + assert ( parent != child ); <nl> AZ ( parent -> flags & VTX_F_COMPLETE ); <nl> AZ ( child -> flags & VTX_F_COMPLETE ); <nl> AZ ( child -> parent ); <nl> vtx_scan_begin ( struct VSLQ * vslq , struct vtx * vtx , const uint32_t * ptr ) <nl> vtx -> reason = reason ; <nl>  <nl> if ( p_vxid == 0 ) <nl> - /* No parent */ <nl> + /* Zero means no parent */ <nl> return ( 0 ); <nl> + if ( p_vxid == vtx -> key . vxid ) <nl> + return ( vtx_diag_tag ( vtx , ptr , " link to self ")); <nl>  <nl> if ( vslq -> grouping == VSL_g_vxid ) <nl> return ( 0 ); /* No links */ <nl> vtx_scan_link ( struct VSLQ * vslq , struct vtx * vtx , const uint32_t * ptr ) <nl> if ( vslq -> grouping == VSL_g_request && vtx -> type == VSL_t_sess ) <nl> return ( 0 ); /* No links */ <nl>  <nl> + if ( c_vxid == 0 ) <nl> + return ( vtx_diag_tag ( vtx , ptr , " illegal link vxid ")); <nl> + if ( c_vxid == vtx -> key . vxid ) <nl> + return ( vtx_diag_tag ( vtx , ptr , " link to self ")); <nl> + <nl> /* Lookup and check child vtx */ <nl> c_vtx = vtx_lookup ( vslq , c_vxid ); <nl> if ( c_vtx == NULL ) {
mmm bin / varnishd / storage_persistent_subr . c <nl> ppp bin / varnishd / storage_persistent_subr . c <nl> smp_def_sign ( const struct smp_sc * sc , struct smp_signctx * ctx , <nl> AZ ( off & 7 ); /* Alignment */ <nl> assert ( strlen ( id ) < sizeof ctx -> ss -> ident ); <nl>  <nl> - memset ( ctx , 0 , sizeof ctx ); <nl> + memset ( ctx , 0 , sizeof * ctx ); <nl> ctx -> ss = ( void *)( sc -> base + off ); <nl> ctx -> unique = sc -> unique ; <nl> ctx -> id = id ;
mmm bin / varnishd / varnishd . c <nl> ppp bin / varnishd / varnishd . c <nl> DebugStunt ( void ) <nl> pipes [ 1 ][ 1 ] = 1 ; <nl>  <nl> /* close the rest */ <nl> - for ( i = 5 ; i < getdtablesize (); i ++) <nl> + j = getdtablesize (); <nl> + for ( i = 5 ; i < j ; i ++) <nl> ( void ) close ( i ); <nl>  <nl> pfd [ 0 ]. fd = pipes [ 0 ][ 0 ];mmm bin / varnishd / mgt_child . c <nl> ppp bin / varnishd / mgt_child . c <nl> DebugStunt ( void ) <nl> pipes [ 1 ][ 1 ] = 1 ; <nl>  <nl> /* close the rest */ <nl> - for ( i = 5 ; i < getdtablesize (); i ++) <nl> + j = getdtablesize (); <nl> + for ( i = 5 ; i < j ; i ++) <nl> ( void ) close ( i ); <nl>  <nl> pfd [ 0 ]. fd = pipes [ 0 ][ 0 ]; <nl> start_child ( struct cli * cli ) <nl> unsigned u ; <nl> char * p ; <nl> struct vev * e ; <nl> - int i , cp [ 2 ]; <nl> + int i , j , cp [ 2 ]; <nl>  <nl> if ( child_state != CH_STOPPED && child_state != CH_DIED ) <nl> return ; <nl> start_child ( struct cli * cli ) <nl> /* Close anything we shouldn ' t know about */ <nl> closelog (); <nl> printf (" Closed fds :"); <nl> - for ( i = STDERR_FILENO + 1 ; i < getdtablesize (); i ++) { <nl> + j = getdtablesize (); <nl> + for ( i = STDERR_FILENO + 1 ; i < j ; i ++) { <nl> if ( vbit_test ( fd_map , i )) <nl> continue ; <nl> if ( close ( i ) == 0 )
mmm bin / varnishtop / varnishtop . c <nl> ppp bin / varnishtop / varnishtop . c <nl> static int end_of_file = 0 ; <nl>  <nl> struct top { <nl> uint8_t tag ; <nl> - char * rec_data ; <nl> + const char * rec_data ; <nl> + char * rec_buf ; <nl> int clen ; <nl> unsigned hash ; <nl> VRB_ENTRY ( top ) e_order ; <nl> accumulate ( struct VSL_data * vsl , struct VSL_transaction * const pt [], <nl> t . hash = u ; <nl> t . tag = tag ; <nl> t . clen = len ; <nl> - t . rec_data = ( char *) VSL_CDATA ( tr -> c -> rec . ptr ); <nl> + t . rec_data = VSL_CDATA ( tr -> c -> rec . ptr ); <nl>  <nl> AZ ( pthread_mutex_lock (& mtx )); <nl> tp = VRB_FIND ( t_key , & h_key , & t ); <nl> accumulate ( struct VSL_data * vsl , struct VSL_transaction * const pt [], <nl> tp -> count = 1 . 0 ; <nl> tp -> clen = len ; <nl> tp -> tag = tag ; <nl> - tp -> rec_data = strdup ( t . rec_data ); <nl> + tp -> rec_buf = strdup ( t . rec_data ); <nl> + tp -> rec_data = tp -> rec_buf ; <nl> AN ( tp -> rec_data ); <nl> VRB_INSERT ( t_key , & h_key , tp ); <nl> VRB_INSERT ( t_order , & h_order , tp ); <nl> update ( int p ) <nl> if ( tp -> count * 10 < t || l > LINES * 10 ) { <nl> VRB_REMOVE ( t_key , & h_key , tp ); <nl> VRB_REMOVE ( t_order , & h_order , tp ); <nl> - free ( tp -> rec_data ); <nl> + free ( tp -> rec_buf ); <nl> free ( tp ); <nl> ntop --; <nl> }
mmm bin / varnishd / cache_cli . c <nl> ppp bin / varnishd / cache_cli . c <nl> CLI_Init ( void ) <nl> vsb_clear ( cli -> sb ); <nl> cli_dispatch ( cli , CLI_cmds , buf ); <nl> vsb_finish ( cli -> sb ); <nl> + AZ ( vsb_overflowed ( cli -> sb )); <nl> i = cli_writeres ( heritage . fds [ 1 ], cli ); <nl> if ( i ) { <nl> VSL ( SLT_Error , 0 , " CLI write failed ( errno =% d )", errno );mmm bin / varnishd / cache_synthetic . c <nl> ppp bin / varnishd / cache_synthetic . c <nl> CLI_Init ( void ) <nl> vsb_clear ( cli -> sb ); <nl> cli_dispatch ( cli , CLI_cmds , buf ); <nl> vsb_finish ( cli -> sb ); <nl> + AZ ( vsb_overflowed ( cli -> sb )); <nl> i = cli_writeres ( heritage . fds [ 1 ], cli ); <nl> if ( i ) { <nl> VSL ( SLT_Error , 0 , " CLI write failed ( errno =% d )", errno ); <nl> SYN_ErrorPage ( struct sess * sp , int status , const char * reason ) <nl> " </ body >\ n " <nl> "</ html >\ n "); <nl> vsb_finish (& vsb ); <nl> + AZ ( vsb_overflowed (& vsb )); <nl> w -> acct . hdrbytes = WRK_Write ( w , vsb_data (& vsb ), vsb_len (& vsb )); <nl> ( void ) WRK_Flush ( w ); <nl> vsb_delete (& vsb );mmm bin / varnishd / mgt_vcc . c <nl> ppp bin / varnishd / mgt_vcc . c <nl> CLI_Init ( void ) <nl> vsb_clear ( cli -> sb ); <nl> cli_dispatch ( cli , CLI_cmds , buf ); <nl> vsb_finish ( cli -> sb ); <nl> + AZ ( vsb_overflowed ( cli -> sb )); <nl> i = cli_writeres ( heritage . fds [ 1 ], cli ); <nl> if ( i ) { <nl> VSL ( SLT_Error , 0 , " CLI write failed ( errno =% d )", errno ); <nl> SYN_ErrorPage ( struct sess * sp , int status , const char * reason ) <nl> " </ body >\ n " <nl> "</ html >\ n "); <nl> vsb_finish (& vsb ); <nl> + AZ ( vsb_overflowed (& vsb )); <nl> w -> acct . hdrbytes = WRK_Write ( w , vsb_data (& vsb ), vsb_len (& vsb )); <nl> ( void ) WRK_Flush ( w ); <nl> vsb_delete (& vsb ); <nl> mgt_run_cc ( const char * source , struct vsb * sb ) <nl> vsb_new (& cmdsb , cmdline , sizeof cmdline , 0 ); <nl> mgt_make_cc_cmd (& cmdsb , sf , of ); <nl> vsb_finish (& cmdsb ); <nl> + AZ ( vsb_overflowed (& cmdsb )); <nl> /* XXX check vsb state */ <nl>  <nl> if ( pipe ( p ) < 0 ) { <nl> mgt_vcc_default ( const char * b_arg , const char * f_arg , int f_fd , int C_flag ) <nl> vf = mgt_VccCompileFile ( sb , f_arg , C_flag , f_fd ); <nl> } <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> fprintf ( stderr , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> mcf_config_inline ( struct cli * cli , const char * const * av , void * priv ) <nl> XXXAN ( sb ); <nl> vf = mgt_VccCompile ( sb , av [ 3 ], NULL , 0 ); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> cli_out ( cli , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> mcf_config_load ( struct cli * cli , const char * const * av , void * priv ) <nl> XXXAN ( sb ); <nl> vf = mgt_VccCompileFile ( sb , av [ 3 ], 0 , - 1 ); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> cli_out ( cli , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb );mmm bin / varnishd / cache_vary . c <nl> ppp bin / varnishd / cache_vary . c <nl> CLI_Init ( void ) <nl> vsb_clear ( cli -> sb ); <nl> cli_dispatch ( cli , CLI_cmds , buf ); <nl> vsb_finish ( cli -> sb ); <nl> + AZ ( vsb_overflowed ( cli -> sb )); <nl> i = cli_writeres ( heritage . fds [ 1 ], cli ); <nl> if ( i ) { <nl> VSL ( SLT_Error , 0 , " CLI write failed ( errno =% d )", errno ); <nl> SYN_ErrorPage ( struct sess * sp , int status , const char * reason ) <nl> " </ body >\ n " <nl> "</ html >\ n "); <nl> vsb_finish (& vsb ); <nl> + AZ ( vsb_overflowed (& vsb )); <nl> w -> acct . hdrbytes = WRK_Write ( w , vsb_data (& vsb ), vsb_len (& vsb )); <nl> ( void ) WRK_Flush ( w ); <nl> vsb_delete (& vsb ); <nl> mgt_run_cc ( const char * source , struct vsb * sb ) <nl> vsb_new (& cmdsb , cmdline , sizeof cmdline , 0 ); <nl> mgt_make_cc_cmd (& cmdsb , sf , of ); <nl> vsb_finish (& cmdsb ); <nl> + AZ ( vsb_overflowed (& cmdsb )); <nl> /* XXX check vsb state */ <nl>  <nl> if ( pipe ( p ) < 0 ) { <nl> mgt_vcc_default ( const char * b_arg , const char * f_arg , int f_fd , int C_flag ) <nl> vf = mgt_VccCompileFile ( sb , f_arg , C_flag , f_fd ); <nl> } <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> fprintf ( stderr , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> mcf_config_inline ( struct cli * cli , const char * const * av , void * priv ) <nl> XXXAN ( sb ); <nl> vf = mgt_VccCompile ( sb , av [ 3 ], NULL , 0 ); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> cli_out ( cli , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> mcf_config_load ( struct cli * cli , const char * const * av , void * priv ) <nl> XXXAN ( sb ); <nl> vf = mgt_VccCompileFile ( sb , av [ 3 ], 0 , - 1 ); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> cli_out ( cli , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> VRY_Create ( const struct sess * sp ) <nl> vsb_clear ( sbh ); <nl> vsb_printf ( sbh , "% c %.* s :% c ", 1 + ( q - p ), q - p , p , 0 ); <nl> vsb_finish ( sbh ); <nl> + AZ ( vsb_overflowed ( sbh )); <nl>  <nl> /* Append to vary matching string */ <nl> vsb_bcat ( sb , vsb_data ( sbh ), vsb_len ( sbh )); <nl> VRY_Create ( const struct sess * sp ) <nl> vsb_printf ( sb , "% c ", 0 ); <nl>  <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> l = vsb_len ( sb ); <nl> sp -> obj -> vary = malloc ( l ); <nl> AN ( sp -> obj -> vary );mmm bin / varnishd / mgt_cli . c <nl> ppp bin / varnishd / mgt_cli . c <nl> CLI_Init ( void ) <nl> vsb_clear ( cli -> sb ); <nl> cli_dispatch ( cli , CLI_cmds , buf ); <nl> vsb_finish ( cli -> sb ); <nl> + AZ ( vsb_overflowed ( cli -> sb )); <nl> i = cli_writeres ( heritage . fds [ 1 ], cli ); <nl> if ( i ) { <nl> VSL ( SLT_Error , 0 , " CLI write failed ( errno =% d )", errno ); <nl> SYN_ErrorPage ( struct sess * sp , int status , const char * reason ) <nl> " </ body >\ n " <nl> "</ html >\ n "); <nl> vsb_finish (& vsb ); <nl> + AZ ( vsb_overflowed (& vsb )); <nl> w -> acct . hdrbytes = WRK_Write ( w , vsb_data (& vsb ), vsb_len (& vsb )); <nl> ( void ) WRK_Flush ( w ); <nl> vsb_delete (& vsb ); <nl> mgt_run_cc ( const char * source , struct vsb * sb ) <nl> vsb_new (& cmdsb , cmdline , sizeof cmdline , 0 ); <nl> mgt_make_cc_cmd (& cmdsb , sf , of ); <nl> vsb_finish (& cmdsb ); <nl> + AZ ( vsb_overflowed (& cmdsb )); <nl> /* XXX check vsb state */ <nl>  <nl> if ( pipe ( p ) < 0 ) { <nl> mgt_vcc_default ( const char * b_arg , const char * f_arg , int f_fd , int C_flag ) <nl> vf = mgt_VccCompileFile ( sb , f_arg , C_flag , f_fd ); <nl> } <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> fprintf ( stderr , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> mcf_config_inline ( struct cli * cli , const char * const * av , void * priv ) <nl> XXXAN ( sb ); <nl> vf = mgt_VccCompile ( sb , av [ 3 ], NULL , 0 ); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> cli_out ( cli , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> mcf_config_load ( struct cli * cli , const char * const * av , void * priv ) <nl> XXXAN ( sb ); <nl> vf = mgt_VccCompileFile ( sb , av [ 3 ], 0 , - 1 ); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> cli_out ( cli , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> VRY_Create ( const struct sess * sp ) <nl> vsb_clear ( sbh ); <nl> vsb_printf ( sbh , "% c %.* s :% c ", 1 + ( q - p ), q - p , p , 0 ); <nl> vsb_finish ( sbh ); <nl> + AZ ( vsb_overflowed ( sbh )); <nl>  <nl> /* Append to vary matching string */ <nl> vsb_bcat ( sb , vsb_data ( sbh ), vsb_len ( sbh )); <nl> VRY_Create ( const struct sess * sp ) <nl> vsb_printf ( sb , "% c ", 0 ); <nl>  <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> l = vsb_len ( sb ); <nl> sp -> obj -> vary = malloc ( l ); <nl> AN ( sp -> obj -> vary ); <nl> mcf_passthru ( struct cli * cli , const char * const * av , void * priv ) <nl> vsb_putc ( sb , '\ n '); <nl> xxxassert (! vsb_overflowed ( sb )); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> i = write ( cli_o , vsb_data ( sb ), vsb_len ( sb )); <nl> xxxassert ( i == vsb_len ( sb )); <nl> vsb_delete ( sb ); <nl> mgt_cli_callback ( const struct ev * e , int what ) <nl> vsb_clear ( cp -> cli -> sb ); <nl> cli_dispatch ( cp -> cli , cli_proto , p ); <nl> vsb_finish ( cp -> cli -> sb ); <nl> + AZ ( vsb_overflowed ( cp -> cli -> sb )); <nl>  <nl> /* send the result back */ <nl> if ( cli_writeres ( cp -> fdo , cp -> cli ))mmm bin / varnishd / varnishd . c <nl> ppp bin / varnishd / varnishd . c <nl> CLI_Init ( void ) <nl> vsb_clear ( cli -> sb ); <nl> cli_dispatch ( cli , CLI_cmds , buf ); <nl> vsb_finish ( cli -> sb ); <nl> + AZ ( vsb_overflowed ( cli -> sb )); <nl> i = cli_writeres ( heritage . fds [ 1 ], cli ); <nl> if ( i ) { <nl> VSL ( SLT_Error , 0 , " CLI write failed ( errno =% d )", errno ); <nl> SYN_ErrorPage ( struct sess * sp , int status , const char * reason ) <nl> " </ body >\ n " <nl> "</ html >\ n "); <nl> vsb_finish (& vsb ); <nl> + AZ ( vsb_overflowed (& vsb )); <nl> w -> acct . hdrbytes = WRK_Write ( w , vsb_data (& vsb ), vsb_len (& vsb )); <nl> ( void ) WRK_Flush ( w ); <nl> vsb_delete (& vsb ); <nl> mgt_run_cc ( const char * source , struct vsb * sb ) <nl> vsb_new (& cmdsb , cmdline , sizeof cmdline , 0 ); <nl> mgt_make_cc_cmd (& cmdsb , sf , of ); <nl> vsb_finish (& cmdsb ); <nl> + AZ ( vsb_overflowed (& cmdsb )); <nl> /* XXX check vsb state */ <nl>  <nl> if ( pipe ( p ) < 0 ) { <nl> mgt_vcc_default ( const char * b_arg , const char * f_arg , int f_fd , int C_flag ) <nl> vf = mgt_VccCompileFile ( sb , f_arg , C_flag , f_fd ); <nl> } <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> fprintf ( stderr , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> mcf_config_inline ( struct cli * cli , const char * const * av , void * priv ) <nl> XXXAN ( sb ); <nl> vf = mgt_VccCompile ( sb , av [ 3 ], NULL , 0 ); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> cli_out ( cli , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> mcf_config_load ( struct cli * cli , const char * const * av , void * priv ) <nl> XXXAN ( sb ); <nl> vf = mgt_VccCompileFile ( sb , av [ 3 ], 0 , - 1 ); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> if ( vsb_len ( sb ) > 0 ) <nl> cli_out ( cli , "% s ", vsb_data ( sb )); <nl> vsb_delete ( sb ); <nl> VRY_Create ( const struct sess * sp ) <nl> vsb_clear ( sbh ); <nl> vsb_printf ( sbh , "% c %.* s :% c ", 1 + ( q - p ), q - p , p , 0 ); <nl> vsb_finish ( sbh ); <nl> + AZ ( vsb_overflowed ( sbh )); <nl>  <nl> /* Append to vary matching string */ <nl> vsb_bcat ( sb , vsb_data ( sbh ), vsb_len ( sbh )); <nl> VRY_Create ( const struct sess * sp ) <nl> vsb_printf ( sb , "% c ", 0 ); <nl>  <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> l = vsb_len ( sb ); <nl> sp -> obj -> vary = malloc ( l ); <nl> AN ( sp -> obj -> vary ); <nl> mcf_passthru ( struct cli * cli , const char * const * av , void * priv ) <nl> vsb_putc ( sb , '\ n '); <nl> xxxassert (! vsb_overflowed ( sb )); <nl> vsb_finish ( sb ); <nl> + AZ ( vsb_overflowed ( sb )); <nl> i = write ( cli_o , vsb_data ( sb ), vsb_len ( sb )); <nl> xxxassert ( i == vsb_len ( sb )); <nl> vsb_delete ( sb ); <nl> mgt_cli_callback ( const struct ev * e , int what ) <nl> vsb_clear ( cp -> cli -> sb ); <nl> cli_dispatch ( cp -> cli , cli_proto , p ); <nl> vsb_finish ( cp -> cli -> sb ); <nl> + AZ ( vsb_overflowed ( cp -> cli -> sb )); <nl>  <nl> /* send the result back */ <nl> if ( cli_writeres ( cp -> fdo , cp -> cli )) <nl> cli_check ( const struct cli * cli ) <nl> return ; <nl> } <nl> vsb_finish ( cli -> sb ); <nl> + AZ ( vsb_overflowed ( cli -> sb )); <nl> fprintf ( stderr , " Error :\ n % s \ n ", vsb_data ( cli -> sb )); <nl> exit ( 2 ); <nl> } <nl> main ( int argc , char * argv []) <nl> if ( cli [ 0 ]. result != CLIS_OK ) { <nl> fprintf ( stderr , " Parameter errors :\ n "); <nl> vsb_finish ( cli [ 0 ]. sb ); <nl> + AZ ( vsb_overflowed ( cli [ 0 ]. sb )); <nl> fprintf ( stderr , "% s \ n ", vsb_data ( cli [ 0 ]. sb )); <nl> exit ( 1 ); <nl> }
mmm bin / varnishd / cache_center . c <nl> ppp bin / varnishd / cache_center . c <nl> cnt_fetch ( struct sess * sp ) <nl> * Space for producing a Content - Length : header including padding <nl> * A billion gigabytes is enough for anybody . <nl> */ <nl> - l += strlen (" Content - Length : XxxXxxXxxXxxXxxXxx " + sizeof ( void *)); <nl> + l += strlen (" Content - Length : XxxXxxXxxXxxXxxXxx ") + sizeof ( void *); <nl>  <nl> if ( sp -> wrk -> ttl < sp -> t_req + params -> shortlived || <nl> sp -> objcore == NULL )
mmm lib / libvcc / vcc_symb . c <nl> ppp lib / libvcc / vcc_symb . c <nl> VCC_Symbol ( struct vcc * tl , struct symbol * parent , <nl> assert ( l > 0 ); <nl>  <nl> VTAILQ_FOREACH ( sym , & parent -> children , list ) { <nl> - if ( sym -> lorev > vhi || sym -> hirev < vlo ) <nl> - continue ; <nl> i = strncasecmp ( sym -> name , b , l ); <nl> if ( i < 0 ) <nl> continue ; <nl> VCC_Symbol ( struct vcc * tl , struct symbol * parent , <nl> } <nl> if ( l > sym -> nlen ) <nl> continue ; <nl> + if ( sym -> lorev > vhi || sym -> hirev < vlo ) <nl> + continue ; <nl> if ( q < e ) <nl> break ; <nl> if (( kind == SYM_NONE && kind == sym -> kind )) <nl> static void <nl> vcc_walksymbols ( struct vcc * tl , const struct symbol * root , <nl> symwalk_f * func , vcc_kind_t kind ) <nl> { <nl> - struct symbol * sym ; <nl> + struct symbol * sym , * sym2 = NULL ; <nl>  <nl> VTAILQ_FOREACH ( sym , & root -> children , list ) { <nl> + if ( sym2 != NULL ) <nl> + assert ( strcasecmp ( sym -> name , sym2 -> name ) >= 0 ); <nl> + sym2 = sym ; <nl> if ( kind == SYM_NONE || kind == sym -> kind ) <nl> func ( tl , sym ); <nl> ERRCHK ( tl );
mmm bin / varnishreplay / varnishreplay . c <nl> ppp bin / varnishreplay / varnishreplay . c <nl> replay_thread ( void * arg ) <nl> freez ( df_c ); <nl> bogus = 0 ; <nl> } <nl> + <nl> + /* leftovers */ <nl> + freez ( msg -> ptr ); <nl> + freez ( msg ); <nl> + freez ( df_H ); <nl> + freez ( df_Host ); <nl> + freez ( df_Uq ); <nl> + freez ( df_m ); <nl> + freez ( df_c ); <nl> + <nl> return ( 0 ); <nl> } <nl> 
mmm bin / varnishd / cache / cache_backend_cfg . c <nl> ppp bin / varnishd / cache / cache_backend_cfg . c <nl> cli_backend_set_health ( struct cli * cli , const char * const * av , void * priv ) <nl>  <nl> static struct cli_proto backend_cmds [] = { <nl> { " backend . list ", " backend . list ", <nl> - "\ tList all backends \ n ", 0 , 1 , " d ", cli_backend_list }, <nl> + "\ tList all backends \ n ", 0 , 1 , "", cli_backend_list }, <nl> { " backend . set_health ", " backend . set_health matcher state ", <nl> - "\ tShow a backend \ n ", 2 , 2 , " d ", cli_backend_set_health }, <nl> + "\ tShow a backend \ n ", 2 , 2 , "", cli_backend_set_health }, <nl> { NULL } <nl> }; <nl> 
mmm bin / varnishd / mgt_vcc . c <nl> ppp bin / varnishd / mgt_vcc . c <nl> mgt_CallCc ( const char * source , struct vsb * sb ) <nl> vsb_printf ( sb , <nl> " Cannot open temporary source file \"% s \": % s \ n ", <nl> sf , strerror ( errno )); <nl> - free ( sf ); <nl> return ( NULL ); <nl> } <nl> fs = fdopen ( sfd , " r +");
mmm bin / varnishncsa / varnishncsa . c <nl> ppp bin / varnishncsa / varnishncsa . c <nl> trimline ( const char * str , const char * end ) <nl> /* nothing */ ; <nl>  <nl> /* trim trailing space */ <nl> - while ( str [ len - 1 ] == ' ') <nl> + while ( len && str [ len - 1 ] == ' ') <nl> -- len ; <nl>  <nl> /* copy and return */
mmm bin / varnishd / varnishd . c <nl> ppp bin / varnishd / varnishd . c <nl> main ( int argc , char * const * argv ) <nl> if ( b_arg != NULL && f_arg != NULL ) { <nl> fprintf ( stderr , " Only one of - b or - f can be specified \ n "); <nl> usage (); <nl> - } else if ( S_arg == NULL && T_arg == NULL ) { <nl> + } <nl> + if ( S_arg == NULL && T_arg == NULL && d_flag == 0 && b_arg == NULL && <nl> + f_arg == NULL ) { <nl> fprintf ( stderr , <nl> " At least one of - d , - b , - f , - S or - T must be specified \ n "); <nl> usage ();
mmm bin / varnishd / cache / cache_vcl . c <nl> ppp bin / varnishd / cache / cache_vcl . c <nl> vcl_call_method ( struct worker * wrk , struct req * req , struct busyobj * bo , <nl> CHECK_OBJ_NOTNULL ( bo , BUSYOBJ_MAGIC ); <nl> vsl = bo -> vsl ; <nl> } <nl> + if ( method == VCL_MET_BACKEND_FETCH || <nl> + method == VCL_MET_PASS || <nl> + method == VCL_MET_MISS || <nl> + method == VCL_MET_PIPE || <nl> + method == VCL_MET_BACKEND_RESPONSE ) { <nl> + /* XXX : temporary workaround */ <nl> + AN ( req ); <nl> + bo = req -> busyobj ; <nl> + CHECK_OBJ_NOTNULL ( bo , BUSYOBJ_MAGIC ); <nl> + } <nl> aws = WS_Snapshot ( wrk -> aws ); <nl> wrk -> handling = 0 ; <nl> wrk -> cur_method = method ; <nl> VSLb ( vsl , SLT_VCL_call , "% s ", VCL_Method_Name ( method )); <nl> - ( void ) func ( wrk , req , NULL , ws ); <nl> + ( void ) func ( wrk , req , bo , ws ); <nl> VSLb ( vsl , SLT_VCL_return , "% s ", VCL_Return_Name ( wrk -> handling )); <nl> wrk -> cur_method = 0 ; <nl> WS_Reset ( wrk -> aws , aws );
mmm bin / varnishd / cache_session . c <nl> ppp bin / varnishd / cache_session . c <nl> SES_RefSrcAddr ( struct sess * sp ) <nl> c3 = c ; <nl> continue ; <nl> } <nl> - TAILQ_REMOVE ( ch , c2 , list ); <nl> - free ( c2 ); <nl> + TAILQ_REMOVE ( ch , c , list ); <nl> + free ( c ); <nl> VSL_stats -> n_srcaddr --; <nl> } <nl> if ( c3 == NULL ) {
mmm bin / varnishd / cache_httpd . c <nl> ppp bin / varnishd / cache_httpd . c <nl> HttpdAnalyze ( struct sess * sp ) <nl>  <nl> sp -> handling = HND_Unclass ; <nl>  <nl> + memset (& sp -> http , 0 , sizeof sp -> http ); <nl> + <nl> /* First , isolate and possibly identify request type */ <nl> sp -> http . req = sp -> rcv ; <nl> for ( p = sp -> rcv ; isalpha (* p ); p ++) <nl> HttpdAnalyze ( struct sess * sp ) <nl> if (* p == '\ r ') <nl> p ++; <nl>  <nl> - memset (& sp -> http , 0 , sizeof sp -> http ); <nl> - <nl> for (; p < sp -> rcv + sp -> rcv_len ; p = r ) { <nl> q = strchr ( p , '\ n '); <nl> r = q + 1 ;
mmm bin / varnishd / cache / cache_req_fsm . c <nl> ppp bin / varnishd / cache / cache_req_fsm . c <nl> CNT_AcctLogCharge ( struct dstat * ds , struct req * req ) <nl>  <nl> a = & req -> acct ; <nl>  <nl> - if (!( req -> res_mode & RES_PIPE )) { <nl> + if ( req -> vsl -> wid && !( req -> res_mode & RES_PIPE )) { <nl> VSLb ( req -> vsl , SLT_ReqAcct , "% ju % ju % ju % ju % ju % ju ", <nl> ( uintmax_t ) a -> req_hdrbytes , <nl> ( uintmax_t ) a -> req_bodybytes ,
mmm bin / varnishd / mgt / mgt_main . c <nl> ppp bin / varnishd / mgt / mgt_main . c <nl> main ( int argc , char * const * argv ) <nl> assert ( VTIM_parse (" Sunday , 06 - Nov - 94 08 : 49 : 37 GMT ") == 784111777 ); <nl> assert ( VTIM_parse (" Sun Nov 6 08 : 49 : 37 1994 ") == 784111777 ); <nl>  <nl> - /* <nl> - * Check that our SHA256 works <nl> - */ <nl> + /* Check that our SHA256 works */ <nl> SHA256_Test (); <nl>  <nl> - /* <nl> - * Create a cli for convenience in otherwise CLI functions <nl> - */ <nl> - <nl> + /* Create a cli for convenience in otherwise CLI functions */ <nl> INIT_OBJ ( cli , CLI_MAGIC ); <nl> cli [ 0 ]. sb = VSB_new_auto (); <nl> XXXAN ( cli [ 0 ]. sb ); <nl> main ( int argc , char * const * argv ) <nl> clilim = 32768 ; <nl> cli [ 0 ]. limit = & clilim ; <nl>  <nl> + /* Various initializations */ <nl> VTAILQ_INIT (& heritage . socks ); <nl> + mgt_evb = vev_new_base (); <nl> + AN ( mgt_evb ); <nl>  <nl> init_params ( cli ); <nl> cli_check ( cli ); <nl> main ( int argc , char * const * argv ) <nl>  <nl> mgt_pid = getpid (); /* daemon () changed this */ <nl>  <nl> - mgt_evb = vev_new_base (); <nl> - XXXAN ( mgt_evb ); <nl> - <nl> if ( d_flag ) <nl> mgt_cli_setup ( 0 , 1 , 1 , " debug ", cli_stdin_close , NULL ); <nl> 
mmm bin / varnishd / cache_pool . c <nl> ppp bin / varnishd / cache_pool . c <nl> WRK_QueueSession ( struct sess * sp ) <nl> unsigned onq ; <nl>  <nl> onq = nq + 1 ; <nl> - if ( onq > nwq ) <nl> + if ( onq >= nwq ) <nl> onq = 0 ; <nl> sp -> workreq . sess = sp ; <nl> qp = wq [ onq ];
mmm bin / varnishd / mgt_child . c <nl> ppp bin / varnishd / mgt_child . c <nl> mgt_sigchld ( struct ev * e , int what ) <nl> ev_poker = NULL ; <nl>  <nl> r = wait4 (- 1 , & status , WNOHANG , NULL ); <nl> - if ( r != child_pid ) { <nl> + if ( r != child_pid || r == - 1 ) { <nl> fprintf ( stderr , " Unknown child died pid =% d status = 0x % x \ n ", <nl> r , status ); <nl> return ( 0 );
mmm bin / varnishd / cache_hash . c <nl> ppp bin / varnishd / cache_hash . c <nl> HSH_Deref ( struct object * o ) <nl> } <nl> assert ( o -> refcnt > 0 ); <nl> r = -- o -> refcnt ; <nl> - hsh_rush ( oh ); <nl> + if ( oh != NULL ) <nl> + hsh_rush ( oh ); <nl> if ( oh != NULL ) { <nl> if (! r ) <nl> VTAILQ_REMOVE (& oh -> objects , o , list );
mmm lib / libvmod_directors / shard_cfg . c <nl> ppp lib / libvmod_directors / shard_cfg . c <nl> shardcfg_backend_cmp ( const struct shard_backend * a , <nl> ai = a -> ident ; <nl> bi = b -> ident ; <nl>  <nl> + assert ( ai || a -> backend ); <nl> + assert ( bi || b -> backend ); <nl> + <nl> /* vcl_names are unique , so we can compare the backend pointers */ <nl> if ( ai == NULL && bi == NULL ) <nl> return a -> backend != b -> backend ; <nl> static int <nl> shardcfg_backend_del_cmp ( const struct shard_backend * task , <nl> const struct shard_backend * b ) <nl> { <nl> - if ( task -> backend && task -> ident == NULL ) <nl> + assert ( task -> backend || task -> ident ); <nl> + <nl> + if ( task -> ident == NULL ) <nl> return task -> backend != b -> backend ; <nl>  <nl> return shardcfg_backend_cmp ( task , b );
mmm bin / varnishd / mgt_event . c <nl> ppp bin / varnishd / mgt_event . c <nl> ev_compact_pfd ( struct evbase * evb ) <nl> DBG ( evb , "...[% d ] fd = % d \ n ", u , p -> fd ); <nl> if ( p -> fd >= 0 ) <nl> continue ; <nl> + if ( u == evb -> lpfd - 1 ) <nl> + break ; <nl> lfd = evb -> pfd [ evb -> lpfd - 1 ]. fd ; <nl> VTAILQ_FOREACH ( ep , & evb -> events , __list ) <nl> if ( ep -> fd == lfd )
mmm bin / varnishd / cache / cache_vrt . c <nl> ppp bin / varnishd / cache / cache_vrt . c <nl> VRT_acl_log ( VRT_CTX , const char * msg ) <nl> { <nl>  <nl> CHECK_OBJ_NOTNULL ( ctx , VRT_CTX_MAGIC ); <nl> - VSLb ( ctx -> vsl , SLT_VCL_acl , "% s ", msg ); <nl> + AN ( msg ); <nl> + if ( ctx -> vsl != NULL ) <nl> + VSLb ( ctx -> vsl , SLT_VCL_acl , "% s ", msg ); <nl> + else <nl> + VSL ( SLT_VCL_acl , 0 , "% s ", msg ); <nl> } <nl>  <nl> /*--------------------------------------------------------------------*/
mmm bin / varnishd / cache_fetch . c <nl> ppp bin / varnishd / cache_fetch . c <nl> FetchBody ( struct sess * sp ) <nl> } else <nl> cls = 0 ; <nl>  <nl> + { <nl> + /* Sanity check fetch methods accounting */ <nl> + struct storage * st ; <nl> + unsigned uu ; <nl> + <nl> + uu = 0 ; <nl> + TAILQ_FOREACH ( st , & sp -> obj -> store , list ) <nl> + uu += st -> len ; <nl> + assert ( uu == sp -> obj -> len ); <nl> + } <nl> + <nl> http_CopyHttp (& sp -> obj -> http , hp ); <nl>  <nl> if ( http_GetHdr ( vc -> http , H_Connection , & b ) && ! strcasecmp ( b , " close "))
mmm bin / varnishhist / varnishhist . c <nl> ppp bin / varnishhist / varnishhist . c <nl> r_hist ( void ) <nl> r = y * m ; <nl> for ( x = 0 ; x < HIST_W ; x ++) { <nl> if ( bucket_miss [ x ] > r ) <nl> - addch ('|'); <nl> - else if ( bucket_hit [ x ] + bucket_miss [ x ] > r ) <nl> addch ('#'); <nl> + else if ( bucket_hit [ x ] + bucket_miss [ x ] > r ) <nl> + addch ('|'); <nl> else <nl> addch (' '); <nl> }
mmm bin / varnishd / waiter / cache_waiter_kqueue . c <nl> ppp bin / varnishd / waiter / cache_waiter_kqueue . c <nl> vwk_fini ( struct waiter * w ) <nl> vwk -> kq = - 1 ; <nl> Lck_Unlock (& vwk -> mtx ); <nl> AZ ( pthread_join ( vwk -> thread , & vp )); <nl> + Lck_Delete (& vwk -> mtx ); <nl> } <nl>  <nl> /*--------------------------------------------------------------------*/
mmm bin / varnishd / varnishd . c <nl> ppp bin / varnishd / varnishd . c <nl> DebugStunt ( void ) <nl> d_child = strtoul ( buf , & p , 0 ); <nl> assert ( p != NULL ); <nl> printf (" New Pid % d \ n ", d_child ); <nl> + assert ( d_child != 0 ); <nl> i = strlen ( p ); <nl> j = write ( pipes [ 1 ][ 1 ], p , i ); <nl> assert ( j == i );
mmm bin / varnishd / mgt_vcc . c <nl> ppp bin / varnishd / mgt_vcc . c <nl> mgt_vcc_default ( const char * b_arg , const char * f_arg , int C_flag ) <nl> if ( C_flag ) { <nl> csrc = VCC_Compile ( sb , buf , NULL ); <nl> fputs ( csrc , stdout ); <nl> - exit ( 0 ); <nl> + return ( 0 ); <nl> } <nl> vf = mgt_VccCompile ( sb , buf , NULL ); <nl> free ( buf ); <nl> } else if ( C_flag ) { <nl> csrc = VCC_CompileFile ( sb , f_arg ); <nl> fputs ( csrc , stdout ); <nl> - exit ( 0 ); <nl> + return ( 0 ); <nl> } else { <nl> vf = mgt_VccCompileFile ( sb , f_arg ); <nl> }mmm bin / varnishd / varnishd . c <nl> ppp bin / varnishd / varnishd . c <nl> mgt_vcc_default ( const char * b_arg , const char * f_arg , int C_flag ) <nl> if ( C_flag ) { <nl> csrc = VCC_Compile ( sb , buf , NULL ); <nl> fputs ( csrc , stdout ); <nl> - exit ( 0 ); <nl> + return ( 0 ); <nl> } <nl> vf = mgt_VccCompile ( sb , buf , NULL ); <nl> free ( buf ); <nl> } else if ( C_flag ) { <nl> csrc = VCC_CompileFile ( sb , f_arg ); <nl> fputs ( csrc , stdout ); <nl> - exit ( 0 ); <nl> + return ( 0 ); <nl> } else { <nl> vf = mgt_VccCompileFile ( sb , f_arg ); <nl> } <nl> main ( int argc , char * argv []) <nl> const char * h_flag = " classic "; <nl> const char * s_arg = " file "; <nl> const char * T_arg = NULL ; <nl> - unsigned C_flag ; <nl> + unsigned C_flag = 0 ; <nl> char * p ; <nl> struct params param ; <nl> struct cli cli [ 1 ]; <nl> main ( int argc , char * argv []) <nl>  <nl> if ( mgt_vcc_default ( b_arg , f_arg , C_flag )) <nl> exit ( 2 ); <nl> + if ( C_flag ) <nl> + exit ( 0 ); <nl>  <nl> setup_storage ( s_arg ); <nl> setup_hash ( h_flag );
mmm bin / varnishd / cache / cache_req_fsm . c <nl> ppp bin / varnishd / cache / cache_req_fsm . c <nl> CNT_Request ( struct worker * wrk , struct req * req ) <nl> CHECK_OBJ_ORNULL ( wrk -> nobjhead , OBJHEAD_MAGIC ); <nl> CHECK_OBJ_NOTNULL ( req , REQ_MAGIC ); <nl>  <nl> - /* <nl> - * We don ' t want the thread workspace to be used for <nl> - * anything of long duration , so mandate that it be <nl> - * empty on state - transitions . <nl> - */ <nl> - WS_Assert ( wrk -> aws ); <nl> - AZ ( WS_Snapshot ( wrk -> aws )); <nl> - <nl> switch ( req -> req_step ) { <nl> # define REQ_STEP ( l , u , arg ) \ <nl> case R_STP_ ## u : \ <nl> CNT_Request ( struct worker * wrk , struct req * req ) <nl> default : <nl> WRONG (" State engine misfire "); <nl> } <nl> - WS_Assert ( wrk -> aws ); <nl> CHECK_OBJ_ORNULL ( wrk -> nobjhead , OBJHEAD_MAGIC ); <nl> } <nl> wrk -> vsl = NULL ;mmm bin / varnishd / cache / cache_req . c <nl> ppp bin / varnishd / cache / cache_req . c <nl> CNT_Request ( struct worker * wrk , struct req * req ) <nl> CHECK_OBJ_ORNULL ( wrk -> nobjhead , OBJHEAD_MAGIC ); <nl> CHECK_OBJ_NOTNULL ( req , REQ_MAGIC ); <nl>  <nl> - /* <nl> - * We don ' t want the thread workspace to be used for <nl> - * anything of long duration , so mandate that it be <nl> - * empty on state - transitions . <nl> - */ <nl> - WS_Assert ( wrk -> aws ); <nl> - AZ ( WS_Snapshot ( wrk -> aws )); <nl> - <nl> switch ( req -> req_step ) { <nl> # define REQ_STEP ( l , u , arg ) \ <nl> case R_STP_ ## u : \ <nl> CNT_Request ( struct worker * wrk , struct req * req ) <nl> default : <nl> WRONG (" State engine misfire "); <nl> } <nl> - WS_Assert ( wrk -> aws ); <nl> CHECK_OBJ_ORNULL ( wrk -> nobjhead , OBJHEAD_MAGIC ); <nl> } <nl> wrk -> vsl = NULL ; <nl> Req_Cleanup ( struct sess * sp , struct worker * wrk , struct req * req ) <nl> req -> is_hit = 0 ; <nl>  <nl> WS_Reset ( req -> ws , 0 ); <nl> - WS_Reset ( wrk -> aws , 0 ); <nl> } <nl>  <nl> /*----------------------------------------------------------------------
mmm include / tbl / req_body . h <nl> ppp include / tbl / req_body . h <nl> REQ_BODY ( PRESENT ) <nl> REQ_BODY ( CHUNKED ) <nl> REQ_BODY ( TAKEN ) <nl> REQ_BODY ( CACHED ) <nl> - REQ_BODY ( DONE ) <nl> REQ_BODY ( FAIL ) <nl> REQ_BODY ( NONE ) <nl> mmm bin / varnishd / cache / cache_http1_fsm . c <nl> ppp bin / varnishd / cache / cache_http1_fsm . c <nl> REQ_BODY ( PRESENT ) <nl> REQ_BODY ( CHUNKED ) <nl> REQ_BODY ( TAKEN ) <nl> REQ_BODY ( CACHED ) <nl> - REQ_BODY ( DONE ) <nl> REQ_BODY ( FAIL ) <nl> REQ_BODY ( NONE ) <nl>  <nl> HTTP1_IterateReqBody ( struct req * req , req_body_iter_f * func , void * priv ) <nl> case REQ_BODY_PRESENT : <nl> case REQ_BODY_CHUNKED : <nl> break ; <nl> - case REQ_BODY_DONE : <nl> case REQ_BODY_TAKEN : <nl> VSLb ( req -> vsl , SLT_VCL_Error , <nl> " Uncached req . body can only be consumed once ."); <nl> int <nl> HTTP1_DiscardReqBody ( struct req * req ) <nl> { <nl>  <nl> - if ( req -> req_body_status == REQ_BODY_DONE ) <nl> - return ( 0 ); <nl> if ( req -> req_body_status == REQ_BODY_FAIL ) <nl> return ( 0 ); <nl> if ( req -> req_body_status == REQ_BODY_TAKEN )mmm bin / varnishd / cache / cache_http1_fetch . c <nl> ppp bin / varnishd / cache / cache_http1_fetch . c <nl> REQ_BODY ( PRESENT ) <nl> REQ_BODY ( CHUNKED ) <nl> REQ_BODY ( TAKEN ) <nl> REQ_BODY ( CACHED ) <nl> - REQ_BODY ( DONE ) <nl> REQ_BODY ( FAIL ) <nl> REQ_BODY ( NONE ) <nl>  <nl> HTTP1_IterateReqBody ( struct req * req , req_body_iter_f * func , void * priv ) <nl> case REQ_BODY_PRESENT : <nl> case REQ_BODY_CHUNKED : <nl> break ; <nl> - case REQ_BODY_DONE : <nl> case REQ_BODY_TAKEN : <nl> VSLb ( req -> vsl , SLT_VCL_Error , <nl> " Uncached req . body can only be consumed once ."); <nl> int <nl> HTTP1_DiscardReqBody ( struct req * req ) <nl> { <nl>  <nl> - if ( req -> req_body_status == REQ_BODY_DONE ) <nl> - return ( 0 ); <nl> if ( req -> req_body_status == REQ_BODY_FAIL ) <nl> return ( 0 ); <nl> if ( req -> req_body_status == REQ_BODY_TAKEN ) <nl> V1F_fetch_hdr ( struct worker * wrk , struct busyobj * bo , struct req * req ) <nl> } else { <nl> i = HTTP1_IterateReqBody ( req , vbf_iter_req_body , wrk ); <nl> } <nl> - if ( req -> req_body_status == REQ_BODY_DONE ) { <nl> + if ( req -> req_body_status == REQ_BODY_TAKEN ) { <nl> retry = - 1 ; <nl> } else if ( req -> req_body_status == REQ_BODY_FAIL ) { <nl> VSLb ( bo -> vsl , SLT_FetchError ,
mmm bin / varnishd / mgt / mgt_shmem . c <nl> ppp bin / varnishd / mgt / mgt_shmem . c <nl> void <nl> mgt_shm_atexit ( void ) <nl> { <nl>  <nl> + /* Do not let VCC kill our VSM */ <nl> + if ( getpid () != mgt_pid ) <nl> + return ; <nl> if ( heritage . vsm != NULL ) <nl> VSM_common_delete (& heritage . vsm ); <nl> }
mmm bin / varnishd / cache / cache_vary . c <nl> ppp bin / varnishd / cache / cache_vary . c <nl> VRY_Match ( struct req * req , const uint8_t * vary ) <nl> vsp [ ln + 1 ] = 0xff ; <nl> vsp [ ln + 2 ] = 0 ; <nl> VRY_Validate ( vsp ); <nl> - req -> vary_l = vsp + 3 ; <nl> + req -> vary_l = vsp + ln + 3 ; <nl>  <nl> i = vry_cmp ( vary , vsp ); <nl> assert ( i == 0 || i == 2 );
mmm bin / varnishd / cache / cache_center . c <nl> ppp bin / varnishd / cache / cache_center . c <nl> cnt_prepresp ( struct sess * sp , struct worker * wrk , struct req * req ) <nl> break ; <nl> if ( bo != NULL ) { <nl> AN ( bo -> do_stream ); <nl> - VDI_CloseFd (& bo -> vbc ); <nl> HSH_Drop ( wrk , & sp -> req -> obj ); <nl> VBO_DerefBusyObj ( wrk , & bo ); <nl> } else {
mmm lib / libvcc / vcc_action . c <nl> ppp lib / libvcc / vcc_action . c <nl> parse_new ( struct vcc * tl ) <nl> vcc_ErrWhere ( tl , tl -> t ); <nl> return ; <nl> } <nl> - XXXAZ ( sy1 ); <nl>  <nl> sy1 = VCC_AddSymbolTok ( tl , tl -> t , SYM_NONE ); // XXX : NONE ? <nl> XXXAN ( sy1 );
mmm bin / varnishd / cache_acceptor . c <nl> ppp bin / varnishd / cache_acceptor . c <nl> vca_write_obj ( struct worker * w , struct sess * sp ) <nl> sp -> obj -> age + sp -> t_req - sp -> obj -> entered ); <nl> sbuf_printf ( w -> sb , " Via : 1 . 1 varnish \ r \ n "); <nl> sbuf_printf ( w -> sb , " X - Varnish : xid % u \ r \ n ", sp -> obj -> xid ); <nl> + if ( http_GetProto ( sp -> http , & r ) && strcmp ( r , " HTTP / 1 . 1 ")) <nl> + sbuf_printf ( w -> sb , " Connection : close \ r \ n "); <nl> sbuf_printf ( w -> sb , "\ r \ n "); <nl> sbuf_finish ( w -> sb ); <nl> vca_write ( sp , sbuf_data ( w -> sb ), sbuf_len ( w -> sb ));
mmm bin / varnishd / tcp . c <nl> ppp bin / varnishd / tcp . c <nl> # include < errno . h > <nl> # include < netdb . h > <nl> # include < stdio . h > <nl> +# include < stdlib . h > <nl> # include < string . h > <nl> # include < unistd . h > <nl>  <nl> accept_filter ( int fd ) <nl> } <nl> # endif <nl>  <nl> + static char * <nl> + strndup ( const char * p , unsigned n ) <nl> +{ <nl> + char * q ; <nl> + <nl> + q = malloc ( n + 1 ); <nl> + if ( q != NULL ) { <nl> + memcpy ( q , p , n ); <nl> + q [ n ] = '\ 0 '; <nl> + } <nl> + return ( q ); <nl> +} <nl> + <nl> int <nl> TCP_parse ( const char * str , char ** addr , char ** port ) <nl> {
mmm bin / varnishncsa / varnishncsa . c <nl> ppp bin / varnishncsa / varnishncsa . c <nl> h_ncsa ( void * priv , enum VSL_tag_e tag , unsigned fd , <nl> const char * p ; <nl> struct vsb * os ; <nl>  <nl> + /* XXX : Ignore fd ' s outside 65536 */ <nl> + if ( fd >= 65536 ) <nl> + return ( reopen ); <nl> + <nl> if ( fd >= nll ) { <nl> struct logline ** newll = ll ; <nl> size_t newnll = nll ;
mmm lib / libvarnish / cli_common . c <nl> ppp lib / libvarnish / cli_common . c <nl> cli_readres ( int fd , unsigned * status , char ** ptr , double tmo ) <nl> * status = CLIS_COMMS ; <nl> if ( ptr != NULL ) <nl> * ptr = strdup (" CLI communication error ( hdr )"); <nl> - return ( 1 ); <nl> + if ( i != 0 ) <nl> + return ( i ); <nl> + return ( 400 ); <nl> } <nl> assert ( i == CLI_LINE0_LEN ); <nl> assert ( res [ 3 ] == ' ');mmm bin / varnishtest / vtc_varnish . c <nl> ppp bin / varnishtest / vtc_varnish . c <nl> cli_readres ( int fd , unsigned * status , char ** ptr , double tmo ) <nl> * status = CLIS_COMMS ; <nl> if ( ptr != NULL ) <nl> * ptr = strdup (" CLI communication error ( hdr )"); <nl> - return ( 1 ); <nl> + if ( i != 0 ) <nl> + return ( i ); <nl> + return ( 400 ); <nl> } <nl> assert ( i == CLI_LINE0_LEN ); <nl> assert ( res [ 3 ] == ' '); <nl> varnish_ask_cli ( const struct varnish * v , const char * cmd , char ** repl ) <nl> assert ( i == strlen ( cmd )); <nl> i = write ( v -> cli_fd , "\ n ", 1 ); <nl> assert ( i == 1 ); <nl> - i = cli_readres ( v -> cli_fd , & retval , & r , 10 . 0 ); <nl> + i = cli_readres ( v -> cli_fd , & retval , & r , 20 . 0 ); <nl> if ( i != 0 ) { <nl> vtc_log ( v -> vl , 0 , " CLI failed (% s ) = % d % u % s ", <nl> cmd , i , retval , r ); <nl> varnish_start ( struct varnish * v ) <nl> return ; <nl> vtc_log ( v -> vl , 2 , " Start "); <nl> u = varnish_ask_cli ( v , " start ", NULL ); <nl> + if ( vtc_error ) <nl> + return ; <nl> assert ( u == CLIS_OK ); <nl> u = varnish_ask_cli ( v , " debug . xid 1000 ", NULL ); <nl> + if ( vtc_error ) <nl> + return ; <nl> assert ( u == CLIS_OK ); <nl> } <nl> 
mmm bin / varnishd / http2 / cache_http2_proto . c <nl> ppp bin / varnishd / http2 / cache_http2_proto . c <nl> h2_end_headers ( struct worker * wrk , struct h2_sess * h2 , <nl> assert ( r2 -> state == H2_S_OPEN ); <nl> h2e = h2h_decode_fini ( h2 , r2 -> decode ); <nl> FREE_OBJ ( r2 -> decode ); <nl> - if ( h2 -> rxf_flags & H2FF_HEADERS_END_STREAM ) <nl> - r2 -> state = H2_S_CLOS_REM ; <nl> h2 -> new_req = NULL ; <nl> + if ( r2 -> req -> req_body_status == REQ_BODY_NONE ) { <nl> + /* REQ_BODY_NONE implies one of the frames in the <nl> + * header block contained END_STREAM */ <nl> + r2 -> state = H2_S_CLOS_REM ; <nl> + } <nl> if ( h2e != NULL ) { <nl> Lck_Lock (& h2 -> sess -> mtx ); <nl> VSLb ( h2 -> vsl , SLT_Debug , " HPACK / FINI % s ", h2e -> name );
mmm bin / varnishd / storage / storage_umem . c <nl> ppp bin / varnishd / storage / storage_umem . c <nl> static void v_matchproto_ ( storage_open_f ) <nl> smu_open ( struct stevedore * st ) <nl> { <nl> struct smu_sc * smu_sc ; <nl> + char ident [ strlen ( st -> ident ) + 1 ]; <nl>  <nl> ASSERT_CLI (); <nl> st -> lru = LRU_Alloc (); <nl> smu_open ( struct stevedore * st ) <nl>  <nl> smu_open_init (); <nl>  <nl> - smu_sc -> smu_cache = umem_cache_createf ( st -> ident , <nl> + AN ( strcpy ( ident , st -> ident )); <nl> + smu_sc -> smu_cache = umem_cache_createf ( ident , <nl> sizeof ( struct smu ), <nl> 0 , // align <nl> smu_smu_constructor ,
mmm bin / varnishd / cache_waiter_poll . c <nl> ppp bin / varnishd / cache_waiter_poll . c <nl> vca_main ( void * arg ) <nl> continue ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> vca_unpoll ( fd ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> }mmm bin / varnishd / cache_waiter_kqueue . c <nl> ppp bin / varnishd / cache_waiter_kqueue . c <nl> vca_main ( void * arg ) <nl> continue ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> vca_unpoll ( fd ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> } <nl> vca_kqueue_main ( void * arg ) <nl> if ( sp -> t_open > deadline ) <nl> break ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> }mmm bin / varnishd / cache_waiter_epoll . c <nl> ppp bin / varnishd / cache_waiter_epoll . c <nl> vca_main ( void * arg ) <nl> continue ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> vca_unpoll ( fd ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> } <nl> vca_kqueue_main ( void * arg ) <nl> if ( sp -> t_open > deadline ) <nl> break ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> } <nl> vca_main ( void * arg ) <nl> if ( sp -> t_open > deadline ) <nl> break ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> }mmm bin / varnishd / cache_waiter_ports . c <nl> ppp bin / varnishd / cache_waiter_ports . c <nl> vca_main ( void * arg ) <nl> continue ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> vca_unpoll ( fd ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> } <nl> vca_kqueue_main ( void * arg ) <nl> if ( sp -> t_open > deadline ) <nl> break ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> } <nl> vca_main ( void * arg ) <nl> if ( sp -> t_open > deadline ) <nl> break ; <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> } <nl> vca_main ( void * arg ) <nl> VTAILQ_REMOVE (& sesshead , sp , list ); <nl> if ( sp -> fd != - 1 ) <nl> vca_del ( sp -> fd ); <nl> + TCP_linger ( sp -> fd , 0 ); <nl> vca_close_session ( sp , " timeout "); <nl> SES_Delete ( sp ); <nl> }
mmm bin / varnishd / cache / cache_director . c <nl> ppp bin / varnishd / cache / cache_director . c <nl> do_list ( struct cli * cli , struct director * d , void * priv ) <nl> if ( d -> vdir -> admin_health == VDI_AH_DELETED ) <nl> return ( 0 ); <nl>  <nl> + // XXX admin health " probe " for the no - probe case is confusing <nl> VCLI_Out ( cli , "\ n %- 30s %- 7s ", d -> vdir -> cli_name , VDI_Ahealth ( d )); <nl>  <nl> if ( d -> vdir -> methods -> list != NULL )
mmm bin / varnishd / cache_http . c <nl> ppp bin / varnishd / cache_http . c <nl> http_Read ( struct http * hp , int fd , void * p , unsigned len ) <nl> b += u ; <nl> len -= u ; <nl> } <nl> - if ( len > 0 ) { <nl> + while ( len > 0 ) { <nl> i = read ( fd , b , len ); <nl> - if ( i < 0 ) <nl> + if ( i <= 0 ) <nl> return ( i ); <nl> u += i ; <nl> + len -= u ; <nl> } <nl> return ( u ); <nl> }
mmm bin / varnishncsa / varnishncsa . c <nl> ppp bin / varnishncsa / varnishncsa . c <nl> dispatch_f ( struct VSL_data * vsl , struct VSL_transaction * const pt [], <nl> for ( t = pt [ 0 ]; t != NULL ; t = *++ pt ) { <nl> CTX . gen ++; <nl> if ( t -> type != VSL_t_req ) <nl> + /* Only look at client requests */ <nl> + continue ; <nl> + if ( t -> reason == VSL_r_esi ) <nl> + /* Skip ESI requests */ <nl> continue ; <nl> CTX . hitmiss = "-"; <nl> CTX . handling = "-";
mmm src / txn . cpp <nl> ppp src / txn . cpp <nl> Nan :: NAN_METHOD_RETURN_TYPE TxnWrap :: putCommon ( Nan :: NAN_METHOD_ARGS_TYPE info , v <nl> } <nl>  <nl> NAN_METHOD ( TxnWrap :: putString ) { <nl> + if (! info [ 2 ]-> IsString ()) <nl> + return Nan :: ThrowError (" Value must be a string ."); <nl> return putCommon ( info , []( Nan :: NAN_METHOD_ARGS_TYPE info , MDB_val & data ) -> void { <nl> CustomExternalStringResource :: writeTo ( Local < String >:: Cast ( info [ 2 ]), & data ); <nl> }, []( MDB_val & data ) -> void {
mmm src / mapi_attr . c <nl> ppp src / mapi_attr . c <nl> mapi_attr_read ( size_t len , unsigned char * buf ) <nl> uint32 i , j ; <nl> assert ( len > 4 ); <nl> uint32 num_properties = GETINT32 ( buf + idx ); <nl> + assert (( num_properties + 1 ) != 0 ); <nl> MAPI_Attr ** attrs = CHECKED_XMALLOC ( MAPI_Attr *, ( num_properties + 1 )); <nl>  <nl> idx += 4 ; <nl> mapi_attr_read ( size_t len , unsigned char * buf ) <nl> /* read the data into a buffer */ <nl> a -> names [ i ]. data <nl> = CHECKED_XMALLOC ( unsigned char , a -> names [ i ]. len ); <nl> + assert (( idx +( a -> names [ i ]. len * 2 )) <= len ); <nl> for ( j = 0 ; j < ( a -> names [ i ]. len >> 1 ); j ++) <nl> a -> names [ i ]. data [ j ] = ( buf + idx )[ j * 2 ]; <nl>  <nl> mapi_attr_read ( size_t len , unsigned char * buf ) <nl> case szMAPI_BINARY : <nl> CHECKINT32 ( idx , len ); v -> len = GETINT32 ( buf + idx ); idx += 4 ; <nl>  <nl> + assert ( v -> len + idx <= len ); <nl> + <nl> if ( a -> type == szMAPI_UNICODE_STRING ) <nl> { <nl> + assert ( v -> len != 0 ); <nl> v -> data . buf = ( unsigned char *) unicode_to_utf8 ( v -> len , buf + idx ); <nl> } <nl> else
mmm modules / stream_out / rtpfmt . c <nl> ppp modules / stream_out / rtpfmt . c <nl> int rtp_packetize_xiph_config ( sout_stream_id_sys_t * id , const char * fmtp , <nl> char * end = strchr ( start , ';'); <nl> assert ( end != NULL ); <nl> size_t len = end - start ; <nl> - char b64 [ len + 1 ]; <nl> + <nl> + char * b64 = malloc ( len + 1 ); <nl> + if (! b64 ) <nl> + return VLC_EGENERIC ; <nl> + <nl> memcpy ( b64 , start , len ); <nl> b64 [ len ] = '\ 0 '; <nl>  <nl> int rtp_packetize_xiph_config ( sout_stream_id_sys_t * id , const char * fmtp , <nl> int i_data ; <nl>  <nl> i_data = vlc_b64_decode_binary (& p_orig , b64 ); <nl> + free ( b64 ); <nl> if ( i_data <= 9 ) <nl> { <nl> free ( p_orig );
mmm modules / demux / mp4 / libmp4 . c <nl> ppp modules / demux / mp4 / libmp4 . c <nl> static int MP4_ReadBox_String ( stream_t * p_stream , MP4_Box_t * p_box ) <nl> { <nl> MP4_READBOX_ENTER ( MP4_Box_data_string_t ); <nl>  <nl> + if ( p_box -> i_size < 8 || p_box -> i_size > SIZE_MAX ) <nl> + MP4_READBOX_EXIT ( 0 ); <nl> + <nl> p_box -> data . p_string -> psz_text = malloc ( p_box -> i_size + 1 - 8 ); /* +\ 0 , - name , - size */ <nl> if ( p_box -> data . p_string -> psz_text == NULL ) <nl> MP4_READBOX_EXIT ( 0 );
mmm modules / codec / schroedinger . c <nl> ppp modules / codec / schroedinger . c <nl> static block_t * Encode ( encoder_t * p_enc , picture_t * p_pic ) <nl> * is appended to the sequence header to allow guard <nl> * against poor streaming servers */ <nl> /* XXX , should this be done using the packetizer ? */ <nl> + <nl> + if ( len > UINT32_MAX - sizeof ( eos ) ) <nl> + return NULL ; <nl> + <nl> p_enc -> fmt_out . p_extra = malloc ( len + sizeof ( eos ) ); <nl> if ( ! p_enc -> fmt_out . p_extra ) <nl> return NULL ;
mmm src / misc / update . c <nl> ppp src / misc / update . c <nl> static bool GetUpdateFile ( update_t * p_update ) <nl> } <nl>  <nl> const int64_t i_read = stream_Size ( p_stream ); <nl> + <nl> + if ( i_read < 0 || i_read >= UINT16_MAX ) <nl> + { <nl> + msg_Err ( p_update -> p_libvlc , " Status file too large "); <nl> + goto error ; <nl> + } <nl> + <nl> psz_update_data = malloc ( i_read + 1 ); /* terminating '\ 0 ' */ <nl> if ( ! psz_update_data ) <nl> goto error ;
mmm src / window . c <nl> ppp src / window . c <nl> win_exchange ( long Prenum ) <nl>  <nl> ( void ) win_comp_pos (); // recompute window positions <nl>  <nl> + if ( wp -> w_buffer != curbuf ) <nl> + reset_VIsual_and_resel (); <nl> + else if ( VIsual_active ) <nl> + wp -> w_cursor = curwin -> w_cursor ; <nl> + <nl> win_enter ( wp , TRUE ); <nl> redraw_all_later ( NOT_VALID ); <nl> } <nl> frame_remove ( frame_T * frp ) <nl> win_alloc_lines ( win_T * wp ) <nl> { <nl> wp -> w_lines_valid = 0 ; <nl> - wp -> w_lines = ALLOC_CLEAR_MULT ( wline_T , Rows ); <nl> + wp -> w_lines = ALLOC_CLEAR_MULT ( wline_T , Rows ); <nl> if ( wp -> w_lines == NULL ) <nl> return FAIL ; <nl> return OK ;mmm src / version . c <nl> ppp src / version . c <nl> win_exchange ( long Prenum ) <nl>  <nl> ( void ) win_comp_pos (); // recompute window positions <nl>  <nl> + if ( wp -> w_buffer != curbuf ) <nl> + reset_VIsual_and_resel (); <nl> + else if ( VIsual_active ) <nl> + wp -> w_cursor = curwin -> w_cursor ; <nl> + <nl> win_enter ( wp , TRUE ); <nl> redraw_all_later ( NOT_VALID ); <nl> } <nl> frame_remove ( frame_T * frp ) <nl> win_alloc_lines ( win_T * wp ) <nl> { <nl> wp -> w_lines_valid = 0 ; <nl> - wp -> w_lines = ALLOC_CLEAR_MULT ( wline_T , Rows ); <nl> + wp -> w_lines = ALLOC_CLEAR_MULT ( wline_T , Rows ); <nl> if ( wp -> w_lines == NULL ) <nl> return FAIL ; <nl> return OK ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4154 , <nl> /**/ <nl> 4153 , <nl> /**/
mmm src / message . c <nl> ppp src / message . c <nl> str2special ( <nl> * sp = str + 1 ; <nl> } <nl> else <nl> - // single - byte character or illegal byte <nl> - * sp = str + 1 ; <nl> + // single - byte character , NUL or illegal byte <nl> + * sp = str + (* str == NUL ? 0 : 1 ); <nl>  <nl> // Make special keys and C0 control characters in <> form , also < M - Space >. <nl> // Use < Space > only for lhs of a mapping .mmm src / version . c <nl> ppp src / version . c <nl> str2special ( <nl> * sp = str + 1 ; <nl> } <nl> else <nl> - // single - byte character or illegal byte <nl> - * sp = str + 1 ; <nl> + // single - byte character , NUL or illegal byte <nl> + * sp = str + (* str == NUL ? 0 : 1 ); <nl>  <nl> // Make special keys and C0 control characters in <> form , also < M - Space >. <nl> // Use < Space > only for lhs of a mapping . <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 9 , <nl> /**/ <nl> 8 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5162 , <nl> /**/ <nl> 5161 , <nl> /**/mmm src / edit . c <nl> ppp src / edit . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5162 , <nl> /**/ <nl> 5161 , <nl> /**/ <nl> ins_bs ( <nl> # endif <nl>  <nl> // delete characters until we are at or before want_vcol <nl> - while ( vcol > want_vcol <nl> + while ( vcol > want_vcol && curwin -> w_cursor . col > 0 <nl> && ( cc = *( ml_get_cursor () - 1 ), VIM_ISWHITE ( cc ))) <nl> ins_bs_one (& vcol ); <nl> 
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3581 , <nl> /**/ <nl> 3580 , <nl> /**/mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3581 , <nl> /**/ <nl> 3580 , <nl> /**/ <nl> ex_put ( exarg_T * eap ) <nl> eap -> forceit = TRUE ; <nl> } <nl> curwin -> w_cursor . lnum = eap -> line2 ; <nl> + check_cursor_col (); <nl> do_put ( eap -> regname , NULL , eap -> forceit ? BACKWARD : FORWARD , 1L , <nl> PUT_LINE | PUT_CURSLINE ); <nl> }
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 378 , <nl> /**/ <nl> 377 , <nl> /**/mmm src / undo . c <nl> ppp src / undo . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 378 , <nl> /**/ <nl> 377 , <nl> /**/ <nl> unserialize_uep ( bufinfo_T * bi , int * error , char_u * file_name ) <nl> { <nl> int i ; <nl> u_entry_T * uep ; <nl> - char_u ** array ; <nl> + char_u ** array = NULL ; <nl> char_u * line ; <nl> int line_len ; <nl>  <nl> unserialize_uep ( bufinfo_T * bi , int * error , char_u * file_name ) <nl> uep -> ue_size = undo_read_4c ( bi ); <nl> if ( uep -> ue_size > 0 ) <nl> { <nl> - array = ( char_u **) U_ALLOC_LINE ( sizeof ( char_u *) * uep -> ue_size ); <nl> + if ( uep -> ue_size < LONG_MAX / ( int ) sizeof ( char_u *)) <nl> + array = ( char_u **) U_ALLOC_LINE ( sizeof ( char_u *) * uep -> ue_size ); <nl> if ( array == NULL ) <nl> { <nl> * error = TRUE ; <nl> unserialize_uep ( bufinfo_T * bi , int * error , char_u * file_name ) <nl> } <nl> vim_memset ( array , 0 , sizeof ( char_u *) * uep -> ue_size ); <nl> } <nl> - else <nl> - array = NULL ; <nl> uep -> ue_array = array ; <nl>  <nl> for ( i = 0 ; i < uep -> ue_size ; ++ i )
mmm src / indent . c <nl> ppp src / indent . c <nl> get_lisp_indent ( void ) <nl> } <nl> } <nl> } <nl> + if (* that == NUL ) <nl> + break ; <nl> } <nl> if (* that == '(' || * that == '[') <nl> ++ parencount ;mmm src / version . c <nl> ppp src / version . c <nl> get_lisp_indent ( void ) <nl> } <nl> } <nl> } <nl> + if (* that == NUL ) <nl> + break ; <nl> } <nl> if (* that == '(' || * that == '[') <nl> ++ parencount ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5122 , <nl> /**/ <nl> 5121 , <nl> /**/
mmm src / textformat . c <nl> ppp src / textformat . c <nl> same_leader ( <nl> if ( leader1_len == 0 ) <nl> return ( leader2_len == 0 ); <nl>  <nl> + char_u * lnum_line = NULL ; <nl> + int line_len = 0 ; <nl> + <nl> // If first leader has ' f ' flag , the lines can be joined only if the <nl> // second line does not have a leader . <nl> // If first leader has ' e ' flag , the lines can never be joined . <nl> same_leader ( <nl> return FALSE ; <nl> if (* p == COM_START ) <nl> { <nl> - if (*( ml_get ( lnum ) + leader1_len ) == NUL ) <nl> + if ( lnum_line == NULL ) <nl> + { <nl> + lnum_line = ml_get ( lnum ); <nl> + line_len = ( int ) STRLEN ( lnum_line ); <nl> + } <nl> + if ( line_len <= leader1_len ) <nl> return FALSE ; <nl> if ( leader2_flags == NULL || leader2_len == 0 ) <nl> return FALSE ;mmm src / version . c <nl> ppp src / version . c <nl> same_leader ( <nl> if ( leader1_len == 0 ) <nl> return ( leader2_len == 0 ); <nl>  <nl> + char_u * lnum_line = NULL ; <nl> + int line_len = 0 ; <nl> + <nl> // If first leader has ' f ' flag , the lines can be joined only if the <nl> // second line does not have a leader . <nl> // If first leader has ' e ' flag , the lines can never be joined . <nl> same_leader ( <nl> return FALSE ; <nl> if (* p == COM_START ) <nl> { <nl> - if (*( ml_get ( lnum ) + leader1_len ) == NUL ) <nl> + if ( lnum_line == NULL ) <nl> + { <nl> + lnum_line = ml_get ( lnum ); <nl> + line_len = ( int ) STRLEN ( lnum_line ); <nl> + } <nl> + if ( line_len <= leader1_len ) <nl> return FALSE ; <nl> if ( leader2_flags == NULL || leader2_len == 0 ) <nl> return FALSE ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1225 , <nl> /**/ <nl> 1224 , <nl> /**/
mmm src / testing . c <nl> ppp src / testing . c <nl> f_assert_fails ( typval_T * argvars , typval_T * rettv ) <nl> in_assert_fails = TRUE ; <nl>  <nl> do_cmdline_cmd ( cmd ); <nl> + <nl> + // reset here for any errors reported below <nl> + trylevel = save_trylevel ; <nl> + suppress_errthrow = FALSE ; <nl> + <nl> if ( called_emsg == called_emsg_before ) <nl> { <nl> prepare_assert_error (& ga ); <nl> f_assert_fails ( typval_T * argvars , typval_T * rettv ) <nl> CHECK_LIST_MATERIALIZE ( list ); <nl> tv = & list -> lv_first -> li_tv ; <nl> expected = tv_get_string_buf_chk ( tv , buf ); <nl> + if ( expected == NULL ) <nl> + goto theend ; <nl> if (! pattern_match ( expected , actual , FALSE )) <nl> { <nl> error_found = TRUE ; <nl> f_assert_fails ( typval_T * argvars , typval_T * rettv ) <nl> { <nl> tv = & list -> lv_u . mat . lv_last -> li_tv ; <nl> expected = tv_get_string_buf_chk ( tv , buf ); <nl> + if ( expected == NULL ) <nl> + goto theend ; <nl> if (! pattern_match ( expected , actual , FALSE )) <nl> { <nl> error_found = TRUE ;mmm src / version . c <nl> ppp src / version . c <nl> f_assert_fails ( typval_T * argvars , typval_T * rettv ) <nl> in_assert_fails = TRUE ; <nl>  <nl> do_cmdline_cmd ( cmd ); <nl> + <nl> + // reset here for any errors reported below <nl> + trylevel = save_trylevel ; <nl> + suppress_errthrow = FALSE ; <nl> + <nl> if ( called_emsg == called_emsg_before ) <nl> { <nl> prepare_assert_error (& ga ); <nl> f_assert_fails ( typval_T * argvars , typval_T * rettv ) <nl> CHECK_LIST_MATERIALIZE ( list ); <nl> tv = & list -> lv_first -> li_tv ; <nl> expected = tv_get_string_buf_chk ( tv , buf ); <nl> + if ( expected == NULL ) <nl> + goto theend ; <nl> if (! pattern_match ( expected , actual , FALSE )) <nl> { <nl> error_found = TRUE ; <nl> f_assert_fails ( typval_T * argvars , typval_T * rettv ) <nl> { <nl> tv = & list -> lv_u . mat . lv_last -> li_tv ; <nl> expected = tv_get_string_buf_chk ( tv , buf ); <nl> + if ( expected == NULL ) <nl> + goto theend ; <nl> if (! pattern_match ( expected , actual , FALSE )) <nl> { <nl> error_found = TRUE ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 404 , <nl> /**/ <nl> 403 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5123 , <nl> /**/ <nl> 5122 , <nl> /**/mmm src / spellsuggest . c <nl> ppp src / spellsuggest . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5123 , <nl> /**/ <nl> 5122 , <nl> /**/ <nl> suggest_trie_walk ( <nl> sp -> ts_isdiff = ( newscore != 0 ) <nl> ? DIFF_YES : DIFF_NONE ; <nl> } <nl> - else if ( sp -> ts_isdiff == DIFF_INSERT ) <nl> + else if ( sp -> ts_isdiff == DIFF_INSERT <nl> + && sp -> ts_fidx > 0 ) <nl> // When inserting trail bytes don ' t advance in the <nl> // bad word . <nl> -- sp -> ts_fidx ;
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3582 , <nl> /**/ <nl> 3581 , <nl> /**/mmm src / spellsuggest . c <nl> ppp src / spellsuggest . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3582 , <nl> /**/ <nl> 3581 , <nl> /**/ <nl> suggest_trie_walk ( <nl> // char , e . g ., " thes ," -> " these ". <nl> p = fword + sp -> ts_fidx ; <nl> MB_PTR_BACK ( fword , p ); <nl> - if (! spell_iswordp ( p , curwin )) <nl> + if (! spell_iswordp ( p , curwin ) && * preword != NUL ) <nl> { <nl> p = preword + STRLEN ( preword ); <nl> MB_PTR_BACK ( preword , p );
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 580 , <nl> /**/ <nl> 579 , <nl> /**/mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 580 , <nl> /**/ <nl> 579 , <nl> /**/ <nl> do_one_cmd ( cmdlinep , sourcing , <nl>  <nl> # ifdef FEAT_WINDOWS <nl> /* : wincmd range depends on the argument . */ <nl> - if ( ea . cmdidx == CMD_wincmd ) <nl> - get_wincmd_addr_type ( p , & ea ); <nl> + if ( ea . cmdidx == CMD_wincmd && p != NULL ) <nl> + get_wincmd_addr_type ( skipwhite ( p ), & ea ); <nl> # endif <nl> } <nl> 
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 221 , <nl> /**/ <nl> 220 , <nl> /**/mmm src / vim9compile . c <nl> ppp src / vim9compile . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 221 , <nl> /**/ <nl> 220 , <nl> /**/ <nl> compile_nested_function ( exarg_T * eap , cctx_T * cctx , garray_T * lines_to_free ) <nl> int r = FAIL ; <nl> compiletype_T compile_type ; <nl> isn_T * funcref_isn = NULL ; <nl> + lvar_T * lvar = NULL ; <nl>  <nl> if ( eap -> forceit ) <nl> { <nl> compile_nested_function ( exarg_T * eap , cctx_T * cctx , garray_T * lines_to_free ) <nl> else <nl> { <nl> // Define a local variable for the function reference . <nl> - lvar_T * lvar = reserve_local ( cctx , func_name , name_end - name_start , <nl> + lvar = reserve_local ( cctx , func_name , name_end - name_start , <nl> TRUE , ufunc -> uf_func_type ); <nl> - <nl> if ( lvar == NULL ) <nl> goto theend ; <nl> if ( generate_FUNCREF ( cctx , ufunc , & funcref_isn ) == FAIL ) <nl> compile_nested_function ( exarg_T * eap , cctx_T * cctx , garray_T * lines_to_free ) <nl> && compile_def_function ( ufunc , TRUE , compile_type , cctx ) == FAIL ) <nl> { <nl> func_ptr_unref ( ufunc ); <nl> + if ( lvar != NULL ) <nl> + // Now the local variable can ' t be used . <nl> + * lvar -> lv_name = '/'; // impossible value <nl> goto theend ; <nl> } <nl> 
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 490 , <nl> /**/ <nl> 489 , <nl> /**/mmm src / ex_getln . c <nl> ppp src / ex_getln . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 490 , <nl> /**/ <nl> 489 , <nl> /**/ <nl> getcmdline_int ( <nl> # endif <nl> expand_T xpc ; <nl> long * b_im_ptr = NULL ; <nl> + buf_T * b_im_ptr_buf = NULL ; // buffer where b_im_ptr is valid <nl> cmdline_info_T save_ccline ; <nl> int did_save_ccline = FALSE ; <nl> int cmdline_type ; <nl> getcmdline_int ( <nl> b_im_ptr = & curbuf -> b_p_iminsert ; <nl> else <nl> b_im_ptr = & curbuf -> b_p_imsearch ; <nl> + b_im_ptr_buf = curbuf ; <nl> if (* b_im_ptr == B_IMODE_LMAP ) <nl> State |= MODE_LANGMAP ; <nl> # ifdef HAVE_INPUT_METHOD <nl> getcmdline_int ( <nl> goto cmdline_not_changed ; <nl>  <nl> case Ctrl_HAT : <nl> - cmdline_toggle_langmap ( b_im_ptr ); <nl> + cmdline_toggle_langmap ( <nl> + buf_valid ( b_im_ptr_buf ) ? b_im_ptr : NULL ); <nl> goto cmdline_not_changed ; <nl>  <nl> // case '@': only in very old vi <nl> returncmd : <nl> # endif <nl>  <nl> # ifdef HAVE_INPUT_METHOD <nl> - if ( b_im_ptr != NULL && * b_im_ptr != B_IMODE_LMAP ) <nl> + if ( b_im_ptr != NULL && buf_valid ( b_im_ptr_buf ) <nl> + && * b_im_ptr != B_IMODE_LMAP ) <nl> im_save_status ( b_im_ptr ); <nl> im_set_active ( FALSE ); <nl> # endif
mmm src / register . c <nl> ppp src / register . c <nl> do_put ( <nl> ptr += yanklen ; <nl>  <nl> // insert block ' s trailing spaces only if there ' s text behind <nl> - if (( j < count - 1 || ! shortline ) && spaces ) <nl> + if (( j < count - 1 || ! shortline ) && spaces > 0 ) <nl> { <nl> vim_memset ( ptr , ' ', ( size_t ) spaces ); <nl> ptr += spaces ; <nl> error : <nl> msgmore ( nr_lines ); <nl> curwin -> w_set_curswant = TRUE ; <nl>  <nl> + // Make sure the cursor is not after the NUL . <nl> + int len = ( int ) STRLEN ( ml_get_curline ()); <nl> + if ( curwin -> w_cursor . col > len ) <nl> + { <nl> + if ( cur_ve_flags == VE_ALL ) <nl> + curwin -> w_cursor . coladd = curwin -> w_cursor . col - len ; <nl> + curwin -> w_cursor . col = len ; <nl> + } <nl> + <nl> end : <nl> if ( cmdmod . cmod_flags & CMOD_LOCKMARKS ) <nl> {mmm src / version . c <nl> ppp src / version . c <nl> do_put ( <nl> ptr += yanklen ; <nl>  <nl> // insert block ' s trailing spaces only if there ' s text behind <nl> - if (( j < count - 1 || ! shortline ) && spaces ) <nl> + if (( j < count - 1 || ! shortline ) && spaces > 0 ) <nl> { <nl> vim_memset ( ptr , ' ', ( size_t ) spaces ); <nl> ptr += spaces ; <nl> error : <nl> msgmore ( nr_lines ); <nl> curwin -> w_set_curswant = TRUE ; <nl>  <nl> + // Make sure the cursor is not after the NUL . <nl> + int len = ( int ) STRLEN ( ml_get_curline ()); <nl> + if ( curwin -> w_cursor . col > len ) <nl> + { <nl> + if ( cur_ve_flags == VE_ALL ) <nl> + curwin -> w_cursor . coladd = curwin -> w_cursor . col - len ; <nl> + curwin -> w_cursor . col = len ; <nl> + } <nl> + <nl> end : <nl> if ( cmdmod . cmod_flags & CMOD_LOCKMARKS ) <nl> { <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1376 , <nl> /**/ <nl> 1375 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 104 , <nl> /**/ <nl> 103 , <nl> /**/mmm src / typval . c <nl> ppp src / typval . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 104 , <nl> /**/ <nl> 103 , <nl> /**/ <nl> eval_string ( char_u ** arg , typval_T * rettv , int evaluate , int interpolate ) <nl> // to 9 characters ( 6 for the char and 3 for a modifier ): <nl> // reserve space for 5 extra . <nl> if (* p == '<') <nl> + { <nl> + int modifiers = 0 ; <nl> + int flags = FSK_KEYCODE | FSK_IN_STRING ; <nl> + <nl> extra += 5 ; <nl> + <nl> + // Skip to the '>' to avoid using '{' inside for string <nl> + // interpolation . <nl> + if ( p [ 1 ] != '*') <nl> + flags |= FSK_SIMPLIFY ; <nl> + if ( find_special_key (& p , & modifiers , flags , NULL ) != 0 ) <nl> + -- p ; // leave " p " on the ">" <nl> + } <nl> } <nl> else if ( interpolate && (* p == '{' || * p == '}')) <nl> {
mmm src / normal . c <nl> ppp src / normal . c <nl> nv_scroll ( cmdarg_T * cap ) <nl> { <nl> ( void ) hasFolding ( curwin -> w_cursor . lnum , <nl> & curwin -> w_cursor . lnum , NULL ); <nl> - -- curwin -> w_cursor . lnum ; <nl> + if ( curwin -> w_cursor . lnum > curwin -> w_topline ) <nl> + -- curwin -> w_cursor . lnum ; <nl> } <nl> } <nl> elsemmm src / version . c <nl> ppp src / version . c <nl> nv_scroll ( cmdarg_T * cap ) <nl> { <nl> ( void ) hasFolding ( curwin -> w_cursor . lnum , <nl> & curwin -> w_cursor . lnum , NULL ); <nl> - -- curwin -> w_cursor . lnum ; <nl> + if ( curwin -> w_cursor . lnum > curwin -> w_topline ) <nl> + -- curwin -> w_cursor . lnum ; <nl> } <nl> } <nl> else <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1189 , <nl> /**/ <nl> 1188 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 18 , <nl> /**/ <nl> 17 , <nl> /**/mmm src / term . c <nl> ppp src / term . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 18 , <nl> /**/ <nl> 17 , <nl> /**/ <nl> check_termcode ( <nl> if (* tp == ESC && ! p_ek && ( State & MODE_INSERT )) <nl> continue ; <nl>  <nl> + tp [ len ] = NUL ; <nl> key_name [ 0 ] = NUL ; // no key name found yet <nl> key_name [ 1 ] = NUL ; // no key name found yet <nl> modifiers = 0 ; // no modifiers yet
mmm src / spell . c <nl> ppp src / spell . c <nl> spell_move_to ( <nl> char_u * line ; <nl> char_u * p ; <nl> char_u * endp ; <nl> - hlf_T attr ; <nl> + hlf_T attr = 0 ; <nl> int len ; <nl> # ifdef FEAT_SYN_HL <nl> int has_syntax = syntax_present ( wp ); <nl> spell_move_to ( <nl>  <nl> while (! got_int ) <nl> { <nl> + int empty_line ; <nl> + <nl> line = ml_get_buf ( wp -> w_buffer , lnum , FALSE ); <nl>  <nl> len = ( int ) STRLEN ( line ); <nl> spell_move_to ( <nl> } <nl>  <nl> // Copy the line into " buf " and append the start of the next line if <nl> - // possible . <nl> + // possible . Note : this ml_get_buf () may make " line " invalid , check <nl> + // for empty line first . <nl> + empty_line = * skipwhite ( line ) == NUL ; <nl> STRCPY ( buf , line ); <nl> if ( lnum < wp -> w_buffer -> b_ml . ml_line_count ) <nl> spell_cat_line ( buf + STRLEN ( buf ), <nl> spell_move_to ( <nl> -- capcol ; <nl>  <nl> // But after empty line check first word in next line <nl> - if (* skipwhite ( line ) == NUL ) <nl> + if ( empty_line ) <nl> capcol = 0 ; <nl> } <nl> mmm src / version . c <nl> ppp src / version . c <nl> spell_move_to ( <nl> char_u * line ; <nl> char_u * p ; <nl> char_u * endp ; <nl> - hlf_T attr ; <nl> + hlf_T attr = 0 ; <nl> int len ; <nl> # ifdef FEAT_SYN_HL <nl> int has_syntax = syntax_present ( wp ); <nl> spell_move_to ( <nl>  <nl> while (! got_int ) <nl> { <nl> + int empty_line ; <nl> + <nl> line = ml_get_buf ( wp -> w_buffer , lnum , FALSE ); <nl>  <nl> len = ( int ) STRLEN ( line ); <nl> spell_move_to ( <nl> } <nl>  <nl> // Copy the line into " buf " and append the start of the next line if <nl> - // possible . <nl> + // possible . Note : this ml_get_buf () may make " line " invalid , check <nl> + // for empty line first . <nl> + empty_line = * skipwhite ( line ) == NUL ; <nl> STRCPY ( buf , line ); <nl> if ( lnum < wp -> w_buffer -> b_ml . ml_line_count ) <nl> spell_cat_line ( buf + STRLEN ( buf ), <nl> spell_move_to ( <nl> -- capcol ; <nl>  <nl> // But after empty line check first word in next line <nl> - if (* skipwhite ( line ) == NUL ) <nl> + if ( empty_line ) <nl> capcol = 0 ; <nl> } <nl>  <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5072 , <nl> /**/ <nl> 5071 , <nl> /**/
mmm src / window . c <nl> ppp src / window . c <nl> wingotofile : <nl> CHECK_CMDWIN ; <nl> if (( len = find_ident_under_cursor (& ptr , FIND_IDENT )) == 0 ) <nl> break ; <nl> + <nl> + // Make a copy , if the line was changed it will be freed . <nl> + ptr = vim_strnsave ( ptr , len ); <nl> + if ( ptr == NULL ) <nl> + break ; <nl> + <nl> find_pattern_in_path ( ptr , 0 , len , TRUE , <nl> Prenum == 0 ? TRUE : FALSE , type , <nl> Prenum1 , ACTION_SPLIT , ( linenr_T ) 1 , ( linenr_T ) MAXLNUM ); <nl> + vim_free ( ptr ); <nl> curwin -> w_set_curswant = TRUE ; <nl> break ; <nl> # endifmmm src / version . c <nl> ppp src / version . c <nl> wingotofile : <nl> CHECK_CMDWIN ; <nl> if (( len = find_ident_under_cursor (& ptr , FIND_IDENT )) == 0 ) <nl> break ; <nl> + <nl> + // Make a copy , if the line was changed it will be freed . <nl> + ptr = vim_strnsave ( ptr , len ); <nl> + if ( ptr == NULL ) <nl> + break ; <nl> + <nl> find_pattern_in_path ( ptr , 0 , len , TRUE , <nl> Prenum == 0 ? TRUE : FALSE , type , <nl> Prenum1 , ACTION_SPLIT , ( linenr_T ) 1 , ( linenr_T ) MAXLNUM ); <nl> + vim_free ( ptr ); <nl> curwin -> w_set_curswant = TRUE ; <nl> break ; <nl> # endif <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4979 , <nl> /**/ <nl> 4978 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 98 , <nl> /**/ <nl> 97 , <nl> /**/mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 98 , <nl> /**/ <nl> 97 , <nl> /**/ <nl> ex_redir ( eap ) <nl> else <nl> EMSG2 ( _ ( e_invarg2 ), eap -> arg ); <nl> } <nl> + <nl> + /* Make sure redirection is not off . Can happen for cmdline completion <nl> + * that indirectly invokes a command to catch its output . */ <nl> + if ( redir_fd != NULL <nl> +# ifdef FEAT_EVAL <nl> + || redir_reg || redir_vname <nl> +# endif <nl> + ) <nl> + redir_off = FALSE ; <nl> } <nl>  <nl> /*mmm src / ex_getln . c <nl> ppp src / ex_getln . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 98 , <nl> /**/ <nl> 97 , <nl> /**/ <nl> ex_redir ( eap ) <nl> else <nl> EMSG2 ( _ ( e_invarg2 ), eap -> arg ); <nl> } <nl> + <nl> + /* Make sure redirection is not off . Can happen for cmdline completion <nl> + * that indirectly invokes a command to catch its output . */ <nl> + if ( redir_fd != NULL <nl> +# ifdef FEAT_EVAL <nl> + || redir_reg || redir_vname <nl> +# endif <nl> + ) <nl> + redir_off = FALSE ; <nl> } <nl>  <nl> /* <nl> getcmdline ( firstc , count , indent ) <nl> */ <nl> for (;;) <nl> { <nl> + redir_off = TRUE ; /* Don ' t redirect the typed command . <nl> + Repeated , because a ": redir " inside <nl> + completion may switch it on . */ <nl> # ifdef USE_ON_FLY_SCROLL <nl> dont_scroll = FALSE ; /* allow scrolling here */ <nl> # endif
mmm src / register . c <nl> ppp src / register . c <nl> error : <nl> len = STRLEN ( y_array [ y_size - 1 ]); <nl> col = ( colnr_T ) len - lendiff ; <nl> if ( col > 1 ) <nl> - curbuf -> b_op_end . col = col - 1 <nl> - - mb_head_off ( y_array [ y_size - 1 ], <nl> + { <nl> + curbuf -> b_op_end . col = col - 1 ; <nl> + if ( len > 0 ) <nl> + curbuf -> b_op_end . col -= mb_head_off ( y_array [ y_size - 1 ], <nl> y_array [ y_size - 1 ] + len - 1 ); <nl> + } <nl> else <nl> curbuf -> b_op_end . col = 0 ; <nl> mmm src / version . c <nl> ppp src / version . c <nl> error : <nl> len = STRLEN ( y_array [ y_size - 1 ]); <nl> col = ( colnr_T ) len - lendiff ; <nl> if ( col > 1 ) <nl> - curbuf -> b_op_end . col = col - 1 <nl> - - mb_head_off ( y_array [ y_size - 1 ], <nl> + { <nl> + curbuf -> b_op_end . col = col - 1 ; <nl> + if ( len > 0 ) <nl> + curbuf -> b_op_end . col -= mb_head_off ( y_array [ y_size - 1 ], <nl> y_array [ y_size - 1 ] + len - 1 ); <nl> + } <nl> else <nl> curbuf -> b_op_end . col = 0 ; <nl>  <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5016 , <nl> /**/ <nl> 5015 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4647 , <nl> /**/ <nl> 4646 , <nl> /**/mmm src / scriptfile . c <nl> ppp src / scriptfile . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4647 , <nl> /**/ <nl> 4646 , <nl> /**/ <nl> get_one_sourceline ( source_cookie_T * sp ) <nl> break ; // all the lines are processed <nl> ga_concat (& ga , (( char_u **) sp -> buflines . ga_data )[ sp -> buf_lnum ]); <nl> sp -> buf_lnum ++; <nl> + if ( ga_grow (& ga , 1 ) == FAIL ) <nl> + break ; <nl> buf = ( char_u *) ga . ga_data ; <nl> + buf [ ga . ga_len ++] = NUL ; <nl> } <nl> else <nl> {
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3625 , <nl> /**/ <nl> 3624 , <nl> /**/mmm src / cindent . c <nl> ppp src / cindent . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3625 , <nl> /**/ <nl> 3624 , <nl> /**/ <nl> get_baseclass_amount ( int col ) <nl> static pos_T * <nl> find_start_brace ( void ) // XXX <nl> { <nl> - pos_T cursor_save ; <nl> - pos_T * trypos ; <nl> - pos_T * pos ; <nl> - static pos_T pos_copy ; <nl> + pos_T cursor_save ; <nl> + pos_T * trypos ; <nl> + pos_T * pos ; <nl> + static pos_T pos_copy ; <nl>  <nl> cursor_save = curwin -> w_cursor ; <nl> while (( trypos = findmatchlimit ( NULL , '{', FM_BLOCKSTOP , 0 )) != NULL ) <nl> find_start_brace ( void ) // XXX <nl> && ( pos = ind_find_start_CORS ( NULL )) == NULL ) // XXX <nl> break ; <nl> if ( pos != NULL ) <nl> - curwin -> w_cursor . lnum = pos -> lnum ; <nl> + curwin -> w_cursor = * pos ; <nl> } <nl> curwin -> w_cursor = cursor_save ; <nl> return trypos ;
mmm src / textobject . c <nl> ppp src / textobject . c <nl> current_quote ( <nl>  <nl> // Find out if we have a quote in the selection . <nl> while ( i <= col_end ) <nl> + { <nl> + // check for going over the end of the line , which can happen if <nl> + // the line was changed after the Visual area was selected . <nl> + if ( line [ i ] == NUL ) <nl> + break ; <nl> if ( line [ i ++] == quotechar ) <nl> { <nl> selected_quote = TRUE ; <nl> break ; <nl> } <nl> + } <nl> } <nl>  <nl> if (! vis_empty && line [ col_start ] == quotechar )mmm src / version . c <nl> ppp src / version . c <nl> current_quote ( <nl>  <nl> // Find out if we have a quote in the selection . <nl> while ( i <= col_end ) <nl> + { <nl> + // check for going over the end of the line , which can happen if <nl> + // the line was changed after the Visual area was selected . <nl> + if ( line [ i ] == NUL ) <nl> + break ; <nl> if ( line [ i ++] == quotechar ) <nl> { <nl> selected_quote = TRUE ; <nl> break ; <nl> } <nl> + } <nl> } <nl>  <nl> if (! vis_empty && line [ col_start ] == quotechar ) <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5120 , <nl> /**/ <nl> 5119 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 151 , <nl> /**/ <nl> 150 , <nl> /**/mmm src / ops . c <nl> ppp src / ops . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 151 , <nl> /**/ <nl> 150 , <nl> /**/ <nl> op_yank ( oap , deleting , mess ) <nl> /* Copy the text from register 0 to the clipboard register . */ <nl> copy_yank_reg (&( y_regs [ PLUS_REGISTER ])); <nl>  <nl> - /* No need to copy to * register upon ' unnamed ' now - see below */ <nl> clip_own_selection (& clip_plus ); <nl> clip_gen_set_selection (& clip_plus ); <nl> - if (! clip_isautosel () && ! did_star ) <nl> + if (! clip_isautosel () && ! did_star && curr == &( y_regs [ PLUS_REGISTER ])) <nl> { <nl> copy_yank_reg (&( y_regs [ STAR_REGISTER ])); <nl> clip_own_selection (& clip_star );
mmm src / testing . c <nl> ppp src / testing . c <nl> ga_concat_shorten_esc ( garray_T * gap , char_u * str ) <nl> { <nl> same_len = 1 ; <nl> s = p ; <nl> - c = mb_ptr2char_adv (& s ); <nl> + c = mb_cptr2char_adv (& s ); <nl> clen = s - p ; <nl> while (* s != NUL && c == mb_ptr2char ( s )) <nl> {mmm src / version . c <nl> ppp src / version . c <nl> ga_concat_shorten_esc ( garray_T * gap , char_u * str ) <nl> { <nl> same_len = 1 ; <nl> s = p ; <nl> - c = mb_ptr2char_adv (& s ); <nl> + c = mb_cptr2char_adv (& s ); <nl> clen = s - p ; <nl> while (* s != NUL && c == mb_ptr2char ( s )) <nl> { <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4397 , <nl> /**/ <nl> 4396 , <nl> /**/
mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> get_address ( <nl>  <nl> // When '/' or '?' follows another address , start from <nl> // there . <nl> - if ( lnum != MAXLNUM ) <nl> - curwin -> w_cursor . lnum = lnum ; <nl> + if ( lnum > 0 && lnum != MAXLNUM ) <nl> + curwin -> w_cursor . lnum = <nl> + lnum > curbuf -> b_ml . ml_line_count <nl> + ? curbuf -> b_ml . ml_line_count : lnum ; <nl>  <nl> // Start a forward search at the end of the line ( unless <nl> // before the first line ).mmm src / version . c <nl> ppp src / version . c <nl> get_address ( <nl>  <nl> // When '/' or '?' follows another address , start from <nl> // there . <nl> - if ( lnum != MAXLNUM ) <nl> - curwin -> w_cursor . lnum = lnum ; <nl> + if ( lnum > 0 && lnum != MAXLNUM ) <nl> + curwin -> w_cursor . lnum = <nl> + lnum > curbuf -> b_ml . ml_line_count <nl> + ? curbuf -> b_ml . ml_line_count : lnum ; <nl>  <nl> // Start a forward search at the end of the line ( unless <nl> // before the first line ). <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3489 , <nl> /**/ <nl> 3488 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3428 , <nl> /**/ <nl> 3427 , <nl> /**/mmm src / normal . c <nl> ppp src / normal . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3428 , <nl> /**/ <nl> 3427 , <nl> /**/ <nl> nv_replace ( cmdarg_T * cap ) <nl> { <nl> /* <nl> * Get ptr again , because u_save and / or showmatch () will have <nl> - * released the line . At the same time we let know that the <nl> - * line will be changed . <nl> + * released the line . This may also happen in ins_copychar (). <nl> + * At the same time we let know that the line will be changed . <nl> */ <nl> - ptr = ml_get_buf ( curbuf , curwin -> w_cursor . lnum , TRUE ); <nl> if ( cap -> nchar == Ctrl_E || cap -> nchar == Ctrl_Y ) <nl> { <nl> int c = ins_copychar ( curwin -> w_cursor . lnum <nl> + ( cap -> nchar == Ctrl_Y ? - 1 : 1 )); <nl> + <nl> + ptr = ml_get_buf ( curbuf , curwin -> w_cursor . lnum , TRUE ); <nl> if ( c != NUL ) <nl> ptr [ curwin -> w_cursor . col ] = c ; <nl> } <nl> else <nl> + { <nl> + ptr = ml_get_buf ( curbuf , curwin -> w_cursor . lnum , TRUE ); <nl> ptr [ curwin -> w_cursor . col ] = cap -> nchar ; <nl> + } <nl> if ( p_sm && msg_silent == 0 ) <nl> showmatch ( cap -> nchar ); <nl> ++ curwin -> w_cursor . col ;
mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> do_cmdline ( <nl>  <nl> // Check for the next breakpoint at or after the ": while " <nl> // or ": for ". <nl> - if ( breakpoint != NULL ) <nl> + if ( breakpoint != NULL && lines_ga . ga_len > current_line ) <nl> { <nl> * breakpoint = dbg_find_breakpoint ( <nl> getline_equal ( fgetline , cookie , getsourceline ),mmm src / version . c <nl> ppp src / version . c <nl> do_cmdline ( <nl>  <nl> // Check for the next breakpoint at or after the ": while " <nl> // or ": for ". <nl> - if ( breakpoint != NULL ) <nl> + if ( breakpoint != NULL && lines_ga . ga_len > current_line ) <nl> { <nl> * breakpoint = dbg_find_breakpoint ( <nl> getline_equal ( fgetline , cookie , getsourceline ), <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 360 , <nl> /**/ <nl> 359 , <nl> /**/
mmm src / register . c <nl> ppp src / register . c <nl> do_put ( <nl> // adjust '] mark <nl> curbuf -> b_op_end . lnum = curwin -> w_cursor . lnum - 1 ; <nl> curbuf -> b_op_end . col = bd . textcol + totlen - 1 ; <nl> + if ( curbuf -> b_op_end . col < 0 ) <nl> + curbuf -> b_op_end . col = 0 ; <nl> curbuf -> b_op_end . coladd = 0 ; <nl> if ( flags & PUT_CURSEND ) <nl> {mmm src / version . c <nl> ppp src / version . c <nl> do_put ( <nl> // adjust '] mark <nl> curbuf -> b_op_end . lnum = curwin -> w_cursor . lnum - 1 ; <nl> curbuf -> b_op_end . col = bd . textcol + totlen - 1 ; <nl> + if ( curbuf -> b_op_end . col < 0 ) <nl> + curbuf -> b_op_end . col = 0 ; <nl> curbuf -> b_op_end . coladd = 0 ; <nl> if ( flags & PUT_CURSEND ) <nl> { <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 765 , <nl> /**/ <nl> 764 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 22 , <nl> /**/ <nl> 21 , <nl> /**/mmm src / globals . h <nl> ppp src / globals . h <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 22 , <nl> /**/ <nl> 21 , <nl> /**/ <nl> EXTERN int State INIT (= NORMAL ); /* This is the current state of the <nl> * command interpreter . */ <nl>  <nl> EXTERN int finish_op INIT (= FALSE );/* TRUE while an operator is pending */ <nl> + EXTERN int opcount INIT (= 0 ); /* count for pending operator */ <nl>  <nl> /* <nl> * ex mode ( Q ) statemmm src / normal . c <nl> ppp src / normal . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 22 , <nl> /**/ <nl> 21 , <nl> /**/ <nl> EXTERN int State INIT (= NORMAL ); /* This is the current state of the <nl> * command interpreter . */ <nl>  <nl> EXTERN int finish_op INIT (= FALSE );/* TRUE while an operator is pending */ <nl> + EXTERN int opcount INIT (= 0 ); /* count for pending operator */ <nl>  <nl> /* <nl> * ex mode ( Q ) state <nl> normal_cmd ( oap , toplevel ) <nl> oparg_T * oap ; <nl> int toplevel ; /* TRUE when called from main () */ <nl> { <nl> - static long opcount = 0 ; /* ca . opcount saved here */ <nl> cmdarg_T ca ; /* command arguments */ <nl> int c ; <nl> int ctrl_w = FALSE ; /* got CTRL - W command */mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 22 , <nl> /**/ <nl> 21 , <nl> /**/ <nl> EXTERN int State INIT (= NORMAL ); /* This is the current state of the <nl> * command interpreter . */ <nl>  <nl> EXTERN int finish_op INIT (= FALSE );/* TRUE while an operator is pending */ <nl> + EXTERN int opcount INIT (= 0 ); /* count for pending operator */ <nl>  <nl> /* <nl> * ex mode ( Q ) state <nl> normal_cmd ( oap , toplevel ) <nl> oparg_T * oap ; <nl> int toplevel ; /* TRUE when called from main () */ <nl> { <nl> - static long opcount = 0 ; /* ca . opcount saved here */ <nl> cmdarg_T ca ; /* command arguments */ <nl> int c ; <nl> int ctrl_w = FALSE ; /* got CTRL - W command */ <nl> ex_normal ( eap ) <nl> tasave_T tabuf ; <nl> int save_insertmode = p_im ; <nl> int save_finish_op = finish_op ; <nl> + int save_opcount = opcount ; <nl> # ifdef FEAT_MBYTE <nl> char_u * arg = NULL ; <nl> int l ; <nl> ex_normal ( eap ) <nl> restart_edit = save_restart_edit ; <nl> p_im = save_insertmode ; <nl> finish_op = save_finish_op ; <nl> + opcount = save_opcount ; <nl> msg_didout |= save_msg_didout ; /* don ' t reset msg_didout now */ <nl>  <nl> /* Restore the state ( needed when called from a function executed for
mmm src / normal . c <nl> ppp src / normal . c <nl> get_visual_text ( <nl> } <nl> if (** pp == NUL ) <nl> * lenp = 0 ; <nl> - if ( has_mbyte && * lenp > 0 ) <nl> - // Correct the length to include all bytes of the last character . <nl> - * lenp += (* mb_ptr2len )(* pp + (* lenp - 1 )) - 1 ; <nl> + if (* lenp > 0 ) <nl> + { <nl> + if ( has_mbyte ) <nl> + // Correct the length to include all bytes of the last <nl> + // character . <nl> + * lenp += (* mb_ptr2len )(* pp + (* lenp - 1 )) - 1 ; <nl> + else if ((* pp )[* lenp - 1 ] == NUL ) <nl> + // Do not include a trailing NUL . <nl> + * lenp -= 1 ; <nl> + } <nl> } <nl> reset_VIsual_and_resel (); <nl> return OK ;mmm src / version . c <nl> ppp src / version . c <nl> get_visual_text ( <nl> } <nl> if (** pp == NUL ) <nl> * lenp = 0 ; <nl> - if ( has_mbyte && * lenp > 0 ) <nl> - // Correct the length to include all bytes of the last character . <nl> - * lenp += (* mb_ptr2len )(* pp + (* lenp - 1 )) - 1 ; <nl> + if (* lenp > 0 ) <nl> + { <nl> + if ( has_mbyte ) <nl> + // Correct the length to include all bytes of the last <nl> + // character . <nl> + * lenp += (* mb_ptr2len )(* pp + (* lenp - 1 )) - 1 ; <nl> + else if ((* pp )[* lenp - 1 ] == NUL ) <nl> + // Do not include a trailing NUL . <nl> + * lenp -= 1 ; <nl> + } <nl> } <nl> reset_VIsual_and_resel (); <nl> return OK ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4956 , <nl> /**/ <nl> 4955 , <nl> /**/
mmm src / spellfile . c <nl> ppp src / spellfile . c <nl> spell_read_tree ( <nl> len = get4c ( fd ); <nl> if ( len < 0 ) <nl> return SP_TRUNCERROR ; <nl> + if ( len >= 0x3ffffff ) <nl> + /* Invalid length , multiply with sizeof ( int ) would overflow . */ <nl> + return SP_FORMERROR ; <nl> if ( len > 0 ) <nl> { <nl> /* Allocate the byte array . */mmm src / version . c <nl> ppp src / version . c <nl> spell_read_tree ( <nl> len = get4c ( fd ); <nl> if ( len < 0 ) <nl> return SP_TRUNCERROR ; <nl> + if ( len >= 0x3ffffff ) <nl> + /* Invalid length , multiply with sizeof ( int ) would overflow . */ <nl> + return SP_FORMERROR ; <nl> if ( len > 0 ) <nl> { <nl> /* Allocate the byte array . */ <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 322 , <nl> /**/ <nl> 321 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1145 , <nl> /**/ <nl> 1144 , <nl> /**/mmm src / eval . c <nl> ppp src / eval . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1145 , <nl> /**/ <nl> 1144 , <nl> /**/ <nl> do_string_sub ( <nl> * - The text after the match . <nl> */ <nl> sublen = vim_regsub (& regmatch , sub , expr , tail , 0 , REGSUB_MAGIC ); <nl> + if ( sublen <= 0 ) <nl> + { <nl> + ga_clear (& ga ); <nl> + break ; <nl> + } <nl> if ( ga_grow (& ga , ( int )(( end - tail ) + sublen - <nl> ( regmatch . endp [ 0 ] - regmatch . startp [ 0 ]))) == FAIL ) <nl> {
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 17 , <nl> /**/ <nl> 16 , <nl> /**/mmm src / window . c <nl> ppp src / window . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 17 , <nl> /**/ <nl> 16 , <nl> /**/ <nl> win_close ( win_T * win , int free_buf ) <nl> */ <nl> if ( wp -> w_buffer != curbuf ) <nl> { <nl> + reset_VIsual_and_resel (); // stop Visual mode <nl> + <nl> other_buffer = TRUE ; <nl> win -> w_closing = TRUE ; <nl> apply_autocmds ( EVENT_BUFLEAVE , NULL , NULL , FALSE , curbuf );
mmm src / undo . c <nl> ppp src / undo . c <nl> u_read_undo ( char_u * name , char_u * hash , char_u * orig_name ) <nl> linenr_T line_lnum ; <nl> colnr_T line_colnr ; <nl> linenr_T line_count ; <nl> - int num_head = 0 ; <nl> + long num_head = 0 ; <nl> long old_header_seq , new_header_seq , cur_header_seq ; <nl> long seq_last , seq_cur ; <nl> long last_save_nr = 0 ; <nl> u_read_undo ( char_u * name , char_u * hash , char_u * orig_name ) <nl> * When there are no headers uhp_table is NULL . */ <nl> if ( num_head > 0 ) <nl> { <nl> - uhp_table = ( u_header_T **) U_ALLOC_LINE ( <nl> + if ( num_head < LONG_MAX / ( long ) sizeof ( u_header_T *)) <nl> + uhp_table = ( u_header_T **) U_ALLOC_LINE ( <nl> num_head * sizeof ( u_header_T *)); <nl> if ( uhp_table == NULL ) <nl> goto error ;mmm src / version . c <nl> ppp src / version . c <nl> u_read_undo ( char_u * name , char_u * hash , char_u * orig_name ) <nl> linenr_T line_lnum ; <nl> colnr_T line_colnr ; <nl> linenr_T line_count ; <nl> - int num_head = 0 ; <nl> + long num_head = 0 ; <nl> long old_header_seq , new_header_seq , cur_header_seq ; <nl> long seq_last , seq_cur ; <nl> long last_save_nr = 0 ; <nl> u_read_undo ( char_u * name , char_u * hash , char_u * orig_name ) <nl> * When there are no headers uhp_table is NULL . */ <nl> if ( num_head > 0 ) <nl> { <nl> - uhp_table = ( u_header_T **) U_ALLOC_LINE ( <nl> + if ( num_head < LONG_MAX / ( long ) sizeof ( u_header_T *)) <nl> + uhp_table = ( u_header_T **) U_ALLOC_LINE ( <nl> num_head * sizeof ( u_header_T *)); <nl> if ( uhp_table == NULL ) <nl> goto error ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 377 , <nl> /**/ <nl> 376 , <nl> /**/
mmm src / regexp . c <nl> ppp src / regexp . c <nl> regmatch ( scan ) <nl> */ <nl> for (;;) <nl> { <nl> - /* Some patterns may cause a long time to match , even though they are not <nl> - * illegal . E . g ., "\([ a - z ]\+\)\+ Q ". Allow breaking them with CTRL - C . */ <nl> + /* Some patterns may take a long time to match , e . g ., "\([ a - z ]\+\)\+ Q ". <nl> + * Allow interrupting them with CTRL - C . */ <nl> fast_breakcheck (); <nl>  <nl> # ifdef DEBUGmmm src / version . c <nl> ppp src / version . c <nl> regmatch ( scan ) <nl> */ <nl> for (;;) <nl> { <nl> - /* Some patterns may cause a long time to match , even though they are not <nl> - * illegal . E . g ., "\([ a - z ]\+\)\+ Q ". Allow breaking them with CTRL - C . */ <nl> + /* Some patterns may take a long time to match , e . g ., "\([ a - z ]\+\)\+ Q ". <nl> + * Allow interrupting them with CTRL - C . */ <nl> fast_breakcheck (); <nl>  <nl> # ifdef DEBUG <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 8 , <nl> /**/ <nl> 7 , <nl> /**/mmm src / regexp_nfa . c <nl> ppp src / regexp_nfa . c <nl> regmatch ( scan ) <nl> */ <nl> for (;;) <nl> { <nl> - /* Some patterns may cause a long time to match , even though they are not <nl> - * illegal . E . g ., "\([ a - z ]\+\)\+ Q ". Allow breaking them with CTRL - C . */ <nl> + /* Some patterns may take a long time to match , e . g ., "\([ a - z ]\+\)\+ Q ". <nl> + * Allow interrupting them with CTRL - C . */ <nl> fast_breakcheck (); <nl>  <nl> # ifdef DEBUG <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 8 , <nl> /**/ <nl> 7 , <nl> /**/ <nl> nfa_regmatch ( prog , start , submatch , m ) <nl> return FALSE ; <nl> } <nl> # endif <nl> + /* Some patterns may take a long time to match , especially when using <nl> + * recursive_regmatch (). Allow interrupting them with CTRL - C . */ <nl> + fast_breakcheck (); <nl> + if ( got_int ) <nl> + return FALSE ; <nl> + <nl> nfa_match = FALSE ; <nl>  <nl> /* Allocate memory for the lists of nodes . */
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5063 , <nl> /**/ <nl> 5062 , <nl> /**/mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5063 , <nl> /**/ <nl> 5062 , <nl> /**/ <nl> theend : <nl> static void <nl> append_command ( char_u * cmd ) <nl> { <nl> - char_u * s = cmd ; <nl> - char_u * d ; <nl> + size_t len = STRLEN ( IObuff ); <nl> + char_u * s = cmd ; <nl> + char_u * d ; <nl>  <nl> + if ( len > IOSIZE - 100 ) <nl> + { <nl> + // Not enough space , truncate and put in "...". <nl> + d = IObuff + IOSIZE - 100 ; <nl> + d -= mb_head_off ( IObuff , d ); <nl> + STRCPY ( d , "..."); <nl> + } <nl> STRCAT ( IObuff , ": "); <nl> d = IObuff + STRLEN ( IObuff ); <nl> while (* s != NUL && d - IObuff + 5 < IOSIZE )
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4219 , <nl> /**/ <nl> 4218 , <nl> /**/mmm src / register . c <nl> ppp src / register . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4219 , <nl> /**/ <nl> 4218 , <nl> /**/ <nl> yank_copy_line ( struct block_def * bd , long y_idx , int exclude_trailing_space ) <nl> { <nl> int s = bd -> textlen + bd -> endspaces ; <nl>  <nl> - while ( VIM_ISWHITE (*( bd -> textstart + s - 1 )) && s > 0 ) <nl> + while ( s > 0 && VIM_ISWHITE (*( bd -> textstart + s - 1 ))) <nl> { <nl> s = s - (* mb_head_off )( bd -> textstart , bd -> textstart + s - 1 ) - 1 ; <nl> pnew --;
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 209 , <nl> /**/ <nl> 208 , <nl> /**/mmm src / if_python . c <nl> ppp src / if_python . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 209 , <nl> /**/ <nl> 208 , <nl> /**/ <nl> py_fix_cursor ( int lo , int hi , int extra ) <nl> curwin -> w_cursor . lnum = lo ; <nl> check_cursor (); <nl> } <nl> + else <nl> + check_cursor_col (); <nl> changed_cline_bef_curs (); <nl> } <nl> invalidate_botline (); <nl> SetBufferLine ( buf_T * buf , int n , PyObject * line , int * len_change ) <nl>  <nl> curbuf = savebuf ; <nl>  <nl> + /* Check that the cursor is not beyond the end of the line now . */ <nl> + if ( buf == curwin -> w_buffer ) <nl> + check_cursor_col (); <nl> + <nl> if ( PyErr_Occurred () || VimErrorCheck ()) <nl> return FAIL ; <nl> 
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 946 , <nl> /**/ <nl> 945 , <nl> /**/mmm src / term . c <nl> ppp src / term . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 946 , <nl> /**/ <nl> 945 , <nl> /**/ <nl> check_termcode ( max_offset , buf , bufsize , buflen ) <nl> * The final byte is ' R '. now it is only used for checking for <nl> * ambiguous - width character state . <nl> */ <nl> + p = tp [ 0 ] == CSI ? tp + 1 : tp + 2 ; <nl> if ((* T_CRV != NUL || * T_U7 != NUL ) <nl> && (( tp [ 0 ] == ESC && tp [ 1 ] == '[' && len >= 3 ) <nl> - || ( tp [ 0 ] == CSI && len >= 2 ))) <nl> + || ( tp [ 0 ] == CSI && len >= 2 )) <nl> + && ( VIM_ISDIGIT (* p ) || * p == '>' || * p == '?')) <nl> { <nl> j = 0 ; <nl> extra = 0 ; <nl> check_termcode ( max_offset , buf , bufsize , buflen ) <nl> && !( tp [ i ] >= '{' && tp [ i ] <= '~') <nl> && ! ASCII_ISALPHA ( tp [ i ]); ++ i ) <nl> if ( tp [ i ] == ';' && ++ j == 1 ) <nl> - extra = atoi (( char *) tp + i + 1 ); <nl> + extra = i + 1 ; <nl> if ( i == len ) <nl> return - 1 ; /* not enough characters */ <nl>  <nl> check_termcode ( max_offset , buf , bufsize , buflen ) <nl> # ifdef FEAT_AUTOCMD <nl> did_cursorhold = TRUE ; <nl> # endif <nl> + if ( extra > 0 ) <nl> + extra = atoi (( char *) tp + extra ); <nl> if ( extra == 2 ) <nl> aw = " single "; <nl> else if ( extra == 3 ) <nl> check_termcode ( max_offset , buf , bufsize , buflen ) <nl> /* rxvt sends its version number : " 20703 " is 2 . 7 . 3 . <nl> * Ignore it for when the user has set ' term ' to xterm , <nl> * even though it ' s an rxvt . */ <nl> + if ( extra > 0 ) <nl> + extra = atoi (( char *) tp + extra ); <nl> if ( extra > 20000 ) <nl> extra = 0 ; <nl> 
mmm src / scriptfile . c <nl> ppp src / scriptfile . c <nl> get_one_sourceline ( source_cookie_T * sp ) <nl> break ; <nl> buf = ( char_u *) ga . ga_data ; <nl> buf [ ga . ga_len ++] = NUL ; <nl> + len = ga . ga_len ; <nl> } <nl> else <nl> { <nl> get_one_sourceline ( source_cookie_T * sp ) <nl> if ( fgets (( char *) buf + ga . ga_len , ga . ga_maxlen - ga . ga_len , <nl> sp -> fp ) == NULL ) <nl> break ; <nl> + len = ga . ga_len + ( int ) STRLEN ( buf + ga . ga_len ); <nl> } <nl> - len = ga . ga_len + ( int ) STRLEN ( buf + ga . ga_len ); <nl> # ifdef USE_CRNL <nl> // Ignore a trailing CTRL - Z , when in Dos mode . Only recognize the <nl> // CTRL - Z by its own , or after a NL .mmm src / version . c <nl> ppp src / version . c <nl> get_one_sourceline ( source_cookie_T * sp ) <nl> break ; <nl> buf = ( char_u *) ga . ga_data ; <nl> buf [ ga . ga_len ++] = NUL ; <nl> + len = ga . ga_len ; <nl> } <nl> else <nl> { <nl> get_one_sourceline ( source_cookie_T * sp ) <nl> if ( fgets (( char *) buf + ga . ga_len , ga . ga_maxlen - ga . ga_len , <nl> sp -> fp ) == NULL ) <nl> break ; <nl> + len = ga . ga_len + ( int ) STRLEN ( buf + ga . ga_len ); <nl> } <nl> - len = ga . ga_len + ( int ) STRLEN ( buf + ga . ga_len ); <nl> # ifdef USE_CRNL <nl> // Ignore a trailing CTRL - Z , when in Dos mode . Only recognize the <nl> // CTRL - Z by its own , or after a NL . <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4974 , <nl> /**/ <nl> 4973 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3923 , <nl> /**/ <nl> 3922 , <nl> /**/mmm src / userfunc . c <nl> ppp src / userfunc . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3923 , <nl> /**/ <nl> 3922 , <nl> /**/ <nl> get_function_args ( <nl> if ( theline == NULL ) <nl> break ; <nl> vim_free (* line_to_free ); <nl> + if (* eap -> cmdlinep == * line_to_free ) <nl> + * eap -> cmdlinep = theline ; <nl> * line_to_free = theline ; <nl> whitep = ( char_u *)" "; <nl> p = skipwhite ( theline );
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3949 , <nl> /**/ <nl> 3948 , <nl> /**/mmm src / regexp . c <nl> ppp src / regexp . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3949 , <nl> /**/ <nl> 3948 , <nl> /**/ <nl> reg_match_visual ( void ) <nl> if ( lnum < top . lnum || lnum > bot . lnum ) <nl> return FALSE ; <nl>  <nl> + col = ( colnr_T )( rex . input - rex . line ); <nl> if ( mode == ' v ') <nl> { <nl> - col = ( colnr_T )( rex . input - rex . line ); <nl> if (( lnum == top . lnum && col < top . col ) <nl> || ( lnum == bot . lnum && col >= bot . col + (* p_sel != ' e '))) <nl> return FALSE ; <nl> reg_match_visual ( void ) <nl> end = end2 ; <nl> if ( top . col == MAXCOL || bot . col == MAXCOL || curswant == MAXCOL ) <nl> end = MAXCOL ; <nl> - cols = win_linetabsize ( wp , rex . line , ( colnr_T )( rex . input - rex . line )); <nl> + <nl> + // getvvcol () flushes rex . line , need to get it again <nl> + rex . line = reg_getline ( rex . lnum ); <nl> + rex . input = rex . line + col ; <nl> + <nl> + cols = win_linetabsize ( wp , rex . line , col ); <nl> if ( cols < start || cols > end - (* p_sel == ' e ')) <nl> return FALSE ; <nl> }
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 352 , <nl> /**/ <nl> 351 , <nl> /**/mmm src / gui_w48 . c <nl> ppp src / gui_w48 . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 352 , <nl> /**/ <nl> 351 , <nl> /**/ <nl> _TextAreaWndProc ( <nl> case WM_NOTIFY : Handle_WM_Notify ( hwnd , ( LPNMHDR ) lParam ); <nl> return TRUE ; <nl> # endif <nl> + /* Workaround for the problem that MyWindowProc () returns FALSE on 64 <nl> + * bit windows when cross - compiled using Mingw libraries . ( Andy <nl> + * Kittner ) */ <nl> + case WM_NCCREATE : <nl> + MyWindowProc ( hwnd , uMsg , wParam , lParam ); <nl> + return TRUE ; <nl>  <nl> - default : <nl> - return MyWindowProc ( hwnd , uMsg , wParam , lParam ); <nl> + default : <nl> + return MyWindowProc ( hwnd , uMsg , wParam , lParam ); <nl> } <nl> } <nl> 
mmm src / diff . c <nl> ppp src / diff . c <nl> diff_mark_adjust_tp ( <nl> for ( i = 0 ; i < DB_COUNT ; ++ i ) <nl> if ( tp -> tp_diffbuf [ i ] != NULL && i != idx ) <nl> { <nl> - dp -> df_lnum [ i ] -= off ; <nl> + if ( dp -> df_lnum [ i ] > off ) <nl> + dp -> df_lnum [ i ] -= off ; <nl> + else <nl> + dp -> df_lnum [ i ] = 1 ; <nl> dp -> df_count [ i ] += n ; <nl> } <nl> } <nl> ex_diffgetput ( exarg_T * eap ) <nl> { <nl> // remember deleting the last line of the buffer <nl> buf_empty = curbuf -> b_ml . ml_line_count == 1 ; <nl> - ml_delete ( lnum ); <nl> - -- added ; <nl> + if ( ml_delete ( lnum ) == OK ) <nl> + -- added ; <nl> } <nl> for ( i = 0 ; i < dp -> df_count [ idx_from ] - start_skip - end_skip ; ++ i ) <nl> {mmm src / version . c <nl> ppp src / version . c <nl> diff_mark_adjust_tp ( <nl> for ( i = 0 ; i < DB_COUNT ; ++ i ) <nl> if ( tp -> tp_diffbuf [ i ] != NULL && i != idx ) <nl> { <nl> - dp -> df_lnum [ i ] -= off ; <nl> + if ( dp -> df_lnum [ i ] > off ) <nl> + dp -> df_lnum [ i ] -= off ; <nl> + else <nl> + dp -> df_lnum [ i ] = 1 ; <nl> dp -> df_count [ i ] += n ; <nl> } <nl> } <nl> ex_diffgetput ( exarg_T * eap ) <nl> { <nl> // remember deleting the last line of the buffer <nl> buf_empty = curbuf -> b_ml . ml_line_count == 1 ; <nl> - ml_delete ( lnum ); <nl> - -- added ; <nl> + if ( ml_delete ( lnum ) == OK ) <nl> + -- added ; <nl> } <nl> for ( i = 0 ; i < dp -> df_count [ idx_from ] - start_skip - end_skip ; ++ i ) <nl> { <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 101 , <nl> /**/ <nl> 100 , <nl> /**/
mmm src / indent . c <nl> ppp src / indent . c <nl> change_indent ( <nl> new_cursor_col += (* mb_ptr2len )( ptr + new_cursor_col ); <nl> else <nl> ++ new_cursor_col ; <nl> + if ( ptr [ new_cursor_col ] == NUL ) <nl> + break ; <nl> vcol += lbr_chartabsize ( ptr , ptr + new_cursor_col , ( colnr_T ) vcol ); <nl> } <nl> vcol = last_vcol ;mmm src / version . c <nl> ppp src / version . c <nl> change_indent ( <nl> new_cursor_col += (* mb_ptr2len )( ptr + new_cursor_col ); <nl> else <nl> ++ new_cursor_col ; <nl> + if ( ptr [ new_cursor_col ] == NUL ) <nl> + break ; <nl> vcol += lbr_chartabsize ( ptr , ptr + new_cursor_col , ( colnr_T ) vcol ); <nl> } <nl> vcol = last_vcol ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4436 , <nl> /**/ <nl> 4435 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 322 , <nl> /**/ <nl> 321 , <nl> /**/mmm src / quickfix . c <nl> ppp src / quickfix . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 322 , <nl> /**/ <nl> 321 , <nl> /**/ <nl> qf_fill_buffer ( qf_list_T * qfl , buf_T * buf , qfline_T * old_last , int qf_winid ) <nl> } <nl>  <nl> // Check if there is anything to display <nl> - if ( qfl != NULL ) <nl> + if ( qfl != NULL && qfl -> qf_start != NULL ) <nl> { <nl> char_u dirname [ MAXPATHL ]; <nl> int invalid_val = FALSE ;
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 316 , <nl> /**/ <nl> 315 , <nl> /**/mmm src / option . c <nl> ppp src / option . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 316 , <nl> /**/ <nl> 315 , <nl> /**/ <nl> check_colorcolumn ( wp ) <nl> int i ; <nl> int j = 0 ; <nl>  <nl> + if ( wp -> w_buffer == NULL ) <nl> + return NULL ; /* buffer was closed */ <nl> + <nl> for ( s = wp -> w_p_cc ; * s != NUL && count < 255 ;) <nl> { <nl> if (* s == '-' || * s == '+')
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4975 , <nl> /**/ <nl> 4974 , <nl> /**/mmm src / ex_getln . c <nl> ppp src / ex_getln . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4975 , <nl> /**/ <nl> 4974 , <nl> /**/ <nl> getcmdline_int ( <nl> int indent , // indent for inside conditionals <nl> int clear_ccline ) // clear ccline first <nl> { <nl> + static int depth = 0 ; // call depth <nl> int c ; <nl> int i ; <nl> int j ; <nl> getcmdline_int ( <nl> int cmdline_type ; <nl> int wild_type ; <nl>  <nl> + // one recursion level deeper <nl> + ++ depth ; <nl> + <nl> if ( ccline . cmdbuff != NULL ) <nl> { <nl> // Being called recursively . Since ccline is global , we need to save <nl> getcmdline_int ( <nl> if ( init_ccline ( firstc , indent ) != OK ) <nl> goto theend ; // out of memory <nl>  <nl> + if ( depth == 50 ) <nl> + { <nl> + // Somehow got into a loop recursively calling getcmdline (), bail out . <nl> + emsg ( _ ( e_command_too_recursive )); <nl> + goto theend ; <nl> + } <nl> + <nl> ExpandInit (& xpc ); <nl> ccline . xpc = & xpc ; <nl>  <nl> theend : <nl> { <nl> char_u * p = ccline . cmdbuff ; <nl>  <nl> + -- depth ; <nl> if ( did_save_ccline ) <nl> restore_cmdline (& save_ccline ); <nl> else
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 801 , <nl> /**/ <nl> 800 , <nl> /**/mmm src / window . c <nl> ppp src / window . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 801 , <nl> /**/ <nl> 800 , <nl> /**/ <nl> win_goto ( wp ) <nl>  <nl> # ifdef FEAT_CONCEAL <nl> /* Conceal cursor line in previous window , unconceal in current window . */ <nl> - if ( win_valid ( owp )) <nl> + if ( win_valid ( owp ) && owp -> w_p_cole > 0 && ! msg_scrolled ) <nl> update_single_line ( owp , owp -> w_cursor . lnum ); <nl> - update_single_line ( curwin , curwin -> w_cursor . lnum ); <nl> + if ( curwin -> w_p_cole > 0 && ! msg_scrolled ) <nl> + need_cursor_line_redraw = TRUE ; <nl> # endif <nl> } <nl> 
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1365 , <nl> /**/ <nl> 1364 , <nl> /**/mmm src / getchar . c <nl> ppp src / getchar . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1365 , <nl> /**/ <nl> 1364 , <nl> /**/ <nl> openscript ( <nl> emsg ( _ ( e_nesting )); <nl> return ; <nl> } <nl> + <nl> + // Disallow sourcing a file in the sandbox , the commands would be executed <nl> + // later , possibly outside of the sandbox . <nl> + if ( check_secure ()) <nl> + return ; <nl> + <nl> # ifdef FEAT_EVAL <nl> if ( ignore_script ) <nl> /* Not reading from script , also don ' t open one . Warning message ? */
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4925 , <nl> /**/ <nl> 4924 , <nl> /**/mmm src / textobject . c <nl> ppp src / textobject . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4925 , <nl> /**/ <nl> 4924 , <nl> /**/ <nl> find_next_quote ( <nl> if ( c == NUL ) <nl> return - 1 ; <nl> else if ( escape != NULL && vim_strchr ( escape , c )) <nl> + { <nl> ++ col ; <nl> + if ( line [ col ] == NUL ) <nl> + return - 1 ; <nl> + } <nl> else if ( c == quotechar ) <nl> break ; <nl> if ( has_mbyte )
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 35 , <nl> /**/ <nl> 34 , <nl> /**/mmm src / spell . c <nl> ppp src / spell . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 35 , <nl> /**/ <nl> 34 , <nl> /**/ <nl> spell_dump_compl ( <nl> n = arridx [ depth ] + curi [ depth ]; <nl> ++ curi [ depth ]; <nl> c = byts [ n ]; <nl> - if ( c == 0 ) <nl> + if ( c == 0 || depth >= MAXWLEN - 1 ) <nl> { <nl> - // End of word , deal with the word . <nl> + // End of word or reached maximum length , deal with the <nl> + // word . <nl> // Don ' t use keep - case words in the fold - case tree , <nl> // they will appear in the keep - case tree . <nl> // Only use the word when the region matches .
mmm src / charset . c <nl> ppp src / charset . c <nl> vim_isupper ( int c ) <nl> return isupper ( c ); <nl> } <nl>  <nl> + int <nl> + vim_isalpha ( int c ) <nl> +{ <nl> + return vim_islower ( c ) || vim_isupper ( c ); <nl> +} <nl> + <nl> int <nl> vim_toupper ( int c ) <nl> {mmm src / filepath . c <nl> ppp src / filepath . c <nl> vim_isupper ( int c ) <nl> return isupper ( c ); <nl> } <nl>  <nl> + int <nl> + vim_isalpha ( int c ) <nl> +{ <nl> + return vim_islower ( c ) || vim_isupper ( c ); <nl> +} <nl> + <nl> int <nl> vim_toupper ( int c ) <nl> { <nl> unix_expandpath ( <nl> else if ( path_end >= path + wildoff <nl> && ( vim_strchr (( char_u *)"*?[{~$", * path_end ) != NULL <nl> || (! p_fic && ( flags & EW_ICASE ) <nl> - && isalpha ( PTR2CHAR ( path_end ))))) <nl> + && vim_isalpha ( PTR2CHAR ( path_end ))))) <nl> e = p ; <nl> if ( has_mbyte ) <nl> {mmm src / version . c <nl> ppp src / version . c <nl> vim_isupper ( int c ) <nl> return isupper ( c ); <nl> } <nl>  <nl> + int <nl> + vim_isalpha ( int c ) <nl> +{ <nl> + return vim_islower ( c ) || vim_isupper ( c ); <nl> +} <nl> + <nl> int <nl> vim_toupper ( int c ) <nl> { <nl> unix_expandpath ( <nl> else if ( path_end >= path + wildoff <nl> && ( vim_strchr (( char_u *)"*?[{~$", * path_end ) != NULL <nl> || (! p_fic && ( flags & EW_ICASE ) <nl> - && isalpha ( PTR2CHAR ( path_end ))))) <nl> + && vim_isalpha ( PTR2CHAR ( path_end ))))) <nl> e = p ; <nl> if ( has_mbyte ) <nl> { <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4418 , <nl> /**/ <nl> 4417 , <nl> /**/
mmm src / spellsuggest . c <nl> ppp src / spellsuggest . c <nl> spell_suggest ( int count ) <nl> curwin -> w_cursor . col = VIsual . col ; <nl> ++ badlen ; <nl> end_visual_mode (); <nl> + // make sure we don ' t include the NUL at the end of the line <nl> + line = ml_get_curline (); <nl> + if ( badlen > STRLEN ( line ) - curwin -> w_cursor . col ) <nl> + badlen = STRLEN ( line ) - curwin -> w_cursor . col ; <nl> } <nl> // Find the start of the badly spelled word . <nl> else if ( spell_move_to ( curwin , FORWARD , TRUE , TRUE , NULL ) == 0mmm src / version . c <nl> ppp src / version . c <nl> spell_suggest ( int count ) <nl> curwin -> w_cursor . col = VIsual . col ; <nl> ++ badlen ; <nl> end_visual_mode (); <nl> + // make sure we don ' t include the NUL at the end of the line <nl> + line = ml_get_curline (); <nl> + if ( badlen > STRLEN ( line ) - curwin -> w_cursor . col ) <nl> + badlen = STRLEN ( line ) - curwin -> w_cursor . col ; <nl> } <nl> // Find the start of the badly spelled word . <nl> else if ( spell_move_to ( curwin , FORWARD , TRUE , TRUE , NULL ) == 0 <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4563 , <nl> /**/ <nl> 4562 , <nl> /**/
mmm src / vim9expr . c <nl> ppp src / vim9expr . c <nl> compile_get_env ( char_u ** arg , cctx_T * cctx ) <nl> len = get_env_len ( arg ); <nl> if ( len == 0 ) <nl> { <nl> - semsg ( _ ( e_syntax_error_at_str ), start - 1 ); <nl> + semsg ( _ ( e_syntax_error_at_str ), start ); <nl> return FAIL ; <nl> } <nl> mmm src / version . c <nl> ppp src / version . c <nl> compile_get_env ( char_u ** arg , cctx_T * cctx ) <nl> len = get_env_len ( arg ); <nl> if ( len == 0 ) <nl> { <nl> - semsg ( _ ( e_syntax_error_at_str ), start - 1 ); <nl> + semsg ( _ ( e_syntax_error_at_str ), start ); <nl> return FAIL ; <nl> } <nl>  <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4049 , <nl> /**/ <nl> 4048 , <nl> /**/
mmm src / ex_getln . c <nl> ppp src / ex_getln . c <nl> cmdline_insert_reg ( int * gotesc UNUSED ) <nl> { <nl> int i ; <nl> int c ; <nl> + int save_new_cmdpos = new_cmdpos ; <nl>  <nl> # ifdef USE_ON_FLY_SCROLL <nl> dont_scroll = TRUE ; // disallow scrolling here <nl> cmdline_insert_reg ( int * gotesc UNUSED ) <nl> # ifdef FEAT_EVAL <nl> /* <nl> * Insert the result of an expression . <nl> - * Need to save the current command line , to be able to enter <nl> - * a new one ... <nl> */ <nl> new_cmdpos = - 1 ; <nl> if ( c == '=') <nl> cmdline_insert_reg ( int * gotesc UNUSED ) <nl> } <nl> # endif <nl> } <nl> + new_cmdpos = save_new_cmdpos ; <nl> + <nl> // remove the double quote <nl> redrawcmd (); <nl> mmm src / version . c <nl> ppp src / version . c <nl> cmdline_insert_reg ( int * gotesc UNUSED ) <nl> { <nl> int i ; <nl> int c ; <nl> + int save_new_cmdpos = new_cmdpos ; <nl>  <nl> # ifdef USE_ON_FLY_SCROLL <nl> dont_scroll = TRUE ; // disallow scrolling here <nl> cmdline_insert_reg ( int * gotesc UNUSED ) <nl> # ifdef FEAT_EVAL <nl> /* <nl> * Insert the result of an expression . <nl> - * Need to save the current command line , to be able to enter <nl> - * a new one ... <nl> */ <nl> new_cmdpos = - 1 ; <nl> if ( c == '=') <nl> cmdline_insert_reg ( int * gotesc UNUSED ) <nl> } <nl> # endif <nl> } <nl> + new_cmdpos = save_new_cmdpos ; <nl> + <nl> // remove the double quote <nl> redrawcmd (); <nl>  <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5148 , <nl> /**/ <nl> 5147 , <nl> /**/
mmm src / eval . c <nl> ppp src / eval . c <nl> eval_lambda ( <nl> ++* arg ; <nl> ret = eval1 ( arg , rettv , evalarg ); <nl> * arg = skipwhite_and_linebreak (* arg , evalarg ); <nl> - if (** arg != ')') <nl> + if (** arg == ')') <nl> + { <nl> + ++* arg ; <nl> + } <nl> + else <nl> { <nl> emsg ( _ ( e_missing_closing_paren )); <nl> ret = FAIL ; <nl> } <nl> - ++* arg ; <nl> } <nl> if ( ret != OK ) <nl> return FAIL ;mmm src / version . c <nl> ppp src / version . c <nl> eval_lambda ( <nl> ++* arg ; <nl> ret = eval1 ( arg , rettv , evalarg ); <nl> * arg = skipwhite_and_linebreak (* arg , evalarg ); <nl> - if (** arg != ')') <nl> + if (** arg == ')') <nl> + { <nl> + ++* arg ; <nl> + } <nl> + else <nl> { <nl> emsg ( _ ( e_missing_closing_paren )); <nl> ret = FAIL ; <nl> } <nl> - ++* arg ; <nl> } <nl> if ( ret != OK ) <nl> return FAIL ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3847 , <nl> /**/ <nl> 3846 , <nl> /**/
mmm src / cindent . c <nl> ppp src / cindent . c <nl> skip_string ( char_u * p ) <nl> while ( vim_isdigit ( p [ i - 1 ])) // '\ 000 ' <nl> ++ i ; <nl> } <nl> - if ( p [ i ] == '\'') // check for trailing ' <nl> + if ( p [ i - 1 ] != NUL && p [ i ] == '\'') // check for trailing ' <nl> { <nl> p += i ; <nl> continue ;mmm src / version . c <nl> ppp src / version . c <nl> skip_string ( char_u * p ) <nl> while ( vim_isdigit ( p [ i - 1 ])) // '\ 000 ' <nl> ++ i ; <nl> } <nl> - if ( p [ i ] == '\'') // check for trailing ' <nl> + if ( p [ i - 1 ] != NUL && p [ i ] == '\'') // check for trailing ' <nl> { <nl> p += i ; <nl> continue ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4968 , <nl> /**/ <nl> 4967 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3612 , <nl> /**/ <nl> 3611 , <nl> /**/mmm src / regexp_nfa . c <nl> ppp src / regexp_nfa . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3612 , <nl> /**/ <nl> 3611 , <nl> /**/ <nl> nfa_regmatch ( <nl> case NFA_MARK_GT : <nl> case NFA_MARK_LT : <nl> { <nl> + size_t col = rex . input - rex . line ; <nl> pos_T * pos = getmark_buf ( rex . reg_buf , t -> state -> val , FALSE ); <nl>  <nl> + // Line may have been freed , get it again . <nl> + if ( REG_MULTI ) <nl> + { <nl> + rex . line = reg_getline ( rex . lnum ); <nl> + rex . input = rex . line + col ; <nl> + } <nl> + <nl> // Compare the mark position to the match position , if the mark <nl> // exists and mark is set in reg_buf . <nl> if ( pos != NULL && pos -> lnum > 0 )mmm src / regexp . c <nl> ppp src / regexp . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3612 , <nl> /**/ <nl> 3611 , <nl> /**/ <nl> nfa_regmatch ( <nl> case NFA_MARK_GT : <nl> case NFA_MARK_LT : <nl> { <nl> + size_t col = rex . input - rex . line ; <nl> pos_T * pos = getmark_buf ( rex . reg_buf , t -> state -> val , FALSE ); <nl>  <nl> + // Line may have been freed , get it again . <nl> + if ( REG_MULTI ) <nl> + { <nl> + rex . line = reg_getline ( rex . lnum ); <nl> + rex . input = rex . line + col ; <nl> + } <nl> + <nl> // Compare the mark position to the match position , if the mark <nl> // exists and mark is set in reg_buf . <nl> if ( pos != NULL && pos -> lnum > 0 ) <nl> typedef struct { <nl> // The current match - position is stord in these variables : <nl> linenr_T lnum ; // line number , relative to first line <nl> char_u * line ; // start of current line <nl> - char_u * input ; // current input , points into " regline " <nl> + char_u * input ; // current input , points into " line " <nl>  <nl> int need_clear_subexpr ; // subexpressions still need to be cleared <nl> # ifdef FEAT_SYN_HL
mmm src / regexp_bt . c <nl> ppp src / regexp_bt . c <nl> regmatch ( <nl> if ( rex . input == rex . line ) <nl> { <nl> // backup to last char of previous line <nl> + if ( rex . lnum == 0 ) <nl> + { <nl> + status = RA_NOMATCH ; <nl> + break ; <nl> + } <nl> -- rex . lnum ; <nl> rex . line = reg_getline ( rex . lnum ); <nl> // Just in case regrepeat () didn ' t countmmm src / version . c <nl> ppp src / version . c <nl> regmatch ( <nl> if ( rex . input == rex . line ) <nl> { <nl> // backup to last char of previous line <nl> + if ( rex . lnum == 0 ) <nl> + { <nl> + status = RA_NOMATCH ; <nl> + break ; <nl> + } <nl> -- rex . lnum ; <nl> rex . line = reg_getline ( rex . lnum ); <nl> // Just in case regrepeat () didn ' t count <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4440 , <nl> /**/ <nl> 4439 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3409 , <nl> /**/ <nl> 3408 , <nl> /**/mmm src / regexp_nfa . c <nl> ppp src / regexp_nfa . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3409 , <nl> /**/ <nl> 3408 , <nl> /**/ <nl> find_match_text ( colnr_T startcol , int regstart , char_u * match_text ) <nl> match = FALSE ; <nl> break ; <nl> } <nl> - len2 += MB_CHAR2LEN ( c2 ); <nl> + len2 += enc_utf8 ? utf_ptr2len ( rex . line + col + len2 ) <nl> + : MB_CHAR2LEN ( c2 ); <nl> } <nl> if ( match <nl> // check that no composing char follows
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 240 , <nl> /**/ <nl> 239 , <nl> /**/mmm src / spellfile . c <nl> ppp src / spellfile . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 240 , <nl> /**/ <nl> 239 , <nl> /**/ <nl> sug_filltree ( spellinfo_T * spin , slang_T * slang ) <nl>  <nl> /* <nl> * Go through the whole case - folded tree , soundfold each word and put it <nl> - * in the trie . <nl> + * in the trie . Bail out if the tree is empty . <nl> */ <nl> byts = slang -> sl_fbyts ; <nl> idxs = slang -> sl_fidxs ; <nl> + if ( byts == NULL || idxs == NULL ) <nl> + return FAIL ; <nl>  <nl> arridx [ 0 ] = 0 ; <nl> curi [ 0 ] = 1 ;
mmm src / eval . c <nl> ppp src / eval . c <nl> eval_expr_typval ( typval_T * expr , typval_T * argv , int argc , typval_T * rettv ) <nl> if ( fc == NULL ) <nl> return FAIL ; <nl>  <nl> - // Shortcut to call a compiled function without overhead . <nl> + // Shortcut to call a compiled function with minimal overhead . <nl> r = call_def_function ( partial -> pt_func , argc , argv , <nl> DEF_USE_PT_ARGV , partial , fc , rettv ); <nl> remove_funccal (); <nl> eval_next_non_blank ( char_u * arg , evalarg_T * evalarg , int * getnext ) <nl>  <nl> if ( next != NULL ) <nl> { <nl> - * getnext = TRUE ; <nl> + * getnext = * p != NL ; <nl> return skipwhite ( next ); <nl> } <nl> }mmm src / version . c <nl> ppp src / version . c <nl> eval_expr_typval ( typval_T * expr , typval_T * argv , int argc , typval_T * rettv ) <nl> if ( fc == NULL ) <nl> return FAIL ; <nl>  <nl> - // Shortcut to call a compiled function without overhead . <nl> + // Shortcut to call a compiled function with minimal overhead . <nl> r = call_def_function ( partial -> pt_func , argc , argv , <nl> DEF_USE_PT_ARGV , partial , fc , rettv ); <nl> remove_funccal (); <nl> eval_next_non_blank ( char_u * arg , evalarg_T * evalarg , int * getnext ) <nl>  <nl> if ( next != NULL ) <nl> { <nl> - * getnext = TRUE ; <nl> + * getnext = * p != NL ; <nl> return skipwhite ( next ); <nl> } <nl> } <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 552 , <nl> /**/ <nl> 551 , <nl> /**/
mmm src / indent . c <nl> ppp src / indent . c <nl> ex_retab ( exarg_T * eap ) <nl> if ( ptr [ col ] == NUL ) <nl> break ; <nl> vcol += chartabsize ( ptr + col , ( colnr_T ) vcol ); <nl> + if ( vcol >= MAXCOL ) <nl> + { <nl> + emsg ( _ ( e_resulting_text_too_long )); <nl> + break ; <nl> + } <nl> if ( has_mbyte ) <nl> col += (* mb_ptr2len )( ptr + col ); <nl> elsemmm src / version . c <nl> ppp src / version . c <nl> ex_retab ( exarg_T * eap ) <nl> if ( ptr [ col ] == NUL ) <nl> break ; <nl> vcol += chartabsize ( ptr + col , ( colnr_T ) vcol ); <nl> + if ( vcol >= MAXCOL ) <nl> + { <nl> + emsg ( _ ( e_resulting_text_too_long )); <nl> + break ; <nl> + } <nl> if ( has_mbyte ) <nl> col += (* mb_ptr2len )( ptr + col ); <nl> else <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4359 , <nl> /**/ <nl> 4358 , <nl> /**/
mmm src / arglist . c <nl> ppp src / arglist . c <nl> do_arg_all ( <nl> tabpage_T * old_curtab , * last_curtab ; <nl> win_T * new_curwin = NULL ; <nl> tabpage_T * new_curtab = NULL ; <nl> + int prev_arglist_locked = arglist_locked ; <nl>  <nl> # ifdef FEAT_CMDWIN <nl> if ( cmdwin_type != 0 ) <nl> do_arg_all ( <nl> // watch out for its size to be changed . <nl> alist = curwin -> w_alist ; <nl> ++ alist -> al_refcount ; <nl> + arglist_locked = TRUE ; <nl>  <nl> old_curwin = curwin ; <nl> old_curtab = curtab ; <nl> do_arg_all ( <nl>  <nl> // Remove the " lock " on the argument list . <nl> alist_unlink ( alist ); <nl> + arglist_locked = prev_arglist_locked ; <nl>  <nl> -- autocmd_no_enter ; <nl> mmm src / version . c <nl> ppp src / version . c <nl> do_arg_all ( <nl> tabpage_T * old_curtab , * last_curtab ; <nl> win_T * new_curwin = NULL ; <nl> tabpage_T * new_curtab = NULL ; <nl> + int prev_arglist_locked = arglist_locked ; <nl>  <nl> # ifdef FEAT_CMDWIN <nl> if ( cmdwin_type != 0 ) <nl> do_arg_all ( <nl> // watch out for its size to be changed . <nl> alist = curwin -> w_alist ; <nl> ++ alist -> al_refcount ; <nl> + arglist_locked = TRUE ; <nl>  <nl> old_curwin = curwin ; <nl> old_curtab = curtab ; <nl> do_arg_all ( <nl>  <nl> // Remove the " lock " on the argument list . <nl> alist_unlink ( alist ); <nl> + arglist_locked = prev_arglist_locked ; <nl>  <nl> -- autocmd_no_enter ; <nl>  <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3884 , <nl> /**/ <nl> 3883 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3564 , <nl> /**/ <nl> 3563 , <nl> /**/mmm src / move . c <nl> ppp src / move . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3564 , <nl> /**/ <nl> 3563 , <nl> /**/ <nl> update_topline ( void ) <nl> check_cursor_lnum (); <nl> curwin -> w_topline = curwin -> w_cursor . lnum ; <nl> curwin -> w_botline = curwin -> w_topline ; <nl> - curwin -> w_valid |= VALID_BOTLINE | VALID_BOTLINE_AP ; <nl> curwin -> w_scbind_pos = 1 ; <nl> return ; <nl> }
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1800 , <nl> /**/ <nl> 1799 , <nl> /**/mmm src / gui_x11 . c <nl> ppp src / gui_x11 . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1800 , <nl> /**/ <nl> 1799 , <nl> /**/ <nl> gui_mch_get_color ( char_u * name ) <nl> guicolor_T <nl> gui_mch_get_rgb_color ( int r , int g , int b ) <nl> { <nl> - char spec [ 8 ]; /* space enough to hold "# RRGGBB " */ <nl> XColor available ; <nl> Colormap colormap ; <nl>  <nl> +/* Using XParseColor () is very slow , put rgb in XColor directly . <nl> + <nl> + char spec [ 8 ]; // space enough to hold "# RRGGBB " <nl> vim_snprintf ( spec , sizeof ( spec ), "#%. 2x %. 2x %. 2x ", r , g , b ); <nl> - colormap = DefaultColormap ( gui . dpy , DefaultScreen ( gui . dpy )); <nl> if ( XParseColor ( gui . dpy , colormap , ( char *) spec , & available ) != 0 <nl> && XAllocColor ( gui . dpy , colormap , & available ) != 0 ) <nl> return ( guicolor_T ) available . pixel ; <nl> +*/ <nl> + colormap = DefaultColormap ( gui . dpy , DefaultScreen ( gui . dpy )); <nl> + vim_memset (& available , 0 , sizeof ( XColor )); <nl> + available . red = r << 8 ; <nl> + available . green = g << 8 ; <nl> + available . blue = b << 8 ; <nl> + if ( XAllocColor ( gui . dpy , colormap , & available ) != 0 ) <nl> + return ( guicolor_T ) available . pixel ; <nl>  <nl> return INVALCOLOR ; <nl> }
mmm src / textformat . c <nl> ppp src / textformat . c <nl> op_format ( <nl> { <nl> curwin -> w_cursor = saved_cursor ; <nl> saved_cursor . lnum = 0 ; <nl> + <nl> + // formatting may have made the cursor position invalid <nl> + check_cursor (); <nl> } <nl>  <nl> if ( oap -> is_VIsual )mmm src / version . c <nl> ppp src / version . c <nl> op_format ( <nl> { <nl> curwin -> w_cursor = saved_cursor ; <nl> saved_cursor . lnum = 0 ; <nl> + <nl> + // formatting may have made the cursor position invalid <nl> + check_cursor (); <nl> } <nl>  <nl> if ( oap -> is_VIsual ) <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5013 , <nl> /**/ <nl> 5012 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1143 , <nl> /**/ <nl> 1142 , <nl> /**/mmm src / buffer . c <nl> ppp src / buffer . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1143 , <nl> /**/ <nl> 1142 , <nl> /**/ <nl> build_stl_str_hl ( <nl> # endif <nl> if ( vim_strchr ( STL_ALL , * s ) == NULL ) <nl> { <nl> + if (* s == NUL ) // can happen with "% 0 " <nl> + break ; <nl> s ++; <nl> continue ; <nl> }
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4218 , <nl> /**/ <nl> 4217 , <nl> /**/mmm src / edit . c <nl> ppp src / edit . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4218 , <nl> /**/ <nl> 4217 , <nl> /**/ <nl> bracketed_paste ( paste_mode_T mode , int drop , garray_T * gap ) <nl> break ; <nl>  <nl> case PASTE_EX : <nl> - if ( gap != NULL && ga_grow ( gap , idx ) == OK ) <nl> + // add one for the NUL that is going to be appended <nl> + if ( gap != NULL && ga_grow ( gap , idx + 1 ) == OK ) <nl> { <nl> mch_memmove (( char *) gap -> ga_data + gap -> ga_len , <nl> buf , ( size_t ) idx );
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3487 , <nl> /**/ <nl> 3486 , <nl> /**/mmm src / drawscreen . c <nl> ppp src / drawscreen . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3487 , <nl> /**/ <nl> 3486 , <nl> /**/ <nl> win_redr_status ( win_T * wp , int ignore_pum UNUSED ) <nl> *( p + len ++) = ' '; <nl> if ( bt_help ( wp -> w_buffer )) <nl> { <nl> - STRCPY ( p + len , _ ("[ Help ]")); <nl> + vim_snprintf (( char *) p + len , MAXPATHL - len , "% s ", _ ("[ Help ]")); <nl> len += ( int ) STRLEN ( p + len ); <nl> } <nl> # ifdef FEAT_QUICKFIX <nl> if ( wp -> w_p_pvw ) <nl> { <nl> - STRCPY ( p + len , _ ("[ Preview ]")); <nl> + vim_snprintf (( char *) p + len , MAXPATHL - len , "% s ", _ ("[ Preview ]")); <nl> len += ( int ) STRLEN ( p + len ); <nl> } <nl> # endif <nl> win_redr_status ( win_T * wp , int ignore_pum UNUSED ) <nl> # endif <nl> ) <nl> { <nl> - STRCPY ( p + len , "[+]"); <nl> - len += 3 ; <nl> + vim_snprintf (( char *) p + len , MAXPATHL - len , "% s ", "[+]"); <nl> + len += ( int ) STRLEN ( p + len ); <nl> } <nl> if ( wp -> w_buffer -> b_p_ro ) <nl> { <nl> - STRCPY ( p + len , _ ("[ RO ]")); <nl> + vim_snprintf (( char *) p + len , MAXPATHL - len , "% s ", _ ("[ RO ]")); <nl> len += ( int ) STRLEN ( p + len ); <nl> } <nl> 
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 598 , <nl> /**/ <nl> 597 , <nl> /**/mmm src / window . c <nl> ppp src / window . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 598 , <nl> /**/ <nl> 597 , <nl> /**/ <nl> win_equal_rec ( <nl> if ( hnc ) // add next_curwin size <nl> { <nl> next_curwin_size -= p_wiw - ( m - n ); <nl> + if ( next_curwin_size < 0 ) <nl> + next_curwin_size = 0 ; <nl> new_size += next_curwin_size ; <nl> room -= new_size - next_curwin_size ; <nl> } <nl> scroll_to_fraction ( win_T * wp , int prev_height ) <nl> void <nl> win_new_width ( win_T * wp , int width ) <nl> { <nl> - wp -> w_width = width ; <nl> + // Should we give an error if width < 0 ? <nl> + wp -> w_width = width < 0 ? 0 : width ; <nl> wp -> w_lines_valid = 0 ; <nl> changed_line_abv_curs_win ( wp ); <nl> // Handled in win_fix_scroll ()
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 498 , <nl> /**/ <nl> 497 , <nl> /**/mmm src / ops . c <nl> ppp src / ops . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 498 , <nl> /**/ <nl> 497 , <nl> /**/ <nl> op_delete ( oap ) <nl> did_yank = TRUE ; <nl> } <nl>  <nl> - /* Yank into small delete register when no register specified and the <nl> - * delete is within one line . */ <nl> - if ( oap -> regname == 0 && oap -> motion_type != MLINE <nl> + /* Yank into small delete register when no named register specified <nl> + * and the delete is within one line . */ <nl> + if (( <nl> +# ifdef FEAT_CLIPBOARD <nl> + (( clip_unnamed & CLIP_UNNAMED ) && oap -> regname == '*') || <nl> + (( clip_unnamed & CLIP_UNNAMED_PLUS ) && oap -> regname == '+') || <nl> +# endif <nl> + oap -> regname == 0 ) && oap -> motion_type != MLINE <nl> && oap -> line_count == 1 ) <nl> { <nl> oap -> regname = '-';
mmm src / ex_getln . c <nl> ppp src / ex_getln . c <nl> init_ccline ( int firstc , int indent ) <nl> ccline . cmdindent = ( firstc > 0 ? indent : 0 ); <nl>  <nl> // alloc initial ccline . cmdbuff <nl> - alloc_cmdbuff ( exmode_active ? 250 : indent + 1 ); <nl> + alloc_cmdbuff ( indent + 50 ); <nl> if ( ccline . cmdbuff == NULL ) <nl> return FAIL ; <nl> ccline . cmdlen = ccline . cmdpos = 0 ;mmm src / version . c <nl> ppp src / version . c <nl> init_ccline ( int firstc , int indent ) <nl> ccline . cmdindent = ( firstc > 0 ? indent : 0 ); <nl>  <nl> // alloc initial ccline . cmdbuff <nl> - alloc_cmdbuff ( exmode_active ? 250 : indent + 1 ); <nl> + alloc_cmdbuff ( indent + 50 ); <nl> if ( ccline . cmdbuff == NULL ) <nl> return FAIL ; <nl> ccline . cmdlen = ccline . cmdpos = 0 ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4214 , <nl> /**/ <nl> 4213 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1247 , <nl> /**/ <nl> 1246 , <nl> /**/mmm src / move . c <nl> ppp src / move . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1247 , <nl> /**/ <nl> 1246 , <nl> /**/ <nl> adjust_skipcol ( void ) <nl> return ; <nl>  <nl> int width1 = curwin -> w_width - curwin_col_off (); <nl> + if ( width1 <= 0 ) <nl> + return ; // no text will be displayed <nl> + <nl> int width2 = width1 + curwin_col_off2 (); <nl> long so = get_scrolloff_value (); <nl> int scrolloff_cols = so == 0 ? 0 : width1 + ( so - 1 ) * width2 ;
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 35 , <nl> /**/ <nl> 34 , <nl> /**/mmm src / os_win32 . c <nl> ppp src / os_win32 . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 35 , <nl> /**/ <nl> 34 , <nl> /**/ <nl> get_exe_name ( void ) <nl> * "! xxd " it ' s found in our starting directory . Needed because <nl> * SearchPath () also looks there . */ <nl> p = mch_getenv (" PATH "); <nl> - if ( STRLEN ( p ) + STRLEN ( exe_path ) + 2 < MAXPATHL ); <nl> + if ( STRLEN ( p ) + STRLEN ( exe_path ) + 2 < MAXPATHL ) <nl> { <nl> STRCPY ( temp , p ); <nl> STRCAT ( temp , ";");
mmm src / undo . c <nl> ppp src / undo . c <nl> u_undo_end ( <nl> } <nl> } <nl> # endif <nl> + if ( VIsual_active ) <nl> + check_pos ( curbuf , & VIsual ); <nl>  <nl> smsg_attr_keep ( 0 , _ ("% ld % s ; % s #% ld % s "), <nl> u_oldcount < 0 ? - u_oldcount : u_oldcount ,mmm src / version . c <nl> ppp src / version . c <nl> u_undo_end ( <nl> } <nl> } <nl> # endif <nl> + if ( VIsual_active ) <nl> + check_pos ( curbuf , & VIsual ); <nl>  <nl> smsg_attr_keep ( 0 , _ ("% ld % s ; % s #% ld % s "), <nl> u_oldcount < 0 ? - u_oldcount : u_oldcount , <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4217 , <nl> /**/ <nl> 4216 , <nl> /**/
mmm src / buffer . c <nl> ppp src / buffer . c <nl> fname_match ( <nl> rmp -> rm_ic = p_fic || ignore_case ; <nl> if ( vim_regexec ( rmp , name , ( colnr_T ) 0 )) <nl> match = name ; <nl> - else <nl> + else if ( rmp -> regprog != NULL ) <nl> { <nl> // Replace $( HOME ) with '~' and try matching again . <nl> p = home_replace_save ( NULL , name );mmm src / version . c <nl> ppp src / version . c <nl> fname_match ( <nl> rmp -> rm_ic = p_fic || ignore_case ; <nl> if ( vim_regexec ( rmp , name , ( colnr_T ) 0 )) <nl> match = name ; <nl> - else <nl> + else if ( rmp -> regprog != NULL ) <nl> { <nl> // Replace $( HOME ) with '~' and try matching again . <nl> p = home_replace_save ( NULL , name ); <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4901 , <nl> /**/ <nl> 4900 , <nl> /**/
mmm src / indent . c <nl> ppp src / indent . c <nl> get_lisp_indent ( void ) <nl> amount += 2 ; <nl> else <nl> { <nl> - that ++; <nl> - amount ++; <nl> + if (* that != NUL ) <nl> + { <nl> + that ++; <nl> + amount ++; <nl> + } <nl> firsttry = amount ; <nl>  <nl> while ( VIM_ISWHITE (* that ))mmm src / version . c <nl> ppp src / version . c <nl> get_lisp_indent ( void ) <nl> amount += 2 ; <nl> else <nl> { <nl> - that ++; <nl> - amount ++; <nl> + if (* that != NUL ) <nl> + { <nl> + that ++; <nl> + amount ++; <nl> + } <nl> firsttry = amount ; <nl>  <nl> while ( VIM_ISWHITE (* that )) <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5151 , <nl> /**/ <nl> 5150 , <nl> /**/
mmm src / mark . c <nl> ppp src / mark . c <nl> movemark ( int count ) <nl> fname2fnum ( jmp ); <nl> if ( jmp -> fmark . fnum != curbuf -> b_fnum ) <nl> { <nl> - // jump to other file <nl> - if ( buflist_findnr ( jmp -> fmark . fnum ) == NULL ) <nl> + // Make a copy , an autocommand may make " jmp " invalid . <nl> + fmark_T fmark = jmp -> fmark ; <nl> + <nl> + // jump to the file with the mark <nl> + if ( buflist_findnr ( fmark . fnum ) == NULL ) <nl> { // Skip this one .. <nl> count += count < 0 ? - 1 : 1 ; <nl> continue ; <nl> } <nl> - if ( buflist_getfile ( jmp -> fmark . fnum , jmp -> fmark . mark . lnum , <nl> - 0 , FALSE ) == FAIL ) <nl> + if ( buflist_getfile ( fmark . fnum , fmark . mark . lnum , 0 , FALSE ) == FAIL ) <nl> return ( pos_T *) NULL ; <nl> // Set lnum again , autocommands my have changed it <nl> - curwin -> w_cursor = jmp -> fmark . mark ; <nl> + curwin -> w_cursor = fmark . mark ; <nl> pos = ( pos_T *)- 1 ; <nl> } <nl> elsemmm src / version . c <nl> ppp src / version . c <nl> movemark ( int count ) <nl> fname2fnum ( jmp ); <nl> if ( jmp -> fmark . fnum != curbuf -> b_fnum ) <nl> { <nl> - // jump to other file <nl> - if ( buflist_findnr ( jmp -> fmark . fnum ) == NULL ) <nl> + // Make a copy , an autocommand may make " jmp " invalid . <nl> + fmark_T fmark = jmp -> fmark ; <nl> + <nl> + // jump to the file with the mark <nl> + if ( buflist_findnr ( fmark . fnum ) == NULL ) <nl> { // Skip this one .. <nl> count += count < 0 ? - 1 : 1 ; <nl> continue ; <nl> } <nl> - if ( buflist_getfile ( jmp -> fmark . fnum , jmp -> fmark . mark . lnum , <nl> - 0 , FALSE ) == FAIL ) <nl> + if ( buflist_getfile ( fmark . fnum , fmark . mark . lnum , 0 , FALSE ) == FAIL ) <nl> return ( pos_T *) NULL ; <nl> // Set lnum again , autocommands my have changed it <nl> - curwin -> w_cursor = jmp -> fmark . mark ; <nl> + curwin -> w_cursor = fmark . mark ; <nl> pos = ( pos_T *)- 1 ; <nl> } <nl> else <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 530 , <nl> /**/ <nl> 529 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 789 , <nl> /**/ <nl> 788 , <nl> /**/mmm src / buffer . c <nl> ppp src / buffer . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 789 , <nl> /**/ <nl> 788 , <nl> /**/ <nl> do_buffer_ext ( <nl> if (( flags & DOBUF_NOPOPUP ) && bt_popup ( buf ) && ! bt_terminal ( buf )) <nl> return OK ; <nl> # endif <nl> + if (( action == DOBUF_GOTO || action == DOBUF_SPLIT ) <nl> + && ( buf -> b_flags & BF_DUMMY )) <nl> + { <nl> + // disallow navigating to the dummy buffer <nl> + semsg ( _ ( e_buffer_nr_does_not_exist ), count ); <nl> + return FAIL ; <nl> + } <nl>  <nl> # ifdef FEAT_GUI <nl> need_mouse_correct = TRUE ;
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 148 , <nl> /**/ <nl> 147 , <nl> /**/mmm src / if_tcl . c <nl> ppp src / if_tcl . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 148 , <nl> /**/ <nl> 147 , <nl> /**/ <nl> tclvimexpr ( <nl> if ( str == NULL ) <nl> Tcl_SetResult ( interp , _ (" invalid expression "), TCL_STATIC ); <nl> else <nl> + { <nl> Tcl_SetResult ( interp , str , TCL_VOLATILE ); <nl> + vim_free ( str ); <nl> + } <nl> err = vimerror ( interp ); <nl> # else <nl> Tcl_SetResult ( interp , _ (" expressions disabled at compile time "), TCL_STATIC );
mmm src / charset . c <nl> ppp src / charset . c <nl> getvcol ( <nl> posptr = NULL ; // continue until the NUL <nl> else <nl> { <nl> - // Special check for an empty line , which can happen on exit , when <nl> - // ml_get_buf () always returns an empty string . <nl> - if (* ptr == NUL ) <nl> - pos -> col = 0 ; <nl> + colnr_T i ; <nl> + <nl> + // In a few cases the position can be beyond the end of the line . <nl> + for ( i = 0 ; i < pos -> col ; ++ i ) <nl> + if ( ptr [ i ] == NUL ) <nl> + { <nl> + pos -> col = i ; <nl> + break ; <nl> + } <nl> posptr = ptr + pos -> col ; <nl> if ( has_mbyte ) <nl> // always start on the first bytemmm src / version . c <nl> ppp src / version . c <nl> getvcol ( <nl> posptr = NULL ; // continue until the NUL <nl> else <nl> { <nl> - // Special check for an empty line , which can happen on exit , when <nl> - // ml_get_buf () always returns an empty string . <nl> - if (* ptr == NUL ) <nl> - pos -> col = 0 ; <nl> + colnr_T i ; <nl> + <nl> + // In a few cases the position can be beyond the end of the line . <nl> + for ( i = 0 ; i < pos -> col ; ++ i ) <nl> + if ( ptr [ i ] == NUL ) <nl> + { <nl> + pos -> col = i ; <nl> + break ; <nl> + } <nl> posptr = ptr + pos -> col ; <nl> if ( has_mbyte ) <nl> // always start on the first byte <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3950 , <nl> /**/ <nl> 3949 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 193 , <nl> /**/ <nl> 192 , <nl> /**/mmm src / ex_getln . c <nl> ppp src / ex_getln . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 193 , <nl> /**/ <nl> 192 , <nl> /**/ <nl> ex_window () <nl> ccline . cmdbuff = vim_strsave (( char_u *)" qa "); <nl> cmdwin_result = CAR ; <nl> } <nl> + else if ( cmdwin_result == Ctrl_C ) <nl> + { <nl> + /* : q or : close , don ' t execute any command <nl> + * and don ' t modify the cmd window . */ <nl> + ccline . cmdbuff = NULL ; <nl> + } <nl> else <nl> ccline . cmdbuff = vim_strsave ( ml_get_curline ()); <nl> if ( ccline . cmdbuff == NULL )mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 193 , <nl> /**/ <nl> 192 , <nl> /**/ <nl> ex_window () <nl> ccline . cmdbuff = vim_strsave (( char_u *)" qa "); <nl> cmdwin_result = CAR ; <nl> } <nl> + else if ( cmdwin_result == Ctrl_C ) <nl> + { <nl> + /* : q or : close , don ' t execute any command <nl> + * and don ' t modify the cmd window . */ <nl> + ccline . cmdbuff = NULL ; <nl> + } <nl> else <nl> ccline . cmdbuff = vim_strsave ( ml_get_curline ()); <nl> if ( ccline . cmdbuff == NULL ) <nl> ex_close ( eap ) <nl> { <nl> # ifdef FEAT_CMDWIN <nl> if ( cmdwin_type != 0 ) <nl> - cmdwin_result = K_IGNORE ; <nl> + cmdwin_result = Ctrl_C ; <nl> else <nl> # endif <nl> if (! text_locked ()
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 466 , <nl> /**/ <nl> 465 , <nl> /**/mmm src / ops . c <nl> ppp src / ops . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 466 , <nl> /**/ <nl> 465 , <nl> /**/ <nl> op_delete ( oap ) <nl> ++ curwin -> w_cursor . lnum ; <nl> del_lines (( long )( oap -> line_count - 2 ), FALSE ); <nl>  <nl> + if ( delete_last_line ) <nl> + oap -> end . lnum = curbuf -> b_ml . ml_line_count ; <nl> + <nl> n = ( oap -> end . col + 1 - ! oap -> inclusive ); <nl> if ( oap -> inclusive && delete_last_line <nl> && n > ( int ) STRLEN ( ml_get ( oap -> end . lnum )))
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 190 , <nl> /**/ <nl> 189 , <nl> /**/mmm src / normal . c <nl> ppp src / normal . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 190 , <nl> /**/ <nl> 189 , <nl> /**/ <nl> nv_brace ( cap ) <nl> clearopbeep ( cap -> oap ); <nl> else <nl> { <nl> + /* Don ' t leave the cursor on the NUL past a line */ <nl> + if ( curwin -> w_cursor . col > 0 && gchar_cursor () == NUL ) <nl> + { <nl> + -- curwin -> w_cursor . col ; <nl> + cap -> oap -> inclusive = TRUE ; <nl> + } <nl> # ifdef FEAT_VIRTUALEDIT <nl> curwin -> w_cursor . coladd = 0 ; <nl> # endif
mmm src / normal . c <nl> ppp src / normal . c <nl> n_start_visual_mode ( int c ) <nl> VIsual_mode = c ; <nl> VIsual_active = TRUE ; <nl> VIsual_reselect = TRUE ; <nl> - trigger_modechanged (); <nl>  <nl> // Corner case : the 0 position in a tab may change when going into <nl> // virtualedit . Recalculate curwin -> w_cursor to avoid bad highlighting . <nl> n_start_visual_mode ( int c ) <nl> foldAdjustVisual (); <nl> # endif <nl>  <nl> + trigger_modechanged (); <nl> setmouse (); <nl> # ifdef FEAT_CONCEAL <nl> // Check if redraw is needed after changing the state .mmm src / version . c <nl> ppp src / version . c <nl> n_start_visual_mode ( int c ) <nl> VIsual_mode = c ; <nl> VIsual_active = TRUE ; <nl> VIsual_reselect = TRUE ; <nl> - trigger_modechanged (); <nl>  <nl> // Corner case : the 0 position in a tab may change when going into <nl> // virtualedit . Recalculate curwin -> w_cursor to avoid bad highlighting . <nl> n_start_visual_mode ( int c ) <nl> foldAdjustVisual (); <nl> # endif <nl>  <nl> + trigger_modechanged (); <nl> setmouse (); <nl> # ifdef FEAT_CONCEAL <nl> // Check if redraw is needed after changing the state . <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3610 , <nl> /**/ <nl> 3609 , <nl> /**/
mmm src / getchar . c <nl> ppp src / getchar . c <nl> add_buff ( <nl> static void <nl> delete_buff_tail ( buffheader_T * buf , int slen ) <nl> { <nl> - int len = ( int ) STRLEN ( buf -> bh_curr -> b_str ); <nl> + int len ; <nl>  <nl> + if ( buf -> bh_curr == NULL || buf -> bh_curr -> b_str == NULL ) <nl> + return ; // nothing to delete <nl> + len = ( int ) STRLEN ( buf -> bh_curr -> b_str ); <nl> if ( len >= slen ) <nl> { <nl> buf -> bh_curr -> b_str [ len - slen ] = NUL ;mmm src / version . c <nl> ppp src / version . c <nl> add_buff ( <nl> static void <nl> delete_buff_tail ( buffheader_T * buf , int slen ) <nl> { <nl> - int len = ( int ) STRLEN ( buf -> bh_curr -> b_str ); <nl> + int len ; <nl>  <nl> + if ( buf -> bh_curr == NULL || buf -> bh_curr -> b_str == NULL ) <nl> + return ; // nothing to delete <nl> + len = ( int ) STRLEN ( buf -> bh_curr -> b_str ); <nl> if ( len >= slen ) <nl> { <nl> buf -> bh_curr -> b_str [ len - slen ] = NUL ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4233 , <nl> /**/ <nl> 4232 , <nl> /**/
mmm src / buffer . c <nl> ppp src / buffer . c <nl> buflist_match ( <nl>  <nl> // First try the short file name , then the long file name . <nl> match = fname_match ( rmp , buf -> b_sfname , ignore_case ); <nl> - if ( match == NULL ) <nl> + if ( match == NULL && rmp -> regprog != NULL ) <nl> match = fname_match ( rmp , buf -> b_ffname , ignore_case ); <nl>  <nl> return match ;mmm src / version . c <nl> ppp src / version . c <nl> buflist_match ( <nl>  <nl> // First try the short file name , then the long file name . <nl> match = fname_match ( rmp , buf -> b_sfname , ignore_case ); <nl> - if ( match == NULL ) <nl> + if ( match == NULL && rmp -> regprog != NULL ) <nl> match = fname_match ( rmp , buf -> b_ffname , ignore_case ); <nl>  <nl> return match ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4938 , <nl> /**/ <nl> 4937 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 339 , <nl> /**/ <nl> 338 , <nl> /**/mmm src / gui_x11 . c <nl> ppp src / gui_x11 . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 339 , <nl> /**/ <nl> 338 , <nl> /**/ <nl> gui_x11_create_blank_mouse ( void ) <nl> { <nl> Pixmap blank_pixmap = XCreatePixmap ( gui . dpy , gui . wid , 1 , 1 , 1 ); <nl> GC gc = XCreateGC ( gui . dpy , blank_pixmap , ( unsigned long ) 0 , ( XGCValues *) 0 ); <nl> - XDrawPoint ( gui . dpy , blank_pixmap , gc , 0 , 0 ); <nl> - XFreeGC ( gui . dpy , gc ); <nl> + <nl> + if ( gc != NULL ) <nl> + { <nl> + XDrawPoint ( gui . dpy , blank_pixmap , gc , 0 , 0 ); <nl> + XFreeGC ( gui . dpy , gc ); <nl> + } <nl> return XCreatePixmapCursor ( gui . dpy , blank_pixmap , blank_pixmap , <nl> - ( XColor *)& gui . norm_pixel , ( XColor *)& gui . norm_pixel , 0 , 0 ); <nl> + ( XColor *)& gui . norm_pixel , ( XColor *)& gui . norm_pixel , 0 , 0 ); <nl> } <nl>  <nl> /*
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 102 , <nl> /**/ <nl> 101 , <nl> /**/mmm src / insexpand . c <nl> ppp src / insexpand . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 102 , <nl> /**/ <nl> 101 , <nl> /**/ <nl> ins_comp_get_next_word_or_line ( <nl> { <nl> char_u * tmp_ptr = ptr ; <nl>  <nl> - if ( compl_status_adding ()) <nl> + if ( compl_status_adding () && compl_length <= ( int ) STRLEN ( tmp_ptr )) <nl> { <nl> tmp_ptr += compl_length ; <nl> // Skip if already inside a word .
mmm src / tag . c <nl> ppp src / tag . c <nl> do_tag ( <nl> char_u * buf_ffname = curbuf -> b_ffname ; // name to use for <nl> // priority computation <nl> int use_tfu = 1 ; <nl> + char_u * tofree = NULL ; <nl>  <nl> // remember the matches for the last used tag <nl> static int num_matches = 0 ; <nl> do_tag ( <nl> * When desired match not found yet , try to find it ( and others ). <nl> */ <nl> if ( use_tagstack ) <nl> - name = tagstack [ tagstackidx ]. tagname ; <nl> + { <nl> + // make a copy , the tagstack may change in ' tagfunc ' <nl> + name = vim_strsave ( tagstack [ tagstackidx ]. tagname ); <nl> + vim_free ( tofree ); <nl> + tofree = name ; <nl> + } <nl> # if defined ( FEAT_QUICKFIX ) <nl> else if ( g_do_tagpreview != 0 ) <nl> name = ptag_entry . tagname ; <nl> end_do_tag : <nl> g_do_tagpreview = 0 ; // don ' t do tag preview next time <nl> # endif <nl>  <nl> + vim_free ( tofree ); <nl> # ifdef FEAT_CSCOPE <nl> return jumped_to_tag ; <nl> # elsemmm src / version . c <nl> ppp src / version . c <nl> do_tag ( <nl> char_u * buf_ffname = curbuf -> b_ffname ; // name to use for <nl> // priority computation <nl> int use_tfu = 1 ; <nl> + char_u * tofree = NULL ; <nl>  <nl> // remember the matches for the last used tag <nl> static int num_matches = 0 ; <nl> do_tag ( <nl> * When desired match not found yet , try to find it ( and others ). <nl> */ <nl> if ( use_tagstack ) <nl> - name = tagstack [ tagstackidx ]. tagname ; <nl> + { <nl> + // make a copy , the tagstack may change in ' tagfunc ' <nl> + name = vim_strsave ( tagstack [ tagstackidx ]. tagname ); <nl> + vim_free ( tofree ); <nl> + tofree = name ; <nl> + } <nl> # if defined ( FEAT_QUICKFIX ) <nl> else if ( g_do_tagpreview != 0 ) <nl> name = ptag_entry . tagname ; <nl> end_do_tag : <nl> g_do_tagpreview = 0 ; // don ' t do tag preview next time <nl> # endif <nl>  <nl> + vim_free ( tofree ); <nl> # ifdef FEAT_CSCOPE <nl> return jumped_to_tag ; <nl> # else <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 246 , <nl> /**/ <nl> 245 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4646 , <nl> /**/ <nl> 4645 , <nl> /**/mmm src / regexp_bt . c <nl> ppp src / regexp_bt . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4646 , <nl> /**/ <nl> 4645 , <nl> /**/ <nl> regmatch ( <nl> int mark = OPERAND ( scan )[ 0 ]; <nl> int cmp = OPERAND ( scan )[ 1 ]; <nl> pos_T * pos ; <nl> + size_t col = REG_MULTI ? rex . input - rex . line : 0 ; <nl>  <nl> pos = getmark_buf ( rex . reg_buf , mark , FALSE ); <nl> + <nl> + // Line may have been freed , get it again . <nl> + if ( REG_MULTI ) <nl> + { <nl> + rex . line = reg_getline ( rex . lnum ); <nl> + rex . input = rex . line + col ; <nl> + } <nl> + <nl> if ( pos == NULL // mark doesn ' t exist <nl> || pos -> lnum <= 0 ) // mark isn ' t set in reg_buf <nl> {
mmm src / insexpand . c <nl> ppp src / insexpand . c <nl> ins_compl_infercase_gettext ( <nl> // growarray . Add the character in the next round . <nl> if ( ga_grow (& gap , IOSIZE ) == FAIL ) <nl> return ( char_u *)"[ failed ]"; <nl> + * p = NUL ; <nl> STRCPY ( gap . ga_data , IObuff ); <nl> gap . ga_len = ( int ) STRLEN ( IObuff ); <nl> }mmm src / version . c <nl> ppp src / version . c <nl> ins_compl_infercase_gettext ( <nl> // growarray . Add the character in the next round . <nl> if ( ga_grow (& gap , IOSIZE ) == FAIL ) <nl> return ( char_u *)"[ failed ]"; <nl> + * p = NUL ; <nl> STRCPY ( gap . ga_data , IObuff ); <nl> gap . ga_len = ( int ) STRLEN ( IObuff ); <nl> } <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 60 , <nl> /**/ <nl> 59 , <nl> /**/
mmm src / vim . h <nl> ppp src / vim . h <nl> || defined ( FEAT_GUI_MAC ) \ <nl> || defined ( FEAT_GUI_W32 ) \ <nl> || defined ( FEAT_GUI_W16 ) \ <nl> - || defined ( FEAT_GUI_BEOS ) \ <nl> - || defined ( FEAT_GUI_AMIGA ) \ <nl> || defined ( FEAT_GUI_PHOTON ) \ <nl> || defined ( FEAT_GUI_KDE ) <nl> # if ! defined ( FEAT_GUI ) && ! defined ( NO_X11_INCLUDES ) <nl> typedef struct VimClipboard <nl> int_u format ; /* Vim ' s own special clipboard format */ <nl> int_u format_raw ; /* Vim ' s raw text clipboard format */ <nl> # endif <nl> -# ifdef FEAT_GUI_BEOS <nl> - /* no clipboard at the moment */ <nl> -# endif <nl> } VimClipboard ; <nl> # else <nl> typedef int VimClipboard ; /* This is required for the prototypes . */
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 46 , <nl> /**/ <nl> 45 , <nl> /**/mmm src / insexpand . c <nl> ppp src / insexpand . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 46 , <nl> /**/ <nl> 45 , <nl> /**/ <nl> ins_compl_add ( <nl> { <nl> if (! match_at_original_text ( match ) <nl> && STRNCMP ( match -> cp_str , str , len ) == 0 <nl> - && match -> cp_str [ len ] == NUL ) <nl> + && (( int ) STRLEN ( match -> cp_str ) <= len <nl> + || match -> cp_str [ len ] == NUL )) <nl> return NOTDONE ; <nl> match = match -> cp_next ; <nl> } while ( match != NULL && ! is_first_match ( match ));
mmm src / help . c <nl> ppp src / help . c <nl> find_help_tags ( <nl> || ( vim_strchr (( char_u *)"% _z @", arg [ 1 ]) != NULL <nl> && arg [ 2 ] != NUL ))) <nl> { <nl> - STRCPY ( d , "/\\\\"); <nl> - STRCPY ( d + 3 , arg + 1 ); <nl> + vim_snprintf (( char *) d , IOSIZE , "/\\\\% s ", arg + 1 ); <nl> // Check for "/\\ _ $", should be "/\\ _ \$" <nl> if ( d [ 3 ] == ' _ ' && d [ 4 ] == '$') <nl> STRCPY ( d + 4 , "\\$");mmm src / version . c <nl> ppp src / version . c <nl> find_help_tags ( <nl> || ( vim_strchr (( char_u *)"% _z @", arg [ 1 ]) != NULL <nl> && arg [ 2 ] != NUL ))) <nl> { <nl> - STRCPY ( d , "/\\\\"); <nl> - STRCPY ( d + 3 , arg + 1 ); <nl> + vim_snprintf (( char *) d , IOSIZE , "/\\\\% s ", arg + 1 ); <nl> // Check for "/\\ _ $", should be "/\\ _ \$" <nl> if ( d [ 3 ] == ' _ ' && d [ 4 ] == '$') <nl> STRCPY ( d + 4 , "\\$"); <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3669 , <nl> /**/ <nl> 3668 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5164 , <nl> /**/ <nl> 5163 , <nl> /**/mmm src / diff . c <nl> ppp src / diff . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5164 , <nl> /**/ <nl> 5163 , <nl> /**/ <nl> diff_mark_adjust_tp ( <nl> // 2 . 3 . 4 . 5 .: inserted / deleted lines touching this diff . <nl> if ( deleted > 0 ) <nl> { <nl> + off = 0 ; <nl> if ( dp -> df_lnum [ idx ] >= line1 ) <nl> { <nl> - off = dp -> df_lnum [ idx ] - lnum_deleted ; <nl> if ( last <= line2 ) <nl> { <nl> // 4 . delete all lines of diff <nl> diff_mark_adjust_tp ( <nl> else <nl> { <nl> // 5 . delete lines at or just before top of diff <nl> + off = dp -> df_lnum [ idx ] - lnum_deleted ; <nl> n = off ; <nl> dp -> df_count [ idx ] -= line2 - dp -> df_lnum [ idx ] + 1 ; <nl> check_unchanged = TRUE ; <nl> diff_mark_adjust_tp ( <nl> } <nl> else <nl> { <nl> - off = 0 ; <nl> if ( last < line2 ) <nl> { <nl> // 2 . delete at end of diff
mmm src / ops . c <nl> ppp src / ops . c <nl> op_replace ( oparg_T * oap , int c ) <nl>  <nl> while ( LTOREQ_POS ( curwin -> w_cursor , oap -> end )) <nl> { <nl> + int done = FALSE ; <nl> + <nl> n = gchar_cursor (); <nl> if ( n != NUL ) <nl> { <nl> op_replace ( oparg_T * oap , int c ) <nl> if ( curwin -> w_cursor . lnum == oap -> end . lnum ) <nl> oap -> end . col += new_byte_len - old_byte_len ; <nl> replace_character ( c ); <nl> + done = TRUE ; <nl> } <nl> else <nl> { <nl> op_replace ( oparg_T * oap , int c ) <nl> if ( curwin -> w_cursor . lnum == oap -> end . lnum ) <nl> getvpos (& oap -> end , end_vcol ); <nl> } <nl> - PBYTE ( curwin -> w_cursor , c ); <nl> + // with " coladd " set may move to just after a TAB <nl> + if ( gchar_cursor () != NUL ) <nl> + { <nl> + PBYTE ( curwin -> w_cursor , c ); <nl> + done = TRUE ; <nl> + } <nl> } <nl> } <nl> - else if ( virtual_op && curwin -> w_cursor . lnum == oap -> end . lnum ) <nl> + if (! done && virtual_op && curwin -> w_cursor . lnum == oap -> end . lnum ) <nl> { <nl> int virtcols = oap -> end . coladd ; <nl> mmm src / version . c <nl> ppp src / version . c <nl> op_replace ( oparg_T * oap , int c ) <nl>  <nl> while ( LTOREQ_POS ( curwin -> w_cursor , oap -> end )) <nl> { <nl> + int done = FALSE ; <nl> + <nl> n = gchar_cursor (); <nl> if ( n != NUL ) <nl> { <nl> op_replace ( oparg_T * oap , int c ) <nl> if ( curwin -> w_cursor . lnum == oap -> end . lnum ) <nl> oap -> end . col += new_byte_len - old_byte_len ; <nl> replace_character ( c ); <nl> + done = TRUE ; <nl> } <nl> else <nl> { <nl> op_replace ( oparg_T * oap , int c ) <nl> if ( curwin -> w_cursor . lnum == oap -> end . lnum ) <nl> getvpos (& oap -> end , end_vcol ); <nl> } <nl> - PBYTE ( curwin -> w_cursor , c ); <nl> + // with " coladd " set may move to just after a TAB <nl> + if ( gchar_cursor () != NUL ) <nl> + { <nl> + PBYTE ( curwin -> w_cursor , c ); <nl> + done = TRUE ; <nl> + } <nl> } <nl> } <nl> - else if ( virtual_op && curwin -> w_cursor . lnum == oap -> end . lnum ) <nl> + if (! done && virtual_op && curwin -> w_cursor . lnum == oap -> end . lnum ) <nl> { <nl> int virtcols = oap -> end . coladd ; <nl>  <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 483 , <nl> /**/ <nl> 482 , <nl> /**/
mmm src / spell . c <nl> ppp src / spell . c <nl> did_set_spelllang ( win_T * wp ) <nl> { <nl> spell_load_lang ( lang ); <nl> // SpellFileMissing autocommands may do anything , including <nl> - // destroying the buffer we are using ... <nl> - if (! bufref_valid (& bufref )) <nl> + // destroying the buffer we are using or closing the window . <nl> + if (! bufref_valid (& bufref ) || ! win_valid_any_tab ( wp )) <nl> { <nl> ret_msg = N_ ( e_spellfilemising_autocommand_deleted_buffer ); <nl> goto theend ;mmm src / version . c <nl> ppp src / version . c <nl> did_set_spelllang ( win_T * wp ) <nl> { <nl> spell_load_lang ( lang ); <nl> // SpellFileMissing autocommands may do anything , including <nl> - // destroying the buffer we are using ... <nl> - if (! bufref_valid (& bufref )) <nl> + // destroying the buffer we are using or closing the window . <nl> + if (! bufref_valid (& bufref ) || ! win_valid_any_tab ( wp )) <nl> { <nl> ret_msg = N_ ( e_spellfilemising_autocommand_deleted_buffer ); <nl> goto theend ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 882 , <nl> /**/ <nl> 881 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 25 , <nl> /**/ <nl> 24 , <nl> /**/mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 25 , <nl> /**/ <nl> 24 , <nl> /**/ <nl> parse_command_modifiers ( <nl> size_t len = STRLEN ( cmd_start ); <nl>  <nl> // Special case : empty command uses "+": <nl> - // "'<,'> mods " -> " mods '<,'>+ <nl> + // "'<,'> mods " -> " mods *+ <nl> + // Use "*" instead of "'<,'>" to avoid the command getting <nl> + // longer , in case is was allocated . <nl> mch_memmove ( orig_cmd , cmd_start , len ); <nl> - STRCPY ( orig_cmd + len , "'<,'>+"); <nl> + STRCPY ( orig_cmd + len , " *+"); <nl> } <nl> else <nl> {
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1378 , <nl> /**/ <nl> 1377 , <nl> /**/mmm src / register . c <nl> ppp src / register . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1378 , <nl> /**/ <nl> 1377 , <nl> /**/ <nl> op_yank ( oparg_T * oap , int deleting , int mess ) <nl> // double - count it . <nl> bd . startspaces = ( ce - cs + 1 ) <nl> - oap -> start . coladd ; <nl> + if ( bd . startspaces < 0 ) <nl> + bd . startspaces = 0 ; <nl> startcol ++; <nl> } <nl> }
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 389 , <nl> /**/ <nl> 388 , <nl> /**/mmm src / errors . h <nl> ppp src / errors . h <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 389 , <nl> /**/ <nl> 388 , <nl> /**/ <nl> EXTERN char e_non_null_dict_required_for_argument_nr [] <nl> EXTERN char e_non_null_list_required_for_argument_nr [] <nl> INIT (= N_ (" E1298 : Non - NULL List required for argument % d ")); <nl> # endif <nl> + EXTERN char e_window_unexpectedly_close_while_searching_for_tags [] <nl> + INIT (= N_ (" E1299 : Window unexpectedly closed while searching for tags "));mmm src / tag . c <nl> ppp src / tag . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 389 , <nl> /**/ <nl> 388 , <nl> /**/ <nl> EXTERN char e_non_null_dict_required_for_argument_nr [] <nl> EXTERN char e_non_null_list_required_for_argument_nr [] <nl> INIT (= N_ (" E1298 : Non - NULL List required for argument % d ")); <nl> # endif <nl> + EXTERN char e_window_unexpectedly_close_while_searching_for_tags [] <nl> + INIT (= N_ (" E1299 : Window unexpectedly closed while searching for tags ")); <nl> do_tag ( <nl> max_num_matches = MAXCOL ; // If less than max_num_matches <nl> // found : all matches found . <nl>  <nl> + // A tag function may do anything , which may cause various <nl> + // information to become invalid . At least check for the tagstack <nl> + // to still be the same . <nl> + if ( tagstack != curwin -> w_tagstack ) <nl> + { <nl> + emsg ( _ ( e_window_unexpectedly_close_while_searching_for_tags )); <nl> + FreeWild ( new_num_matches , new_matches ); <nl> + break ; <nl> + } <nl> + <nl> // If there already were some matches for the same name , move them <nl> // to the start . Avoids that the order changes when using <nl> // ": tnext " and jumping to another file .
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5163 , <nl> /**/ <nl> 5162 , <nl> /**/mmm src / diff . c <nl> ppp src / diff . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5163 , <nl> /**/ <nl> 5162 , <nl> /**/ <nl> diff_buf_delete ( buf_T * buf ) <nl> tp -> tp_diffbuf [ i ] = NULL ; <nl> tp -> tp_diff_invalid = TRUE ; <nl> if ( tp == curtab ) <nl> - diff_redraw ( TRUE ); <nl> + { <nl> + // don ' t redraw right away , more might change or buffer state <nl> + // is invalid right now <nl> + need_diff_redraw = TRUE ; <nl> + redraw_later ( VALID ); <nl> + } <nl> } <nl> } <nl> } <nl> diff_redraw ( <nl>  <nl> need_diff_redraw = FALSE ; <nl> FOR_ALL_WINDOWS ( wp ) <nl> - if ( wp -> w_p_diff ) <nl> + // when closing windows or wiping buffers skip invalid window <nl> + if ( wp -> w_p_diff && buf_valid ( wp -> w_buffer )) <nl> { <nl> redraw_win_later ( wp , SOME_VALID ); <nl> if ( wp != curwin )
mmm src / eval . c <nl> ppp src / eval . c <nl> num_divide ( varnumber_T n1 , varnumber_T n2 , int * failed ) <nl> else <nl> result = VARNUM_MAX ; <nl> } <nl> + else if ( n1 == VARNUM_MIN && n2 == - 1 ) <nl> + { <nl> + // specific case : trying to do VARNUM_MIN / - 1 results in a positive <nl> + // number that doesn ' t fit in varnumber_T and causes an FPE <nl> + result = VARNUM_MAX ; <nl> + } <nl> else <nl> result = n1 / n2 ; <nl>  <nl> var2fpos ( <nl> } <nl>  <nl> /* <nl> - * Convert list in " arg " into position " psop " and optional file number " fnump ". <nl> + * Convert list in " arg " into position " posp " and optional file number " fnump ". <nl> * When " fnump " is NULL there is no file number , only 3 items : [ lnum , col , off ] <nl> * Note that the column is passed on as - is , the caller may want to decrement <nl> * it to use 1 for the first column .mmm src / version . c <nl> ppp src / version . c <nl> num_divide ( varnumber_T n1 , varnumber_T n2 , int * failed ) <nl> else <nl> result = VARNUM_MAX ; <nl> } <nl> + else if ( n1 == VARNUM_MIN && n2 == - 1 ) <nl> + { <nl> + // specific case : trying to do VARNUM_MIN / - 1 results in a positive <nl> + // number that doesn ' t fit in varnumber_T and causes an FPE <nl> + result = VARNUM_MAX ; <nl> + } <nl> else <nl> result = n1 / n2 ; <nl>  <nl> var2fpos ( <nl> } <nl>  <nl> /* <nl> - * Convert list in " arg " into position " psop " and optional file number " fnump ". <nl> + * Convert list in " arg " into position " posp " and optional file number " fnump ". <nl> * When " fnump " is NULL there is no file number , only 3 items : [ lnum , col , off ] <nl> * Note that the column is passed on as - is , the caller may want to decrement <nl> * it to use 1 for the first column . <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 804 , <nl> /**/ <nl> 803 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 805 , <nl> /**/ <nl> 804 , <nl> /**/mmm src / quickfix . c <nl> ppp src / quickfix . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 805 , <nl> /**/ <nl> 804 , <nl> /**/ <nl> qf_update_buffer ( qf_info_T * qi , qfline_T * old_last ) <nl> qf_winid = win -> w_id ; <nl> } <nl>  <nl> + // autocommands may cause trouble <nl> + incr_quickfix_busy (); <nl> + <nl> if ( old_last == NULL ) <nl> // set curwin / curbuf to buf and save a few things <nl> aucmd_prepbuf (& aco , buf ); <nl> qf_update_buffer ( qf_info_T * qi , qfline_T * old_last ) <nl> // when the added lines are not visible . <nl> if (( win = qf_find_win ( qi )) != NULL && old_line_count < win -> w_botline ) <nl> redraw_buf_later ( buf , UPD_NOT_VALID ); <nl> + <nl> + // always called after incr_quickfix_busy () <nl> + decr_quickfix_busy (); <nl> } <nl> } <nl> 
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1402 , <nl> /**/ <nl> 1401 , <nl> /**/mmm src / vim9class . c <nl> ppp src / vim9class . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1402 , <nl> /**/ <nl> 1401 , <nl> /**/ <nl> class_object_index ( <nl> cl = rettv -> vval . v_object -> obj_class ; <nl> } <nl>  <nl> + if ( cl == NULL ) <nl> + { <nl> + emsg ( _ ( e_incomplete_type )); <nl> + return FAIL ; <nl> + } <nl> + <nl> if (* name_end == '(') <nl> { <nl> int on_class = rettv -> v_type == VAR_CLASS ;
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1531 , <nl> /**/ <nl> 1530 , <nl> /**/mmm src / register . c <nl> ppp src / register . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1531 , <nl> /**/ <nl> 1530 , <nl> /**/ <nl> get_register ( <nl> if ( copy ) <nl> { <nl> // If we run out of memory some or all of the lines are empty . <nl> - if ( reg -> y_size == 0 ) <nl> + if ( reg -> y_size == 0 || y_current -> y_array == NULL ) <nl> reg -> y_array = NULL ; <nl> else <nl> reg -> y_array = ALLOC_MULT ( char_u *, reg -> y_size );
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 211 , <nl> /**/ <nl> 210 , <nl> /**/mmm src / vim9cmds . c <nl> ppp src / vim9cmds . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 211 , <nl> /**/ <nl> 210 , <nl> /**/ <nl> compile_lock_unlock ( <nl> size_t len ; <nl> char_u * buf ; <nl> isntype_T isn = ISN_EXEC ; <nl> + char * cmd = eap -> cmdidx == CMD_lockvar ? " lockvar " : " unlockvar "; <nl>  <nl> if ( cctx -> ctx_skip == SKIP_YES ) <nl> return OK ; <nl>  <nl> + if (* p == NUL ) <nl> + { <nl> + semsg ( _ ( e_argument_required_for_str ), cmd ); <nl> + return FAIL ; <nl> + } <nl> + <nl> // Cannot use : lockvar and : unlockvar on local variables . <nl> if ( p [ 1 ] != ':') <nl> { <nl> compile_lock_unlock ( <nl> ret = FAIL ; <nl> else <nl> { <nl> - char * cmd = eap -> cmdidx == CMD_lockvar ? " lockvar " : " unlockvar "; <nl> - <nl> if ( deep < 0 ) <nl> vim_snprintf (( char *) buf , len , "% s ! % s ", cmd , p ); <nl> else
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 11 , <nl> /**/ <nl> 10 , <nl> /**/mmm src / register . c <nl> ppp src / register . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 11 , <nl> /**/ <nl> 10 , <nl> /**/ <nl> do_put ( <nl> vim_memset ( ptr , ' ', ( size_t ) spaces ); <nl> ptr += spaces ; <nl> } <nl> + else <nl> + totlen -= spaces ; // didn ' t use these spaces <nl> } <nl>  <nl> // may insert some spaces after the new text
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4009 , <nl> /**/ <nl> 4008 , <nl> /**/mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4009 , <nl> /**/ <nl> 4008 , <nl> /**/ <nl> find_ex_command ( <nl> } <nl>  <nl> // Check for "++ nr " and "-- nr ". <nl> - if ( p == eap -> cmd && p [ 0 ] == p [ 1 ] && (* p == '+' || * p == '-')) <nl> + if ( p == eap -> cmd && p [ 0 ] != NUL && p [ 0 ] == p [ 1 ] <nl> + && (* p == '+' || * p == '-')) <nl> { <nl> eap -> cmdidx = * p == '+' ? CMD_increment : CMD_decrement ; <nl> return eap -> cmd + 2 ;mmm src / vim9compile . c <nl> ppp src / vim9compile . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4009 , <nl> /**/ <nl> 4008 , <nl> /**/ <nl> find_ex_command ( <nl> } <nl>  <nl> // Check for "++ nr " and "-- nr ". <nl> - if ( p == eap -> cmd && p [ 0 ] == p [ 1 ] && (* p == '+' || * p == '-')) <nl> + if ( p == eap -> cmd && p [ 0 ] != NUL && p [ 0 ] == p [ 1 ] <nl> + && (* p == '+' || * p == '-')) <nl> { <nl> eap -> cmdidx = * p == '+' ? CMD_increment : CMD_decrement ; <nl> return eap -> cmd + 2 ; <nl> compile_def_function ( <nl> cmd = ea . cmd ; <nl> if ((* cmd != '$' || starts_with_colon ) <nl> && ( starts_with_colon || !(* cmd == '\'' <nl> - || ( cmd [ 0 ] == cmd [ 1 ] && (* cmd == '+' || * cmd == '-'))))) <nl> + || ( cmd [ 0 ] != NUL && cmd [ 0 ] == cmd [ 1 ] <nl> + && (* cmd == '+' || * cmd == '-'))))) <nl> { <nl> ea . cmd = skip_range ( ea . cmd , TRUE , NULL ); <nl> if ( ea . cmd > cmd )
mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> do_exedit ( <nl> # endif <nl> ) <nl> { <nl> - // Can ' t edit another file when " curbuf_lock " is set . Only ": edit " <nl> - // can bring us here , others are stopped earlier . <nl> - if (* eap -> arg != NUL && curbuf_locked ()) <nl> + // Can ' t edit another file when " textlock " or " curbuf_lock " is set . <nl> + // Only ": edit " or ": script " can bring us here , others are stopped <nl> + // earlier . <nl> + if (* eap -> arg != NUL && text_or_buf_locked ()) <nl> return ; <nl>  <nl> n = readonlymode ;mmm src / version . c <nl> ppp src / version . c <nl> do_exedit ( <nl> # endif <nl> ) <nl> { <nl> - // Can ' t edit another file when " curbuf_lock " is set . Only ": edit " <nl> - // can bring us here , others are stopped earlier . <nl> - if (* eap -> arg != NUL && curbuf_locked ()) <nl> + // Can ' t edit another file when " textlock " or " curbuf_lock " is set . <nl> + // Only ": edit " or ": script " can bring us here , others are stopped <nl> + // earlier . <nl> + if (* eap -> arg != NUL && text_or_buf_locked ()) <nl> return ; <nl>  <nl> n = readonlymode ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5126 , <nl> /**/ <nl> 5125 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 260 , <nl> /**/ <nl> 259 , <nl> /**/mmm src / quickfix . c <nl> ppp src / quickfix . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 260 , <nl> /**/ <nl> 259 , <nl> /**/ <nl> call_qftf_func ( qf_list_T * qfl , int qf_winid , long start_idx , long end_idx ) <nl> { <nl> callback_T * cb = & qftf_cb ; <nl> list_T * qftf_list = NULL ; <nl> + static int recursive = FALSE ; <nl> + <nl> + if ( recursive ) <nl> + return NULL ; // this doesn ' t work properly recursively <nl> + recursive = TRUE ; <nl>  <nl> // If ' quickfixtextfunc ' is set , then use the user - supplied function to get <nl> // the text to display . Use the local value of ' quickfixtextfunc ' if it is <nl> call_qftf_func ( qf_list_T * qfl , int qf_winid , long start_idx , long end_idx ) <nl>  <nl> // create the dict argument <nl> if (( d = dict_alloc_lock ( VAR_FIXED )) == NULL ) <nl> + { <nl> + recursive = FALSE ; <nl> return NULL ; <nl> + } <nl> dict_add_number ( d , " quickfix ", ( long ) IS_QF_LIST ( qfl )); <nl> dict_add_number ( d , " winid ", ( long ) qf_winid ); <nl> dict_add_number ( d , " id ", ( long ) qfl -> qf_id ); <nl> call_qftf_func ( qf_list_T * qfl , int qf_winid , long start_idx , long end_idx ) <nl> dict_unref ( d ); <nl> } <nl>  <nl> + recursive = FALSE ; <nl> return qftf_list ; <nl> } <nl> 
mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> append_command ( char_u * cmd ) <nl>  <nl> STRCAT ( IObuff , ": "); <nl> d = IObuff + STRLEN ( IObuff ); <nl> - while (* s != NUL && d - IObuff < IOSIZE - 7 ) <nl> + while (* s != NUL && d - IObuff + 5 < IOSIZE ) <nl> { <nl> if ( enc_utf8 ? ( s [ 0 ] == 0xc2 && s [ 1 ] == 0xa0 ) : * s == 0xa0 ) <nl> { <nl> append_command ( char_u * cmd ) <nl> STRCPY ( d , "< a0 >"); <nl> d += 4 ; <nl> } <nl> + else if ( d - IObuff + (* mb_ptr2len )( s ) + 1 >= IOSIZE ) <nl> + break ; <nl> else <nl> MB_COPY_CHAR ( s , d ); <nl> }mmm src / version . c <nl> ppp src / version . c <nl> append_command ( char_u * cmd ) <nl>  <nl> STRCAT ( IObuff , ": "); <nl> d = IObuff + STRLEN ( IObuff ); <nl> - while (* s != NUL && d - IObuff < IOSIZE - 7 ) <nl> + while (* s != NUL && d - IObuff + 5 < IOSIZE ) <nl> { <nl> if ( enc_utf8 ? ( s [ 0 ] == 0xc2 && s [ 1 ] == 0xa0 ) : * s == 0xa0 ) <nl> { <nl> append_command ( char_u * cmd ) <nl> STRCPY ( d , "< a0 >"); <nl> d += 4 ; <nl> } <nl> + else if ( d - IObuff + (* mb_ptr2len )( s ) + 1 >= IOSIZE ) <nl> + break ; <nl> else <nl> MB_COPY_CHAR ( s , d ); <nl> } <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4895 , <nl> /**/ <nl> 4894 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 212 , <nl> /**/ <nl> 211 , <nl> /**/mmm src / vim9cmds . c <nl> ppp src / vim9cmds . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 212 , <nl> /**/ <nl> 211 , <nl> /**/ <nl> free_locals ( cctx_T * cctx ) <nl> int <nl> check_vim9_unlet ( char_u * name ) <nl> { <nl> + if (* name == NUL ) <nl> + { <nl> + semsg ( _ ( e_argument_required_for_str ), " unlet "); <nl> + return FAIL ; <nl> + } <nl> + <nl> if ( name [ 1 ] != ':' || vim_strchr (( char_u *)" gwtb ", * name ) == NULL ) <nl> { <nl> // " unlet s : var " is allowed in legacy script .
mmm src / ex_cmds . c <nl> ppp src / ex_cmds . c <nl> ex_copy ( linenr_T line1 , linenr_T line2 , linenr_T n ) <nl> } <nl>  <nl> appended_lines_mark ( n , count ); <nl> + if ( VIsual_active ) <nl> + check_pos ( curbuf , & VIsual ); <nl>  <nl> msgmore (( long ) count ); <nl> }mmm src / version . c <nl> ppp src / version . c <nl> ex_copy ( linenr_T line1 , linenr_T line2 , linenr_T n ) <nl> } <nl>  <nl> appended_lines_mark ( n , count ); <nl> + if ( VIsual_active ) <nl> + check_pos ( curbuf , & VIsual ); <nl>  <nl> msgmore (( long ) count ); <nl> } <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4215 , <nl> /**/ <nl> 4214 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4074 , <nl> /**/ <nl> 4073 , <nl> /**/mmm src / drawscreen . c <nl> ppp src / drawscreen . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4074 , <nl> /**/ <nl> 4073 , <nl> /**/ <nl> win_redr_status ( win_T * wp , int ignore_pum UNUSED ) <nl> p = NameBuff ; <nl> len = ( int ) STRLEN ( p ); <nl>  <nl> - if ( bt_help ( wp -> w_buffer ) <nl> + if (( bt_help ( wp -> w_buffer ) <nl> # ifdef FEAT_QUICKFIX <nl> - || wp -> w_p_pvw <nl> + || wp -> w_p_pvw <nl> # endif <nl> - || bufIsChanged ( wp -> w_buffer ) <nl> - || wp -> w_buffer -> b_p_ro ) <nl> + || bufIsChanged ( wp -> w_buffer ) <nl> + || wp -> w_buffer -> b_p_ro ) <nl> + && len < MAXPATHL - 1 ) <nl> *( p + len ++) = ' '; <nl> if ( bt_help ( wp -> w_buffer )) <nl> {
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 6 , <nl> /**/ <nl> 5 , <nl> /**/mmm src / tag . c <nl> ppp src / tag . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 6 , <nl> /**/ <nl> 5 , <nl> /**/ <nl> get_tags ( list , pat ) <nl> /* Skip field without colon . */ <nl> while (* p != NUL && * p >= ' ') <nl> ++ p ; <nl> + if (* p == NUL ) <nl> + break ; <nl> } <nl> } <nl> }
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3741 , <nl> /**/ <nl> 3740 , <nl> /**/mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 3741 , <nl> /**/ <nl> 3740 , <nl> /**/ <nl> ex_open ( exarg_T * eap ) <nl> regmatch . regprog = vim_regcomp ( eap -> arg , magic_isset () ? RE_MAGIC : 0 ); <nl> if ( regmatch . regprog != NULL ) <nl> { <nl> + // make a copy of the line , when searching for a mark it might be <nl> + // flushed <nl> + char_u * line = vim_strsave ( ml_get_curline ()); <nl> + <nl> regmatch . rm_ic = p_ic ; <nl> - p = ml_get_curline (); <nl> - if ( vim_regexec (& regmatch , p , ( colnr_T ) 0 )) <nl> - curwin -> w_cursor . col = ( colnr_T )( regmatch . startp [ 0 ] - p ); <nl> + if ( vim_regexec (& regmatch , line , ( colnr_T ) 0 )) <nl> + curwin -> w_cursor . col = ( colnr_T )( regmatch . startp [ 0 ] - line ); <nl> else <nl> emsg ( _ ( e_nomatch )); <nl> vim_regfree ( regmatch . regprog ); <nl> + vim_free ( line ); <nl> } <nl> // Move to the NUL , ignore any other arguments . <nl> eap -> arg += STRLEN ( eap -> arg );
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1367 , <nl> /**/ <nl> 1366 , <nl> /**/mmm src / move . c <nl> ppp src / move . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 1367 , <nl> /**/ <nl> 1366 , <nl> /**/ <nl> scrolldown ( <nl> col -= width1 ; <nl> ++ row ; <nl> } <nl> - if ( col > width2 ) <nl> + if ( col > width2 && width2 > 0 ) <nl> { <nl> row += col / width2 ; <nl> col = col % width2 ;
mmm src / term . c <nl> ppp src / term . c <nl> check_shellsize ( void ) <nl> if ( Rows < min_rows ()) // need room for one window and command line <nl> Rows = min_rows (); <nl> limit_screen_size (); <nl> + <nl> + // make sure these values are not invalid <nl> + if ( cmdline_row >= Rows ) <nl> + cmdline_row = Rows - 1 ; <nl> + if ( msg_row >= Rows ) <nl> + msg_row = Rows - 1 ; <nl> } <nl>  <nl> /*mmm src / version . c <nl> ppp src / version . c <nl> check_shellsize ( void ) <nl> if ( Rows < min_rows ()) // need room for one window and command line <nl> Rows = min_rows (); <nl> limit_screen_size (); <nl> + <nl> + // make sure these values are not invalid <nl> + if ( cmdline_row >= Rows ) <nl> + cmdline_row = Rows - 1 ; <nl> + if ( msg_row >= Rows ) <nl> + msg_row = Rows - 1 ; <nl> } <nl>  <nl> /* <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5160 , <nl> /**/ <nl> 5159 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4977 , <nl> /**/ <nl> 4976 , <nl> /**/mmm src / ex_cmds . c <nl> ppp src / ex_cmds . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4977 , <nl> /**/ <nl> 4976 , <nl> /**/ <nl> ex_substitute ( exarg_T * eap ) <nl> // Save flags for recursion . They can change for e . g . <nl> // : s /^/\= execute (" s #^## gn ") <nl> subflags_save = subflags ; <nl> + <nl> + // Disallow changing text or switching window in an expression . <nl> + ++ textwinlock ; <nl> # endif <nl> // get length of substitution part <nl> sublen = vim_regsub_multi (& regmatch , <nl> sub_firstlnum - regmatch . startpos [ 0 ]. lnum , <nl> sub , sub_firstline , FALSE , magic_isset (), TRUE ); <nl> # ifdef FEAT_EVAL <nl> + -- textwinlock ; <nl> + <nl> // If getting the substitute string caused an error , don ' t do <nl> // the replacement . <nl> // Don ' t keep flags set by a recursive call . <nl> ex_substitute ( exarg_T * eap ) <nl> mch_memmove ( new_end , sub_firstline + copycol , ( size_t ) copy_len ); <nl> new_end += copy_len ; <nl>  <nl> +# ifdef FEAT_EVAL <nl> + ++ textwinlock ; <nl> +# endif <nl> ( void ) vim_regsub_multi (& regmatch , <nl> sub_firstlnum - regmatch . startpos [ 0 ]. lnum , <nl> sub , new_end , TRUE , magic_isset (), TRUE ); <nl> +# ifdef FEAT_EVAL <nl> + -- textwinlock ; <nl> +# endif <nl> sub_nsubs ++; <nl> did_sub = TRUE ; <nl> 
mmm src / normal . c <nl> ppp src / normal . c <nl> nv_brackets ( cmdarg_T * cap ) <nl> clearop ( cap -> oap ); <nl> else <nl> { <nl> + // Make a copy , if the line was changed it will be freed . <nl> + ptr = vim_strnsave ( ptr , len ); <nl> + if ( ptr == NULL ) <nl> + return ; <nl> + <nl> find_pattern_in_path ( ptr , 0 , len , TRUE , <nl> cap -> count0 == 0 ? ! isupper ( cap -> nchar ) : FALSE , <nl> (( cap -> nchar & 0xf ) == (' d ' & 0xf )) ? FIND_DEFINE : FIND_ANY , <nl> nv_brackets ( cmdarg_T * cap ) <nl> islower ( cap -> nchar ) ? ACTION_SHOW : ACTION_GOTO , <nl> cap -> cmdchar == ']' ? curwin -> w_cursor . lnum + 1 : ( linenr_T ) 1 , <nl> ( linenr_T ) MAXLNUM ); <nl> + vim_free ( ptr ); <nl> curwin -> w_set_curswant = TRUE ; <nl> } <nl> }mmm src / version . c <nl> ppp src / version . c <nl> nv_brackets ( cmdarg_T * cap ) <nl> clearop ( cap -> oap ); <nl> else <nl> { <nl> + // Make a copy , if the line was changed it will be freed . <nl> + ptr = vim_strnsave ( ptr , len ); <nl> + if ( ptr == NULL ) <nl> + return ; <nl> + <nl> find_pattern_in_path ( ptr , 0 , len , TRUE , <nl> cap -> count0 == 0 ? ! isupper ( cap -> nchar ) : FALSE , <nl> (( cap -> nchar & 0xf ) == (' d ' & 0xf )) ? FIND_DEFINE : FIND_ANY , <nl> nv_brackets ( cmdarg_T * cap ) <nl> islower ( cap -> nchar ) ? ACTION_SHOW : ACTION_GOTO , <nl> cap -> cmdchar == ']' ? curwin -> w_cursor . lnum + 1 : ( linenr_T ) 1 , <nl> ( linenr_T ) MAXLNUM ); <nl> + vim_free ( ptr ); <nl> curwin -> w_set_curswant = TRUE ; <nl> } <nl> } <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5024 , <nl> /**/ <nl> 5023 , <nl> /**/
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 218 , <nl> /**/ <nl> 217 , <nl> /**/mmm src / edit . c <nl> ppp src / edit . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 218 , <nl> /**/ <nl> 217 , <nl> /**/ <nl> edit_unputchar ( void ) <nl> * Only works when cursor is in the line that changes . <nl> */ <nl> void <nl> - display_dollar ( colnr_T col ) <nl> + display_dollar ( colnr_T col_arg ) <nl> { <nl> + colnr_T col = col_arg < 0 ? 0 : col_arg ; <nl> colnr_T save_col ; <nl>  <nl> if (! redrawing ())
mmm src / version . c <nl> ppp src / version . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 2136 , <nl> /**/ <nl> 2135 , <nl> /**/mmm src / window . c <nl> ppp src / window . c <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 2136 , <nl> /**/ <nl> 2135 , <nl> /**/ <nl> win_enter_ext ( <nl> # ifdef FEAT_JOB_CHANNEL <nl> entering_window ( curwin ); <nl> # endif <nl> + // Careful : autocommands may close the window and make " wp " invalid <nl> if ( trigger_new_autocmds ) <nl> apply_autocmds ( EVENT_WINNEW , NULL , NULL , FALSE , curbuf ); <nl> if ( trigger_enter_autocmds ) <nl> win_enter_ext ( <nl> # endif <nl> curwin -> w_redr_status = TRUE ; <nl> # ifdef FEAT_TERMINAL <nl> - if ( bt_terminal ( wp -> w_buffer )) <nl> + if ( bt_terminal ( curwin -> w_buffer )) <nl> // terminal is likely in another mode <nl> redraw_mode = TRUE ; <nl> # endif
mmm src / ex_getln . c <nl> ppp src / ex_getln . c <nl> cmdline_erase_chars ( <nl> { <nl> while ( p > ccline . cmdbuff && vim_isspace ( p [- 1 ])) <nl> -- p ; <nl> - i = vim_iswordc ( p [- 1 ]); <nl> - while ( p > ccline . cmdbuff && ! vim_isspace ( p [- 1 ]) <nl> - && vim_iswordc ( p [- 1 ]) == i ) <nl> - -- p ; <nl> + if ( p > ccline . cmdbuff ) <nl> + { <nl> + i = vim_iswordc ( p [- 1 ]); <nl> + while ( p > ccline . cmdbuff && ! vim_isspace ( p [- 1 ]) <nl> + && vim_iswordc ( p [- 1 ]) == i ) <nl> + -- p ; <nl> + } <nl> } <nl> else <nl> -- p ;mmm src / version . c <nl> ppp src / version . c <nl> cmdline_erase_chars ( <nl> { <nl> while ( p > ccline . cmdbuff && vim_isspace ( p [- 1 ])) <nl> -- p ; <nl> - i = vim_iswordc ( p [- 1 ]); <nl> - while ( p > ccline . cmdbuff && ! vim_isspace ( p [- 1 ]) <nl> - && vim_iswordc ( p [- 1 ]) == i ) <nl> - -- p ; <nl> + if ( p > ccline . cmdbuff ) <nl> + { <nl> + i = vim_iswordc ( p [- 1 ]); <nl> + while ( p > ccline . cmdbuff && ! vim_isspace ( p [- 1 ]) <nl> + && vim_iswordc ( p [- 1 ]) == i ) <nl> + -- p ; <nl> + } <nl> } <nl> else <nl> -- p ; <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 4899 , <nl> /**/ <nl> 4898 , <nl> /**/
mmm src / buffer . c <nl> ppp src / buffer . c <nl> can_unload_buffer ( buf_T * buf ) <nl> } <nl> } <nl> if (! can_unload ) <nl> - semsg ( _ ( e_attempt_to_delete_buffer_that_is_in_use_str ), buf -> b_fname ); <nl> + { <nl> + char_u * fname = buf -> b_fname != NULL ? buf -> b_fname : buf -> b_ffname ; <nl> + <nl> + semsg ( _ ( e_attempt_to_delete_buffer_that_is_in_use_str ), <nl> + fname != NULL ? fname : ( char_u *)"[ No Name ]"); <nl> + } <nl> return can_unload ; <nl> } <nl> mmm src / version . c <nl> ppp src / version . c <nl> can_unload_buffer ( buf_T * buf ) <nl> } <nl> } <nl> if (! can_unload ) <nl> - semsg ( _ ( e_attempt_to_delete_buffer_that_is_in_use_str ), buf -> b_fname ); <nl> + { <nl> + char_u * fname = buf -> b_fname != NULL ? buf -> b_fname : buf -> b_ffname ; <nl> + <nl> + semsg ( _ ( e_attempt_to_delete_buffer_that_is_in_use_str ), <nl> + fname != NULL ? fname : ( char_u *)"[ No Name ]"); <nl> + } <nl> return can_unload ; <nl> } <nl>  <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 614 , <nl> /**/ <nl> 613 , <nl> /**/mmm src / spell . c <nl> ppp src / spell . c <nl> can_unload_buffer ( buf_T * buf ) <nl> } <nl> } <nl> if (! can_unload ) <nl> - semsg ( _ ( e_attempt_to_delete_buffer_that_is_in_use_str ), buf -> b_fname ); <nl> + { <nl> + char_u * fname = buf -> b_fname != NULL ? buf -> b_fname : buf -> b_ffname ; <nl> + <nl> + semsg ( _ ( e_attempt_to_delete_buffer_that_is_in_use_str ), <nl> + fname != NULL ? fname : ( char_u *)"[ No Name ]"); <nl> + } <nl> return can_unload ; <nl> } <nl>  <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 614 , <nl> /**/ <nl> 613 , <nl> /**/ <nl> spell_load_lang ( char_u * lang ) <nl> sl . sl_slang = NULL ; <nl> sl . sl_nobreak = FALSE ; <nl>  <nl> + // Disallow deleting the current buffer . Autocommands can do weird things <nl> + // and cause " lang " to be freed . <nl> + ++ curbuf -> b_locked ; <nl> + <nl> // We may retry when no spell file is found for the language , an <nl> // autocommand may load it then . <nl> for ( round = 1 ; round <= 2 ; ++ round ) <nl> spell_load_lang ( char_u * lang ) <nl> STRCPY ( fname_enc + STRLEN ( fname_enc ) - 3 , " add . spl "); <nl> do_in_runtimepath ( fname_enc , DIP_ALL , spell_load_cb , & sl ); <nl> } <nl> + <nl> + -- curbuf -> b_locked ; <nl> } <nl>  <nl> /*
mmm src / regexp . c <nl> ppp src / regexp . c <nl> cstrchr ( char_u * s , int c ) <nl> { <nl> if ( enc_utf8 && c > 0x80 ) <nl> { <nl> - if ( utf_fold ( utf_ptr2char ( p )) == cc ) <nl> + int uc = utf_ptr2char ( p ); <nl> + <nl> + // Do not match an illegal byte . E . g . 0xff matches 0xc3 0xbf , <nl> + // not 0xff . <nl> + if (( uc < 0x80 || uc != * p ) && utf_fold ( uc ) == cc ) <nl> return p ; <nl> } <nl> else if (* p == c || * p == cc )mmm src / version . c <nl> ppp src / version . c <nl> cstrchr ( char_u * s , int c ) <nl> { <nl> if ( enc_utf8 && c > 0x80 ) <nl> { <nl> - if ( utf_fold ( utf_ptr2char ( p )) == cc ) <nl> + int uc = utf_ptr2char ( p ); <nl> + <nl> + // Do not match an illegal byte . E . g . 0xff matches 0xc3 0xbf , <nl> + // not 0xff . <nl> + if (( uc < 0x80 || uc != * p ) && utf_fold ( uc ) == cc ) <nl> return p ; <nl> } <nl> else if (* p == c || * p == cc ) <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 105 , <nl> /**/ <nl> 104 , <nl> /**/
mmm src / ex_docmd . c <nl> ppp src / ex_docmd . c <nl> parse_cmd_address ( exarg_T * eap , char ** errormsg , int silent ) <nl> curwin -> w_cursor . lnum = eap -> line2 ; <nl>  <nl> // Don ' t leave the cursor on an illegal line or column , but do <nl> - // accept zero as address , so 0 ;/ PATTERN / works correctly . <nl> + // accept zero as address , so 0 ;/ PATTERN / works correctly <nl> + // ( where zero usually means to use the first line ). <nl> // Check the cursor position before returning . <nl> if ( eap -> line2 > 0 ) <nl> check_cursor (); <nl> + else <nl> + check_cursor_col (); <nl> need_check_cursor = TRUE ; <nl> } <nl> }mmm src / version . c <nl> ppp src / version . c <nl> parse_cmd_address ( exarg_T * eap , char ** errormsg , int silent ) <nl> curwin -> w_cursor . lnum = eap -> line2 ; <nl>  <nl> // Don ' t leave the cursor on an illegal line or column , but do <nl> - // accept zero as address , so 0 ;/ PATTERN / works correctly . <nl> + // accept zero as address , so 0 ;/ PATTERN / works correctly <nl> + // ( where zero usually means to use the first line ). <nl> // Check the cursor position before returning . <nl> if ( eap -> line2 > 0 ) <nl> check_cursor (); <nl> + else <nl> + check_cursor_col (); <nl> need_check_cursor = TRUE ; <nl> } <nl> } <nl> static char *( features []) = <nl>  <nl> static int included_patches [] = <nl> { /* Add new patch number below this line */ <nl> +/**/ <nl> + 5150 , <nl> /**/ <nl> 5149 , <nl> /**/
mmm src / daemon / protocols / lldp . c <nl> ppp src / daemon / protocols / lldp . c <nl> lldp_decode ( struct lldpd * cfg , char * frame , int s , <nl> case LLDP_TLV_MGMT_ADDR : <nl> CHECK_TLV_SIZE ( 1 , " Management address "); <nl> addr_str_length = PEEK_UINT8 ; <nl> + if ( addr_str_length > sizeof ( addr_str_buffer )) { <nl> + log_warnx (" lldp ", " too large management address on % s ", <nl> + hardware -> h_ifname ); <nl> + goto malformed ; <nl> + } <nl> CHECK_TLV_SIZE ( 1 + addr_str_length , " Management address "); <nl> PEEK_BYTES ( addr_str_buffer , addr_str_length ); <nl> addr_length = addr_str_length - 1 ; <nl> lldp_decode ( struct lldpd * cfg , char * frame , int s , <nl> CHECK_TLV_SIZE ( 1 + addr_str_length + 5 , " Management address "); <nl> iface_subtype = PEEK_UINT8 ; <nl> iface_number = PEEK_UINT32 ; <nl> - <nl> + <nl> af = lldpd_af_from_lldp_proto ( addr_family ); <nl> if ( af == LLDPD_AF_UNSPEC ) <nl> break ; <nl> lldp_decode ( struct lldpd * cfg , char * frame , int s , <nl> TAILQ_INSERT_TAIL (& chassis -> c_mgmt , mgmt , m_entries ); <nl> break ; <nl> case LLDP_TLV_ORG : <nl> - CHECK_TLV_SIZE ( 4 , " Organisational "); <nl> + CHECK_TLV_SIZE ( 1 + ( int ) sizeof ( orgid ), " Organisational "); <nl> PEEK_BYTES ( orgid , sizeof ( orgid )); <nl> tlv_subtype = PEEK_UINT8 ; <nl> if ( memcmp ( dot1 , orgid , sizeof ( orgid )) == 0 ) {
mmm modules / pico_ipv4 . c <nl> ppp modules / pico_ipv4 . c <nl> static int pico_ipv4_process_in ( struct pico_stack * S , struct pico_protocol * self <nl> f -> transport_hdr = (( uint8_t *) f -> net_hdr ) + PICO_SIZE_IP4HDR + option_len ; <nl> f -> transport_len = ( uint16_t )( short_be ( hdr -> len ) - PICO_SIZE_IP4HDR - option_len ); <nl> f -> net_len = ( uint16_t )( PICO_SIZE_IP4HDR + option_len ); <nl> + <nl> + if (( f -> net_hdr + f -> net_len ) > ( f -> buffer + f -> buffer_len )) { <nl> + pico_frame_discard ( f ); <nl> + return 0 ; <nl> + } <nl> # if defined ( PICO_SUPPORT_IPV4FRAG ) || defined ( PICO_SUPPORT_IPV6FRAG ) <nl> f -> frag = short_be ( hdr -> frag ); <nl> # endifmmm modules / pico_tcp . c <nl> ppp modules / pico_tcp . c <nl> static int pico_ipv4_process_in ( struct pico_stack * S , struct pico_protocol * self <nl> f -> transport_hdr = (( uint8_t *) f -> net_hdr ) + PICO_SIZE_IP4HDR + option_len ; <nl> f -> transport_len = ( uint16_t )( short_be ( hdr -> len ) - PICO_SIZE_IP4HDR - option_len ); <nl> f -> net_len = ( uint16_t )( PICO_SIZE_IP4HDR + option_len ); <nl> + <nl> + if (( f -> net_hdr + f -> net_len ) > ( f -> buffer + f -> buffer_len )) { <nl> + pico_frame_discard ( f ); <nl> + return 0 ; <nl> + } <nl> # if defined ( PICO_SUPPORT_IPV4FRAG ) || defined ( PICO_SUPPORT_IPV6FRAG ) <nl> f -> frag = short_be ( hdr -> frag ); <nl> # endif <nl> static inline void tcp_parse_option_mss ( struct pico_socket_tcp * t , uint8_t len , <nl> if ( tcpopt_len_check ( idx , len , PICO_TCPOPTLEN_MSS ) < 0 ) <nl> return ; <nl>  <nl> + if ((* idx + PICO_TCPOPTLEN_MSS ) > len ) <nl> + return ; <nl> + <nl> t -> mss_ok = 1 ; <nl> mss = short_from ( opt + * idx ); <nl> * idx += ( uint32_t ) sizeof ( uint16_t ); <nl> static int tcp_parse_options ( struct pico_frame * f ) <nl> uint8_t * opt = f -> transport_hdr + PICO_SIZE_TCPHDR ; <nl> uint32_t i = 0 ; <nl> f -> timestamp = 0 ; <nl> + <nl> + if ( f -> buffer + f -> buffer_len > f -> transport_hdr + f -> transport_len ) <nl> + return - 1 ; <nl> + <nl> while ( i < ( f -> transport_len - PICO_SIZE_TCPHDR )) { <nl> uint8_t type = opt [ i ++]; <nl> uint8_t len ;
mmm libyara / modules / pe . c <nl> ppp libyara / modules / pe . c <nl> IMPORTED_FUNCTION * pe_parse_import_descriptor ( <nl> IMPORTED_FUNCTION * imported_func = ( IMPORTED_FUNCTION *) <nl> yr_calloc ( 1 , sizeof ( IMPORTED_FUNCTION )); <nl>  <nl> + if (! imported_func ) <nl> + continue ; <nl> + <nl> imported_func -> name = name ; <nl> imported_func -> next = NULL ; <nl>  <nl> IMPORTED_FUNCTION * pe_parse_import_descriptor ( <nl> IMPORTED_FUNCTION * imported_func = ( IMPORTED_FUNCTION *) <nl> yr_calloc ( 1 , sizeof ( IMPORTED_FUNCTION )); <nl>  <nl> + if (! imported_func ) <nl> + continue ; <nl> + <nl> imported_func -> name = name ; <nl> imported_func -> next = NULL ; <nl> 
mmm libyara / modules / pe . c <nl> ppp libyara / modules / pe . c <nl> limitations under the License . <nl>  <nl>  <nl> # define fits_in_pe ( pe , pointer , size ) \ <nl> - (( uint8_t *)( pointer ) + size <= pe -> data + pe -> data_size ) <nl> + ( size <= pe -> data_size && \ <nl> + ( uint8_t *)( pointer ) >= pe -> data && \ <nl> + ( uint8_t *)( pointer ) + size <= pe -> data + pe -> data_size ) <nl>  <nl>  <nl> # define struct_fits_in_pe ( pe , pointer , struct_type ) \
mmm libyara / modules / pe . c <nl> ppp libyara / modules / pe . c <nl> void pe_parse_version_info ( <nl>  <nl> version_info = ( PVERSION_INFO ) ( pe -> data + version_info_offset ); <nl>  <nl> - if (! fits_in_pe ( pe , version_info -> Key , sizeof (" VS_VERSION_INFO "))) <nl> + if (! fits_in_pe ( pe , version_info -> Key , sizeof (" VS_VERSION_INFO ") * 2 )) <nl> return ; <nl>  <nl> if ( strcmp_w ( version_info -> Key , " VS_VERSION_INFO ") != 0 ) <nl> void pe_parse_version_info ( <nl>  <nl> string_file_info = ADD_OFFSET ( version_info , sizeof ( VERSION_INFO ) + 86 ); <nl>  <nl> - while ( fits_in_pe ( pe , string_file_info -> Key , sizeof (" StringFileInfo ")) && <nl> + while ( fits_in_pe ( pe , string_file_info -> Key , sizeof (" StringFileInfo ") * 2 ) && <nl> strcmp_w ( string_file_info -> Key , " StringFileInfo ") == 0 ) <nl> { <nl> PVERSION_INFO string_table = ADD_OFFSET (
mmm libyara / atoms . c <nl> ppp libyara / atoms . c <nl> int yr_atoms_table_quality ( <nl> } <nl> else <nl> { <nl> - if ( atom_length == YR_MAX_ATOM_LENGTH ) <nl> - return table [ middle ]. quality ; <nl> - <nl> int i = middle + 1 ; <nl> int quality = table [ middle ]. quality ; <nl> int min_quality = quality ; <nl>  <nl> + if ( atom_length == YR_MAX_ATOM_LENGTH ) <nl> + return table [ middle ]. quality ; <nl> + <nl> while ( i < end && memcmp ( table [ i ]. atom , atom , atom_length ) == 0 ) <nl> { <nl> if ( min_quality > table [ i ]. quality )
mmm libyara / pe . h <nl> ppp libyara / pe . h <nl> typedef struct _IMAGE_FILE_HEADER { <nl>  <nl>  <nl> # define IMAGE_FILE_MACHINE_I386 0x014c // Intel 386 . <nl> -# define IMAGE_FILE_MACHINE_X64 0x8664 // Intel x64 . <nl> +# define IMAGE_FILE_MACHINE_AMD64 0x8664 // Intel x64 . <nl>  <nl> // <nl> // Directory format .mmm libyara / exefiles . c <nl> ppp libyara / exefiles . c <nl> typedef struct _IMAGE_FILE_HEADER { <nl>  <nl>  <nl> # define IMAGE_FILE_MACHINE_I386 0x014c // Intel 386 . <nl> -# define IMAGE_FILE_MACHINE_X64 0x8664 // Intel x64 . <nl> +# define IMAGE_FILE_MACHINE_AMD64 0x8664 // Intel x64 . <nl>  <nl> // <nl> // Directory format . <nl> PIMAGE_NT_HEADERS yr_get_pe_header ( <nl>  <nl> if ( pe_header -> Signature == IMAGE_NT_SIGNATURE && <nl> ( pe_header -> FileHeader . Machine == IMAGE_FILE_MACHINE_I386 || <nl> - pe_header -> FileHeader . Machine == IMAGE_FILE_MACHINE_X64 ) && <nl> + pe_header -> FileHeader . Machine == IMAGE_FILE_MACHINE_AMD64 ) && <nl> buffer_length > headers_size ) <nl> { <nl> return pe_header ;
mmm libyara / modules / pe . c <nl> ppp libyara / modules / pe . c <nl> void pe_parse_certificates ( <nl> } <nl> BIO_read ( date_bio , p , date_bio -> num_write ); <nl> p [ date_bio -> num_write ] = '\ x0 '; <nl> - set_string ( p , pe -> object , " signatures [% i ]. notBefore ", counter ); <nl> + set_string ( p , pe -> object , " signatures [% i ]. not_before ", counter ); <nl> yr_free ( p ); <nl> date_time = X509_get_notAfter ( cert ); <nl> ASN1_TIME_print ( date_bio , date_time ); <nl> void pe_parse_certificates ( <nl> } <nl> BIO_read ( date_bio , p , date_length ); <nl> p [ date_length ] = '\ x0 '; <nl> - set_string ( p , pe -> object , " signatures [% i ]. notAfter ", counter ); <nl> + set_string ( p , pe -> object , " signatures [% i ]. not_after ", counter ); <nl> yr_free ( p ); <nl> } <nl> BIO_set_close ( date_bio , BIO_CLOSE ); <nl> begin_declarations ; <nl> declare_integer (" version "); <nl> declare_string (" algorithm "); <nl> declare_string (" serial "); <nl> - declare_string (" notBefore "); <nl> - declare_string (" notAfter "); <nl> + declare_string (" not_before "); <nl> + declare_string (" not_after "); <nl> end_struct_array (" signatures "); <nl> declare_integer (" number_of_signatures "); <nl> 
mmm libyara / ast . c <nl> ppp libyara / ast . c <nl> void free_term ( TERM * term ) <nl>  <nl> free_term ((( TERM_STRING *) term )-> offset ); <nl> break ; <nl> + <nl> + case TERM_TYPE_STRING_OFFSET : <nl> + <nl> + free_term ((( TERM_STRING *) term )-> index ); <nl> + break ; <nl>  <nl> case TERM_TYPE_STRING_IN_RANGE : <nl> 
mmm libyara / re . c <nl> ppp libyara / re . c <nl> int yr_re_fast_exec ( <nl>  <nl> for ( i = repeat_any_args -> min + 1 ; i <= repeat_any_args -> max ; i ++) <nl> { <nl> - next_input = input + i * input_incr ; <nl> - <nl> if ( bytes_matched + i >= max_bytes_matched ) <nl> break ; <nl>  <nl> + next_input = input + i * input_incr ; <nl> + <nl> if ( *( next_opcode ) != RE_OPCODE_LITERAL || <nl> (*( next_opcode ) == RE_OPCODE_LITERAL && <nl> *( next_opcode + 1 ) == * next_input )) <nl> int yr_re_fast_exec ( <nl>  <nl> input += input_incr * repeat_any_args -> min ; <nl> bytes_matched += repeat_any_args -> min ; <nl> + bytes_matched = yr_min ( bytes_matched , max_bytes_matched ); <nl> ip = next_opcode ; <nl>  <nl> break ;mmm libyara / scan . c <nl> ppp libyara / scan . c <nl> int yr_re_fast_exec ( <nl>  <nl> for ( i = repeat_any_args -> min + 1 ; i <= repeat_any_args -> max ; i ++) <nl> { <nl> - next_input = input + i * input_incr ; <nl> - <nl> if ( bytes_matched + i >= max_bytes_matched ) <nl> break ; <nl>  <nl> + next_input = input + i * input_incr ; <nl> + <nl> if ( *( next_opcode ) != RE_OPCODE_LITERAL || <nl> (*( next_opcode ) == RE_OPCODE_LITERAL && <nl> *( next_opcode + 1 ) == * next_input )) <nl> int yr_re_fast_exec ( <nl>  <nl> input += input_incr * repeat_any_args -> min ; <nl> bytes_matched += repeat_any_args -> min ; <nl> + bytes_matched = yr_min ( bytes_matched , max_bytes_matched ); <nl> ip = next_opcode ; <nl>  <nl> break ; <nl> int _yr_scan_match_callback ( <nl> // total match length is the sum of backward and forward matches . <nl> match_length += callback_args -> forward_matches ; <nl>  <nl> + // make sure that match fits into the data . <nl> + assert ( match_offset + match_length <= callback_args -> data_size ); <nl> + <nl> if ( callback_args -> full_word ) <nl> { <nl> if ( flags & RE_FLAGS_WIDE )
mmm common . h <nl> ppp common . h <nl> bool compile_files ( <nl> { <nl> for ( int i = 0 ; i < argc - 1 ; i ++) <nl> { <nl> + FILE * rule_file ; <nl> const char * ns ; <nl> const char * file_name ; <nl> char * colon = ( char *) strchr ( argv [ i ], ':'); <nl> bool compile_files ( <nl> ns = NULL ; <nl> } <nl>  <nl> - FILE * rule_file = fopen ( file_name , " r "); <nl> + if ( strcmp ( file_name , "-") == 0 ) <nl> + rule_file = stdin ; <nl> + else <nl> + rule_file = fopen ( file_name , " r "); <nl>  <nl> if ( rule_file == NULL ) <nl> {
mmm libyara / libyara . c <nl> ppp libyara / libyara . c <nl> limitations under the License . <nl> # include < ctype . h > <nl>  <nl> # include < yara / error . h > <nl> -# include < yara / mem . h > <nl> # include < yara / re . h > <nl> # include < yara / modules . h > <nl> - <nl> +# include < yara / mem . h > <nl>  <nl> # ifdef _WIN32 <nl> # define snprintf _snprintfmmm libyara / include / yara / mem . h <nl> ppp libyara / include / yara / mem . h <nl> limitations under the License . <nl> # include < ctype . h > <nl>  <nl> # include < yara / error . h > <nl> -# include < yara / mem . h > <nl> # include < yara / re . h > <nl> # include < yara / modules . h > <nl> - <nl> +# include < yara / mem . h > <nl>  <nl> # ifdef _WIN32 <nl> # define snprintf _snprintf <nl> limitations under the License . <nl> # ifdef DMALLOC <nl>  <nl> # define yr_malloc malloc <nl> +# define yr_calloc calloc <nl> # define yr_realloc realloc <nl> # define yr_free free <nl> # define yr_strdup strdup
mmm libyara / modules / pe . c <nl> ppp libyara / modules / pe . c <nl> PIMAGE_DATA_DIRECTORY pe_get_directory_entry ( <nl> { <nl> PIMAGE_DATA_DIRECTORY result ; <nl>  <nl> - if ( pe -> header -> FileHeader . Machine == 0x8664 ) // is a 64 - bit PE ? <nl> + if ( pe -> header -> FileHeader . Machine == IMAGE_FILE_MACHINE_AMD64 ) <nl> result = &(( PIMAGE_NT_HEADERS64 ) pe -> header )-> <nl> OptionalHeader . DataDirectory [ entry ]; <nl> else <nl> void pe_parse ( <nl> char section_name [ IMAGE_SIZEOF_SHORT_NAME + 1 ]; <nl>  <nl> # define OptionalHeader ( field ) \ <nl> - ( pe -> header -> FileHeader . Machine == 0x8664 ? \ <nl> + ( pe -> header -> FileHeader . Machine == IMAGE_FILE_MACHINE_AMD64 ? \ <nl> (( PIMAGE_NT_HEADERS64 ) pe -> header )-> OptionalHeader . field : \ <nl> pe -> header -> OptionalHeader . field ) <nl> 
mmm libyara / modules / pe . c <nl> ppp libyara / modules / pe . c <nl> IMPORTED_FUNCTION * pe_parse_import_descriptor ( <nl> // I ' ve seen binaries where OriginalFirstThunk is zero . In this case <nl> // use FirstThunk . <nl>  <nl> - if ( offset < 0 ) <nl> + if ( offset <= 0 ) <nl> offset = pe_rva_to_offset ( pe , import_descriptor -> FirstThunk ); <nl>  <nl> if ( offset < 0 )
mmm ext / redcarpet / html . c <nl> ppp ext / redcarpet / html . c <nl> rndr_quote ( struct buf * ob , const struct buf * text , void * opaque ) <nl> if (! text || ! text -> size ) <nl> return 0 ; <nl>  <nl> + struct html_renderopt * options = opaque ; <nl> + <nl> BUFPUTSL ( ob , "< q >"); <nl> - bufput ( ob , text -> data , text -> size ); <nl> + <nl> + if ( options -> flags & HTML_ESCAPE ) <nl> + escape_html ( ob , text -> data , text -> size ); <nl> + else <nl> + bufput ( ob , text -> data , text -> size ); <nl> + <nl> BUFPUTSL ( ob , "</ q >"); <nl>  <nl> return 1 ;
mmm 3rdparty / expat / lib / xmlparse . c <nl> ppp 3rdparty / expat / lib / xmlparse . c <nl> doProlog ( XML_Parser parser , <nl> return XML_ERROR_UNCLOSED_TOKEN ; <nl> case XML_TOK_PARTIAL_CHAR : <nl> return XML_ERROR_PARTIAL_CHAR ; <nl> + case - XML_TOK_PROLOG_S : <nl> + tok = - tok ; <nl> + break ; <nl> case XML_TOK_NONE : <nl> # ifdef XML_DTD <nl> /* for internal PE NOT referenced between declarations */mmm 3rdparty / expat / lib / xmltok_impl . c <nl> ppp 3rdparty / expat / lib / xmltok_impl . c <nl> doProlog ( XML_Parser parser , <nl> return XML_ERROR_UNCLOSED_TOKEN ; <nl> case XML_TOK_PARTIAL_CHAR : <nl> return XML_ERROR_PARTIAL_CHAR ; <nl> + case - XML_TOK_PROLOG_S : <nl> + tok = - tok ; <nl> + break ; <nl> case XML_TOK_NONE : <nl> # ifdef XML_DTD <nl> /* for internal PE NOT referenced between declarations */ <nl> PREFIX ( updatePosition )( const ENCODING * enc , <nl> const char * end , <nl> POSITION * pos ) <nl> { <nl> - while ( ptr != end ) { <nl> + while ( ptr < end ) { <nl> switch ( BYTE_TYPE ( enc , ptr )) { <nl> # define LEAD_CASE ( n ) \ <nl> case BT_LEAD ## n : \
mmm src / ui . cpp <nl> ppp src / ui . cpp <nl> public : <nl> if ( ms_uiThread ) <nl> { <nl> ms_uiThread -> Join (); <nl> + delete ms_uiThread ; <nl> ms_uiThread = NULL ; <nl> } <nl> }
mmm src / ui . cpp <nl> ppp src / ui . cpp <nl> void CenterWindowOnHostApplication ( wxTopLevelWindow * win ) <nl> EnumWindows ( EnumProcessWindowsCallback , ( LPARAM ) & data ); <nl>  <nl> if ( data . biggest . IsEmpty ()) <nl> - return ; // no window to center on <nl> + { <nl> + // no parent window to center on , so center on the screen <nl> + win -> Center (); <nl> + return ; <nl> + } <nl>  <nl> const wxRect & host ( data . biggest ); <nl> 
mmm src / ui . cpp <nl> ppp src / ui . cpp <nl> BOOL CALLBACK EnumProcessWindowsCallback ( HWND handle , LPARAM lParam ) <nl>  <nl> RECT rwin ; <nl> GetWindowRect ( handle , & rwin ); <nl> + if ( MonitorFromRect (& rwin , MONITOR_DEFAULTTONULL ) == NULL ) <nl> + return TRUE ; // window is offscreen <nl> + <nl> wxRect r ( rwin . left , rwin . top , rwin . right - rwin . left , rwin . bottom - rwin . top ); <nl> if ( r . width * r . height > data . biggest . width * data . biggest . height ) <nl> data . biggest = r ;
mmm src / gui / curses / gui - curses - chat . c <nl> ppp src / gui / curses / gui - curses - chat . c <nl> gui_chat_draw ( struct t_gui_buffer * buffer , int clear_chat ) <nl>  <nl> if ( clear_chat ) <nl> { <nl> - snprintf ( format_empty , 32 , "%%-% ds ", ptr_win -> win_chat_width ); <nl> + snprintf ( format_empty , sizeof ( format_empty ), <nl> + "%%-% ds ", ptr_win -> win_chat_width ); <nl> for ( i = 0 ; i < ptr_win -> win_chat_height ; i ++) <nl> { <nl> mvwprintw ( GUI_WINDOW_OBJECTS ( ptr_win )-> win_chat , i , 0 ,
mmm src / plugins / irc / irc - ctcp . c <nl> ppp src / plugins / irc / irc - ctcp . c <nl> irc_ctcp_dcc_filename_without_quotes ( const char * filename ) <nl> int length ; <nl>  <nl> length = strlen ( filename ); <nl> - if ( length > 0 ) <nl> + if ( length > 1 ) <nl> { <nl> if (( filename [ 0 ] == '\"') && ( filename [ length - 1 ] == '\"')) <nl> return weechat_strndup ( filename + 1 , length - 2 );
mmm src / plugins / irc / irc - mode . c <nl> ppp src / plugins / irc / irc - mode . c <nl> irc_mode_channel_set ( struct t_irc_server * server , <nl> int smart_filter ; <nl> struct t_irc_nick * ptr_nick ; <nl> struct t_irc_modelist * ptr_modelist ; <nl> + struct t_irc_modelist_item * ptr_item ; <nl>  <nl> if (! server || ! channel || ! modes ) <nl> return 0 ; <nl> irc_mode_channel_set ( struct t_irc_server * server , <nl> } <nl> else if ( set_flag == '-') <nl> { <nl> - irc_modelist_item_free ( ptr_modelist , <nl> - irc_modelist_item_search ( ptr_modelist , ptr_arg )); <nl> + ptr_item = irc_modelist_item_search ( ptr_modelist , <nl> + ptr_arg ); <nl> + if ( ptr_item ) <nl> + irc_modelist_item_free ( ptr_modelist , ptr_item ); <nl> } <nl> } <nl> }
mmm src / core / wee - string . c <nl> ppp src / core / wee - string . c <nl> string_replace_regex ( const char * string , void * regex , const char * replace , <nl> int length , length_replace , start_offset , i , rc , end , last_match ; <nl> regmatch_t regex_match [ 100 ]; <nl>  <nl> - if (! string ) <nl> + if (! string || ! regex ) <nl> return NULL ; <nl>  <nl> length = strlen ( string ) + 1 ;
mmm src / gui / curses / gui - curses - bar - window . c <nl> ppp src / gui / curses / gui - curses - bar - window . c <nl> gui_bar_window_draw ( struct t_gui_bar_window * bar_window , <nl>  <nl> /* move cursor if it was asked in an item content ( input_text does that <nl> to move cursor in user input text ) */ <nl> - if ( window && ( gui_current_window == window ) <nl> + if ((! window || ( gui_current_window == window )) <nl> && ( bar_window -> cursor_x >= 0 ) && ( bar_window -> cursor_y >= 0 )) <nl> { <nl> move ( bar_window -> cursor_y , bar_window -> cursor_x );
mmm src / plugins / irc / irc - command . c <nl> ppp src / plugins / irc / irc - command . c <nl> irc_command_server ( void * data , struct t_gui_buffer * buffer , int argc , <nl> if ( irc_current_server ) <nl> { <nl> ptr_server = irc_current_server -> next_server ; <nl> + if (! ptr_server ) <nl> + ptr_server = irc_servers ; <nl> while ( ptr_server != irc_current_server ) <nl> { <nl> if ( ptr_server -> buffer )
mmm weechat / src / plugins / scripts / perl / weechat - perl . c <nl> ppp weechat / src / plugins / scripts / perl / weechat - perl . c <nl> weechat_perl_cmd ( t_weechat_plugin * plugin , <nl> ptr_handler -> handler_args ); <nl> } <nl> } <nl> + if (! handler_found ) <nl> + plugin -> print_server ( plugin , " ( none )"); <nl> break ; <nl> case 1 : <nl> if ( plugin -> ascii_strcasecmp ( plugin , argv [ 0 ], " autoload ") == 0 )mmm src / plugins / scripts / perl / weechat - perl . c <nl> ppp src / plugins / scripts / perl / weechat - perl . c <nl> weechat_perl_cmd ( t_weechat_plugin * plugin , <nl> ptr_handler -> handler_args ); <nl> } <nl> } <nl> + if (! handler_found ) <nl> + plugin -> print_server ( plugin , " ( none )"); <nl> break ; <nl> case 1 : <nl> if ( plugin -> ascii_strcasecmp ( plugin , argv [ 0 ], " autoload ") == 0 ) <nl> weechat_perl_cmd ( t_weechat_plugin * plugin , <nl> ptr_handler -> handler_args ); <nl> } <nl> } <nl> + if (! handler_found ) <nl> + plugin -> print_server ( plugin , " ( none )"); <nl> break ; <nl> case 1 : <nl> if ( plugin -> ascii_strcasecmp ( plugin , argv [ 0 ], " autoload ") == 0 )
mmm src / plugins / irc / irc - server . c <nl> ppp src / plugins / irc / irc - server . c <nl> irc_server_msgq_flush () <nl> /* new_msg = plugin_modifier_exec ( PLUGIN_MODIFIER_IRC_IN , <nl> irc_recv_msgq -> server -> name , <nl> ptr_data );*/ <nl> + new_msg = NULL ; <nl> + <nl> /* no changes in new message */ <nl> if ( new_msg && ( strcmp ( ptr_data , new_msg ) == 0 )) <nl> {
mmm src / plugins / trigger / trigger - callback . c <nl> ppp src / plugins / trigger / trigger - callback . c <nl> trigger_callback_replace_regex ( struct t_trigger * trigger , <nl>  <nl> for ( i = 0 ; i < trigger -> regex_count ; i ++) <nl> { <nl> + /* if regex is not set ( invalid ), skip it */ <nl> + if (! trigger -> regex [ i ]. regex ) <nl> + continue ; <nl> + <nl> ptr_key = ( trigger -> regex [ i ]. variable ) ? <nl> trigger -> regex [ i ]. variable : <nl> trigger_hook_regex_default_var [ weechat_config_integer ( trigger -> options [ TRIGGER_OPTION_HOOK ])];
mmm src / plugins / scripts / lua / weechat - lua - api . c <nl> ppp src / plugins / scripts / lua / weechat - lua - api . c <nl> weechat_lua_api_config_reload_cb ( void * data , <nl> { <nl> lua_argv [ 0 ] = ( script_callback -> data ) ? script_callback -> data : empty_arg ; <nl> lua_argv [ 1 ] = script_ptr2str ( config_file ); <nl> - lua_argv [ 2 ] = NULL ; <nl>  <nl> rc = ( int *) weechat_lua_exec ( script_callback -> script , <nl> WEECHAT_SCRIPT_EXEC_INT ,
mmm src / gui / gui - buffer . c <nl> ppp src / gui / gui - buffer . c <nl> gui_buffer_local_var_remove ( struct t_gui_buffer * buffer , <nl> buffer -> local_variables = local_var -> next_var ; <nl> if ( buffer -> last_local_var == local_var ) <nl> buffer -> last_local_var = local_var -> prev_var ; <nl> + <nl> + free ( local_var ); <nl> } <nl>  <nl> /*
mmm src / core / hook / wee - hook - line . c <nl> ppp src / core / hook / wee - hook - line . c <nl> hook_line_free_data ( struct t_hook * hook ) <nl> if (! hook || ! hook -> hook_data ) <nl> return ; <nl>  <nl> + if ( HOOK_LINE ( hook , buffers )) <nl> + { <nl> + string_free_split ( HOOK_LINE ( hook , buffers )); <nl> + HOOK_LINE ( hook , buffers ) = NULL ; <nl> + } <nl> if ( HOOK_LINE ( hook , tags_array )) <nl> { <nl> string_free_split_tags ( HOOK_LINE ( hook , tags_array ));
mmm src / gui / gui - buffer . c <nl> ppp src / gui / gui - buffer . c <nl> gui_buffer_set_highlight_words_list ( struct t_gui_buffer * buffer , <nl> } <nl>  <nl> gui_buffer_set_highlight_words ( buffer , words ); <nl> + <nl> + free ( words ); <nl> } <nl>  <nl> /*
mmm src / core / wee - hook . c <nl> ppp src / core / wee - hook . c <nl> hook_process_hashtable ( struct t_weechat_plugin * plugin , <nl>  <nl> hook_add_to_list ( new_hook ); <nl>  <nl> + if ( weechat_debug_core >= 1 ) <nl> + { <nl> + gui_chat_printf ( NULL , <nl> + " debug : hook_process : command =\"% s \", " <nl> + " options =\"% s \", timeout =% d ", <nl> + new_hook_process -> command , <nl> + hashtable_get_string ( new_hook_process -> options , <nl> + " keys_values "), <nl> + new_hook_process -> timeout ); <nl> + } <nl> + <nl> hook_process_run ( new_hook ); <nl>  <nl> return new_hook ;
mmm src / core / wee - command . c <nl> ppp src / core / wee - command . c <nl> COMMAND_CALLBACK ( cursor ) <nl> gui_cursor_move_xy ( x , y ); <nl> } <nl> } <nl> + free ( str_x ); <nl> } <nl> } <nl> else
mmm src / gui / gui - buffer . c <nl> ppp src / gui / gui - buffer . c <nl> gui_buffer_move_to_number ( struct t_gui_buffer * buffer , int number ) <nl> if ( ptr_buffer == ptr_last_buffer ) <nl> break ; <nl> } <nl> - gui_buffers -> prev_buffer = buffer ; <nl> - buffer -> prev_buffer = NULL ; <nl> - buffer -> next_buffer = gui_buffers ; <nl> - gui_buffers = buffer ; <nl> + gui_buffers -> prev_buffer = ptr_last_buffer ; <nl> + ptr_first_buffer -> prev_buffer = NULL ; <nl> + ptr_last_buffer -> next_buffer = gui_buffers ; <nl> + gui_buffers = ptr_first_buffer ; <nl> } <nl> else <nl> { <nl> gui_buffer_move_to_number ( struct t_gui_buffer * buffer , int number ) <nl> for ( ptr_buffer_pos = gui_buffers ; ptr_buffer_pos ; <nl> ptr_buffer_pos = ptr_buffer_pos -> next_buffer ) <nl> { <nl> - if ( ptr_buffer_pos -> number == number ) <nl> + if ( ptr_buffer_pos -> number >= number ) <nl> break ; <nl> } <nl> if ( ptr_buffer_pos )
mmm src / plugins / relay / irc / relay - irc . c <nl> ppp src / plugins / relay / irc / relay - irc . c <nl> relay_irc_recv ( struct t_relay_client * client , const char * data ) <nl> /* server capabilities */ <nl> if ( irc_command && ( weechat_strcasecmp ( irc_command , " cap ") == 0 )) <nl> { <nl> - if (( irc_argc > 0 ) && irc_argv ) <nl> + if ( irc_argc > 0 ) <nl> { <nl> relay_irc_recv_command_capab ( client , <nl> irc_argc , irc_argv , irc_argv_eol );
mmm src / gui / curses / gui - curses - window . c <nl> ppp src / gui / curses / gui - curses - window . c <nl> gui_window_set_custom_color_fg ( WINDOW * window , int fg ) <nl> { <nl> current_bg = window_current_style_bg ; <nl>  <nl> + gui_window_remove_color_style ( window , A_BOLD ); <nl> + <nl> if (( fg > 0 ) && ( fg & GUI_COLOR_EXTENDED_FLAG )) <nl> { <nl> gui_window_set_color ( window , <nl> gui_window_set_custom_color_fg ( WINDOW * window , int fg ) <nl> } <nl> else if ( fg < GUI_CURSES_NUM_WEECHAT_COLORS ) <nl> { <nl> - gui_window_remove_color_style ( window , A_BOLD ); <nl> attributes = gui_weechat_colors [ fg ]. attributes ; <nl> gui_window_set_color_style ( window , attributes ); <nl> fg = gui_weechat_colors [ fg ]. foreground ;
mmm src / plugins / irc / irc - server . c <nl> ppp src / plugins / irc / irc - server . c <nl> irc_server_msgq_flush () <nl> free ( command ); <nl> if ( channel ) <nl> free ( channel ); <nl> + if ( arguments ) <nl> + free ( arguments ); <nl> if ( msg_decoded ) <nl> free ( msg_decoded ); <nl> if ( msg_decoded_without_color )
mmm config . h <nl> ppp config . h <nl> namespace CryptoPP { } <nl> # define __USE_W32_SOCKETS <nl> # endif <nl>  <nl> - typedef unsigned char byte ; // put in global namespace to avoid ambiguity with other byte typedefs <nl> +// Originally in global namespace to avoid ambiguity with other byte typedefs . <nl> +// Moved to Crypto ++ namespace due to C ++ 17 , std :: byte and potential compile problems . Also see <nl> +// http :// www . cryptopp . com / wiki / std :: byte and http :// github . com / weidai11 / cryptopp / issues / 442 <nl> +// typedef unsigned char byte ; <nl> +# define CRYPTOPP_NO_GLOBAL_BYTE 1 <nl>  <nl> NAMESPACE_BEGIN ( CryptoPP ) <nl>  <nl> + typedef unsigned char byte ; <nl> typedef unsigned short word16 ; <nl> typedef unsigned int word32 ; <nl> 
mmm randpool . cpp <nl> ppp randpool . cpp <nl> NAMESPACE_BEGIN ( CryptoPP ) <nl> RandomPool :: RandomPool () <nl> : m_pCipher ( new AES :: Encryption ), m_keySet ( false ) <nl> { <nl> + memset ( m_key , 0 , m_key . SizeInBytes ()); <nl> + memset ( m_seed , 0 , m_seed . SizeInBytes ()); <nl> } <nl>  <nl> void RandomPool :: IncorporateEntropy ( const byte * input , size_t length )
mmm stdcpp . h <nl> ppp stdcpp . h <nl> # include < algorithm > <nl> # include < functional > <nl>  <nl> +// R - value references and std :: move <nl> +# if defined ( __cplusplus >= 201103L ) <nl> +# include < utility > <nl> +# endif <nl> + <nl> # ifdef CRYPTOPP_INCLUDE_VECTOR_CC <nl> // workaround needed on Sun Studio 12u1 Sun C ++ 5 . 10 SunOS_i386 128229 - 02 2009 / 09 / 21 <nl> # include < vector . cc >
mmm misc . h <nl> ppp misc . h <nl> inline T1 RoundUpToMultipleOf ( const T1 & n , const T2 & m ) <nl> //! \ details Internally the function calls C ++ 11 ' s < tt > alignof </ tt > if available . If not available , <nl> //! then the function uses compiler specific extensions such as < tt > __alignof </ tt > and <nl> //! < tt > _alignof_ </ tt >. If an extension is not available , then the function uses <nl> -//! < tt > __BIGGEST_ALIGNMENT__ </ tt >. < tt > sizeof ( T )</ tt > is used if the others are not available . <nl> +//! < tt > __BIGGEST_ALIGNMENT__ </ tt > if < tt > __BIGGEST_ALIGNMENT__ </ tt > is smaller than < tt > sizeof ( T )</ tt >. <nl> +//! < tt > sizeof ( T )</ tt > is used if all others are not available . <nl> //! In < em > all </ em > cases , if < tt > CRYPTOPP_ALLOW_UNALIGNED_DATA_ACCESS </ tt > is defined , then the <nl> //! function returns 1 . <nl> template < class T > <nl> inline unsigned int GetAlignmentOf ( T * dummy = NULL ) // VC60 workaround <nl> return __alignof ( T ); <nl> # elif defined ( __GNUC__ ) <nl> return __alignof__ ( T ); <nl> -# elif __BIGGEST_ALIGNMENT__ <nl> - return __BIGGEST_ALIGNMENT__ ; <nl> # elif CRYPTOPP_BOOL_SLOW_WORD64 <nl> return UnsignedMin ( 4U , sizeof ( T )); <nl> # else <nl> +# if __BIGGEST_ALIGNMENT__ <nl> + if ( __BIGGEST_ALIGNMENT__ < sizeof ( T )) <nl> + return __BIGGEST_ALIGNMENT__ ; <nl> + else <nl> +# endif <nl> return sizeof ( T ); <nl> # endif <nl> }
mmm config . h <nl> ppp config . h <nl> typedef unsigned int word32 ; <nl> # if defined ( _MSC_VER ) || defined ( __BORLANDC__ ) <nl> typedef unsigned __int64 word64 ; <nl> # define W64LIT ( x ) x ## ui64 <nl> -# elif ( _LP64 || __LP64__ ) && ! defined ( __SUNPRO_CC ) <nl> +# elif ( _LP64 || __LP64__ ) <nl> typedef unsigned long word64 ; <nl> # define W64LIT ( x ) x ## UL <nl> # else
mmm kalyna . cpp <nl> ppp kalyna . cpp <nl> void Kalyna :: Base :: UncheckedSetKey ( const byte * key , unsigned int keylen , const N <nl>  <nl> void Kalyna :: Base :: ProcessAndXorBlock ( const byte * inBlock , const byte * xorBlock , byte * outBlock ) const <nl> { <nl> + // Timing attack countermeasure . see comments in Rijndael for more details <nl> + const int cacheLineSize = GetCacheLineSize (); <nl> + volatile word32 _u = 0 ; <nl> + word32 u = _u ; <nl> + <nl> + for ( unsigned int i = 0 ; i < COUNTOF ( KalynaTab :: S ); i += cacheLineSize ) <nl> + u &= * reinterpret_cast < const word32 *>( KalynaTab :: S + i ); <nl> + m_wspace [ 0 ] = u ; <nl> + <nl> switch (( m_nb << 8 ) | m_nk ) <nl> { <nl> case ( 2 << 8 ) | 2 :
mmm ppc - simd . cpp <nl> ppp ppc - simd . cpp <nl> // is needed because additional CXXFLAGS are required to enable the <nl> // appropriate instructions sets in some build configurations . <nl>  <nl> +// TODO : we still need to implement Power8 SHA . Once we have Power8 SHA , <nl> +// we should be able to use CRYPTOPP_POWER8_AES_AVAILABLE and <nl> +// CRYPTOPP_POWER8_SHA_AVAILABLE instead of the broader <nl> +// CRYPTOPP_POWER8_AVAILABLE . The change will need to be coordinated <nl> +// with the defines in config . h . <nl> + <nl> +// TODO : Bob Wilkinson reported we are misdetecting CRYPTOPP_POWER8_AVAILABLE . <nl> +// The problem is , the updated compiler supports them but the down - level <nl> +// assembler and linker do not . We will probably need to fix that through <nl> +// the makefile , similar to the way x86 AES and SHA is handled . Another <nl> +// twist is , we don ' t have access to a test machine and it must be fixed <nl> +// for two compilers ( IBM XL C / C ++ and GCC ). Ugh ... <nl> + <nl> # include " pch . h " <nl> # include " config . h " <nl> # include " stdcpp . h " <nl> bool CPU_ProbePower8 () <nl> { <nl> # if defined ( CRYPTOPP_NO_CPU_FEATURE_PROBES ) <nl> return false ; <nl> -# elif ( CRYPTOPP_POWER7_AVAILABLE ) <nl> +# elif ( CRYPTOPP_POWER8_AVAILABLE ) <nl> # if defined ( CRYPTOPP_GNU_STYLE_INLINE_ASSEMBLY ) <nl>  <nl> // longjmp and clobber warnings . Volatile is required .
mmm secblock . h <nl> ppp secblock . h <nl> public : <nl>  <nl> if ( t . m_size ) <nl> { <nl> + const size_type oldSize = m_size ; <nl> if ( this != & t ) // s += t <nl> { <nl> - const size_type oldSize = m_size ; <nl> Grow ( m_size + t . m_size ); <nl> memcpy_s ( m_ptr + oldSize , ( m_size - oldSize )* sizeof ( T ), t . m_ptr , t . m_size * sizeof ( T )); <nl> } <nl> else // t += t <nl> { <nl> - SecBlock result ( m_size + t . m_size ); <nl> - if ( m_size ) { memcpy_s ( result . m_ptr , result . m_size * sizeof ( T ), m_ptr , m_size * sizeof ( T ));} <nl> - memcpy_s ( result . m_ptr + m_size , ( result . m_size - m_size )* sizeof ( T ), t . m_ptr , t . m_size * sizeof ( T )); <nl> - swap ( result ); <nl> + Grow ( m_size * 2 ); <nl> + memcpy_s ( m_ptr + oldSize , ( m_size - oldSize )* sizeof ( T ), m_ptr , oldSize * sizeof ( T )); <nl> } <nl> } <nl> return * this ;
mmm ecp . cpp <nl> ppp ecp . cpp <nl> ECP :: ECP ( BufferedTransformation & bt ) <nl> GetField (). BERDecodeElement ( seq , m_b ); <nl> // skip optional seed <nl> if (! seq . EndReached ()) <nl> - BERDecodeOctetString ( seq , TheBitBucket ()); <nl> + { <nl> + SecByteBlock seed ; <nl> + unsigned int unused ; <nl> + BERDecodeBitString ( seq , seed , unused ); <nl> + } <nl> seq . MessageEnd (); <nl> } <nl> 
mmm queue . cpp <nl> ppp queue . cpp <nl> public : <nl>  <nl> inline size_t Put ( const byte * begin , size_t length ) <nl> { <nl> + if (! begin || ! length ) return length ; <nl> size_t l = STDMIN ( length , MaxSize ()- m_tail ); <nl> if ( buf + m_tail != begin ) <nl> memcpy ( buf + m_tail , begin , l ); <nl> public : <nl>  <nl> inline size_t Peek ( byte * target , size_t copyMax ) const <nl> { <nl> + if (! target || ! copyMax ) return 0 ; <nl> size_t len = STDMIN ( copyMax , m_tail - m_head ); <nl> memcpy ( target , buf + m_head , len ); <nl> return len ;
mmm secblock . h <nl> ppp secblock . h <nl> public : <nl> /// \ since Crypto ++ 6 . 0 <nl> # if defined ( CRYPTOPP_DOXYGEN_PROCESSING ) <nl> static const size_type ELEMS_MAX = ...; <nl> +# elif defined ( _MSC_VER ) && ( _MSC_VER <= 1400 ) <nl> + static const size_type ELEMS_MAX = (~( size_type ) 0 )/ sizeof ( T ); <nl> # elif defined ( CRYPTOPP_CXX11_ENUM ) <nl> enum : size_type { ELEMS_MAX = SIZE_MAX / sizeof ( T )}; <nl> # else <nl> public : <nl> /// \ since Crypto ++ 6 . 0 <nl> # if defined ( CRYPTOPP_DOXYGEN_PROCESSING ) <nl> static const size_type ELEMS_MAX = ...; <nl> +# elif defined ( _MSC_VER ) && ( _MSC_VER <= 1400 ) <nl> + static const size_type ELEMS_MAX = (~( size_type ) 0 )/ sizeof ( T ); <nl> # elif defined ( CRYPTOPP_CXX11_ENUM ) <nl> enum : size_type { ELEMS_MAX = A :: ELEMS_MAX }; <nl> # else
mmm config . h <nl> ppp config . h <nl> const lword LWORD_MAX = W64LIT ( 0xffffffffffffffff ); <nl> # else <nl> # define CRYPTOPP_NATIVE_DWORD_AVAILABLE 1 <nl> # if defined ( __alpha__ ) || defined ( __ia64__ ) || defined ( _ARCH_PPC64 ) || defined ( __x86_64__ ) || defined ( __mips64 ) || defined ( __sparc64__ ) <nl> -# if defined ( __GNUC__ ) && ! defined ( __INTEL_COMPILER ) && !( CRYPTOPP_GCC_VERSION == 40001 && defined ( __APPLE__ )) && !( defined ( __GNUC__ ) && CRYPTOPP_GCC_VERSION < 50000 && defined ( _ARCH_PPC64 )) && CRYPTOPP_GCC_VERSION >= 30400 <nl> + # if (( CRYPTOPP_GCC_VERSION >= 30400 ) || ( CRYPTOPP_LLVM_CLANG_VERSION >= 30000 ) || ( CRYPTOPP_APPLE_CLANG_VERSION >= 40300 )) && ( __SIZEOF_INT128__ >= 16 ) <nl> // GCC 4 . 0 . 1 on MacOS X is missing __umodti3 and __udivti3 <nl> // GCC 4 . 8 . 3 and bad uint128_t ops on PPC64 / POWER7 ( Issue 421 ) <nl> // mode ( TI ) division broken on amd64 with GCC earlier than GCC 3 . 4
mmm sha - simd . cpp <nl> ppp sha - simd . cpp <nl> # undef CRYPTOPP_ARM_SHA_AVAILABLE <nl> # endif <nl>  <nl> -# if ( CRYPTOPP_CLMUL_AVAILABLE ) <nl> +# if ( CRYPTOPP_SHANI_AVAILABLE ) <nl> # include < nmmintrin . h > <nl> # include < immintrin . h > <nl> # endifmmm crc - simd . cpp <nl> ppp crc - simd . cpp <nl> # undef CRYPTOPP_ARM_SHA_AVAILABLE <nl> # endif <nl>  <nl> -# if ( CRYPTOPP_CLMUL_AVAILABLE ) <nl> +# if ( CRYPTOPP_SHANI_AVAILABLE ) <nl> # include < nmmintrin . h > <nl> # include < immintrin . h > <nl> # endif <nl> # undef CRYPTOPP_ARM_CRC32_AVAILABLE <nl> # endif <nl>  <nl> -# if ( CRYPTOPP_CLMUL_AVAILABLE ) <nl> +# if ( CRYPTOPP_SSE42_AVAILABLE ) <nl> # include < nmmintrin . h > <nl> # endif <nl> 
mmm zrtp / ZrtpSdesStream . cpp <nl> ppp zrtp / ZrtpSdesStream . cpp <nl> bool ZrtpSdesStream :: createSdesProfile ( char * cryptoString , size_t * maxLen ) { <nl> /* Get B64 code for master key and master salt */ <nl> b64Len = b64Encode ( keySalt , keyLenBytes + saltLenBytes , b64keySalt , sizeof ( b64keySalt )); <nl> b64keySalt [ b64Len ] = '\ 0 '; <nl> - * maxLen = snprintf ( cryptoString , * maxLen , "% d % s inline :% s ", tag , pSuite -> name , b64keySalt ); <nl> + memset ( cryptoString , 0 , * maxLen ); <nl> + * maxLen = snprintf ( cryptoString , * maxLen - 1 , "% d % s inline :% s ", tag , pSuite -> name , b64keySalt ); <nl>  <nl> memset ( keySalt , 0 , sizeof ( keySalt )); <nl> return true ;
mmm cryptcommon / ZrtpRandom . cpp <nl> ppp cryptcommon / ZrtpRandom . cpp <nl> int ZrtpRandom :: getRandomData ( uint8_t * buffer , uint32_t length ) { <nl> uint8_t rdata [ AES_BLOCK_SIZE ]; <nl> uint32_t generated = length ; <nl>  <nl> + initialize (); <nl> + <nl> lockRandom . Lock (); <nl>  <nl> /* <nl> int ZrtpRandom :: getRandomData ( uint8_t * buffer , uint32_t length ) { <nl>  <nl> int ZrtpRandom :: addEntropy ( const uint8_t * buffer , uint32_t length ) <nl> { <nl> + initialize (); <nl> + <nl> if ( buffer && length ) { <nl> sha512_hash ( buffer , length , & mainCtx ); <nl> } <nl> void ZrtpRandom :: initialize () { <nl> return ; <nl> } <nl> sha512_begin (& mainCtx ); <nl> + initialized = true ; <nl> lockRandom . Unlock (); <nl> } <nl> 
mmm src / game_config . cpp <nl> ppp src / game_config . cpp <nl> namespace game_config <nl> namespace sounds { <nl> const std :: string turn_bell = " bell . wav ", <nl> timer_bell = " timer . wav ", <nl> - receive_message = " chat - 3 . ogg ", <nl> + receive_message = " chat - 1 . ogg , chat - 2 . ogg , chat - 3 . ogg , chat - 4 . ogg ", <nl> receive_message_highlight = " chat - highlight . ogg ", <nl> receive_message_friend = " chat - 4 . ogg ", <nl> receive_message_server = " receive . wav ",
mmm src / playturn . cpp <nl> ppp src / playturn . cpp <nl> void turn_info :: left_click ( const SDL_MouseButtonEvent & event ) <nl> enemy == units_ . end () && ! current_route_ . steps . empty () && <nl> current_route_ . steps . front () == selected_hex_ ) { <nl>  <nl> + const std :: vector < gamemap :: location > steps = current_route_ . steps ; <nl> const size_t moves = move_unit (& gui_ , gameinfo_ , status_ , map_ , units_ , teams_ , <nl> - current_route_ . steps ,& recorder ,& undo_stack_ ,& next_unit_ ); <nl> + steps ,& recorder ,& undo_stack_ ,& next_unit_ ); <nl>  <nl> cursor :: set ( cursor :: NORMAL ); <nl>  <nl> void turn_info :: left_click ( const SDL_MouseButtonEvent & event ) <nl>  <nl> redo_stack_ . clear (); <nl>  <nl> - assert ( moves <= current_route_ . steps . size ()); <nl> - const gamemap :: location & dst = current_route_ . steps [ moves - 1 ]; <nl> + assert ( moves <= steps . size ()); <nl> + const gamemap :: location & dst = steps [ moves - 1 ]; <nl> const unit_map :: const_iterator u = units_ . find ( dst ); <nl>  <nl> // u may be equal to units_ . end () in the case of e . g . a [ teleport ] <nl> if ( u != units_ . end ()) { <nl> // Reselect the unit if the move was interrupted <nl> - if ( dst != current_route_ . steps . back ()) { <nl> + if ( dst != steps . back ()) { <nl> selected_hex_ = dst ; <nl> gui_ . select_hex ( dst ); <nl> }
mmm src / menu_events . cpp <nl> ppp src / menu_events . cpp <nl> namespace events { <nl> } <nl> } else if ( cmd == " clear ") { <nl> gui_ -> clear_chat_messages (); <nl> - } else if ( cmd == " sunset ") { <nl> + } else if ( game_config :: debug && cmd == " sunset ") { <nl> int delay = lexical_cast_default < int >( data ); <nl> gui_ -> sunset ( delay ); <nl> } else if ( cmd == " w ") {
mmm src / game_events . cpp <nl> ppp src / game_events . cpp <nl> namespace { <nl> state_of_game -> get_variable ( var_name +". recruit ") = side_data [" recruit "]; <nl> state_of_game -> get_variable ( var_name +". fog ") = side_data [" fog "]; <nl> state_of_game -> get_variable ( var_name +". shroud ") = side_data [" shroud "]; <nl> + state_of_game -> get_variable ( var_name +". hidden ") = side_data [" hidden "]; <nl>  <nl> state_of_game -> get_variable ( var_name +". income ") = lexical_cast_default < std :: string >((* teams )[ team_index ]. income (),""); <nl> state_of_game -> get_variable ( var_name +". village_gold ") = lexical_cast_default < std :: string >((* teams )[ team_index ]. village_gold (),"");
mmm src / about . cpp <nl> ppp src / about . cpp <nl> std :: vector < std :: string > get_text () { <nl> "- Michel Loos ", <nl> "- Renato Cunha ", <nl> "- Sérgio de Miranda Costa ", <nl> + "- Tiago Souza ( Salvador )", <nl>  <nl> " _ " N_ ("+ Russian Translation "), <nl> "- Alexandr Menovchicov ",
mmm src / whiteboard / manager . cpp <nl> ppp src / whiteboard / manager . cpp <nl> void manager :: print_help_once () <nl>  <nl> void manager :: set_active ( bool active ) <nl> { <nl> - if ( is_observer ()) <nl> + if ( wait_for_side_init_ <nl> + || executing_actions_ <nl> + || is_observer () <nl> + || resources :: controller -> is_linger_mode ()) <nl> { <nl> active_ = false ; <nl> - LOG_WB << " Whiteboard can ' t be activated by observers .\ n "; <nl> + LOG_WB << " Whiteboard can ' t be activated now .\ n "; <nl> } <nl> else if ( active != active_ ) <nl> { <nl> void manager :: set_invert_behavior ( bool invert ) <nl> bool block_whiteboard_activation = false ; <nl> if ( wait_for_side_init_ <nl> || executing_actions_ <nl> - || is_observer ()) <nl> + || is_observer () <nl> + || resources :: controller -> is_linger_mode ()) <nl> { <nl> block_whiteboard_activation = true ; <nl> }
mmm src / gui / auxiliary / event / dispatcher_private . hpp <nl> ppp src / gui / auxiliary / event / dispatcher_private . hpp <nl> struct find < false > <nl> // MSVC 2008 doesn ' t like operator () here so changed the name . <nl> return functor . template oper < item >( event ); <nl> } else { <nl> - typedef typename boost :: mpl :: next < itor >:: type itor ; <nl> - return find < boost :: is_same < itor , end >:: value > <nl> - :: execute (( itor *) 0 , ( end *) 0 , event , functor ); <nl> + typedef typename boost :: mpl :: next < itor >:: type titor ; <nl> + return find < boost :: is_same < titor , end >:: value > <nl> + :: execute (( titor *) 0 , ( end *) 0 , event , functor ); <nl> } <nl> } <nl> };
mmm src / server / game . cpp <nl> ppp src / server / game . cpp <nl> void game :: transfer_side_control ( const network :: connection sock , const simple_wm <nl> return ; <nl> } <nl> sides_ [ side_num - 1 ] = 0 ; <nl> - bool host_leave = false ; <nl> // If the old player lost his last side , make him an observer . <nl> if ( std :: find ( sides_ . begin (), sides_ . end (), old_player ) == sides_ . end ()) { <nl> observers_ . push_back ( old_player ); <nl> void game :: transfer_side_control ( const network :: connection sock , const simple_wm <nl> simple_wml :: document observer_join ; <nl> observer_join . root (). add_child (" observer "). set_attr_dup (" name ", old_player_name . c_str ()); <nl> send_data ( observer_join , old_player ); <nl> - // If the old player was the host of the game , choose another player . <nl> - /* if ( old_player == owner_ ) { <nl> - host_leave = true ; <nl> - if ( players_ . empty ()) { <nl> - owner_ = newplayer -> first ; <nl> - } else { <nl> - owner_ = players_ . front (); <nl> - } <nl> - notify_new_host (); <nl> - }*/ <nl> } <nl> change_controller ( side_num - 1 , newplayer , false ); <nl>  <nl> - if ( host_leave ) transfer_ai_sides (); <nl> - <nl> // If we gave the new side to an observer add him to players_ . <nl> if ( is_observer ( newplayer -> first )) { <nl> players_ . push_back ( newplayer -> first );
mmm src / server / server . cpp <nl> ppp src / server / server . cpp <nl> void server :: run () <nl> continue ; <nl> } <nl>  <nl> + if ( username == " server ") { <nl> + network :: send_data ( construct_error ( <nl> + " The nick ' server ' is reserved and can not be used by players "), sock ); <nl> + continue ; <nl> + } <nl> + <nl> // check the username isn ' t already taken <nl> player_map :: const_iterator p ; <nl> for ( p = players_ . begin (); p != players_ . end (); ++ p ) {
mmm src / random . cpp <nl> ppp src / random . cpp <nl> # include " random . hpp " <nl> # include " log . hpp " <nl>  <nl> +# include < boost / random / random_device . hpp > <nl>  <nl> # include < cassert > <nl> # include < cstdlib > <nl> +# include < limits > <nl> # include < random > <nl> -# include < boost / random / random_device . hpp > <nl>  <nl> static lg :: log_domain log_random (" random "); <nl> # define DBG_RND LOG_STREAM ( debug , log_random ) <nl> static lg :: log_domain log_random (" random "); <nl> # define WRN_RND LOG_STREAM ( warn , log_random ) <nl> # define ERR_RND LOG_STREAM ( err , log_random ) <nl>  <nl> + static_assert ( std :: numeric_limits < double >:: is_iec559 , " Floating point representation is not IEEE 754 - compliant "); <nl> + <nl> namespace { <nl>  <nl> class rng_default : public randomness :: rng
mmm src / campaign_server / campaign_server . cpp <nl> ppp src / campaign_server / campaign_server . cpp <nl> namespace { <nl> net_manager_ ( min_thread , max_thread ), <nl> server_manager_ ( load_config ()), <nl> hooks_ (), <nl> - input_ ( 0 ) <nl> + input_ ( 0 ), <nl> + compress_level_ ( 0 ) <nl> { <nl> if ( cfg_ . child (" campaigns ") == NULL ) { <nl> cfg_ . add_child (" campaigns ");
mmm src / server / room_manager . cpp <nl> ppp src / server / room_manager . cpp <nl> void room_manager :: load_config ( const config & cfg ) <nl> { <nl> filename_ = cfg [" room_save_file "]; <nl> compress_stored_rooms_ = utils :: string_bool ( cfg [" compress_stored_rooms "], true ); <nl> - new_room_policy_ = pp_from_string ( cfg [" new_room_policy "]); <nl> + PRIVILEGE_POLICY pp = pp_from_string ( cfg [" new_room_policy "]); <nl> + if ( pp != PP_COUNT ) new_room_policy_ = pp ; <nl> } <nl>  <nl> const std :: string & room_manager :: storage_filename () const
mmm src / about . cpp <nl> ppp src / about . cpp <nl> std :: vector < std :: string > get_text () { <nl>  <nl> " _ " N_ ("+ Developers "), <nl> "- Alfredo Beaumont ( ziberpunk )", <nl> + "- Bram Ridder ( Morloth )", <nl> "- Cedric Duval ", <nl> "- Cyril Bouthors ( CyrilB )", <nl> "- Guillaume Melquiond ( silene )", <nl> std :: vector < std :: string > get_text () { <nl> "- Justin Zaun ( jzaun )", <nl>  <nl> " _ " N_ ("+ Multiplayer Maps "), <nl> + "- Mike Quinones ( Doc Paterson )", <nl> "- Peter Groen ( pg )", <nl> "- Tom Chance ( telex4 )", <nl> - "- Mike Quinones ( Doc Paterson )", <nl>  <nl> " _ " N_ ("+ Packagers "), <nl> "- Cyril Bouthors ( CyrilB )",
mmm src / units / map . hpp <nl> ppp src / units / map . hpp <nl> public : <nl> size_t size () const { return lmap_ . size (); } <nl> size_t num_iters () const ; <nl>  <nl> + bool empty () const { return lmap_ . empty (); } <nl> + <nl> void clear ( bool force = false ); <nl>  <nl> /**
mmm src / hotkeys . cpp <nl> ppp src / hotkeys . cpp <nl> void delete_all_wml_hotkeys () <nl> } <nl> } <nl>  <nl> -// retunrs weather a hotkey was deleted . <nl> +// Returns whether a hotkey was deleted . <nl> bool remove_wml_hotkey ( const std :: string & id ) <nl> { <nl> hotkey :: hotkey_command & command = get_hotkey_command ( id );
mmm src / sdl_utils . cpp <nl> ppp src / sdl_utils . cpp <nl> void draw_unit_ellipse ( SDL_Surface * target , short colour , const SDL_Rect & clip , <nl> xpos = behind -> w - xpos - 1 ; <nl>  <nl> int ypos = yit - unity ; <nl> - if ( xit >= clip . x && yit >= clip . y && xit < clip . x + clip . w && yit < clip . y + clip . h && <nl> - xpos >= 0 && ypos >= 0 && xpos < behind -> w && ypos < behind -> h && <nl> + if ( xit >= clip . x && yit >= clip . y && xit < clip . x + clip . w && yit + 1 < clip . y + clip . h && <nl> + xpos >= 0 && ypos >= 0 && xpos < behind -> w && ypos + 1 < behind -> h && <nl> pixels [ ypos *( behind -> w + pad ) + xpos ] == 0 ) { <nl> - SDL_Rect rect = { xit , yit , 1 , 1 }; <nl> + SDL_Rect rect = { xit , yit , 1 , 2 }; <nl> SDL_FillRect ( target ,& rect , colour ); <nl> } <nl>  <nl> yit = yloc + height / 2 + y ; <nl> ypos = yit - unity ; <nl> - if ( xit >= clip . x && yit >= clip . y && xit < clip . x + clip . w && yit < clip . y + clip . h && <nl> - xpos >= 0 && ypos >= 0 && xpos < behind -> w && ypos < behind -> h ) { <nl> - SDL_Rect rect = { xit , yit , 1 , 1 }; <nl> + if ( xit >= clip . x && yit >= clip . y && xit < clip . x + clip . w && yit + 1 < clip . y + clip . h && <nl> + xpos >= 0 && ypos >= 0 && xpos < behind -> w && ypos + 1 < behind -> h ) { <nl> + SDL_Rect rect = { xit , yit , 1 , 2 }; <nl> SDL_FillRect ( target ,& rect , colour ); <nl> } <nl> }
mmm src / time_of_day . cpp <nl> ppp src / time_of_day . cpp <nl> time_of_day :: time_of_day ( const config & cfg ) <nl> time_of_day :: time_of_day () <nl> : lawful_bonus ( 0 ) <nl> , bonus_modified ( 0 ) <nl> +, image () <nl> , name (" NULL_TOD ") <nl> , id (" nulltod ") <nl> +, image_mask () <nl> , red ( 0 ) <nl> , green ( 0 ) <nl> , blue ( 0 ) <nl> +, sounds () <nl> { <nl> } <nl> 
mmm src / variable . cpp <nl> ppp src / variable . cpp <nl> # include " log . hpp " <nl> # include " unit . hpp " <nl> # include " unit_map . hpp " <nl> +# include " team . hpp " <nl>  <nl> static lg :: log_domain log_engine (" engine "); <nl> # define LOG_NG LOG_STREAM ( info , log_engine ) <nl> void scoped_weapon_info :: activate () <nl>  <nl> void scoped_recall_unit :: activate () <nl> { <nl> + const std :: vector < team >& teams = teams_manager :: get_teams (); <nl> + std :: vector < team >:: const_iterator team_it ; <nl> + for ( team_it = teams . begin (); team_it != teams . end (); team_it ++) { <nl> + if ( team_it -> save_id () == player_ ) <nl> + break ; <nl> + } <nl> + <nl> player_info * const player = repos -> get_player ( player_ ); <nl> - if ( player != NULL ) { <nl> - if ( player -> available_units . size () > recall_index_ ) { <nl> + if ( team_it != teams . end ()) { <nl> + if ( team_it -> recall_list (). size () > recall_index_ ) { <nl> config tmp_cfg ; <nl> - player -> available_units [ recall_index_ ]. write ( tmp_cfg ); <nl> + team_it -> recall_list ()[ recall_index_ ]. write ( tmp_cfg ); <nl> tmp_cfg [" x "] = " recall "; <nl> tmp_cfg [" y "] = " recall "; <nl> LOG_NG << " auto - storing $" << name () << " for player : " << player_
mmm src / unit . cpp <nl> ppp src / unit . cpp <nl> void unit :: write ( config & cfg ) const <nl> break ; <nl> case unit_type :: LIMINAL : <nl> cfg [" alignment "] = " liminal "; <nl> + break ; <nl> default : <nl> cfg [" alignment "] = " neutral "; <nl> }
mmm src / campaign_server / campaign_server . cpp <nl> ppp src / campaign_server / campaign_server . cpp <nl> void server :: handle_read_from_fifo ( const boost :: system :: error_code & error , std :: <nl>  <nl> if ( ctl == " shut_down ") { <nl> LOG_CS << " Shut down requested by admin , shutting down ...\ n "; <nl> + throw server_shutdown (" Shut down via fifo command "); <nl> } else if ( ctl == " readonly ") { <nl> if ( ctl . args_count ()) { <nl> cfg_ [" read_only "] = read_only_ = utils :: string_bool ( ctl [ 1 ], true );
mmm src / scripting / lua_unit_attacks . cpp <nl> ppp src / scripting / lua_unit_attacks . cpp <nl> static int impl_unit_attack_match ( lua_State * L ) <nl> { <nl> const_attack_ptr atk = luaW_toweapon ( L , 1 ); <nl> config cfg = luaW_checkconfig ( L , 2 ); <nl> + if (! atk ) { <nl> + return luaL_argerror ( L , 1 , " invalid attack "); <nl> + } <nl> lua_pushboolean ( L , atk -> matches_filter ( cfg )); <nl> return 1 ; <nl> }
mmm src / server / game . cpp <nl> ppp src / server / game . cpp <nl> bool game :: describe_slots () { <nl> std :: string descr = buf . str (); <nl>  <nl> if ((* description_ )[" slots "] != descr ) { <nl> - description_ -> set_attr_dup (" slots ", descr ); <nl> + description_ -> set_attr_dup (" slots ", descr . c_str ()); <nl> return true ; <nl> } else { <nl> return false ;
mmm src / scripting / lua_gui2 . cpp <nl> ppp src / scripting / lua_gui2 . cpp <nl> int intf_set_dialog_callback ( lua_State * L ) <nl> if ( gui2 :: clickable_item * c = dynamic_cast < gui2 :: clickable_item *>( w )) { <nl> static dialog_callback_wrapper wrapper ; <nl> c -> connect_click_handler ( std :: bind (& dialog_callback_wrapper :: forward , wrapper , w )); <nl> - } else if ( gui2 :: selectable_item * s = dynamic_cast < gui2 :: selectable_item *>( w )) { <nl> - connect_signal_notify_modified ( dynamic_cast < gui2 :: widget &>(* s ), std :: bind ( dialog_callback , _1 )); <nl> - } else if ( gui2 :: integer_selector * s = dynamic_cast < gui2 :: integer_selector *>( w )) { <nl> - connect_signal_notify_modified ( dynamic_cast < gui2 :: widget &>(* s ), std :: bind ( dialog_callback , _1 )); <nl> + } else if ( gui2 :: selectable_item * si = dynamic_cast < gui2 :: selectable_item *>( w )) { <nl> + connect_signal_notify_modified ( dynamic_cast < gui2 :: widget &>(* si ), std :: bind ( dialog_callback , _1 )); <nl> + } else if ( gui2 :: integer_selector * is = dynamic_cast < gui2 :: integer_selector *>( w )) { <nl> + connect_signal_notify_modified ( dynamic_cast < gui2 :: widget &>(* is ), std :: bind ( dialog_callback , _1 )); <nl> } <nl> # ifdef GUI2_EXPERIMENTAL_LISTBOX <nl> else if ( gui2 :: list_view * l = dynamic_cast < gui2 :: list_view *>( w )) {
mmm src / playturn . cpp <nl> ppp src / playturn . cpp <nl> bool turn_slice ( game_data & gameinfo , game_state & state_of_game , <nl> unit_map :: const_iterator it = units . find ( next_unit ); <nl> if ( it != units . end ()) { <nl> for (++ it ; it != units . end (); ++ it ) { <nl> - if ( unit_can_move ( it -> first , units , map , teams )) { <nl> + if ( it -> second . side () == team_num && <nl> + unit_can_move ( it -> first , units , map , teams )) { <nl> break ; <nl> } <nl> } <nl> bool turn_slice ( game_data & gameinfo , game_state & state_of_game , <nl>  <nl> if ( it == units . end ()) { <nl> for ( it = units . begin (); it != units . end (); ++ it ) { <nl> - if ( unit_can_move ( it -> first , units , map , teams )) { <nl> + if ( it -> second . side () == team_num && <nl> + unit_can_move ( it -> first , units , map , teams )) { <nl> break ; <nl> } <nl> }
mmm src / savegame . cpp <nl> ppp src / savegame . cpp <nl> bool loadgame :: load_multiplayer_game () <nl> return false ; <nl> } <nl>  <nl> + if ( is_replay_save ( summary_ )) { <nl> + gui2 :: show_transient_message ( video_ , _ (" Load Game "), _ (" Replays are not supported in multiplayer mode .")); <nl> + return false ; <nl> + } <nl> + <nl> if ( gamestate_ . classification (). campaign_type != game_classification :: CAMPAIGN_TYPE :: MULTIPLAYER ) { <nl> gui2 :: show_transient_error_message ( video_ , _ (" This is not a multiplayer save .")); <nl> return false ;
mmm src / map . cpp <nl> ppp src / map . cpp <nl> bool gamemap :: is_village ( const gamemap :: location & loc ) const <nl>  <nl> bool gamemap :: gives_healing ( const gamemap :: location & loc ) const <nl> { <nl> - return is_village ( loc ); <nl> + return on_board ( loc ) && gives_healing ( get_terrain ( loc )); <nl> } <nl>  <nl> bool gamemap :: is_castle ( const gamemap :: location & loc ) const
mmm src / about . cpp <nl> ppp src / about . cpp <nl> void show_about ( display & disp ) <nl> text . push_back ("+ Developers "); <nl> text . push_back ("- Alfredo Beaumont ( ziberpunk )"); <nl> text . push_back ("- Cyril Bouthors ( CyrilB )"); <nl> - text . push_back ("- Guillaume Duwelz - Rebert "); <nl> text . push_back ("- Isaac Clerencia "); <nl> text . push_back ("- J . R . Blain ( Cowboy )"); <nl> text . push_back ("- Justin Zaun ( jzaun )");
mmm src / serialization / binary_or_text . cpp <nl> ppp src / serialization / binary_or_text . cpp <nl>  <nl> bool detect_format_and_read ( config & cfg , std :: istream & in ) <nl> { <nl> - try { <nl> + unsigned char c = in . peek (); <nl> + if ( c < 5 ) { <nl> read_compressed ( cfg , in ); <nl> return true ; <nl> - } catch ( config :: error &) { <nl> + } else { <nl> + read ( cfg , in ); <nl> + return false ; <nl> } <nl> - <nl> - read ( cfg , in ); <nl> - return false ; <nl> } <nl>  <nl> void write_possibly_compressed ( std :: string const & filename , config & cfg , bool compress )
mmm src / generate_report . cpp <nl> ppp src / generate_report . cpp <nl> Units cannot be killed by poison alone . The poison will not reduce it below 1 HP <nl> break ; <nl>  <nl> const t_translation :: t_letter terrain = map . get_terrain ( mouseover ); <nl> + if ( terrain == t_translation :: OFF_MAP_USER ) <nl> + break ; <nl> + <nl> const t_translation :: t_list & underlying = map . underlying_union_terrain ( terrain ); <nl>  <nl> if ( map . is_village ( mouseover )) { <nl> Units cannot be killed by poison alone . The poison will not reduce it below 1 HP <nl> break ; <nl> } <nl>  <nl> + const t_translation :: t_letter terrain = map [ mouseover . x ][ mouseover . y ]; <nl> + <nl> + if ( terrain == t_translation :: OFF_MAP_USER ) <nl> + break ; <nl> + <nl> str << mouseover ; <nl>  <nl> if ( u == units . end () || current_team . shrouded ( mouseover . x , mouseover . y )) <nl> break ; <nl>  <nl> - const t_translation :: t_letter terrain = map [ mouseover . x ][ mouseover . y ]; <nl> - <nl> const int move_cost = u -> second . movement_cost ( terrain ); <nl> const int defense = 100 - u -> second . defense_modifier ( terrain ); <nl> 
mmm src / sdl_utils . cpp <nl> ppp src / sdl_utils . cpp <nl> void fill_rect_alpha ( SDL_Rect & rect , Uint32 colour , Uint8 alpha , surface const & <nl>  <nl> surface get_surface_portion ( surface const & src , SDL_Rect & area ) <nl> { <nl> - if ( area . x >= src -> w || area . y >= src -> h ) { <nl> - std :: cerr << " illegal surface portion ...\ n "; <nl> + // check if there is something in the portion <nl> + if ( area . x >= src -> w || area . y >= src -> h || area . x + area . w < 0 || area . y + area . h < 0 ) { <nl> + // std :: cerr << " illegal surface portion ...\ n "; <nl> return NULL ; <nl> } <nl>  <nl> if ( area . x + area . w > src -> w ) { <nl> area . w = src -> w - area . x ; <nl> } <nl> - <nl> if ( area . y + area . h > src -> h ) { <nl> area . h = src -> h - area . y ; <nl> } <nl> surface get_surface_portion ( surface const & src , SDL_Rect & area ) <nl> return NULL ; <nl> } <nl>  <nl> - SDL_Rect dstarea = { 0 , 0 , 0 , 0 }; <nl> - <nl> - SDL_BlitSurface ( src ,& area , dst ,& dstarea ); <nl> + SDL_BlitSurface ( src ,& area , dst , NULL ); <nl>  <nl> return dst ; <nl> }mmm src / sdl_utils . hpp <nl> ppp src / sdl_utils . hpp <nl> void fill_rect_alpha ( SDL_Rect & rect , Uint32 colour , Uint8 alpha , surface const & <nl>  <nl> surface get_surface_portion ( surface const & src , SDL_Rect & area ) <nl> { <nl> - if ( area . x >= src -> w || area . y >= src -> h ) { <nl> - std :: cerr << " illegal surface portion ...\ n "; <nl> + // check if there is something in the portion <nl> + if ( area . x >= src -> w || area . y >= src -> h || area . x + area . w < 0 || area . y + area . h < 0 ) { <nl> + // std :: cerr << " illegal surface portion ...\ n "; <nl> return NULL ; <nl> } <nl>  <nl> if ( area . x + area . w > src -> w ) { <nl> area . w = src -> w - area . x ; <nl> } <nl> - <nl> if ( area . y + area . h > src -> h ) { <nl> area . h = src -> h - area . y ; <nl> } <nl> surface get_surface_portion ( surface const & src , SDL_Rect & area ) <nl> return NULL ; <nl> } <nl>  <nl> - SDL_Rect dstarea = { 0 , 0 , 0 , 0 }; <nl> - <nl> - SDL_BlitSurface ( src ,& area , dst ,& dstarea ); <nl> + SDL_BlitSurface ( src ,& area , dst , NULL ); <nl>  <nl> return dst ; <nl> } <nl> surface darken_image ( surface const & surf ); <nl> surface recolor_image ( surface surf , const std :: map < Uint32 , Uint32 >& map_rgb ); <nl>  <nl> surface brighten_image ( surface const & surf , fixed_t amount ); <nl> +// send NULL if the portion is outside of the surface <nl> surface get_surface_portion ( surface const & surf , SDL_Rect & rect ); <nl> surface adjust_surface_alpha ( surface const & surf , fixed_t amount , bool optimize = true ); <nl> surface adjust_surface_alpha_add ( surface const & surf , int amount );
mmm src / play_controller . cpp <nl> ppp src / play_controller . cpp <nl> bool play_controller :: in_context_menu ( hotkey :: HOTKEY_COMMAND command ) const <nl> case hotkey :: HOTKEY_RECRUIT : <nl> case hotkey :: HOTKEY_REPEAT_RECRUIT : <nl> case hotkey :: HOTKEY_RECALL : { <nl> + wb :: scoped_planned_pathfind_map future ; //< lasts until method returns . <nl> // last_hex_ is set by mouse_events :: mouse_motion <nl> // Enable recruit / recall on castle / keep tiles <nl> for ( unit_map :: const_iterator leader = units_ . begin (); <nl> leader != units_ . end ();++ leader ) { <nl> if ( leader -> can_recruit () && <nl> - leader -> side () == player_number_ && <nl> + leader -> side () == resources :: screen -> viewing_side () && <nl> can_recruit_on ( map_ , leader -> get_location (), mouse_handler_ . get_last_hex ())) <nl> return true ; <nl> }mmm src / playsingle_controller . cpp <nl> ppp src / playsingle_controller . cpp <nl> bool play_controller :: in_context_menu ( hotkey :: HOTKEY_COMMAND command ) const <nl> case hotkey :: HOTKEY_RECRUIT : <nl> case hotkey :: HOTKEY_REPEAT_RECRUIT : <nl> case hotkey :: HOTKEY_RECALL : { <nl> + wb :: scoped_planned_pathfind_map future ; //< lasts until method returns . <nl> // last_hex_ is set by mouse_events :: mouse_motion <nl> // Enable recruit / recall on castle / keep tiles <nl> for ( unit_map :: const_iterator leader = units_ . begin (); <nl> leader != units_ . end ();++ leader ) { <nl> if ( leader -> can_recruit () && <nl> - leader -> side () == player_number_ && <nl> + leader -> side () == resources :: screen -> viewing_side () && <nl> can_recruit_on ( map_ , leader -> get_location (), mouse_handler_ . get_last_hex ())) <nl> return true ; <nl> } <nl> # include " save_blocker . hpp " <nl> # include " soundsource . hpp " <nl> # include " storyscreen / interface . hpp " <nl> +# include " whiteboard / manager . hpp " <nl>  <nl> static lg :: log_domain log_engine (" engine "); <nl> # define ERR_NG LOG_STREAM ( err , log_engine ) <nl> void playsingle_controller :: init_gui (){ <nl> void playsingle_controller :: recruit (){ <nl> if (! browse_ ) <nl> menu_handler_ . recruit ( player_number_ , mouse_handler_ . get_last_hex ()); <nl> + else if ( resources :: whiteboard -> is_active ()) <nl> + menu_handler_ . recruit ( resources :: screen -> viewing_side (), mouse_handler_ . get_last_hex ()); <nl> } <nl>  <nl> void playsingle_controller :: repeat_recruit (){ <nl> if (! browse_ ) <nl> menu_handler_ . repeat_recruit ( player_number_ , mouse_handler_ . get_last_hex ()); <nl> + else if ( resources :: whiteboard -> is_active ()) <nl> + menu_handler_ . repeat_recruit ( resources :: screen -> viewing_side (), mouse_handler_ . get_last_hex ()); <nl> } <nl>  <nl> void playsingle_controller :: recall (){ <nl> if (! browse_ ) <nl> menu_handler_ . recall ( player_number_ , mouse_handler_ . get_last_hex ()); <nl> + else if ( resources :: whiteboard -> is_active ()) <nl> + menu_handler_ . recall ( resources :: screen -> viewing_side (), mouse_handler_ . get_last_hex ()); <nl> } <nl>  <nl> void playsingle_controller :: toggle_shroud_updates (){ <nl> bool playsingle_controller :: can_execute_command ( hotkey :: HOTKEY_COMMAND command , <nl> case hotkey :: HOTKEY_ADD_WAYPOINT : <nl> case hotkey :: HOTKEY_UNIT_HOLD_POSITION : <nl> case hotkey :: HOTKEY_END_UNIT_TURN : <nl> + return ! browse_ && ! linger_ && ! events :: commands_disabled ; <nl> case hotkey :: HOTKEY_RECRUIT : <nl> case hotkey :: HOTKEY_REPEAT_RECRUIT : <nl> case hotkey :: HOTKEY_RECALL : <nl> - return ! browse_ && ! linger_ && ! events :: commands_disabled ; <nl> + return (! browse_ || resources :: whiteboard -> is_active ()) && ! linger_ && ! events :: commands_disabled ; <nl> case hotkey :: HOTKEY_ENDTURN : <nl> return (! browse_ || linger_ ) && ! events :: commands_disabled ; <nl> 
mmm src / map . cpp <nl> ppp src / map . cpp <nl> bool gamemap :: location :: matches_range ( const std :: string & xloc , const std :: string <nl> std :: vector < std :: string > xlocs = utils :: split ( xloc ); <nl> std :: vector < std :: string > ylocs = utils :: split ( yloc ); <nl>  <nl> - int size ; <nl> + size_t size ; <nl> for ( size = xlocs . size (); size < ylocs . size (); ++ size ) { <nl> xlocs . push_back (""); <nl> } <nl> while ( size > ylocs . size ()) { <nl> ylocs . push_back (""); <nl> } <nl> - for ( int i = 0 ; i != size ; ++ i ) { <nl> + for ( size_t i = 0 ; i != size ; ++ i ) { <nl> if ( matches_range ( xlocs [ i ], ylocs [ i ])) <nl> return true ; <nl> }
mmm src / playcampaign . cpp <nl> ppp src / playcampaign . cpp <nl> LEVEL_RESULT play_game ( display & disp , game_state & gamestate , const config & game_ <nl> gamestate . set_variables (* gamestate . snapshot . child (" variables ")); <nl> } <nl> // Replace game label with that from snapshot <nl> - if (! state . snapshot [" label "]. empty ()){ <nl> - state . label = state . snapshot [" label "]; <nl> + if (! gamestate . snapshot [" label "]. empty ()){ <nl> + gamestate . label = gamestate . snapshot [" label "]; <nl> } <nl> // get the current gold values of players so they don ' t start with the amount <nl> // they had at the start of the scenario
mmm src / upload_log . cpp <nl> ppp src / upload_log . cpp <nl> upload_log :: upload_log ( bool enable ) : <nl>  <nl> void upload_log :: read_replay () <nl> { <nl> - if ( ! uploader_settings :: new_uploader || ! enabled_ || ! game_config :: debug ) { <nl> + if ( ! uploader_settings :: new_uploader || ! enabled_ || game_config :: debug ) { <nl> return ; <nl> } <nl>  <nl> void upload_log :: start ( game_state & state , const std :: string map_data ) <nl> delete game_ ; <nl> game_ = new config (); <nl> (* game_ )[" time "] = lexical_cast < std :: string >( SDL_GetTicks () / 1000 ); <nl> - (* game_ )[" campaign "] = state . classification (). campaign_define ; <nl> + (* game_ )[" campaign "] = state . classification (). campaign_type ; <nl> (* game_ )[" difficulty "] = state . classification (). difficulty ; <nl> - (* game_ )[" scenario "] = state . classification (). scenario ; <nl> + (* game_ )[" scenario "] = state . classification (). label ; <nl> if ( uploader_settings :: new_uploader ) { <nl> // replace newlines in map definition with semicolons so that braindead server - side wml parser doesn ' t get confused <nl> std :: string encoded_map ( map_data );
mmm src / team . cpp <nl> ppp src / team . cpp <nl> void team :: team_info :: read ( const config & cfg ) <nl> if (! user_team_name . translatable ()) <nl> user_team_name = t_string :: from_serialized ( user_team_name ); <nl>  <nl> - if ( cfg . has_attribute (" ai_config ")) { <nl> - ai :: manager :: get_singleton (). add_ai_for_side_from_file ( side , cfg [" ai_config "], true ); <nl> - } else { <nl> - ai :: manager :: get_singleton (). add_ai_for_side_from_config ( side , cfg , true ); <nl> + display * disp = display :: get_singleton (); <nl> + <nl> + if ( disp && ! disp -> in_editor ()) { <nl> + if ( cfg . has_attribute (" ai_config ")) { <nl> + ai :: manager :: get_singleton (). add_ai_for_side_from_file ( side , cfg [" ai_config "], true ); <nl> + } else { <nl> + ai :: manager :: get_singleton (). add_ai_for_side_from_config ( side , cfg , true ); <nl> + } <nl> } <nl>  <nl> std :: vector < std :: string > recruits = utils :: split ( cfg [" recruit "]);
mmm src / variable_info . cpp <nl> ppp src / variable_info . cpp <nl> namespace <nl> char * endptr ; <nl> int res = strtol ( index_str , & endptr , 10 ); <nl>  <nl> - if (* endptr != ']' || res > int ( game_config :: max_loop )) <nl> + if (* endptr != ']' || res > int ( game_config :: max_loop ) || endptr == index_str ) <nl> { <nl> throw invalid_variablename_exception (); <nl> }
mmm src / editor / editor_controller . cpp <nl> ppp src / editor / editor_controller . cpp <nl> bool editor_controller :: execute_command ( hotkey :: HOTKEY_COMMAND command , int inde <nl> gui_ -> init_flags (); <nl> return true ; <nl>  <nl> + // Transitions <nl> + case HOTKEY_EDITOR_PARTIAL_UPDATE_TRANSITIONS : <nl> + context_manager_ -> set_update_trasitions_mode ( 2 ); <nl> + return true ; <nl> case HOTKEY_EDITOR_AUTO_UPDATE_TRANSITIONS : <nl> + context_manager_ -> set_update_trasitions_mode ( 1 ); <nl> + return true ; <nl> + case HOTKEY_EDITOR_NO_UPDATE_TRANSITIONS : <nl> + context_manager_ -> set_update_trasitions_mode ( 0 ); <nl> + return true ; <nl> + case HOTKEY_EDITOR_TOGGLE_TRANSITIONS : <nl> if ( context_manager_ -> toggle_update_transitions ()) <nl> return true ; <nl> // else intentionally fall through <nl> case HOTKEY_EDITOR_UPDATE_TRANSITIONS : <nl> context_manager_ -> refresh_all (); <nl> return true ; <nl> + // Refresh <nl> case HOTKEY_EDITOR_REFRESH : <nl> context_manager_ -> reload_map (); <nl> return true ;
mmm src / actions . cpp <nl> ppp src / actions . cpp <nl> int battle_context :: choose_defender_weapon ( const unit & attacker , const unit & def <nl> choices . push_back ( i ); <nl> } <nl> } <nl> - if ( choices . size () == 0 ) <nl> + if ( choices . empty ()) <nl> return - 1 ; <nl> if ( choices . size () == 1 ) <nl> return choices [ 0 ]; <nl> int battle_context :: choose_attacker_weapon ( const unit & attacker , const unit & def <nl> choices . push_back ( i ); <nl> } <nl> } <nl> - if ( choices . size () == 0 ) <nl> + if ( choices . empty ()) <nl> return - 1 ; <nl> if ( choices . size () == 1 ) { <nl> * defender_weapon = choose_defender_weapon ( attacker , defender , choices [ 0 ], units , <nl> void calculate_healing ( int side , bool update_display ) <nl> healers . push_back ( units . find ( heal_loc -> loc )); <nl> } <nl>  <nl> - if ( healers . size () > 0 ) { <nl> + if (! healers . empty ()) { <nl> DBG_NG << " Unit has " << healers . size () << " potential healers \ n "; <nl> } <nl>  <nl> void calculate_healing ( int side , bool update_display ) <nl> healing = neg_max ; <nl> } <nl>  <nl> - if ( healers . size () > 0 ) { <nl> + if (! healers . empty ()) { <nl> DBG_NG << " Just before healing animations , unit has " << healers . size () << " potential healers \ n "; <nl> } <nl> 
mmm src / menu_events . cpp <nl> ppp src / menu_events . cpp <nl> void console_handler :: do_layers () <nl> const mouse_handler & mousehandler = menu_handler_ . pc_ . get_mouse_handler_base (); <nl> const map_location & loc = mousehandler . get_last_hex (); <nl>  <nl> - gui2 :: dialogs :: terrain_layers :: display ( disp , loc , disp . video ()); <nl> + // <nl> + // It ' s possible to invoke this dialog even if loc isn ' t a valid hex . I ' m not sure <nl> + // exactly how that happens , but it does seem to occur when moving the mouse outside <nl> + // the window to the menu bar . Not sure if there ' s a bug when the set - last - hex code <nl> + // in that case , but this check at least ensures the dialog is only ever shown for a <nl> + // valid , on - map location . Otherwise , an assertion gets thrown . <nl> + // <nl> + // -- vultraz , 2017 - 09 - 21 <nl> + // <nl> + if ( disp . get_map (). on_board_with_border ( loc )) { <nl> + gui2 :: dialogs :: terrain_layers :: display ( disp , loc , disp . video ()); <nl> + } <nl> } <nl>  <nl> void console_handler :: do_fps ()
mmm src / playcampaign . cpp <nl> ppp src / playcampaign . cpp <nl> LEVEL_RESULT play_game ( game_display & disp , saved_game & gamestate , <nl> if ( is_unit_test ) { <nl> return res ; <nl> } <nl> + // in this case we might have skipped state . set_snapshot which means wew cannot do gamestate . convert_to_start_save (); <nl> + if ( res == QUIT ) <nl> + { <nl> + return res ; <nl> + } <nl>  <nl> // Save - management options fire on game end . <nl> // This means : ( a ) we have a victory , or <nl> LEVEL_RESULT play_game ( game_display & disp , saved_game & gamestate , <nl> // On DEFEAT , QUIT , or OBSERVER_END , we ' re done now <nl>  <nl> // If there is no next scenario we ' re done now . <nl> - if ( res == QUIT || ! end_level . proceed_to_next_level || gamestate . carryover_sides_start [" next_scenario "]. empty ()) <nl> + if (! end_level . proceed_to_next_level || gamestate . carryover_sides_start [" next_scenario "]. empty ()) <nl> { <nl> return res ; <nl> }
mmm src / dialogs . cpp <nl> ppp src / dialogs . cpp <nl> bool animate_unit_advancement ( unit_map & units , map_location loc , game_display & g <nl> :: advance_unit ( units , loc , chosen_unit ); <nl> } else { <nl> unit amla_unit ( u -> second ); <nl> + config mod_option (* mod_options [ choice - options . size ()]); <nl>  <nl> LOG_NG << " firing advance event ( AMLA )\ n "; <nl> game_events :: fire (" advance ", loc ); <nl>  <nl> amla_unit . get_experience (- amla_unit . max_experience ()); // subtract xp required <nl> - amla_unit . add_modification (" advance ",* mod_options [ choice - options . size ()]); <nl> + amla_unit . add_modification (" advance ", mod_option ); <nl> units . replace ( new std :: pair < map_location , unit >( loc , amla_unit )); <nl>  <nl> LOG_NG << " firing post_advance event ( AMLA )\ n ";
mmm src / multiplayer_connect . cpp <nl> ppp src / multiplayer_connect . cpp <nl> config connect :: side :: get_config () const <nl> break ; <nl> case CNTR_EMPTY : <nl> description = N_ ("( Empty slot )"); <nl> + res [" no_leader "] = " yes "; <nl> break ; <nl> default : <nl> wassert ( false );
mmm src / mp_options . cpp <nl> ppp src / mp_options . cpp <nl> void manager :: init_info ( const config & cfg , const std :: string & key ) <nl> entry [" id "] = comp [" id "]; <nl> entry [" name "] = comp [" name "]; <nl>  <nl> - if ( comp . has_child (" options ") && ( comp [" allow_new_game "]. to_bool ( true )) { <nl> + if ( comp . has_child (" options ") && comp [" allow_new_game "]. to_bool ( true )) { <nl> const config & options = comp . child (" options "); <nl>  <nl> BOOST_FOREACH ( const config :: any_child & c ,
mmm src / playmp_controller . cpp <nl> ppp src / playmp_controller . cpp <nl> bool playmp_controller :: can_execute_command ( const hotkey :: hotkey_command & cmd , i <nl> switch ( command ){ <nl> case hotkey :: HOTKEY_ENDTURN : <nl> if ( linger_ ) <nl> - { <nl> - return is_host_ ; <nl> + { <nl> + bool has_next_scenario = ! resources :: gamedata -> next_scenario (). empty () && <nl> + resources :: gamedata -> next_scenario () != " null "; <nl> + return is_host_ || ! has_next_scenario ; <nl> } <nl> else <nl> {mmm src / play_controller . cpp <nl> ppp src / play_controller . cpp <nl> bool playmp_controller :: can_execute_command ( const hotkey :: hotkey_command & cmd , i <nl> switch ( command ){ <nl> case hotkey :: HOTKEY_ENDTURN : <nl> if ( linger_ ) <nl> - { <nl> - return is_host_ ; <nl> + { <nl> + bool has_next_scenario = ! resources :: gamedata -> next_scenario (). empty () && <nl> + resources :: gamedata -> next_scenario () != " null "; <nl> + return is_host_ || ! has_next_scenario ; <nl> } <nl> else <nl> { <nl> void play_controller :: set_defeat_music_list ( const std :: string & list ) <nl>  <nl> void play_controller :: check_victory () <nl> { <nl> + if ( linger_ ) <nl> + { <nl> + return ; <nl> + } <nl> std :: set < unsigned > not_defeated ; <nl> for ( unit_map :: const_iterator i = units_ . begin (), <nl> i_end = units_ . end (); i != i_end ; ++ i )
mmm src / formula_string_utils . hpp <nl> ppp src / formula_string_utils . hpp <nl> class variable_set ; <nl>  <nl> namespace utils { <nl>  <nl> +/** <nl> + * Determines if a string might contain variables to interpolate . <nl> + * This can allow one to skip future interpolations ( plural -- if there is only <nl> + * one interpolation , the savings are not worth this check ). In this spirit , <nl> + * precision is sacrificed in the name of efficiency ; the check is quick and <nl> + * allows false positives , but there are no false negatives . ( A false negative <nl> + * would lead to incorrect behavior , whereas a false positive leads to merely <nl> + * inefficient behavior .) In practice , false positives should be uncommon enough <nl> + * to not worry about . <nl> + */ <nl> + inline bool might_contain_variables ( const std :: string & str ) <nl> +{ return str . find ('$') != std :: string :: npos ; } <nl> + <nl> /** <nl> * Function which will interpolate variables , starting with '$' in the string <nl> * ' str ' with the equivalent symbols in the given symbol table . If ' symbols '
mmm src / game_events . cpp <nl> ppp src / game_events . cpp <nl> WML_HANDLER_FUNCTION ( replace_schedule , /* event_info */, cfg ) <nl> ERR_NG << " attempted to to replace ToD schedule with empty schedule \ n "; <nl> } else { <nl> resources :: tod_manager -> replace_schedule ( cfg . get_parsed_config ()); <nl> + resources :: screen -> new_turn (); <nl> LOG_NG << " replaced ToD schedule \ n "; <nl> } <nl> }
mmm src / unit_types . cpp <nl> ppp src / unit_types . cpp <nl> void unit_type_data :: set_config ( config & cfg ) <nl> base_tree . push_back ( id ); <nl> while ( const config & bu = ut . child (" base_unit ")) <nl> { <nl> - if ( std :: find ( base_tree . begin (), base_tree . end (), bu [" id "]) != base_tree . end ()) { <nl> + if ( std :: find ( base_tree . begin (), base_tree . end (), bu [" id "]. str ()) != base_tree . end ()) { <nl> // If you want to allow diamond - style inheritance , replace the config :: error throw with a continue <nl>  <nl> std :: stringstream ss ;
mmm src / persist_context . cpp <nl> ppp src / persist_context . cpp <nl> bool persist_file_context :: clear_var ( const std :: string & global , bool immediate ) <nl> while (( active -> empty ()) && (! namespace_ . lineage_ . empty ())) { <nl> name_space prev = namespace_ . prev (); <nl> active = get_node ( cfg_ , prev ); <nl> + /// @ todo : This assertion replaces a seg fault . Still need to fix the <nl> + /// real bug ( documented as bug # 21093 ). <nl> + assert ( active != NULL ); <nl> active -> clear_children ( namespace_ . node_ ); <nl> if ( active -> has_child (" variables ") && active -> child (" variables "). empty ()) { <nl> active -> clear_children (" variables ");
mmm src / actions / undo . cpp <nl> ppp src / actions / undo . cpp <nl> void undo_list :: add_auto_shroud ( bool turned_on ) <nl> /// @ todo : Consecutive shroud actions can be collapsed into one . <nl>  <nl> // Do not call add (), as this should not clear the redo stack . <nl> - undos_ . push_back ( new undo :: auto_shroud_action ( turned_on )); <nl> + add ( new undo :: auto_shroud_action ( turned_on )); <nl> } <nl>  <nl> /** <nl> void undo_list :: add_update_shroud () <nl> { <nl> /// @ todo : Consecutive shroud actions can be collapsed into one . <nl>  <nl> - // Do not call add (), as this should not clear the redo stack . <nl> - undos_ . push_back ( new undo :: update_shroud_action ()); <nl> + add ( new undo :: update_shroud_action ()); <nl> } <nl>  <nl> 
mmm src / tools / exploder_utils . cpp <nl> ppp src / tools / exploder_utils . cpp <nl> void masked_overwrite_surface ( surface dest , surface src , surface mask , int x , in <nl> small_shift_before = 0 ; <nl> } <nl>  <nl> - if ( x + src_width <= dest -> w ) { <nl> + if ( x + src_width <= unsigned ( dest -> w )) { <nl> small_shift_after = 0 ; <nl> } else { <nl> small_shift_after = src_width - ( dest -> w - x ); <nl> void masked_overwrite_surface ( surface dest , surface src , surface mask , int x , in <nl> y = 0 ; <nl> } <nl>  <nl> - if ( y + src_height > dest -> h ) { <nl> + if ( y + src_height > unsigned ( dest -> h )) { <nl> src_height = dest -> h - y ; <nl> } <nl> 
mmm src / game . cpp <nl> ppp src / game . cpp <nl> void game_controller :: start_wesnothd () <nl>  <nl> std :: string config = " data / lan_server . cfg "; <nl> # ifndef _WIN32 <nl> - config = game_config :: wesnothd_name +" - c " + config + " - d - t 2 - T 5 "; <nl> + config = "\"" + game_config :: wesnothd_name +"\" - c " + config + " - d - t 2 - T 5 "; <nl> LOG_GENERAL << " Starting wesnothd : "<< config << "\ n "; <nl> if ( std :: system ( config . c_str ()) != 0 ) <nl> # else <nl> LOG_GENERAL << " Starting wesnothd \ n "; <nl> // Wesnothd_start . bat has to be included in windows as windows don ' t know how to start <nl> // background job <nl> - if ( std :: system ((" cmd / C start / B " + game_config :: wesnothd_name + " - c " + config + " - t 2 - T 5 "). c_str ()) != 0 ) <nl> + if ( std :: system ((" cmd / C start / B \"" + game_config :: wesnothd_name + "\" - c " + config + " - t 2 - T 5 "). c_str ()) != 0 ) <nl> # endif <nl> { <nl> LOG_GENERAL << " Failed to run server start script \ n ";
mmm src / text . cpp <nl> ppp src / text . cpp <nl> void ttext :: recalculate ( const bool force ) const <nl> << " result " << rect_ <nl> << ".\ n "; <nl>  <nl> - if ( rect_ . width > maximum_width_ ) { <nl> + if ( maximum_width_ != - 1 && rect_ . width > maximum_width_ ) { <nl> ERR_GUI_L << " ttext ::" << __func__ <nl> << " text '" << gui2 :: debug_truncate ( text_ ) <nl> << " ' width " << rect_ . width
mmm src / unit . cpp <nl> ppp src / unit . cpp <nl> void unit :: refresh_unit ( display & disp , gamemap :: location hex , bool with_status ) <nl>  <nl> if (! anim_ ) set_standing ( disp ); <nl> const gamemap :: TERRAIN terrain = map . get_terrain ( hex ); <nl> - const double submerge = is_flying () ? 0 . 0 : map . get_terrain_info ( terrain ). unit_submerge (); <nl> + const double submerge = is_flying () ? 0 . 0 : map . get_terrain_info ( terrain ). unit_submerge () * disp . zoom (); <nl> const int height_adjust = is_flying () ? 0 : int ( map . get_terrain_info ( terrain ). unit_height_adjust () * disp . zoom ()); <nl>  <nl> std :: string image_name ; <nl> void unit :: refresh_unit ( display & disp , gamemap :: location hex , bool with_status ) <nl>  <nl> if ( facing_ == gamemap :: location :: NORTH_WEST || facing_ == gamemap :: location :: SOUTH_WEST ) { <nl> const int d = disp . hex_size () / 2 ; <nl> - unit_anim_halo_ = halo :: add ( x + d - current_frame . halo_x , <nl> - y + d + current_frame . halo_y , <nl> + unit_anim_halo_ = halo :: add ( x + d -( current_frame . halo_x * disp . zoom ()), <nl> + y + d +( current_frame . halo_y * disp . zoom ()), <nl> current_frame . halo [ sub_halo ]. first ); <nl> } else { <nl> const int d = disp . hex_size () / 2 ; <nl> - unit_anim_halo_ = halo :: add ( x + d + current_frame . halo_x , <nl> - y + d + current_frame . halo_y , <nl> + unit_anim_halo_ = halo :: add ( x + d +( current_frame . halo_x * disp . zoom ()), <nl> + y + d +( current_frame . halo_y * disp . zoom ()), <nl> current_frame . halo [ sub_halo ]. first , <nl> halo :: REVERSE ); <nl> } <nl> void unit :: refresh_unit ( display & disp , gamemap :: location hex , bool with_status ) <nl> } else { <nl> loc = image :: locator ( image_name ); <nl> } <nl> - surface image ( image :: get_image ( loc , get_state (" stoned ")==" yes "? image :: GREYED : image :: UNSCALED )); <nl> + surface image ( image :: get_image ( loc , get_state (" stoned ")==" yes "? image :: GREYED : image :: SCALED )); <nl> if ( image == NULL ) { <nl> image = still_image (); <nl> } <nl> void unit :: refresh_unit ( display & disp , gamemap :: location hex , bool with_status ) <nl> if ( max_hitpoints () > 0 ) { <nl> unit_energy = double ( hitpoints ())/ double ( max_hitpoints ()); <nl> } <nl> - disp . draw_bar (* energy_file , x - 5 , y - height_adjust ,( max_hitpoints ()* 2 )/ 3 , unit_energy , hp_color (), bar_alpha ); <nl> + disp . draw_bar (* energy_file , x -( 5 * disp . zoom ()), y - height_adjust ,( max_hitpoints ()* 2 )/ 3 , unit_energy , hp_color (), bar_alpha ); <nl>  <nl> if ( experience () > 0 && can_advance ()) { <nl> const double filled = double ( experience ())/ double ( max_experience ());
mmm src / game_launcher . cpp <nl> ppp src / game_launcher . cpp <nl> game_launcher :: game_launcher ( const commandline_options & cmdline_opts , const char <nl> const std :: string app_basename = filesystem :: base_name ( appname ); <nl> jump_to_editor_ = app_basename . find (" editor ") != std :: string :: npos ; <nl>  <nl> + if ( cmdline_opts_ . core_id ) { <nl> + preferences :: set_core_id (* cmdline_opts_ . core_id ); <nl> + } <nl> if ( cmdline_opts_ . campaign ) { <nl> jump_to_campaign_ . jump_ = true ; <nl> jump_to_campaign_ . campaign_id_ = * cmdline_opts_ . campaign ;
mmm src / hotkey / command_executor . cpp <nl> ppp src / hotkey / command_executor . cpp <nl> void execute_command ( display & disp , const hotkey_command & command , command_execu <nl> } <nl> case LUA_CONSOLE : { <nl> if (! disp . in_game ()) { <nl> - WRN_G << " caution : attempting to interface console with game lua kernel when we are not in game ...\ n "; <nl> + // WRN_G << " caution : attempting to interface console with game lua kernel when we are not in game ...\ n "; <nl> + gui2 :: tlua_interpreter :: display ( disp . video (), gui2 :: tlua_interpreter :: APP ); <nl> + } else { <nl> + gui2 :: tlua_interpreter :: display ( disp . video (), gui2 :: tlua_interpreter :: GAME ); <nl> } <nl> - gui2 :: tlua_interpreter :: display ( disp . video (), gui2 :: tlua_interpreter :: GAME ); <nl> break ; <nl> } <nl> default :
mmm src / multiplayer_ui . cpp <nl> ppp src / multiplayer_ui . cpp <nl> std :: string get_colour_string ( int id ) <nl> } <nl> } <nl>  <nl> - chat :: chat () <nl> + chat :: chat () : <nl> + message_history_ (), <nl> + last_update_ () <nl> { <nl> } <nl>  <nl> ui :: ui ( game_display & disp , const std :: string & title , const config & cfg , chat & c , <nl> chat_textbox_ ( disp . video (), 100 , "", false ), <nl> users_menu_ ( disp . video (), std :: vector < std :: string >(), false , - 1 , - 1 , NULL , & umenu_style ), <nl>  <nl> + user_list_ (), <nl> selected_game_ (""), <nl>  <nl> result_ ( CONTINUE ),
mmm src / unit . cpp <nl> ppp src / unit . cpp <nl> const std :: vector < attack_type >& unit :: attacks () const <nl>  <nl> int unit :: movement_cost ( const gamemap & map , gamemap :: TERRAIN terrain ) const <nl> { <nl> - if ( type_ -> level () == 0 && terrain == gamemap :: TOWER ) <nl> - return 100 ; <nl> +// don ' t allow level 0 units to take villages - removed until AI <nl> +// is smart enough to deal with this . <nl> +// if ( type_ -> level () == 0 && terrain == gamemap :: TOWER ) <nl> +// return 100 ; <nl>  <nl> const int res = type_ -> movement_type (). movement_cost ( map , terrain ); <nl> 
mmm src / mouse_events . cpp <nl> ppp src / mouse_events . cpp <nl> bool mouse_handler :: left_click ( int x , int y , const bool browse ) <nl>  <nl> gui (). unhighlight_reach (); <nl>  <nl> + // If the whiteboard is active , it intercepts any unit movement <nl> if ( resources :: whiteboard -> is_active ()) { <nl> // Unselect the current hex , and create planned move for whiteboard <nl> selected_hex_ = map_location (); <nl> bool mouse_handler :: left_click ( int x , int y , const bool browse ) <nl> { <nl> resources :: whiteboard -> save_temp_move (); <nl> } <nl> - } else { <nl> + // Otherwise proceed to normal unit movement , unless the selected unit already has actions <nl> + // from the whiteboard . <nl> + } else if (! resources :: whiteboard -> unit_has_actions (* u )) { <nl> // register the mouse - UI waypoints into the unit ' s waypoints <nl> u -> waypoints () = waypoints_ ; <nl> 
mmm src / addon / manager . cpp <nl> ppp src / addon / manager . cpp <nl> namespace { <nl> if ( res == gui2 :: twindow :: OK ) { <nl> delete_remote_addon ( disp , addon , connection ); <nl> } <nl> - <nl> - return ; <nl> } <nl> - <nl> // Handle publish option <nl> - if ( index >= int ( addons . size ())) { <nl> + else if ( index >= int ( addons . size ())) { <nl> const std :: string & addon = publish_options [ index - int ( addons . size ())]; <nl> upload_addon_to_server ( disp , addon , connection ); <nl> - return ; <nl> } <nl> - <nl> - if ( check_whether_overwrite ( disp , addons [ index ], publish_options )) <nl> + // Handle download option <nl> + else if ( check_whether_overwrite ( disp , addons [ index ], publish_options )) <nl> { <nl> // Handle download <nl> install_addon ( disp , addons [ index ], titles [ index ], types [ index ], <nl> namespace { <nl> } <nl> } <nl>  <nl> - // Show the dialog again , and position it on the same item installed <nl> + // Show the dialog again , and position it on the last selected item <nl> download_addons ( disp , remote_address , update_mode , do_refresh , index ); <nl>  <nl> } catch ( config :: error & e ) {
mmm doc / doxygen / doxygen . cpp <nl> ppp doc / doxygen / doxygen . cpp <nl> @ li < a href =" hierarchy . html "> Classes </ a > <nl> @ li < a href =" files . html "> Source Files </ a > <nl> @ li < a href =" todo . html "> Todo </ a > <nl> + @ li < a href =" deprecated . html "> Deprecated </ a > <nl> </ td > <nl> </ tr > <nl> </ table >
mmm src / controller_base . cpp <nl> ppp src / controller_base . cpp <nl> # include " controller_base . hpp " <nl> # include " dialogs . hpp " <nl> # include " mouse_handler_base . hpp " <nl> +# include " foreach . hpp " <nl>  <nl> controller_base :: controller_base ( <nl> int ticks , const config & game_config , CVideo & /* video */) : <nl> bool controller_base :: handle_scroll ( CKey & key , int mousex , int mousey , int mouse <nl> bool scrolling = false ; <nl> bool mouse_in_window = ( SDL_GetAppState () & SDL_APPMOUSEFOCUS ) <nl> || utils :: string_bool ( preferences :: get (" scroll_when_mouse_outside "), true ); <nl> - const int scroll_threshold = ( preferences :: mouse_scroll_enabled ()) <nl> + int scroll_threshold = ( preferences :: mouse_scroll_enabled ()) <nl> ? preferences :: mouse_scroll_threshold () <nl> : 0 ; <nl> - <nl> + foreach ( const theme :: menu & m , get_display (). get_theme (). menus ()) { <nl> + if ( point_in_rect ( mousex , mousey , m . get_location ())) { <nl> + scroll_threshold = 0 ; <nl> + } <nl> + } <nl> if (( key [ SDLK_UP ] && have_keyboard_focus ()) <nl> || ( mousey < scroll_threshold && mouse_in_window )) { <nl> get_display (). scroll ( 0 ,- preferences :: scroll_speed ());
mmm src / unit . cpp <nl> ppp src / unit . cpp <nl> bool unit :: internal_matches_filter ( const vconfig & cfg , const gamemap :: location & <nl> std :: vector < std :: pair < int , int > >:: const_iterator range , range_end = ranges . end (); <nl> for ( range = ranges . begin (); range != range_end ; ++ range ) { <nl> for ( int i = range -> first ; i <= range -> second ; ++ i ) { <nl> - if ( i > 0 && i <= teams_ -> size ()) { <nl> + if ( i > 0 && static_cast < size_t >( i ) <= teams_ -> size ()) { <nl> viewers . insert ( i ); <nl> } <nl> } <nl> bool unit :: internal_matches_filter ( const vconfig & cfg , const gamemap :: location & <nl> } else { <nl> // if viewing_side is not defined , default to all enemies <nl> const team & my_team = (* teams_ )[ this -> side ()- 1 ]; <nl> - for ( int i = 1 ; i <= teams_ -> size (); ++ i ) { <nl> + for ( size_t i = 1 ; i <= teams_ -> size (); ++ i ) { <nl> if ( my_team . is_enemy ( i )) { <nl> viewers . insert ( i ); <nl> }
mmm src / game . cpp <nl> ppp src / game . cpp <nl> static int process_command_args ( int argc , char ** argv ) { <nl> python_ai :: invoke (" documentation "); <nl> return 0 ; <nl> } else if ( val == "-- python - shell ") { <nl> - int ret = python_ai :: run_shell (); <nl> + python_ai :: run_shell (); <nl> return 0 ; <nl> # endif <nl> } else if ( val == "-- config - dir ") {
mmm src / units / attack_type . cpp <nl> ppp src / units / attack_type . cpp <nl> static bool matches_simple_filter ( const attack_type & attack , const config & fil <nl> const std :: vector < std :: string > filter_name = utils :: split ( filter [" name "]); <nl> const std :: vector < std :: string > filter_type = utils :: split ( filter [" type "]); <nl> const std :: string filter_special = filter [" special "]; <nl> + const std :: string filter_special_active = filter [" special_active "]; <nl> const std :: string filter_formula = filter [" formula "]; <nl>  <nl> if ( ! filter_range . empty () && std :: find ( filter_range . begin (), filter_range . end (), attack . range ()) == filter_range . end () ) <nl> static bool matches_simple_filter ( const attack_type & attack , const config & fil <nl> if ( ! filter_special . empty () && ! attack . get_special_bool ( filter_special , true ) ) <nl> return false ; <nl>  <nl> + if ( ! filter_special_active . empty () && ! attack . get_special_bool ( filter_special_active , false ) ) <nl> + return false ; <nl> + <nl> if (! filter_formula . empty ()) { <nl> try { <nl> const wfl :: attack_type_callable callable ( attack );
mmm src / addon / info . cpp <nl> ppp src / addon / info . cpp <nl> void addon_info :: read ( const config & cfg ) <nl> } <nl>  <nl> this -> depends = utils :: split ( cfg [" dependencies "]. str ()); <nl> + this -> feedback_url = cfg [" feedback_url "]. str (); <nl>  <nl> this -> updated = cfg [" timestamp "]. to_time_t (); <nl> } <nl> void addon_info :: write ( config & cfg ) const <nl> } <nl>  <nl> cfg [" dependencies "] = utils :: join ( this -> depends ); <nl> + cfg [" feedback_url "] = this -> feedback_url ; <nl>  <nl> cfg [" timestamp "] = this -> updated ; <nl> }mmm src / addon / info . hpp <nl> ppp src / addon / info . hpp <nl> void addon_info :: read ( const config & cfg ) <nl> } <nl>  <nl> this -> depends = utils :: split ( cfg [" dependencies "]. str ()); <nl> + this -> feedback_url = cfg [" feedback_url "]. str (); <nl>  <nl> this -> updated = cfg [" timestamp "]. to_time_t (); <nl> } <nl> void addon_info :: write ( config & cfg ) const <nl> } <nl>  <nl> cfg [" dependencies "] = utils :: join ( this -> depends ); <nl> + cfg [" feedback_url "] = this -> feedback_url ; <nl>  <nl> cfg [" timestamp "] = this -> updated ; <nl> } <nl> struct addon_info <nl> std :: vector < std :: string > depends ; <nl> // std :: vector < addon_dependency > conflicts , recommends , replaces ; <nl>  <nl> + std :: string feedback_url ; <nl> + <nl> time_t updated ; <nl>  <nl> // Artificial upload order index used to preserve add - ons upload order <nl> struct addon_info <nl> , version (), author (), size (), downloads () <nl> , uploads (), type (), locales () <nl> , depends () <nl> + , feedback_url () <nl> , updated (), order () <nl> {} <nl>  <nl> struct addon_info <nl> , version (), author (), size (), downloads () <nl> , uploads (), type (), locales () <nl> , depends () <nl> + , feedback_url () <nl> , updated (), order () <nl> { <nl> this -> read ( cfg ); <nl> struct addon_info <nl> this -> type = o . type ; <nl> this -> locales = o . locales ; <nl> this -> depends = o . depends ; <nl> + this -> feedback_url = o . feedback_url ; <nl> this -> updated = o . updated ; <nl> this -> order = o . order ; <nl> }
mmm src / whiteboard / highlight_visitor . cpp <nl> ppp src / whiteboard / highlight_visitor . cpp <nl> action_ptr highlight_visitor :: get_execute_target () <nl> } <nl> action_ptr highlight_visitor :: get_delete_target () <nl> { <nl> - action_ptr action ; <nl> + action_ptr action = action_ptr (); <nl> if ( owner_unit_ ) <nl> { <nl> - action = * side_actions_ -> find_last_action_of (* owner_unit_ ); <nl> + side_actions :: iterator it = side_actions_ -> find_last_action_of (* owner_unit_ ); <nl> + if ( it != side_actions_ -> end ()) <nl> + { <nl> + action = * it ; <nl> + } <nl> } <nl> return action ; <nl> }
mmm src / reports . cpp <nl> ppp src / reports . cpp <nl> static config unit_weapons ( reports :: context & rc , const unit * attacker , const ma <nl> for ( unsigned int i = 0 ; i < attacker -> attacks (). size (); i ++) { <nl> // skip weapons with attack_weight = 0 <nl> if ( attacker -> attacks ()[ i ]. attack_weight () > 0 ) { <nl> - battle_context weapon ( rc . units (), attacker_pos , defender -> get_location (), i , - 1 , 0 . 0 , nullptr , attacker ); <nl> - weapons . push_back ( weapon ); <nl> + weapons . emplace_back ( rc . units (), attacker_pos , defender -> get_location (), i , - 1 , 0 . 0 , nullptr , attacker ); <nl> } <nl> } <nl> mmm src / mouse_events . cpp <nl> ppp src / mouse_events . cpp <nl> static config unit_weapons ( reports :: context & rc , const unit * attacker , const ma <nl> for ( unsigned int i = 0 ; i < attacker -> attacks (). size (); i ++) { <nl> // skip weapons with attack_weight = 0 <nl> if ( attacker -> attacks ()[ i ]. attack_weight () > 0 ) { <nl> - battle_context weapon ( rc . units (), attacker_pos , defender -> get_location (), i , - 1 , 0 . 0 , nullptr , attacker ); <nl> - weapons . push_back ( weapon ); <nl> + weapons . emplace_back ( rc . units (), attacker_pos , defender -> get_location (), i , - 1 , 0 . 0 , nullptr , attacker ); <nl> } <nl> } <nl>  <nl> int mouse_handler :: fill_weapon_choices ( <nl> best = bc_vector . size (); <nl> } <nl>  <nl> - bc_vector . push_back ( bc ); <nl> + bc_vector . push_back ( std :: move ( bc )); <nl> } <nl> } <nl> mmm src / actions / attack . hpp <nl> ppp src / actions / attack . hpp <nl> static config unit_weapons ( reports :: context & rc , const unit * attacker , const ma <nl> for ( unsigned int i = 0 ; i < attacker -> attacks (). size (); i ++) { <nl> // skip weapons with attack_weight = 0 <nl> if ( attacker -> attacks ()[ i ]. attack_weight () > 0 ) { <nl> - battle_context weapon ( rc . units (), attacker_pos , defender -> get_location (), i , - 1 , 0 . 0 , nullptr , attacker ); <nl> - weapons . push_back ( weapon ); <nl> + weapons . emplace_back ( rc . units (), attacker_pos , defender -> get_location (), i , - 1 , 0 . 0 , nullptr , attacker ); <nl> } <nl> } <nl>  <nl> int mouse_handler :: fill_weapon_choices ( <nl> best = bc_vector . size (); <nl> } <nl>  <nl> - bc_vector . push_back ( bc ); <nl> + bc_vector . push_back ( std :: move ( bc )); <nl> } <nl> } <nl>  <nl> public : <nl> battle_context ( const battle_context_unit_stats & att , const battle_context_unit_stats & def ); <nl>  <nl> battle_context ( const battle_context & other ); <nl> + battle_context ( battle_context && other ) = default ; <nl>  <nl> battle_context & operator =( const battle_context & other ); <nl> + battle_context & operator =( battle_context && other ) = default ; <nl>  <nl> /** This method returns the statistics of the attacker . */ <nl> const battle_context_unit_stats & get_attacker_stats () const
mmm src / unit . cpp <nl> ppp src / unit . cpp <nl> void unit :: restart_animation ( const game_display & disp , int start_time ) { <nl> } <nl>  <nl> void unit :: set_facing ( gamemap :: location :: DIRECTION dir ) { <nl> - wassert ( dir != gamemap :: location :: NDIRECTIONS ); <nl> - facing_ = dir ; <nl> + if ( dir != gamemap :: location :: NDIRECTIONS ) { <nl> + facing_ = dir ; <nl> + } <nl> + // else look at yourself ( not available so continue to face the same direction ) <nl> } <nl>  <nl> void unit :: redraw_unit ( game_display & disp , const gamemap :: location & loc )
mmm src / gui / widgets / window . hpp <nl> ppp src / gui / widgets / window . hpp <nl> public : <nl> static void set_sunset ( const unsigned interval ) <nl> { sunset_ = interval ? interval : 5 ; } <nl>  <nl> + bool get_need_layout () const { return need_layout_ ; } <nl> + <nl> private : <nl>  <nl> /** Needed so we can change what ' s drawn on the screen . */
mmm src / playlevel . cpp <nl> ppp src / playlevel . cpp <nl> LEVEL_RESULT play_level ( game_data & gameinfo , const config & game_config , <nl> if ( first_human_team != - 1 ) { <nl> clear_shroud ( gui , status , map , gameinfo , units , teams , first_human_team ); <nl> LOG_NG << " b " << ( SDL_GetTicks () - ticks ) << "\ n "; <nl> - gui . scroll_to_tile ( map . starting_position ( first_human_team + 1 ). x , map . starting_position ( first_human_team + 1 ). y , display :: WARP ); <nl> + gui . scroll_to_tile ( map . starting_position ( first_human_team + 1 ). x , <nl> + map . starting_position ( first_human_team + 1 ). y , display :: WARP ); <nl> LOG_NG << " c " << ( SDL_GetTicks () - ticks ) << "\ n "; <nl> } <nl>  <nl> LEVEL_RESULT play_level ( game_data & gameinfo , const config & game_config , <nl> events :: raise_draw_event (); <nl> if (! loading_game ) { <nl> game_events :: fire (" start "); <nl> + game_events :: set_variable (" turn_number ", " 1 "); <nl> } <nl>  <nl> gui . draw ();
mmm src / soundsource . cpp <nl> ppp src / soundsource . cpp <nl> void positional_source :: write_config ( config & cfg ) const <nl> cfg [" delay "] = str_cast < unsigned int >( this -> min_delay_ ); <nl> cfg [" chance "] = str_cast < unsigned int >( this -> chance_ ); <nl> cfg [" check_fogged "] = this -> check_fogged_ ? " yes " : " no "; <nl> - cfg [" check_shrouded "] = this -> check_fogged_ ? " yes " : " no "; <nl> + cfg [" check_shrouded "] = this -> check_shrouded_ ? " yes " : " no "; <nl>  <nl> cfg [" x "] = cfg [" y "] = ""; <nl> bool first_loc = true ;
mmm src / reports . cpp <nl> ppp src / reports . cpp <nl> Units cannot be killed by poison alone . The poison will not reduce it below 1 HP <nl>  <nl> if ( flag_icon . empty ()) { <nl> flag_icon = game_config :: flag_icon_image ; <nl> - old_rgb = game_config :: flag_rgb ; <nl> - new_rgb = team :: get_side_colour_index ( playing_side ); <nl> - mods = "~ RC (" + old_rgb + ">" + new_rgb + ")"; <nl> } <nl> + old_rgb = game_config :: flag_rgb ; <nl> + new_rgb = team :: get_side_colour_index ( playing_side ); <nl> + mods = "~ RC (" + old_rgb + ">" + new_rgb + ")"; <nl>  <nl> // remove animation stuff we don ' t care about <nl> // const std :: vector < std :: string > items = utils :: split ( flag );
mmm src / hotkey / hotkey_item . cpp <nl> ppp src / hotkey / hotkey_item . cpp <nl> void save_hotkeys ( config & cfg ) <nl> cfg . clear_children (" hotkey "); <nl>  <nl> BOOST_FOREACH ( hotkey_ptr item , hotkeys_ ) { <nl> - if (! item -> is_default ()) { <nl> + if (! item -> is_default () && item -> active ()) { <nl> item -> save ( cfg . add_child (" hotkey ")); <nl> } <nl> }
mmm src / menu_events . cpp <nl> ppp src / menu_events . cpp <nl> private : <nl> if ( network :: nconnections () == 0 ) { <nl> std :: cerr << " showing ai formula ...\ n "; <nl> textbox_info_ . show ( gui :: TEXTBOX_AI , sgettext (" prompt ^ Command :"), "", false , * gui_ ); <nl> - } else { <nl> - add_chat_message ( time ( NULL ), _ (" ai "), 0 , " Formula commandline not available in network games "); <nl> } <nl> } <nl> 
mmm src / help . cpp <nl> ppp src / help . cpp <nl> public : <nl> std :: string lang_unit = type -> type_name (); <nl> std :: string ref_id ; <nl> if ( description_type (* type ) == FULL_DESCRIPTION ) { <nl> - ref_id = unit_prefix + type -> id (); <nl> + const std :: string section_prefix = type -> variations (). empty () ? "" : ".."; <nl> + ref_id = section_prefix + unit_prefix + type -> id (); <nl> } else { <nl> ref_id = unknown_unit_topic ; <nl> lang_unit += " (?)";
mmm src / saved_game . cpp <nl> ppp src / saved_game . cpp <nl> void saved_game :: expand_random_scenario () <nl> LOG_NG << " randomly generating scenario ...\ n "; <nl> const cursor :: setter cursor_setter ( cursor :: WAIT ); <nl>  <nl> - starting_pos_ = random_generate_scenario ( starting_pos_ [" scenario_generation "], <nl> + config scenario_new = random_generate_scenario ( starting_pos_ [" scenario_generation "], <nl> starting_pos_ . child (" generator ")); <nl> + // Preserve " story " form the scenario toplevel . <nl> + BOOST_FOREACH ( config & story , starting_pos_ . child_range (" story ")) <nl> + { <nl> + scenario_new . add_child (" story ", story ); <nl> + } <nl> + starting_pos_ = scenario_new ; <nl> } <nl> // it looks like we support a map = where map = filename equals more or less map_data ={ filename } <nl> if ( starting_pos_ [" map_data "]. empty () && starting_pos_ [" map "] != "") {
mmm src / play_controller . cpp <nl> ppp src / play_controller . cpp <nl> void play_controller :: show_help (){ <nl> } <nl>  <nl> void play_controller :: undo (){ <nl> + // deselect unit ( only here , not to be done when undoing attack - move ) <nl> + mouse_handler_ . deselect_hex (); <nl> menu_handler_ . undo ( player_number_ ); <nl> } <nl>  <nl> void play_controller :: redo (){ <nl> + // deselect unit ( only here , not to be done when undoing attack - move ) <nl> + mouse_handler_ . deselect_hex (); <nl> menu_handler_ . redo ( player_number_ ); <nl> } <nl>  <nl> void play_controller :: init_side ( const unsigned int team_index , bool /* is_replay * <nl> gui_ -> invalidate_all (); <nl> } <nl>  <nl> - if (! recorder . is_skipping ()){ <nl> + if (! recorder . is_skipping () && ! skip_replay_ ){ <nl> gui_ -> scroll_to_leader ( units_ , player_number_ ); <nl> } <nl> }
mmm src / display . cpp <nl> ppp src / display . cpp <nl> void display :: clear_redraw_observers () <nl>  <nl> void display :: draw ( bool update , bool force ) { <nl> // log_scope (" display :: draw "); <nl> - if ( screen_ . update_locked ()) { <nl> + if ( screen_ . update_locked () || ( SDL_GetAppState () & SDL_APPACTIVE ) == 0 ) { <nl> return ; <nl> } <nl> bool changed = draw_init ();
mmm src / multiplayer_connect . cpp <nl> ppp src / multiplayer_connect . cpp <nl> config connect :: side :: get_config () const <nl> } <nl> { <nl> res [" id "] = res [" save_id "]; <nl> - const config & ai_cfg = ai :: configuration :: get_ai_config_for ( ai_algorithm_ ); <nl> - res . add_child (" ai ", ai_cfg ); <nl> utils :: string_map symbols ; <nl> - symbols [" playername "] = ai_cfg [" description "]; <nl> + if ( allow_player_ ) { <nl> + const config & ai_cfg = ai :: configuration :: get_ai_config_for ( ai_algorithm_ ); <nl> + res . add_child (" ai ", ai_cfg ); <nl> + symbols [" playername "] = ai_cfg [" description "]; <nl> + } else { // do not import default ai cfg here - all is set by scenario config <nl> + symbols [" playername "] = _ (" Computer Player "); <nl> + } <nl> symbols [" side "] = res [" side "]. str (); <nl> description = vgettext ("$ playername $ side ", symbols ); <nl> }
mmm src / tools / schema / tag . cpp <nl> ppp src / tools / schema / tag . cpp <nl> void class_tag :: add_tag ( const std :: string & path , const class_tag & tag , <nl> it -> second . add_keys ( tag . keys_ ); <nl> it -> second . add_links ( tag . links_ ); <nl> } <nl> + links_ . erase ( tag . get_name ()); <nl> return ; <nl> } <nl> std :: string :: size_type pos = path . find ('/'); <nl> void class_tag :: append_super ( const class_tag & tag , const std :: string & path ){ <nl> add_keys ( tag . keys_ ); <nl> add_links ( tag . links_ ); <nl> for ( tag_map :: const_iterator i = tag . tags_ . begin (); i != tag . tags_ . end ();++ i ){ <nl> + links_ . erase ( i -> first ); <nl> add_link ( path + "/" + i -> first ); <nl> + <nl> } <nl> } <nl> 
mmm src / gui / dialogs / addon / manager . cpp <nl> ppp src / gui / dialogs / addon / manager . cpp <nl> void addon_manager :: apply_filters ( window & window ) <nl> template < void ( addon_manager ::* fptr )( const addon_info & addon , window & window )> <nl> void addon_manager :: execute_action_on_selected_addon ( window & window ) <nl> { <nl> + // Explicitly return to the main page if we ' re in low - res mode so the list is visible . <nl> + if ( stacked_widget * stk = find_widget < stacked_widget >(& window , " main_stack ", false , false )) { <nl> + stk -> select_layer ( 0 ); <nl> + } <nl> + <nl> addon_list & addons = find_widget < addon_list >(& window , " addons ", false ); <nl> const addon_info * addon = addons . get_selected_addon (); <nl> 
mmm src / wesnothd_connection . hpp <nl> ppp src / wesnothd_connection . hpp <nl> public : <nl> // Destroys this object . <nl> void stop (); <nl>  <nl> + bool socket_open () const <nl> + { <nl> + return socket_ . is_open (); <nl> + } <nl> + <nl> /** True if connected and no high - level operation is in progress */ <nl> bool handshake_finished () const <nl> {mmm src / wesnothd_connection . cpp <nl> ppp src / wesnothd_connection . cpp <nl> public : <nl> // Destroys this object . <nl> void stop (); <nl>  <nl> + bool socket_open () const <nl> + { <nl> + return socket_ . is_open (); <nl> + } <nl> + <nl> /** True if connected and no high - level operation is in progress */ <nl> bool handshake_finished () const <nl> { <nl> void wesnothd_connection :: send_data ( const configr_of & request ) <nl> void wesnothd_connection :: cancel () <nl> { <nl> MPTEST_LOG ; <nl> - if ( socket_ . is_open ()) { <nl> + if ( socket_open ()) { <nl> boost :: system :: error_code ec ; <nl>  <nl> # ifdef _MSC_VERmmm src / game_initialization / multiplayer . cpp <nl> ppp src / game_initialization / multiplayer . cpp <nl> public : <nl> // Destroys this object . <nl> void stop (); <nl>  <nl> + bool socket_open () const <nl> + { <nl> + return socket_ . is_open (); <nl> + } <nl> + <nl> /** True if connected and no high - level operation is in progress */ <nl> bool handshake_finished () const <nl> { <nl> void wesnothd_connection :: send_data ( const configr_of & request ) <nl> void wesnothd_connection :: cancel () <nl> { <nl> MPTEST_LOG ; <nl> - if ( socket_ . is_open ()) { <nl> + if ( socket_open ()) { <nl> boost :: system :: error_code ec ; <nl>  <nl> # ifdef _MSC_VER <nl> std :: pair < wesnothd_connection_ptr , config > open_connection ( std :: string host ) <nl> return std :: make_pair ( std :: move ( sock ), config ()); <nl> } <nl>  <nl> + if (! sock -> socket_open ()) { <nl> + throw wesnothd_error (" The server has shut down or restarted ."); <nl> + } <nl> + <nl> data . clear (); <nl> sock -> wait_and_receive_data ( data ); <nl> 
mmm src / multiplayer . cpp <nl> ppp src / multiplayer . cpp <nl> void play_multiplayer ( display & disp , game_data & units_data , config cfg , <nl> } else if ( result < int ( choices . size ()/ 3 )* 2 ) { <nl> controller = " ai "; <nl> result -= choices . size ()/ 3 ; <nl> + sides [ res ]-> values [" description "] = ""; <nl> } else { <nl> controller = " network "; <nl> result -= ( choices . size ()/ 3 )* 2 ;
mmm src / gui / widgets / window . cpp <nl> ppp src / gui / widgets / window . cpp <nl> void window :: layout () <nl>  <nl> log_scope2 ( log_gui_layout , LOG_SCOPE_HEADER ); <nl>  <nl> - point size = get_best_size (); <nl> const point mouse = get_mouse_position (); <nl>  <nl> variables_ . add (" mouse_x ", wfl :: variant ( mouse . x )); <nl> variables_ . add (" mouse_y ", wfl :: variant ( mouse . y )); <nl> variables_ . add (" window_width ", wfl :: variant ( 0 )); <nl> variables_ . add (" window_height ", wfl :: variant ( 0 )); <nl> - variables_ . add (" best_window_width ", wfl :: variant ( size . x )); <nl> - variables_ . add (" best_window_height ", wfl :: variant ( size . y )); <nl> variables_ . add (" size_request_mode ", wfl :: variant (" maximum ")); <nl>  <nl> get_screen_size_variables ( variables_ ); <nl> void window :: layout () <nl> } <nl>  <nl> /***** Get the best location for the window *****/ <nl> - size = get_best_size (); <nl> + point size = get_best_size (); <nl> assert ( size . x <= maximum_width && size . y <= maximum_height ); <nl>  <nl> point origin ( 0 , 0 );
mmm src / cursor . cpp <nl> ppp src / cursor . cpp <nl> void draw ( surface screen ) <nl> if ( use_colour_cursors () == false ) { <nl> return ; <nl> } <nl> - <nl> + <nl> + if ( current_cursor == NUM_CURSORS ) { <nl> + return ; <nl> + } <nl> + <nl> if (! colour_ready ) { <nl> // display start to draw cursor <nl> // so it can now display colour cursor <nl> void draw ( surface screen ) <nl> set ( current_cursor ); <nl> } <nl>  <nl> - if ( current_cursor == NUM_CURSORS ) { <nl> - return ; <nl> - } <nl> - <nl> if ( have_focus == false ) { <nl> cursor_buf = NULL ; <nl> return ;
mmm src / loadscreen . cpp <nl> ppp src / loadscreen . cpp <nl> loadscreen :: loadscreen ( CVideo & screen , const int & percent ): <nl> setconfig_counter ( 0 ), <nl> parser_counter ( 0 ), <nl> screen_ ( screen ), <nl> + textarea_ (), <nl> + logo_surface_ ( NULL ), <nl> logo_drawn_ ( false ), <nl> pby_offset_ ( 0 ), <nl> prcnt_ ( percent )
mmm src / game_preferences . cpp <nl> ppp src / game_preferences . cpp <nl> void encounter_start_units ( unit_map & units ){ <nl> void encounter_recallable_units ( std :: vector < team >& teams ){ <nl> for ( std :: vector < team >:: iterator help_team_it = teams . begin (); <nl> help_team_it != teams . end (); ++ help_team_it ) { <nl> - for ( std :: vector < unit >:: iterator help_recall_it = help_team_it -> recall_list (). begin (); help_recall_it != help_team_it -> recall_list (). end (); help_recall_it ++) { <nl> + for ( std :: vector < unit >:: iterator help_recall_it = help_team_it -> recall_list (). begin (); help_recall_it != help_team_it -> recall_list (). end (); ++ help_recall_it ) { <nl> encountered_units_set . insert ( help_recall_it -> type_id ()); <nl> } <nl> } <nl> } <nl>  <nl> void encounter_map_terrain ( gamemap & map ){ <nl> - for ( int map_x = 0 ; map_x < map . w (); map_x ++) { <nl> - for ( int map_y = 0 ; map_y < map . h (); map_y ++) { <nl> + for ( int map_x = 0 ; map_x < map . w (); ++ map_x ) { <nl> + for ( int map_y = 0 ; map_y < map . h (); ++ map_y ) { <nl> const t_translation :: t_terrain t = map . get_terrain ( map_location ( map_x , map_y )); <nl> preferences :: encountered_terrains (). insert ( t ); <nl> const t_translation :: t_list & underlaying_list = map . underlying_union_terrain ( map_location ( map_x , map_y )); <nl> - for ( std :: vector < t_translation :: t_terrain >:: const_iterator ut = underlaying_list . begin (); ut != underlaying_list . end (); ut ++) { <nl> + for ( std :: vector < t_translation :: t_terrain >:: const_iterator ut = underlaying_list . begin (); ut != underlaying_list . end (); ++ ut ) { <nl> preferences :: encountered_terrains (). insert (* ut ); <nl> }; <nl> }
mmm src / font . cpp <nl> ppp src / font . cpp <nl> struct font_id <nl> }; <nl> bool operator <( const font_id & o ) const <nl> { <nl> - return subset < o . subset || subset == o . subset && size < o . size ; <nl> + return subset < o . subset || ( subset == o . subset && size < o . size ); <nl> }; <nl>  <nl> subset_id subset ;
mmm src / network_worker . cpp <nl> ppp src / network_worker . cpp <nl> static int process_queue ( void * shard_num ) <nl> } else { <nl> std :: string buffer ( buf . begin (), buf . end ()); <nl> std :: istringstream stream ( buffer ); <nl> - // Binary wml starts with a char < 4 , the first char of a gzip header is 31 <nl> - // so test that here and use the proper reader . <nl> try { <nl> - if ( stream . peek () == 31 ) { <nl> - read_gz ( received_data -> config_buf , stream ); <nl> - } else { <nl> - /// @ todo Possibly complain more loudly <nl> - ERR_NW << " Receiving binary WML . Who is sending this ?\ n "; <nl> - } <nl> + read_gz ( received_data -> config_buf , stream ); <nl> } catch ( config :: error & e ) <nl> { <nl> received_data -> config_error = e . message ;
mmm src / version . cpp <nl> ppp src / version . cpp <nl> version_info :: version_info ( unsigned int major , unsigned int minor , unsigned int <nl> } <nl>  <nl> version_info :: version_info ( const std :: string & str ) <nl> - : special_ (""), special_separator_ ('\ 0 '), sane_ ( true ) <nl> + : nums_ () <nl> + , special_ ("") <nl> + , special_separator_ ('\ 0 ') <nl> + , sane_ ( true ) <nl> { <nl> const std :: vector < std :: string > string_parts = utils :: split ( str ,'.'); <nl> // first two components are required to be valid numbers , though
mmm src / server / server . cpp <nl> ppp src / server / server . cpp <nl> std :: string server :: process_command ( const std :: string & query ) { <nl> } else if ( command == " status ") { <nl> out << " STATUS REPORT \ n "; <nl> for ( player_map :: const_iterator pl = players_ . begin (); pl != players_ . end (); ++ pl ) { <nl> - if ( parameters == "" || utils :: wildcard_string_match ( pl -> second . name (), parameters )) { <nl> + if ( parameters == "" <nl> + || utils :: wildcard_string_match ( pl -> second . name (), parameters ) <nl> + || utils :: wildcard_string_match ( network :: ip_address ( pl -> first ), parameters )) { <nl> const network :: connection_stats & stats = network :: get_connection_stats ( pl -> first ); <nl> const int time_connected = stats . time_connected / 1000 ; <nl> const int seconds = time_connected % 60 ; <nl> std :: string server :: process_command ( const std :: string & query ) { <nl> } else { <nl> out << " Command '" << command << "' is not recognized .\ n "; <nl> out << " Available commands are : ( lobby ) msg < message >, motd [< message >]" <nl> - ", status [< nickmask >], metrics , ( k ) ban ( s ) [< mask >], unban < ipmask >" <nl> + ", status [< mask >], metrics , ( k ) ban ( s ) [< mask >], unban < ipmask >" <nl> ", kick < mask >"; <nl> } <nl> 
mmm src / image_modifications . cpp <nl> ppp src / image_modifications . cpp <nl> surface rotate_modification :: operator ()( const surface & src ) const <nl> case 180 : return rotate_180_surface ( src ); <nl> case 270 : return rotate_90_surface ( src , false ); <nl> case 360 : return src ; <nl> - default : return rotate_any_surface ( src , normalized , zoom_ , offset_ ); <nl> } <nl>  <nl> - // Other values are not supported . Ignore them . <nl> - return src ; <nl> + return rotate_any_surface ( src , normalized , zoom_ , offset_ ); <nl> } <nl>  <nl> surface gs_modification :: operator ()( const surface & src ) const <nl> REGISTER_MOD_PARSER ( ROTATE , args ) <nl> lexical_cast_default < int >( slice_params [ 1 ]), <nl> lexical_cast_default < int >( slice_params [ 2 ])); <nl> break ; <nl> - default : <nl> - return NULL ; <nl> - break ; <nl> } <nl> return NULL ; <nl> }
mmm src / chat_command_handler . hpp <nl> ppp src / chat_command_handler . hpp <nl> protected : <nl> _ (" Mute / Unmute all observers . ( toggles )"), ""); <nl> register_command (" ping ", & chat_command_handler :: do_network_send , <nl> ""); <nl> - register_command (" green ", & chat_command_handler :: do_network_send_req_arg , <nl> - "", "", " A "); <nl> - register_command (" red ", & chat_command_handler :: do_network_send_req_arg , <nl> - "", "", " A "); <nl> - register_command (" yellow ", & chat_command_handler :: do_network_send_req_arg , <nl> - "", "", " A "); <nl> register_command (" report ", & chat_command_handler :: do_network_send_req_arg , <nl> _ (" Report abuse , rule violations , etc . to the server moderators . " <nl> " Make sure to mention relevant nicknames , etc ."), "");mmm src / chat_events . cpp <nl> ppp src / chat_events . cpp <nl> protected : <nl> _ (" Mute / Unmute all observers . ( toggles )"), ""); <nl> register_command (" ping ", & chat_command_handler :: do_network_send , <nl> ""); <nl> - register_command (" green ", & chat_command_handler :: do_network_send_req_arg , <nl> - "", "", " A "); <nl> - register_command (" red ", & chat_command_handler :: do_network_send_req_arg , <nl> - "", "", " A "); <nl> - register_command (" yellow ", & chat_command_handler :: do_network_send_req_arg , <nl> - "", "", " A "); <nl> register_command (" report ", & chat_command_handler :: do_network_send_req_arg , <nl> _ (" Report abuse , rule violations , etc . to the server moderators . " <nl> " Make sure to mention relevant nicknames , etc ."), ""); <nl> void chat_handler :: send_command ( const std :: string & cmd , const std :: string & args <nl> else if ( cmd == " ping ") { <nl> data [ cmd ] = std :: to_string ( time ( nullptr )); <nl> } <nl> - else if ( cmd == " green ") { <nl> - data . add_child (" query ")[" type "] = " lobbymsg @" + args ; <nl> - } <nl> - else if ( cmd == " red ") { <nl> - data . add_child (" query ")[" type "] = " lobbymsg #" + args ; <nl> - } <nl> - else if ( cmd == " yellow ") { <nl> - data . add_child (" query ")[" type "] = " lobbymsg < 255 , 255 , 0 >" + args ; <nl> - } <nl> else if ( cmd == " report ") { <nl> data . add_child (" query ")[" type "] = " report " + args ; <nl> }
mmm src / widgets / textbox . cpp <nl> ppp src / widgets / textbox . cpp <nl> void textbox :: set_text ( std :: string text ) <nl> { <nl> text_ = string_to_wstring ( text ); <nl> cursor_ = text_ . size (); <nl> + selstart_ = - 1 ; <nl> + selend_ = - 1 ; <nl> set_dirty ( true ); <nl> update_text_cache ( true ); <nl> } <nl> void textbox :: clear () <nl> cursor_ = 0 ; <nl> cursor_pos_ = 0 ; <nl> text_pos_ = 0 ; <nl> + selstart_ = - 1 ; <nl> + selend_ = - 1 ; <nl> set_dirty ( true ); <nl> update_text_cache ( true ); <nl> }
mmm src / campaign_server / campaign_server . cpp <nl> ppp src / campaign_server / campaign_server . cpp <nl> namespace { <nl>  <nl> // Copy over COPYING . txt <nl> std :: string contents = read_file (" COPYING . txt "); <nl> - config & copying = dir . add_child (" file "); <nl> - copying [" name "] = " COPYING . txt "; <nl> - copying [" contents "] = contents ; <nl> - <nl> if ( contents . empty ()) { <nl> LOG_CS << " Could not find COPYING . txt , path is \"" <nl> << game_config :: path << "\"\ n "; <nl> + return ; <nl> } <nl> + config & copying = dir . add_child (" file "); <nl> + copying [" name "] = " COPYING . txt "; <nl> + copying [" contents "] = contents ; <nl> + <nl> } <nl> void campaign_server :: convert_binary_to_gzip () <nl> {
mmm src / actions . cpp <nl> ppp src / actions . cpp <nl> bool can_recruit_on ( const gamemap & map , const map_location & leader , const map_lo <nl> if (! map . is_castle ( loc )) <nl> return false ; <nl>  <nl> + if (! map . is_keep ( leader )) <nl> + return false ; <nl> + <nl> castle_cost_calculator calc ( map ); <nl> // The limit computed in the third argument is more than enough for <nl> // any convex castle on the map . Strictly speaking it could be
mmm src / gui / dialogs / lobby / lobby . cpp <nl> ppp src / gui / dialogs / lobby / lobby . cpp <nl> tlobby_main :: tlobby_main ( const config & game_config , <nl> { <nl> // Need to set this in the constructor , pre_show () is too late <nl> set_show_even_without_video ( true ); <nl> + set_allow_plugin_skip ( false ); <nl> } <nl>  <nl> struct lobby_delay_gamelist_update_guard <nl> void tlobby_main :: pre_show ( twindow & window ) <nl> game_config :: lobby_network_timer , std :: bind (& tlobby_main :: network_handler , this ), true ); <nl>  <nl> // Set up Lua plugin context <nl> - set_allow_plugin_skip ( false ); <nl> plugins_context_ . reset ( new plugins_context (" Multiplayer Lobby ")); <nl>  <nl> plugins_context_ -> set_callback (" join ", [&, this ]( const config &) {mmm src / gui / dialogs / multiplayer / mp_create_game . cpp <nl> ppp src / gui / dialogs / multiplayer / mp_create_game . cpp <nl> tlobby_main :: tlobby_main ( const config & game_config , <nl> { <nl> // Need to set this in the constructor , pre_show () is too late <nl> set_show_even_without_video ( true ); <nl> + set_allow_plugin_skip ( false ); <nl> } <nl>  <nl> struct lobby_delay_gamelist_update_guard <nl> void tlobby_main :: pre_show ( twindow & window ) <nl> game_config :: lobby_network_timer , std :: bind (& tlobby_main :: network_handler , this ), true ); <nl>  <nl> // Set up Lua plugin context <nl> - set_allow_plugin_skip ( false ); <nl> plugins_context_ . reset ( new plugins_context (" Multiplayer Lobby ")); <nl>  <nl> plugins_context_ -> set_callback (" join ", [&, this ]( const config &) { <nl> tmp_create_game :: tmp_create_game ( const config & cfg , ng :: create_engine & create_en <nl>  <nl> create_engine_ . get_state () = saved_game (); <nl> create_engine_ . get_state (). classification (). campaign_type = game_classification :: CAMPAIGN_TYPE :: MULTIPLAYER ; <nl> + <nl> + // Need to set this in the constructor , pre_show () is too late <nl> + set_allow_plugin_skip ( false ); <nl> } <nl>  <nl> void tmp_create_game :: pre_show ( twindow & window ) <nl> void tmp_create_game :: pre_show ( twindow & window ) <nl> // <nl> // Set up the Lua plugin context <nl> // <nl> - set_allow_plugin_skip ( false ); <nl> plugins_context_ . reset ( new plugins_context (" Multiplayer Create ")); <nl>  <nl> plugins_context_ -> set_callback (" create ", [& window ]( const config &) { window . set_retval ( twindow :: OK ); }, false );mmm src / gui / dialogs / title_screen . cpp <nl> ppp src / gui / dialogs / title_screen . cpp <nl> tlobby_main :: tlobby_main ( const config & game_config , <nl> { <nl> // Need to set this in the constructor , pre_show () is too late <nl> set_show_even_without_video ( true ); <nl> + set_allow_plugin_skip ( false ); <nl> } <nl>  <nl> struct lobby_delay_gamelist_update_guard <nl> void tlobby_main :: pre_show ( twindow & window ) <nl> game_config :: lobby_network_timer , std :: bind (& tlobby_main :: network_handler , this ), true ); <nl>  <nl> // Set up Lua plugin context <nl> - set_allow_plugin_skip ( false ); <nl> plugins_context_ . reset ( new plugins_context (" Multiplayer Lobby ")); <nl>  <nl> plugins_context_ -> set_callback (" join ", [&, this ]( const config &) { <nl> tmp_create_game :: tmp_create_game ( const config & cfg , ng :: create_engine & create_en <nl>  <nl> create_engine_ . get_state () = saved_game (); <nl> create_engine_ . get_state (). classification (). campaign_type = game_classification :: CAMPAIGN_TYPE :: MULTIPLAYER ; <nl> + <nl> + // Need to set this in the constructor , pre_show () is too late <nl> + set_allow_plugin_skip ( false ); <nl> } <nl>  <nl> void tmp_create_game :: pre_show ( twindow & window ) <nl> void tmp_create_game :: pre_show ( twindow & window ) <nl> // <nl> // Set up the Lua plugin context <nl> // <nl> - set_allow_plugin_skip ( false ); <nl> plugins_context_ . reset ( new plugins_context (" Multiplayer Create ")); <nl>  <nl> plugins_context_ -> set_callback (" create ", [& window ]( const config &) { window . set_retval ( twindow :: OK ); }, false ); <nl> void ttitle_screen :: basic_callback ( twindow & window , tresult res ) <nl>  <nl> ttitle_screen :: ttitle_screen ( game_launcher & game ) : result_ ( REDRAW_BACKGROUND ), game_ ( game ), debug_clock_ ( nullptr ) <nl> { <nl> + // Need to set this in the constructor , pre_show () / post_build () is too late <nl> + set_allow_plugin_skip ( false ); <nl> } <nl>  <nl> ttitle_screen ::~ ttitle_screen () <nl> debug_tooltip ( twindow & window , bool & handled , const tpoint & coordinate ) <nl> void ttitle_screen :: pre_show ( twindow & window ) <nl> { <nl> set_restore ( false ); <nl> - set_allow_plugin_skip ( false ); <nl> window . set_click_dismiss ( false ); <nl> window . set_enter_disabled ( true ); <nl> window . set_escape_disabled ( true );
mmm src / actions . cpp <nl> ppp src / actions . cpp <nl> void calculate_healing ( display & disp , const gamemap & map , <nl>  <nl> int pos_max = i -> second . max_hitpoints () - i -> second . hitpoints (); <nl> int neg_max = -( i -> second . hitpoints () - 1 ); <nl> + if ( healing > 0 && pos_max <= 0 ) { <nl> + // Do not try to " heal " if HP >= max HP <nl> + continue ; <nl> + } <nl> if ( healing > pos_max ) { <nl> healing = pos_max ; <nl> } else if ( healing < neg_max ) {
mmm src / game_events . cpp <nl> ppp src / game_events . cpp <nl> bool event_handler :: handle_event_command ( const queued_event & event_info , <nl> const int side = lexical_cast_default < int >( side_str . value (), - 1 ); <nl>  <nl> // Select advancement if it is on the playing side and the player is a human <nl> - const bool sel = ( side == u . side () && (* teams )[ side - 1 ]. is_human ()); <nl> + const bool sel = ( side == static_cast < int >( u . side ()) <nl> + && (* teams )[ side - 1 ]. is_human ()); <nl>  <nl> // The code in dialogs :: advance_unit tests whether the unit can advance <nl> dialogs :: advance_unit (* game_data_ptr , * game_map , * units , loc , * screen , ! sel , true );
mmm src / terrain_filter . cpp <nl> ppp src / terrain_filter . cpp <nl> namespace { <nl> bool terrain_matches_filter ( const gamemap & map , const gamemap :: location & loc , const vconfig & cfg , <nl> const gamestatus & game_status , const unit_map & units , const bool flat_tod , <nl> const size_t max_loop ) <nl> -{ <nl> +{ <nl> + if (! map . on_board ( loc )) return false ; <nl> + <nl> // handle radius <nl> const size_t radius = minimum < size_t >( max_loop , <nl> lexical_cast_default < size_t >( cfg [" radius "], 0 ));
mmm src / image . cpp <nl> ppp src / image . cpp <nl> static void precache_file_existence_internal ( const std :: string & dir , const std :: <nl> return ; <nl> precached_dirs . insert ( checked_dir ); <nl>  <nl> + if (! filesystem :: is_directory ( checked_dir )) <nl> + return ; <nl> + <nl> std :: vector < std :: string > files_found ; <nl> std :: vector < std :: string > dirs_found ; <nl> filesystem :: get_files_in_dir ( checked_dir , & files_found , & dirs_found ,
mmm src / savegame . cpp <nl> ppp src / savegame . cpp <nl> # include " foreach . hpp " <nl> # include " game_end_exceptions . hpp " <nl> # include " game_events . hpp " <nl> -# include " game_preferences . hpp " // FIXME : get rid of this one <nl> # include " gettext . hpp " <nl> # include " gui / dialogs / game_save . hpp " <nl> # include " gui / widgets / window . hpp " <nl> bool savegame :: save_game_interactive ( display & gui , const std :: string & message , <nl> } <nl>  <nl> std :: string filename = filename_ ; <nl> - if ( res == gui2 :: twindow :: OK && savegame_manager :: save_game_exists ( filename , preferences :: compress_saves ())) { <nl> + if ( res == gui2 :: twindow :: OK && savegame_manager :: save_game_exists ( filename , compress_saves_ )) { <nl> std :: stringstream s ; <nl> s << _ (" Save already exists . Do you want to overwrite it ?") <nl> << std :: endl << _ (" Name : ") << filename ;
mmm src / hotkey / hotkey_command . cpp <nl> ppp src / hotkey / hotkey_command . cpp <nl> hotkey :: hotkey_command_temp hotkey_list_ [] = { <nl> { hotkey :: HOTKEY_CLEAR_MSG , " clearmessages ", N_ (" Clear Messages "), false , hotkey :: SCOPE_GAME , "" }, <nl>  <nl> { hotkey :: HOTKEY_LANGUAGE , " changelanguage ", N_ (" Change Language "), false , hotkey :: SCOPE_MAIN_MENU , "" }, <nl> - { hotkey :: TITLE_SCREEN__RELOAD_WML , " title_screen__reload_wml ", N_ (" Refresh WML "), true , hotkey :: SCOPE_MAIN_MENU , "" }, <nl> + { hotkey :: TITLE_SCREEN__RELOAD_WML , " title_screen__reload_wml ", N_ (" Refresh WML "), true , hotkey :: SCOPE_GENERAL , "" }, <nl> { hotkey :: TITLE_SCREEN__NEXT_TIP , " title_screen__next_tip ", N_ (" Next Tip of the Day "), false , hotkey :: SCOPE_MAIN_MENU , "" }, <nl> { hotkey :: TITLE_SCREEN__PREVIOUS_TIP , " title_screen__previous_tip ", N_ (" Previous Tip of the Day "), false , hotkey :: SCOPE_MAIN_MENU , "" }, <nl> { hotkey :: TITLE_SCREEN__TUTORIAL , " title_screen__tutorial ", N_ (" Start Tutorial "), false , hotkey :: SCOPE_MAIN_MENU , "" },
mmm src / network_asio . hpp <nl> ppp src / network_asio . hpp <nl> class connection <nl> std :: size_t bytes_transferred , <nl> config & response <nl> ); <nl> - boost :: optional < std :: size_t > bytes_to_read_ ; <nl> + std :: size_t bytes_to_read_ ; <nl> std :: size_t bytes_read_ ; <nl>  <nl> public : <nl> class connection <nl> /** True if connected and no high - level operation is in progress */ <nl> bool done () const { return done_ ; } <nl>  <nl> - const boost :: optional < std :: size_t >& bytes_to_read () const <nl> + std :: size_t bytes_to_read () const <nl> { <nl> return bytes_to_read_ ; <nl> }mmm src / network_asio . cpp <nl> ppp src / network_asio . cpp <nl> class connection <nl> std :: size_t bytes_transferred , <nl> config & response <nl> ); <nl> - boost :: optional < std :: size_t > bytes_to_read_ ; <nl> + std :: size_t bytes_to_read_ ; <nl> std :: size_t bytes_read_ ; <nl>  <nl> public : <nl> class connection <nl> /** True if connected and no high - level operation is in progress */ <nl> bool done () const { return done_ ; } <nl>  <nl> - const boost :: optional < std :: size_t >& bytes_to_read () const <nl> + std :: size_t bytes_to_read () const <nl> { <nl> return bytes_to_read_ ; <nl> } <nl> connection :: connection ( const std :: string & host , const std :: string & service ) <nl> , write_buf_ () <nl> , read_buf_ () <nl> , handshake_response_ () <nl> - , bytes_to_read_ () <nl> + , bytes_to_read_ ( 0 ) <nl> , bytes_read_ ( 0 ) <nl> { <nl> resolver_ . async_resolve ( <nl> std :: size_t connection :: is_read_complete ( <nl> bytes_to_read_ = ntohl ( data_size . num ) + 4 ; <nl> } <nl> # if BOOST_VERSION >= 103700 <nl> - return bytes_to_read_ . get () - bytes_transferred ; <nl> + return bytes_to_read_ - bytes_transferred ; <nl> # else <nl> - return bytes_to_read_ . get () == bytes_transferred ; <nl> + return bytes_to_read_ == bytes_transferred ; <nl> # endif <nl> } <nl> } <nl> void connection :: handle_read ( <nl> ) <nl> { <nl> std :: cout << " Read " << bytes_transferred << " bytes .\ n "; <nl> - bytes_to_read_ . reset (); <nl> + bytes_to_read_ = 0 ; <nl> done_ = true ; <nl> if ( ec && ec != boost :: asio :: error :: eof ) <nl> throw error ( ec );mmm src / gui / dialogs / network_transmission . cpp <nl> ppp src / gui / dialogs / network_transmission . cpp <nl> class connection <nl> std :: size_t bytes_transferred , <nl> config & response <nl> ); <nl> - boost :: optional < std :: size_t > bytes_to_read_ ; <nl> + std :: size_t bytes_to_read_ ; <nl> std :: size_t bytes_read_ ; <nl>  <nl> public : <nl> class connection <nl> /** True if connected and no high - level operation is in progress */ <nl> bool done () const { return done_ ; } <nl>  <nl> - const boost :: optional < std :: size_t >& bytes_to_read () const <nl> + std :: size_t bytes_to_read () const <nl> { <nl> return bytes_to_read_ ; <nl> } <nl> connection :: connection ( const std :: string & host , const std :: string & service ) <nl> , write_buf_ () <nl> , read_buf_ () <nl> , handshake_response_ () <nl> - , bytes_to_read_ () <nl> + , bytes_to_read_ ( 0 ) <nl> , bytes_read_ ( 0 ) <nl> { <nl> resolver_ . async_resolve ( <nl> std :: size_t connection :: is_read_complete ( <nl> bytes_to_read_ = ntohl ( data_size . num ) + 4 ; <nl> } <nl> # if BOOST_VERSION >= 103700 <nl> - return bytes_to_read_ . get () - bytes_transferred ; <nl> + return bytes_to_read_ - bytes_transferred ; <nl> # else <nl> - return bytes_to_read_ . get () == bytes_transferred ; <nl> + return bytes_to_read_ == bytes_transferred ; <nl> # endif <nl> } <nl> } <nl> void connection :: handle_read ( <nl> ) <nl> { <nl> std :: cout << " Read " << bytes_transferred << " bytes .\ n "; <nl> - bytes_to_read_ . reset (); <nl> + bytes_to_read_ = 0 ; <nl> done_ = true ; <nl> if ( ec && ec != boost :: asio :: error :: eof ) <nl> throw error ( ec ); <nl> void tnetwork_transmission :: pump_monitor :: process ( events :: pump_info &) <nl> window_ . get (). set_retval ( twindow :: OK ); <nl> } else { <nl> if ( connection_ . bytes_to_read ()) { <nl> - size_t total = connection_ . bytes_to_read (). get (); <nl> + size_t total = connection_ . bytes_to_read (); <nl> size_t completed = connection_ . bytes_read (); <nl> find_widget < tprogress_bar >(&( window_ . get ()), " progress ", false ) <nl> . set_percentage (( completed * 100 )/ total );
mmm src / filesystem . cpp <nl> ppp src / filesystem . cpp <nl> std :: string get_wml_location ( const std :: string & filename , const std :: string & cur <nl> return result ; <nl> } <nl>  <nl> - if ( ends_with ( filename , ". pbl ")) { <nl> + if ( looks_like_pbl ( filename )) { <nl> ERR_FS << " Illegal path '" << filename << "' (. pbl files are not allowed )." << std :: endl ; <nl> return result ; <nl> }mmm src / filesystem_boost . cpp <nl> ppp src / filesystem_boost . cpp <nl> std :: string get_wml_location ( const std :: string & filename , const std :: string & cur <nl> return result ; <nl> } <nl>  <nl> - if ( ends_with ( filename , ". pbl ")) { <nl> + if ( looks_like_pbl ( filename )) { <nl> ERR_FS << " Illegal path '" << filename << "' (. pbl files are not allowed )." << std :: endl ; <nl> return result ; <nl> } <nl> static bool is_legal_file ( const std :: string & filename ) <nl> return false ; <nl> } <nl>  <nl> - if ( ends_with ( filename , ". pbl ")) { <nl> + if ( looks_like_pbl ( filename )) { <nl> ERR_FS << " Illegal path '" << filename << "' (. pbl files are not allowed )." << std :: endl ; <nl> return false ; <nl> }
mmm src / formula_ai . cpp <nl> ppp src / formula_ai . cpp <nl> public : <nl> class fallback_function : public function_expression { <nl> public : <nl> explicit fallback_function ( const args_list & args ) <nl> - : function_expression (" fallback ", args , 1 , 1 ) <nl> + : function_expression (" fallback ", args , 0 , 1 ) <nl> {} <nl> private : <nl> variant execute ( const formula_callable & variables ) const { <nl> + if ( args (). size () == 0 ) <nl> + return variant ( new fallback_callable ("")); <nl> return variant ( new fallback_callable ( args ()[ 0 ]-> evaluate ( variables ). as_string ())); <nl> } <nl> };
mmm src / playlevel . cpp <nl> ppp src / playlevel . cpp <nl> redo_turn : <nl> info [" type "] = " termination "; <nl> info [" condition "] = " game over "; <nl> network :: send_data ( cfg ); <nl> - } else <nl> + } else { <nl> gui :: show_dialog ( gui , NULL , _ (" Game Over "), <nl> _ (" The game is over ."), gui :: OK_ONLY ); <nl> + return QUIT ; <nl> + } <nl> } <nl>  <nl> if ( end_level . result == QUIT ) {
mmm src / pathfind . cpp <nl> ppp src / pathfind . cpp <nl> static void find_routes ( const gamemap & map , const unit_map & units , <nl> if ( old_move_left >= new_turns_moves + new_move_left ) <nl> continue ; <nl>  <nl> + paths :: route & src_route = routes [ loc ]; <nl> + <nl> if (! ignore_units ) { <nl> // we can not traverse enemies <nl> const unit_map :: const_iterator unit_it = <nl> static void find_routes ( const gamemap & map , const unit_map & units , <nl> && enemy_zoc ( map , units , teams , currentloc , viewing_team , u . side (), see_all ) <nl> && ! u . get_ability_bool (" skirmisher ", currentloc )) { <nl> new_move_left = 0 ; <nl> - // Recheck if we already have a better route , but now with the ZoC effect <nl> - if ( old_move_left >= new_turns_moves + 0 ) <nl> + // Recheck if we already have a better route , but now with the ZoC effect . <nl> + // Since the ZOC is cancelling the remaining move points , the game cannot <nl> + // notice a difference between a short and a long path . So check the path <nl> + // length too in case of equality . <nl> + if ( old_move_left > new_turns_moves + 0 || <nl> + ( old_move_left == new_turns_moves + 0 && <nl> + old_rt -> second . steps . size () <= src_route . steps . size () + 1 )) <nl> continue ; <nl> } <nl> } <nl>  <nl> - paths :: route & src_route = routes [ loc ]; <nl> paths :: route & new_route = routes [ currentloc ]; <nl> new_route . steps = src_route . steps ; <nl> new_route . steps . push_back ( loc );
mmm src / sdl / surface . hpp <nl> ppp src / sdl / surface . hpp <nl> public : <nl> return * this ; <nl> } <nl>  <nl> + // Intended to be used when SDL has already freed the surface <nl> + void clear_without_free () { surface_ = nullptr ; } <nl> + <nl> operator SDL_Surface *() const { return surface_ ; } <nl>  <nl> SDL_Surface * get () const { return surface_ ; }mmm src / video . cpp <nl> ppp src / video . cpp <nl> public : <nl> return * this ; <nl> } <nl>  <nl> + // Intended to be used when SDL has already freed the surface <nl> + void clear_without_free () { surface_ = nullptr ; } <nl> + <nl> operator SDL_Surface *() const { return surface_ ; } <nl>  <nl> SDL_Surface * get () const { return surface_ ; } <nl> void CVideo :: update_framebuffer () <nl> if (! frameBuffer ) { <nl> frameBuffer = fb ; <nl> } else { <nl> + // Because SDL has already freed the old framebuffer , <nl> + // ensure that we won ' t attempt to free it . <nl> + frameBuffer . clear_without_free (); <nl> frameBuffer . assign ( fb ); <nl> } <nl> }
mmm src / unit_display . cpp <nl> ppp src / unit_display . cpp <nl> void unit_attack ( <nl> int damage_left = damage ; <nl> while ( damage_left > 0 && ! animator . would_end ()) { <nl> int step_left = ( animator . get_end_time () - animator . get_animation_time () )/ 50 ; <nl> - int removed_hp = damage_left / step_left ; <nl> + int removed_hp = step_left ? damage_left / step_left : 1 ; <nl> if ( removed_hp < 1 ) removed_hp = 1 ; <nl> if ( step_left < 1 ) step_left = 1 ; <nl> defender . take_hit ( removed_hp );
mmm src / mouse_events . cpp <nl> ppp src / mouse_events . cpp <nl> paths :: route mouse_handler :: get_route ( unit_map :: const_iterator un , map_location <nl>  <nl> void mouse_handler :: mouse_press ( const SDL_MouseButtonEvent & event , const bool browse ) <nl> { <nl> - if ( attackmove_ ) return ; <nl> mouse_handler_base :: mouse_press ( event , browse ); <nl> } <nl>  <nl> bool mouse_handler :: left_click ( int x , int y , const bool browse ) <nl>  <nl> gui (). unhighlight_reach (); <nl> move_unit_along_current_route ( check_shroud ); <nl> - } else { <nl> + } else if (! attackmove_ ) { <nl> + // we block selection during attack + move ( because motion is blocked ) <nl> + // FIXME : deal with selected event when commands_disabled <nl> // we select a ( maybe empty ) hex <nl> select_hex ( hex , browse ); <nl> }
mmm src / network_worker . cpp <nl> ppp src / network_worker . cpp <nl> SOCKET_STATE receive_buf ( TCPsocket sock , std :: vector < char >& buf ) <nl> } <nl> } <nl>  <nl> - const ssize_t res = SDLNet_TCP_Recv ( sock , beg , end - beg ); <nl> + const int res = SDLNet_TCP_Recv ( sock , beg , end - beg ); <nl> if ( res <= 0 ) { <nl> if ( SDLNet_CheckSockets ( set , 15000 ) <= 0 ) { <nl> ERR_NW << " SDLNet_CheckSockets : " << strerror ( errno ) << "\ n ";
mmm src / network_worker . cpp <nl> ppp src / network_worker . cpp <nl> manager ::~ manager () <nl> cond [ shard ]-> notify_all (); <nl>  <nl> for ( std :: map < Uint32 , threading :: thread *>:: const_iterator i = threads [ shard ]. begin (); i != threads [ shard ]. end (); ++ i ) { <nl> + <nl> DBG_NW << " waiting for thread " << i -> first << " to exit ...\ n "; <nl> delete i -> second ; <nl> DBG_NW << " thread exited ...\ n "; <nl> manager ::~ manager () <nl> // will access memory already freed by way of <nl> // stale mutex . Bad things will follow . ;) <nl> threads [ shard ]. clear (); <nl> + // Have to clean up to_clear so no bogus clearing of threads <nl> + to_clear [ shard ]. clear (); <nl> delete cond [ shard ]; <nl> cond [ shard ] = NULL ; <nl> delete shard_mutexes [ shard ];
mmm src / image . cpp <nl> ppp src / image . cpp <nl> surface locator :: load_image_sub_file () const <nl> } <nl> } <nl> else { <nl> - // Deprecated 1 . 6 palette switch syntax <nl> + ///@ Deprecated 1 . 6 palette switch syntax <nl> if ( field . find ('=') != std :: string :: npos ) { <nl> lg :: wml_error << " the ~ RC () image function cannot be used for palette switch ( A = B ) in 1 . 7 . x ; use ~ PAL ( A > B ) instead \ n "; <nl> }
mmm src / menu_events . cpp <nl> ppp src / menu_events . cpp <nl> void console_handler :: do_set_var () { <nl> } <nl> } <nl> void console_handler :: do_show_var () { <nl> - gui2 :: show_transient_message ((* menu_handler_ . gui_ ). video (),"", resources :: gamedata -> get_variable ( get_data ())); <nl> + gui2 :: show_transient_message ((* menu_handler_ . gui_ ). video (),"", resources :: gamedata -> get_variable_const ( get_data ())); <nl> } <nl>  <nl> 
mmm src / actions / attack . cpp <nl> ppp src / actions / attack . cpp <nl> int battle_context :: choose_attacker_weapon ( const unit & attacker , <nl> attacker_combatant_ = new combatant (* attacker_stats_ ); <nl> defender_combatant_ = new combatant (* defender_stats_ , prev_def ); <nl> attacker_combatant_ -> fight (* defender_combatant_ ); <nl> + } else { <nl> + if ( attacker_stats_ -> disable ) { <nl> + delete attacker_stats_ ; <nl> + attacker_stats_ = nullptr ; <nl> + continue ; <nl> + } <nl> } <nl> if (! best_att_comb || better_combat (* attacker_combatant_ , * defender_combatant_ , <nl> * best_att_comb , * best_def_comb , harm_weight )) {
mmm src / multiplayer_lobby . cpp <nl> ppp src / multiplayer_lobby . cpp <nl> void gamebrowser :: draw_row ( const size_t index , const SDL_Rect & item_rect , ROW_TY <nl> xpos += vision_icon -> w + h_padding_ ; <nl>  <nl> const surface status_text ( font :: get_rendered_text ( game . status , font :: SIZE_NORMAL , font_color )); <nl> - const surface vision_text ( font :: get_rendered_text ( font :: make_text_ellipsis ( game . vision , font :: SIZE_NORMAL , maximum < int >(( item_rect . x + item_rect . w - margin_ - status_text -> w - 2 * h_padding_ ) - xpos , 0 )), font :: SIZE_NORMAL , font :: NORMAL_COLOUR )); <nl> + <nl> + const int status_text_width = status_text ? status_text -> w : 0 ; <nl> + const surface vision_text ( font :: get_rendered_text ( font :: make_text_ellipsis ( game . vision , font :: SIZE_NORMAL , maximum < int >(( item_rect . x + item_rect . w - margin_ - status_text_width - 2 * h_padding_ ) - xpos , 0 )), font :: SIZE_NORMAL , font :: NORMAL_COLOUR )); <nl> // draw vision text <nl> video (). blit_surface ( xpos , ypos , vision_text ); <nl>  <nl> // draw status text <nl> - xpos = item_rect . x + item_rect . w - margin_ - status_text -> w ; <nl> - video (). blit_surface ( xpos , ypos , status_text ); <nl> + xpos = item_rect . x + item_rect . w - margin_ - status_text_width ; <nl> + if ( status_text ) { <nl> + video (). blit_surface ( xpos , ypos , status_text ); <nl> + } <nl>  <nl> // if ( selected_ == index ) <nl> // draw_solid_tinted_rectangle ( item_rect . x , item_rect . y , item_rect . w , item_rect . h , 153 , 0 , 0 , 0 . 3 , video (). getSurface ());
mmm src / gui / widgets / scrollbar_container . cpp <nl> ppp src / gui / widgets / scrollbar_container . cpp <nl> static void set_scrollbar_mode ( tgrid * scrollbar_grid , tscrollbar_ * scrollbar , <nl> return ; <nl> } <nl>  <nl> - <nl> scrollbar -> set_item_count ( items ); <nl> + scrollbar -> set_item_position ( 0 ); <nl> scrollbar -> set_visible_items ( visible_items ); <nl>  <nl> if ( scrollbar_mode == tscrollbar_container :: auto_visible ) {
mmm src / gui / widgets / widget . cpp <nl> ppp src / gui / widgets / widget . cpp <nl> void widget :: set_visible ( const visibility visible ) <nl> visible_ = visible ; <nl>  <nl> if ( need_resize ) { <nl> - if ( new_widgets ) { <nl> + if ( visible == visibility :: visible && new_widgets ) { <nl> event :: message message ; <nl> fire ( event :: REQUEST_PLACEMENT , * this , message ); <nl> } else {
mmm src / mouse_handler_base . hpp <nl> ppp src / mouse_handler_base . hpp <nl> public : <nl> * @ returns true when the click should not process the event further . <nl> * This means do not treat the call as a start of drag movement . <nl> */ <nl> - virtual bool right_click ( int /* x */, int /* y */, const bool /* browse */) <nl> + virtual bool right_click ( int x , int y , const bool browse ) <nl> { <nl> - return true ; <nl> + return right_click_show_menu ( x , y , browse ); <nl> } <nl>  <nl> /**
mmm src / upload_log . cpp <nl> ppp src / upload_log . cpp <nl> upload_log ::~ upload_log () <nl> if ( game_finished ( game_ )) <nl> config_ . add_child (" game ", * game_ ); <nl>  <nl> + if ( game_ ) <nl> + delete game_ ; <nl> + <nl> if ( enabled_ && ! config_ . empty () && ! game_config :: debug ) { <nl> config_ [" version "] = VERSION ; <nl> config_ [" format_version "] = " 1 "; <nl> void upload_log :: start ( game_state & state , const team & team , <nl> if ( game_finished ( game_ )) <nl> config_ . add_child (" game ", * game_ ); <nl>  <nl> + <nl> + if ( game_ ) <nl> + delete game_ ; <nl> game_ = new config (); <nl> (* game_ )[" time "] = lexical_cast < std :: string >( SDL_GetTicks () / 1000 ); <nl> (* game_ )[" campaign "] = state . campaign_define ;
mmm src / image . cpp <nl> ppp src / image . cpp <nl> static bool localized_file_uptodate ( const std :: string & loc_file ) <nl> // First call , parse track index to collect fuzzy files by path . <nl> std :: string fsep = "\ xC2 \ xA6 "; // UTF - 8 for " broken bar " <nl> std :: string trackpath = filesystem :: get_binary_file_location ("", " l10n - track "); <nl> + <nl> + // l10n - track file not present . Assume image is up - to - date . <nl> + if ( trackpath . empty ()) { <nl> + return true ; <nl> + } <nl> + <nl> std :: string contents = filesystem :: read_file ( trackpath ); <nl>  <nl> for ( const std :: string & line : utils :: split ( contents , '\ n ')) {
mmm src / sdl / alpha . cpp <nl> ppp src / sdl / alpha . cpp <nl> int SDL_SetAlpha ( SDL_Surface * surface , Uint32 flag , Uint8 alpha ) <nl> { <nl> if ( flag & SDL_SRCALPHA ) { <nl> + // Need to specify the alpha blend mode if not setting alpha as opaque <nl> + int blendModeResult = SDL_SetSurfaceBlendMode ( surface , SDL_BLENDMODE_BLEND ); <nl> + if ( blendModeResult != 0 ) <nl> + return blendModeResult ; <nl> + <nl> return SDL_SetSurfaceAlphaMod ( surface , alpha ); <nl> } else { <nl> return SDL_SetSurfaceAlphaMod ( surface , SDL_ALPHA_OPAQUE );
mmm src / multiplayer_wait . cpp <nl> ppp src / multiplayer_wait . cpp <nl> void wait :: start_game () <nl>  <nl> LOG_NW << " starting game \ n "; <nl> sound :: play_UI_sound ( game_config :: sounds :: mp_game_begins ); <nl> + game_display :: get_singleton ()-> send_notification ( _ (" Wesnoth "), _ (" Game has begun !")); <nl> } <nl>  <nl> void wait :: layout_children ( const SDL_Rect & rect )
mmm src / unit_display . cpp <nl> ppp src / unit_display . cpp <nl> void unit_attack ( <nl> int swing , std :: string hit_text ) <nl> { <nl> game_display * disp = game_display :: get_singleton (); <nl> - if (! disp ) return ; <nl> + if (! disp || preferences :: show_combat () == false ) return ; <nl> unit_map & units = disp -> get_units (); <nl> disp -> select_hex ( gamemap :: location :: null_location ); <nl> - const bool hide = disp -> video (). update_locked () || disp -> fogged ( a ) && disp -> fogged ( b ) <nl> - || preferences :: show_combat () == false ; <nl> + const bool hide = disp -> video (). update_locked () || disp -> fogged ( a ) && disp -> fogged ( b ); <nl>  <nl> if (! hide ) { <nl> disp -> scroll_to_tiles ( a , b , game_display :: ONSCREEN ); <nl> void unit_attack ( <nl> } <nl> } <nl>  <nl> - <nl> - <nl> - <nl> - <nl> animator . start_animations (); <nl> animator . wait_for_end (); <nl> 
mmm src / editor / editor_controller . cpp <nl> ppp src / editor / editor_controller . cpp <nl> bool editor_controller :: can_execute_command ( const hotkey :: hotkey_command & cmd , i <nl> std :: string dummy ; <nl> return context_manager_ -> modified_maps ( dummy ) > 1 ; <nl> } <nl> - case HOTKEY_EDITOR_MAP_SWITCH : <nl> case HOTKEY_EDITOR_PLAYLIST : <nl> + case HOTKEY_EDITOR_SCHEDULE : <nl> + return ! context_manager_ -> get_map_context (). is_pure_map (); <nl> + case HOTKEY_EDITOR_MAP_SWITCH : <nl> case HOTKEY_EDITOR_MAP_CLOSE : <nl> return true ; <nl> case HOTKEY_EDITOR_MAP_REVERT :mmm src / hotkey / hotkey_command . cpp <nl> ppp src / hotkey / hotkey_command . cpp <nl> bool editor_controller :: can_execute_command ( const hotkey :: hotkey_command & cmd , i <nl> std :: string dummy ; <nl> return context_manager_ -> modified_maps ( dummy ) > 1 ; <nl> } <nl> - case HOTKEY_EDITOR_MAP_SWITCH : <nl> case HOTKEY_EDITOR_PLAYLIST : <nl> + case HOTKEY_EDITOR_SCHEDULE : <nl> + return ! context_manager_ -> get_map_context (). is_pure_map (); <nl> + case HOTKEY_EDITOR_MAP_SWITCH : <nl> case HOTKEY_EDITOR_MAP_CLOSE : <nl> return true ; <nl> case HOTKEY_EDITOR_MAP_REVERT : <nl> hotkey :: hotkey_command_temp hotkey_list_ [] = { <nl> // These are not really hotkey items but menu entries to get expanded . <nl> // They need to have their own hotkey to control their active state . <nl> { hotkey :: HOTKEY_EDITOR_PLAYLIST , " editor - playlist ", N_ (" Switch Time of Day "), true , scope_editor , "" }, <nl> + { hotkey :: HOTKEY_EDITOR_SCHEDULE , " menu - editor - schedule ", "", true , hotkey :: SCOPE_EDITOR , "" }, <nl> { hotkey :: HOTKEY_EDITOR_MAP_SWITCH , " editor - switch - map ", N_ (" Switch Map "), true , scope_editor , "" }, <nl> { hotkey :: HOTKEY_EDITOR_LOCAL_TIME , " menu - editor - local - time ", N_ (" Assign Local Time "), true , scope_editor , "" }, <nl> mmm src / hotkey / hotkey_command . hpp <nl> ppp src / hotkey / hotkey_command . hpp <nl> bool editor_controller :: can_execute_command ( const hotkey :: hotkey_command & cmd , i <nl> std :: string dummy ; <nl> return context_manager_ -> modified_maps ( dummy ) > 1 ; <nl> } <nl> - case HOTKEY_EDITOR_MAP_SWITCH : <nl> case HOTKEY_EDITOR_PLAYLIST : <nl> + case HOTKEY_EDITOR_SCHEDULE : <nl> + return ! context_manager_ -> get_map_context (). is_pure_map (); <nl> + case HOTKEY_EDITOR_MAP_SWITCH : <nl> case HOTKEY_EDITOR_MAP_CLOSE : <nl> return true ; <nl> case HOTKEY_EDITOR_MAP_REVERT : <nl> hotkey :: hotkey_command_temp hotkey_list_ [] = { <nl> // These are not really hotkey items but menu entries to get expanded . <nl> // They need to have their own hotkey to control their active state . <nl> { hotkey :: HOTKEY_EDITOR_PLAYLIST , " editor - playlist ", N_ (" Switch Time of Day "), true , scope_editor , "" }, <nl> + { hotkey :: HOTKEY_EDITOR_SCHEDULE , " menu - editor - schedule ", "", true , hotkey :: SCOPE_EDITOR , "" }, <nl> { hotkey :: HOTKEY_EDITOR_MAP_SWITCH , " editor - switch - map ", N_ (" Switch Map "), true , scope_editor , "" }, <nl> { hotkey :: HOTKEY_EDITOR_LOCAL_TIME , " menu - editor - local - time ", N_ (" Assign Local Time "), true , scope_editor , "" }, <nl>  <nl> enum HOTKEY_COMMAND { <nl> HOTKEY_EDITOR_PALETTE_GROUPS , HOTKEY_EDITOR_PALETTE_UPSCROLL , HOTKEY_EDITOR_PALETTE_DOWNSCROLL , <nl>  <nl> HOTKEY_EDITOR_PLAYLIST , <nl> + HOTKEY_EDITOR_SCHEDULE , <nl> HOTKEY_EDITOR_LOCAL_TIME , <nl> HOTKEY_EDITOR_UNIT_FACING , <nl> 
mmm src / config_adapter . cpp <nl> ppp src / config_adapter . cpp <nl> void get_player_info ( const config & cfg , game_state & gamestate , std :: string save_ <nl> LOG_NG << " found gold : '" << gold << "'\ n "; <nl>  <nl> int ngold = lexical_cast_default < int >( gold ); <nl> - if ( player != NULL && player -> gold >= ngold ) { <nl> + if ( ( player != NULL && player -> gold >= ngold ) || snapshot ) { <nl> ngold = player -> gold ; <nl> } <nl> 
mmm src / team . cpp <nl> ppp src / team . cpp <nl> void team :: team_info :: write ( config & cfg ) const <nl> cfg [" hidden "] = hidden ; <nl> cfg [" suppress_end_turn_confirmation "] = no_turn_confirmation ; <nl> cfg [" scroll_to_leader "] = scroll_to_leader ; <nl> - cfg [" controller "] = controller_string (); <nl> + cfg [" controller "] = ( controller == IDLE ? " human " : controller_string ()); <nl>  <nl> std :: stringstream can_recruit_str ; <nl> for ( std :: set < std :: string >:: const_iterator cr = can_recruit . begin (); cr != can_recruit . end (); ++ cr ) {
mmm src / serialization / schema_validator . cpp <nl> ppp src / serialization / schema_validator . cpp <nl> std :: string at ( const std :: string & file , int line ){ <nl>  <nl> void extra_tag_error ( const std :: string & file , int line , <nl> const std :: string & name ){ <nl> - WRN_VL << at ( file , line ) << ": extra tag "<< name << "\ n "; <nl> + ERR_VL << at ( file , line ) << ": extra tag "<< name << "\ n "; <nl> } <nl> void wrong_tag_error ( const std :: string & file , int line , <nl> const std :: string & name ){ <nl> std :: ostringstream ss ; <nl> ss << at ( file , line ) << ": wrong tag "<< name << "\ n "; <nl> - WRN_VL << ss . str (); <nl> + ERR_VL << ss . str (); <nl> // throw twml_exception (" Validation error ", ss . str ()); <nl> } <nl> void missing_tag_error ( const std :: string & file , int line , <nl> const std :: string & name ){ <nl> - WRN_VL << at ( file , line ) << ": missing tag "<< name << "\ n "; <nl> + ERR_VL << at ( file , line ) << ": missing tag "<< name << "\ n "; <nl> } <nl> void extra_key_error ( const std :: string & file , int line , <nl> const std :: string & tag , const std :: string & key <nl> ){ <nl> - WRN_VL << at ( file , line ) << ": In tag "<< tag <nl> + ERR_VL << at ( file , line ) << ": In tag "<< tag <nl> << " which begins here , " << " key "<< key << " wasn ' t allowed \ n "; <nl> } <nl> void missing_key_error ( const std :: string & file , int line , <nl> const std :: string & tag , const std :: string & key <nl> ){ <nl> - WRN_VL << at ( file , line ) << ": In tag "<< tag <nl> + ERR_VL << at ( file , line ) << ": In tag "<< tag <nl> << " which begins here , " << " missing key "<< key << "\ n "; <nl> } <nl> void wrong_value_error ( const std :: string & file , int line , <nl> const std :: string & tag , const std :: string & key , <nl> const std :: string & value ){ <nl> - WRN_VL << at ( file , line ) << ": In tag "<< tag <nl> + ERR_VL << at ( file , line ) << ": In tag "<< tag <nl> << " which begins here , " << " key "<< key << " have wrong value " <nl> << value << "\ n "; <nl> }
mmm src / ai / default / ai . cpp <nl> ppp src / ai / default / ai . cpp <nl> bool ai_default_recruitment_stage :: recruit_usage ( const std :: string & usage ) <nl>  <nl> if ( imc != maximum_counts_ . end ()) { <nl> int count_active = 0 ; <nl> - for ( unit_map :: iterator u = get_info (). units . begin (); u != get_info (). units . end (); u ++) { <nl> + for ( unit_map :: iterator u = get_info (). units . begin (); u != get_info (). units . end (); ++ u ) { <nl> if (( u -> second . side ()== get_side ()) && (! u -> second . incapacitated ()) && ( u -> second . type_id () == name )) { <nl> - count_active ++; <nl> + ++ count_active ; <nl> } <nl> } <nl> 
mmm src / upload_log . cpp <nl> ppp src / upload_log . cpp <nl> void upload_log :: start ( game_state & state , const team & team , <nl> if ( uploader_settings :: new_uploader ) { <nl> // replace newlines in map definition with semicolons so that braindead server - side wml parser doesn ' t get confused <nl> std :: string encoded_map ( map_data ); <nl> - for ( int idx = 0 ; idx < encoded_map . length (); idx ++) { <nl> + for ( size_t idx = 0 ; idx < encoded_map . length (); idx ++) { <nl> if ( encoded_map [ idx ] == '\ n ') <nl> encoded_map [ idx ] = ';'; <nl> }
mmm src / gui / dialogs / drop_down_menu . cpp <nl> ppp src / gui / dialogs / drop_down_menu . cpp <nl> namespace <nl> */ <nl> grid * row_grid = list . get_row_grid ( list . get_selected_row ()); <nl> if ( toggle_button * checkbox = find_widget < toggle_button >( row_grid , " checkbox ", false , false )) { <nl> - checkbox -> set_value_bool (! checkbox -> get_value_bool ()); <nl> + checkbox -> set_value_bool (! checkbox -> get_value_bool (), true ); <nl> } <nl> } <nl> 
mmm src / gamestatus . cpp <nl> ppp src / gamestatus . cpp <nl> void game_state :: get_player_info ( const config & side_cfg , <nl> for ( std :: vector < unit >:: iterator it = player -> available_units . begin (); <nl> it != player -> available_units . end (); ++ it ) { <nl> if ( it -> can_recruit ()) { <nl> - new_unit = * it ; <nl> - new_unit . set_game_context (& units , & map , & tod_mng , & teams ); <nl> player -> available_units . erase ( it ); <nl> break ; <nl> } <nl> } <nl> + } <nl> + <nl> + if ( player_cfg != NULL ) { <nl> for ( std :: vector < unit >:: iterator it = teams . back (). recall_list (). begin (); <nl> it != teams . back (). recall_list (). end (); ++ it ) { <nl> if ( it -> can_recruit ()) { <nl> + new_unit = * it ; <nl> + new_unit . set_game_context (& units , & map , & tod_mng , & teams ); <nl> teams . back (). recall_list (). erase ( it ); <nl> break ; <nl> }
mmm src / display . cpp <nl> ppp src / display . cpp <nl> # include " units / drawer . hpp " <nl> # include " whiteboard / manager . hpp " <nl> # include " show_dialog . hpp " <nl> +# include " gui / dialogs / loadscreen . hpp " <nl>  <nl> # include < SDL_image . h > <nl>  <nl> void display :: handle_window_event ( const SDL_Event & event ) { <nl> } <nl>  <nl> void display :: handle_event ( const SDL_Event & event ) { <nl> + if ( gui2 :: tloadscreen :: displaying ()) { <nl> + return ; <nl> + } <nl> if ( event . type == DRAW_ALL_EVENT ) { <nl> draw (); <nl> }
mmm src / synced_commands . cpp <nl> ppp src / synced_commands . cpp <nl> SYNCED_COMMAND_HANDLER_FUNCTION ( attack , child , /* use_undo */, show , error_handler <nl> } <nl> } <nl>  <nl> - if ( size_t ( weapon_num ) >= u -> attacks (). size ()) { <nl> + if ( static_cast < unsigned >( weapon_num ) >= u -> attacks (). size ()) { <nl> error_handler (" illegal weapon type in attack \ n ", true ); <nl> return false ; <nl> }
mmm src / widgets / scrollpane . cpp <nl> ppp src / widgets / scrollpane . cpp <nl> void scrollpane :: draw () <nl>  <nl> void scrollpane :: scroll ( unsigned int pos ) <nl> { <nl> - if (( int ) pos == content_pos_ . y ) <nl> + if ( static_cast < int >( pos ) == content_pos_ . y ) <nl> return ; <nl>  <nl> content_pos_ . y = pos ;
mmm src / multiplayer_connect . cpp <nl> ppp src / multiplayer_connect . cpp <nl> int mp_connect :: load_map ( const std :: string & era , int map , int num_turns , int vil <nl>  <nl> load_game (* data_ , game , * state_ ); <nl>  <nl> - state_ -> gold = - 10000 ; <nl> state_ -> available_units . clear (); <nl> state_ -> can_recruit . clear (); <nl>  <nl> int mp_connect :: load_map ( const std :: string & era , int map , int num_turns , int vil <nl>  <nl> level_ = level_ptr ; <nl> state_ -> label = level_ -> values [" name "]; <nl> + state_ -> gold = - 10000 ; <nl>  <nl> std :: map < int , std :: string > res_to_id ; <nl> for ( config :: child_list :: const_iterator i = levels . begin (); i != levels . end (); ++ i ){
mmm src / gui / widgets / grid . hpp <nl> ppp src / gui / widgets / grid . hpp <nl> class grid : public widget <nl> public : <nl> explicit grid ( const unsigned rows = 0 , const unsigned cols = 0 ); <nl>  <nl> + /** Delete the copy constructor . */ <nl> + grid ( const grid &) = delete ; <nl> + <nl> + /** Delete the move assignment operator . */ <nl> + grid & operator =( const grid &) = delete ; <nl> + <nl> virtual ~ grid (); <nl>  <nl> /***** ***** ***** ***** LAYOUT FLAGS ***** ***** ***** *****/
mmm src / about . cpp <nl> ppp src / about . cpp <nl> std :: vector < std :: string > get_text () { <nl> " _ " N_ ("+ Catalan Translation "), <nl> "- Carles Company ( brrr )", <nl> "- Dan Rosàs Garcia ( focks )", <nl> + "- Jonatan Alamà ( tin )", <nl> "- Jordà Polo ( ettin )", <nl> "- Mark Recasens ", <nl> "- Pau Rul · lan Ferragut ",
mmm src / widgets / menu . cpp <nl> ppp src / widgets / menu . cpp <nl> size_t menu :: get_item_height ( int ) const <nl>  <nl> void menu :: process_help_string ( int mousex , int mousey ) <nl> { <nl> + if ( hidden ()) return ; <nl> + <nl> const std :: pair < int , int > loc ( hit ( mousex , mousey ), hit_column ( mousex )); <nl> if ( loc == cur_help_ ) { <nl> return ; <nl> void menu :: process_help_string ( int mousex , int mousey ) <nl> help_string_ = - 1 ; <nl> } <nl> if ( size_t ( loc . first ) < items_ . size ()) { <nl> - const std :: vector < std :: string >& row = items_ [ loc . first ]. help ; <nl> + const std :: vector < std :: string >& row = items_ [ item_pos_ [ loc . first ]]. help ; <nl> if ( size_t ( loc . second ) < row . size ()) { <nl> const std :: string & help = row [ loc . second ]; <nl> if ( help . empty () == false ) {
mmm src / events . cpp <nl> ppp src / events . cpp <nl> void pump () <nl> SDL_UserEvent user_event ; <nl> user_event . type = DOUBLE_CLICK_EVENT ; <nl> user_event . code = 0 ; <nl> - user_event . data1 = reinterpret_cast < void *>( event . button . x ); <nl> - user_event . data2 = reinterpret_cast < void *>( event . button . y ); <nl> + user_event . data1 = new int ( event . button . x ); <nl> + user_event . data2 = new int ( event . button . y ); <nl> :: SDL_PushEvent ( reinterpret_cast < SDL_Event *>(& user_event )); <nl> } <nl>  <nl> void raise_draw_event () <nl> } <nl> } <nl>  <nl> -} <nl> \ No newline at end of file <nl> +}mmm src / widgets / menu . cpp <nl> ppp src / widgets / menu . cpp <nl> void pump () <nl> SDL_UserEvent user_event ; <nl> user_event . type = DOUBLE_CLICK_EVENT ; <nl> user_event . code = 0 ; <nl> - user_event . data1 = reinterpret_cast < void *>( event . button . x ); <nl> - user_event . data2 = reinterpret_cast < void *>( event . button . y ); <nl> + user_event . data1 = new int ( event . button . x ); <nl> + user_event . data2 = new int ( event . button . y ); <nl> :: SDL_PushEvent ( reinterpret_cast < SDL_Event *>(& user_event )); <nl> } <nl>  <nl> void raise_draw_event () <nl> } <nl> } <nl>  <nl> -} <nl> \ No newline at end of file <nl> +} <nl> void menu :: handle_event ( const SDL_Event & event ) <nl> x = event . button . x ; <nl> y = event . button . y ; <nl> } else { <nl> - x = reinterpret_cast < int >( event . user . data1 ); <nl> - y = reinterpret_cast < int >( event . user . data2 ); <nl> + x = *( reinterpret_cast < int *>( event . user . data1 )); <nl> + y = *( reinterpret_cast < int *>( event . user . data2 )); <nl> + delete reinterpret_cast < int *>( event . user . data1 ); <nl> + delete reinterpret_cast < int *>( event . user . data2 ); <nl> } <nl>  <nl> const int item = hit ( x , y );
mmm src / game . cpp <nl> ppp src / game . cpp <nl> void game_controller :: reset_game_cfg () <nl> defines_map_ [" APPLE "] = preproc_define (); <nl> # endif <nl>  <nl> - defines_map_ [" NORMAL "] = preproc_define (); <nl> - defines_map_ [" MEDIUM "] = preproc_define (); <nl> - <nl> if ( multiplayer_mode_ ) { <nl> defines_map_ [" MULTIPLAYER "] = preproc_define (); <nl> + } else { <nl> + defines_map_ [" NORMAL "] = preproc_define (); <nl> + defines_map_ [" MEDIUM "] = preproc_define (); <nl> } <nl>  <nl> refresh_game_cfg ();
mmm src / map . cpp <nl> ppp src / map . cpp <nl> bool gamemap :: filter_location ( const gamemap :: location & loc , const config & /* con * <nl> void gamemap :: write_terrain ( const gamemap :: location & loc , config & cfg ) const <nl> { <nl> // will need to be updated for multi - letter terrain -- Sapient <nl> - char * loc_terrain = " "; <nl> + char loc_terrain [] = " "; <nl> * loc_terrain = get_terrain ( loc ); <nl> cfg [" terrain "] = loc_terrain ; <nl> }
mmm src / filesystem_boost . cpp <nl> ppp src / filesystem_boost . cpp <nl> static bool is_legal_file ( const std :: string & filename ) <nl> return false ; <nl> } <nl>  <nl> + if ( ends_with ( filename , ". pbl ")) { <nl> + ERR_FS << " Illegal path '" << filename << "' (. pbl files are not allowed )." << std :: endl ; <nl> + return false ; <nl> + } <nl> + <nl> return true ; <nl> } <nl> mmm src / filesystem . cpp <nl> ppp src / filesystem . cpp <nl> static bool is_legal_file ( const std :: string & filename ) <nl> return false ; <nl> } <nl>  <nl> + if ( ends_with ( filename , ". pbl ")) { <nl> + ERR_FS << " Illegal path '" << filename << "' (. pbl files are not allowed )." << std :: endl ; <nl> + return false ; <nl> + } <nl> + <nl> return true ; <nl> } <nl>  <nl> std :: string get_wml_location ( const std :: string & filename , const std :: string & cur <nl> return result ; <nl> } <nl>  <nl> + if ( ends_with ( filename , ". pbl ")) { <nl> + ERR_FS << " Illegal path '" << filename << "' (. pbl files are not allowed )." << std :: endl ; <nl> + return result ; <nl> + } <nl> + <nl> bool already_found = false ; <nl>  <nl> if ( filename [ 0 ] == '~')
mmm src / editor / palette / editor_palettes . cpp <nl> ppp src / editor / palette / editor_palettes . cpp <nl> template < class Item > <nl> void editor_palette < Item >:: swap () <nl> { <nl> std :: swap ( selected_fg_item_ , selected_bg_item_ ); <nl> + select_fg_item ( selected_fg_item_ ); <nl> + select_bg_item ( selected_bg_item_ ); <nl> + set_dirty (); <nl> } <nl> template void editor_palette < t_translation :: t_terrain >:: swap (); <nl> template void editor_palette < unit_type >:: swap ();mmm src / editor / editor_controller . cpp <nl> ppp src / editor / editor_controller . cpp <nl> template < class Item > <nl> void editor_palette < Item >:: swap () <nl> { <nl> std :: swap ( selected_fg_item_ , selected_bg_item_ ); <nl> + select_fg_item ( selected_fg_item_ ); <nl> + select_bg_item ( selected_bg_item_ ); <nl> + set_dirty (); <nl> } <nl> template void editor_palette < t_translation :: t_terrain >:: swap (); <nl> template void editor_palette < unit_type >:: swap (); <nl> bool editor_controller :: execute_command ( hotkey :: HOTKEY_COMMAND command , int inde <nl> return true ; <nl> case HOTKEY_EDITOR_PALETTE_ITEM_SWAP : <nl> toolkit_ -> get_palette_manager ()-> active_palette (). swap (); <nl> - toolkit_ -> set_mouseover_overlay (); <nl> return true ; <nl> case HOTKEY_EDITOR_PARTIAL_UNDO : <nl> if ( dynamic_cast < const editor_action_chain *>( context_manager_ -> get_map_context (). last_undo_action ()) != NULL ) {
mmm src / widgets / textbox . cpp <nl> ppp src / widgets / textbox . cpp <nl> void textbox :: draw_cursor ( int pos ) const <nl>  <nl> void textbox :: draw () const <nl> { <nl> - if ( location (). h == 0 ) <nl> + if ( location (). x == 0 ) <nl> return ; <nl>  <nl> if ( buffer_ . get () != NULL ) { <nl> void textbox :: draw () const <nl>  <nl> void textbox :: handle_event ( const SDL_Event & event ) <nl> { <nl> + if ( location (). x == 0 ) <nl> + return ; <nl> + <nl> int mousex , mousey ; <nl> SDL_GetMouseState (& mousex ,& mousey ); <nl> 
mmm src / extracts . cpp <nl> ppp src / extracts . cpp <nl> void APar_Print_TrackDetails ( TrackInfo * track_info ) { <nl> } <nl>  <nl> void APar_ExtractDetails ( FILE * isofile , uint8_t optional_output ) { <nl> - char uint32_buffer [ 5 ]; <nl> + char uint32_buffer [ 8 ]; <nl> Trackage track = { 0 }; <nl>  <nl> AtomicInfo * mvhdAtom = APar_FindAtom (" moov . mvhd ", false , VERSIONED_ATOM , 0 );
mmm source / core / ScpFileSystem . cpp <nl> ppp source / core / ScpFileSystem . cpp <nl> void __fastcall TSCPFileSystem :: SCPSink ( const UnicodeString TargetDir , <nl> { <nl> FTerminal -> LogEvent ( FORMAT ( L " Warning : Remote host set a compound pathname '% s '", ( Line ))); <nl> } <nl> + if (( Level == 0 ) && ( OnlyFileName != UnixExtractFileName ( FileName ))) <nl> + { <nl> + SCPError ( LoadStr ( UNREQUESTED_FILE ), False ); <nl> + } <nl>  <nl> FullFileName = SourceDir + OnlyFileName ; <nl> OperationProgress -> SetFile ( FullFileName );mmm source / resource / TextsCore . h <nl> ppp source / resource / TextsCore . h <nl> void __fastcall TSCPFileSystem :: SCPSink ( const UnicodeString TargetDir , <nl> { <nl> FTerminal -> LogEvent ( FORMAT ( L " Warning : Remote host set a compound pathname '% s '", ( Line ))); <nl> } <nl> + if (( Level == 0 ) && ( OnlyFileName != UnixExtractFileName ( FileName ))) <nl> + { <nl> + SCPError ( LoadStr ( UNREQUESTED_FILE ), False ); <nl> + } <nl>  <nl> FullFileName = SourceDir + OnlyFileName ; <nl> OperationProgress -> SetFile ( FullFileName ); <nl> # define S3_STATUS_ACCESS_DENIED 746 <nl> # define UNKNOWN_FILE_ENCRYPTION 747 <nl> # define INVALID_ENCRYPT_KEY 748 <nl> +# define UNREQUESTED_FILE 749 <nl>  <nl> # define CORE_CONFIRMATION_STRINGS 300 <nl> # define CONFIRM_PROLONG_TIMEOUT3 301
mmm wsutil / filesystem . c <nl> ppp wsutil / filesystem . c <nl> create_persconffile_profile ( const char * profilename , char ** pf_dir_path_return ) <nl> * doing a " stat ()" on it . If it ' s a drive letter , <nl> * or if the " stat ()" succeeds , we assume it exists . <nl> */ <nl> - pf_dir_path_copy = pf_dir_path ; <nl> + pf_dir_path_copy = g_strdup ( pf_dir_path ); <nl> pf_dir_parent_path = get_dirname ( pf_dir_path_copy ); <nl> pf_dir_parent_path_len = strlen ( pf_dir_parent_path ); <nl> if ( pf_dir_parent_path_len > 0 <nl> create_persconffile_profile ( const char * profilename , char ** pf_dir_path_return ) <nl> save_errno = errno ; <nl> * pf_dir_path_return = pf_dir_path ; <nl> errno = save_errno ; <nl> + g_free ( pf_dir_path_copy ); <nl> return - 1 ; <nl> } <nl> /* <nl> create_persconffile_profile ( const char * profilename , char ** pf_dir_path_return ) <nl> ret = ws_mkdir ( pf_dir_parent_path , 0755 ); <nl> if ( ret == - 1 ) { <nl> * pf_dir_path_return = pf_dir_parent_path ; <nl> + g_free ( pf_dir_path ); <nl> return - 1 ; <nl> } <nl> }
mmm epan / dissectors / packet - gtpv2 . c <nl> ppp epan / dissectors / packet - gtpv2 . c <nl> dissect_gtpv2_mm_context_utms_qq ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tr <nl> proto_tree_add_item ( tree , hf_gtpv2_ik , tvb , offset , 16 , ENC_NA ); <nl> offset += 16 ; <nl>  <nl> - if ( nr_qua ) <nl> - { <nl> - offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qui ); <nl> + if ( nr_qua ) { <nl> + offset = dissect_gtpv2_authentication_quadruplets ( tvb , tree , offset , nr_qua ); <nl> } <nl>  <nl> if ( nr_qui ) {
mmm epan / dissectors / packet - ipsec . c <nl> ppp epan / dissectors / packet - ipsec . c <nl> dissect_esp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> /* Copy back the Authentication which was not encrypted */ <nl> if ( decrypted_len >= esp_auth_len ) <nl> { <nl> - tvb_memcpy ( tvb , decrypted_data + decrypted_len - esp_auth_len , sizeof ( struct newesp )+ decrypted_len - esp_auth_len , esp_auth_len ); <nl> + tvb_memcpy ( tvb , decrypted_data + decrypted_len - esp_auth_len , ( gint )( sizeof ( struct newesp )+ decrypted_len - esp_auth_len ), esp_auth_len ); <nl> } <nl>  <nl> /* Decryption has finished */
mmm packet - smb - browse . c <nl> ppp packet - smb - browse . c <nl> * Routines for SMB Browser packet dissection <nl> * Copyright 1999 , Richard Sharpe < rsharpe @ ns . aus . com > <nl> * <nl> - * $ Id : packet - smb - browse . c , v 1 . 15 2001 / 08 / 01 03 : 47 : 00 guy Exp $ <nl> + * $ Id : packet - smb - browse . c , v 1 . 16 2001 / 08 / 01 08 : 12 : 15 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> dissect_mailslot_browse ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * parent_tr <nl> proto_tree_add_item ( tree , hf_update_count , tvb , offset , 1 , TRUE ); <nl> offset += 1 ; <nl>  <nl> - /* periodicity */ <nl> + /* periodicity ( in milliseconds ) */ <nl> periodicity = tvb_get_letohl ( tvb , offset ); <nl> proto_tree_add_uint_format ( tree , hf_periodicity , tvb , offset , 4 , <nl> periodicity , <nl> dissect_mailslot_browse ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * parent_tr <nl> * <nl> * The document at <nl> * <nl> - * http :// www . samba . org / samba / ftp / specs / smbpub . txt <nl> + * http :// www . samba . org / samba / ftp / specs / brow_rev . txt <nl> * <nl> * gives both formats of host announcement packets , saying that <nl> * "[ The first ] format seems wrong ", that one being what appears to <nl> dissect_mailslot_lanman ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * parent_tr <nl> proto_tree_add_item ( tree , hf_os_minor , tvb , offset , 1 , TRUE ); <nl> offset += 1 ; <nl>  <nl> - /* periodicity */ <nl> - periodicity = tvb_get_letohs ( tvb , offset ); <nl> + /* periodicity ( in seconds ; convert to milliseconds ) */ <nl> + periodicity = tvb_get_letohs ( tvb , offset )* 1000 ; <nl> proto_tree_add_uint_format ( tree , hf_periodicity , tvb , offset , 2 , <nl> periodicity , <nl> " Update Periodicity : % s ",
mmm packet - esis . c <nl> ppp packet - esis . c <nl> * Routines for ISO / OSI End System to Intermediate System <nl> * Routing Exchange Protocol ISO 9542 . <nl> * <nl> - * $ Id : packet - esis . c , v 1 . 27 2002 / 08 / 28 21 : 00 : 13 jmayer Exp $ <nl> + * $ Id : packet - esis . c , v 1 . 28 2003 / 02 / 25 19 : 07 : 07 guy Exp $ <nl> * Ralf Schneider < Ralf . Schneider @ t - online . de > <nl> * <nl> * Ethereal - Network traffic analyzer <nl> proto_reg_handoff_esis ( void ) <nl> dissector_handle_t esis_handle ; <nl>  <nl> esis_handle = create_dissector_handle ( dissect_esis , proto_esis ); <nl> + register_dissector (" esis ", dissect_esis , proto_esis ); <nl> dissector_add (" osinl ", NLPID_ISO9542_ESIS , esis_handle ); <nl> }
mmm gtk / uat_gui . c <nl> ppp gtk / uat_gui . c <nl> static void uat_edit_dialog ( uat_t * uat , gint row , gboolean copy ) { <nl> if ( copy && row >= 0 ) { <nl> dd -> rec = g_malloc0 ( uat -> record_size ); <nl> if ( uat -> copy_cb ) { <nl> - uat -> copy_cb ( dd -> rec , UAT_INDEX_PTR ( uat , row ), uat -> record_size ); <nl> + uat -> copy_cb ( dd -> rec , UAT_INDEX_PTR ( uat , row ), ( unsigned int ) uat -> record_size ); <nl> } <nl> dd -> is_new = TRUE ; <nl> } else if ( row >= 0 ) {
mmm epan / dissectors / packet - 6lowpan . c <nl> ppp epan / dissectors / packet - 6lowpan . c <nl> dissect_6lowpan_iphc_nhc ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , gi <nl> /* Get and display the checksum . */ <nl> if (!( udp_flags & LOWPAN_NHC_UDP_CHECKSUM )) { <nl> /* Parse the checksum . */ <nl> - udp . checksum = tvb_get_ntohs ( tvb , offset ); <nl> + tvb_memcpy ( tvb , & udp . checksum , offset , sizeof ( udp . checksum )); <nl> proto_tree_add_checksum ( tree , tvb , offset , hf_6lowpan_udp_checksum , - 1 , NULL , pinfo , 0 , ENC_BIG_ENDIAN , PROTO_CHECKSUM_NO_FLAGS ); <nl> offset += 2 ; <nl> }
mmm epan / dissectors / packet - rlc . c <nl> ppp epan / dissectors / packet - rlc . c <nl> translate_hex_key ( gchar * char_key ){ <nl> key_in = g_malloc0 ( sizeof ( guint8 )* 16 ); <nl> /* memset ( key_in , 0 , 16 ); <nl> */ <nl> - j = ( strlen ( char_key )/ 2 )- 1 ; <nl> + j = ( int )( strlen ( char_key )/ 2 )- 1 ; <nl> /* Translate " hex - string " into a byte aligned block */ <nl> - for ( i = strlen ( char_key ); i > 0 ; i -= 2 ){ <nl> + for ( i = ( int ) strlen ( char_key ); i > 0 ; i -= 2 ){ <nl>  <nl> key_in [ j ] = ( ( guint8 ) ( strtol ( & char_key [ i - 2 ], NULL , 16 ) )); <nl> char_key [ i - 2 ] = '\ 0 '; <nl> translate_hex_key ( gchar * char_key ){ <nl> * @ return tvb Returns a deciphered tvb <nl> */ <nl>  <nl> - static tvbuff_t * rlc_decipher_tvb ( tvbuff_t * tvb , packet_info * pinfo , guint32 counter , guint8 rbid , gboolean dir ){ <nl> # if ! HAVE_UMTS_KASUMI <nl> + static tvbuff_t * rlc_decipher_tvb ( tvbuff_t * tvb _U_ , packet_info * pinfo , guint32 counter _U_ , guint8 rbid _U_ , gboolean dir _U_ ){ <nl> /* Check if we have a KASUMI implementatation */ <nl> expert_add_info_format ( pinfo , NULL , PI_UNDECODED , PI_WARN , " Unable to decipher packet since KASUMI implementation is missing ."); <nl> return NULL ; <nl> # else <nl> + static tvbuff_t * rlc_decipher_tvb ( tvbuff_t * tvb , packet_info * pinfo , guint32 counter , guint8 rbid , gboolean dir ){ <nl> guint64 i ; <nl> guint8 * out = NULL ,* key_in = NULL ; <nl> tvbuff_t * t ;
mmm ui / qt / progress_frame . cpp <nl> ppp ui / qt / progress_frame . cpp <nl> # include " wireshark_application . h " <nl>  <nl> // To do : <nl> -// - Use a different icon ? <nl> // - Add an NSProgressIndicator to the dock icon on OS X . <nl> // - Start adding the progress bar to dialogs . <nl> // - Don ' t complain so loudly when the user stops a capture . <nl> static const int app_update_freq_ = 100 ; // ms <nl> void <nl> update_progress_dlg ( progdlg_t * dlg , gfloat percentage , const gchar *) <nl> { <nl> - if (! dlg || dlg -> elapsed_timer -> elapsed () < app_update_freq_ ) return ; <nl> + if (! dlg ) return ; <nl> + if ( dlg -> elapsed_timer -> isValid () && ! dlg -> elapsed_timer -> hasExpired ( app_update_freq_ )) return ; <nl> + dlg -> elapsed_timer -> restart (); <nl>  <nl> dlg -> progress_frame -> setValue ( percentage * 100 ); <nl>  <nl> update_progress_dlg ( progdlg_t * dlg , gfloat percentage , const gchar *) <nl> * Flush out the update and process any input events . <nl> */ <nl> WiresharkApplication :: processEvents (); <nl> - dlg -> elapsed_timer -> restart (); <nl>  <nl> /* Redraw so the progress bar shows the update */ <nl> dlg -> progress_frame -> update (); <nl> struct progdlg * ProgressFrame :: showProgress ( bool animate , bool terminate_is_stop <nl> { <nl> setMaximumValue ( 100 ); <nl> ui -> progressBar -> setValue ( value ); <nl> - progress_dialog_ . elapsed_timer -> start (); <nl> + progress_dialog_ . elapsed_timer -> invalidate (); <nl> emit showRequested ( animate , terminate_is_stop , stop_flag ); <nl> return & progress_dialog_ ; <nl> }
mmm epan / dissectors / packet - csn1 . c <nl> ppp epan / dissectors / packet - csn1 . c <nl> csnStreamDissector ( proto_tree * tree , csnStream_t * ar , const CSN_DESCR * pDescr , t <nl> proto_tree * test_tree ; <nl>  <nl> descr [ 0 ] = pChoice -> descr ; <nl> + memset (& descr [ 1 ], 0x00 , sizeof ( CSN_DESCR )); <nl> descr [ 1 ]. type = CSN_END ; <nl> pui8 = pui8DATA ( data , pDescr -> offset ); <nl> * pui8 = i ;
mmm wiretap / file_access . c <nl> ppp wiretap / file_access . c <nl> cleanup_open_routines ( void ) <nl> guint i ; <nl> struct open_info * i_open ; <nl>  <nl> - for ( i = 0 , i_open = open_routines ; i < open_info_arr -> len ; i ++, i_open ++) { <nl> - if ( i_open -> extensions != NULL ) <nl> - g_strfreev ( i_open -> extensions_set ); <nl> - } <nl> + if ( open_routines != NULL ) { <nl> + for ( i = 0 , i_open = open_routines ; i < open_info_arr -> len ; i ++, i_open ++) { <nl> + if ( i_open -> extensions != NULL ) <nl> + g_strfreev ( i_open -> extensions_set ); <nl> + } <nl>  <nl> - g_array_free ( open_info_arr , TRUE ); <nl> + g_array_free ( open_info_arr , TRUE ); <nl> + } <nl> } <nl>  <nl> /*
mmm epan / dissectors / packet - vtp . c <nl> ppp epan / dissectors / packet - vtp . c <nl> dissect_vtp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> while ( tvb_reported_length_remaining ( tvb , offset ) > 0 ) { <nl> vlan_info_len = <nl> dissect_vlan_info ( tvb , pinfo , offset , vtp_tree ); <nl> - if ( vlan_info_len < 0 ) <nl> + if ( vlan_info_len <= 0 ) <nl> break ; <nl> offset += vlan_info_len ; <nl> }
mmm gtk / main . c <nl> ppp gtk / main . c <nl> main ( int argc , char * argv []) <nl> /* read in rc file from global and personal configuration paths . */ <nl> rc_file = get_datafile_path ( RC_FILE ); <nl> gtk_rc_parse ( rc_file ); <nl> + g_free ( rc_file ); <nl> rc_file = get_persconffile_path ( RC_FILE , FALSE , FALSE ); <nl> gtk_rc_parse ( rc_file ); <nl> + g_free ( rc_file ); <nl>  <nl> font_init (); <nl>  <nl> main ( int argc , char * argv []) <nl> u3_deregister_pid (); <nl>  <nl> epan_cleanup (); <nl> - g_free ( rc_file ); <nl>  <nl> # ifdef HAVE_AIRPDCAP <nl> /* Davide Schiera ( 2006 - 11 - 18 ): destroy AirPDcap context */
mmm file . c <nl> ppp file . c <nl> /* file . c <nl> * File I / O routines <nl> * <nl> - * $ Id : file . c , v 1 . 219 2000 / 09 / 11 22 : 43 : 02 sharpe Exp $ <nl> + * $ Id : file . c , v 1 . 220 2000 / 09 / 12 03 : 27 : 00 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ zing . org > <nl> rescan_packets ( capture_file * cf , const char * action , gboolean refilter , <nl>  <nl> if ( redissect ) { <nl> /* Since all state for the frame was destroyed , mark the frame <nl> - * as not visited , and null out the pointer to the per - frame <nl> + * as not visited , free the GSList referring to the state <nl> * data ( the per - frame data itself was freed by <nl> - * " init_all_protocols ()"). */ <nl> + * " init_all_protocols ()"), and null out the GSlist pointer . */ <nl> fdata -> flags . visited = 0 ; <nl> - <nl> - /* If there is any per - frame data , delete that , as what it points to <nl> - * has gone as well . <nl> - */ <nl> - <nl> if ( fdata -> pfd ) { <nl> g_slist_free ( fdata -> pfd ); <nl> } <nl> rescan_packets ( capture_file * cf , const char * action , gboolean refilter , <nl> until it finishes . Should we just stick them with that ? */ <nl> for (; fdata != NULL ; fdata = fdata -> next ) { <nl> fdata -> flags . visited = 0 ; <nl> + if ( fdata -> pfd ) { <nl> + g_slist_free ( fdata -> pfd ); <nl> + } <nl> fdata -> pfd = NULL ; <nl> } <nl> }
mmm ui / capture . c <nl> ppp ui / capture . c <nl> capture_start ( capture_options * capture_opts , capture_session * cap_session , void ( <nl> GString * source ; <nl>  <nl> cap_session -> state = CAPTURE_PREPARING ; <nl> + cap_session -> count = 0 ; <nl> g_log ( LOG_DOMAIN_CAPTURE , G_LOG_LEVEL_MESSAGE , " Capture Start ..."); <nl> source = get_iface_list_string ( capture_opts , IFLIST_SHOW_FILTER ); <nl> cf_set_tempfile_source (( capture_file *) cap_session -> cf , source -> str );
mmm epan / register . c <nl> ppp epan / register . c <nl> register_all_protocols ( register_cb cb , gpointer cb_data ) <nl> } <nl> g_thread_join ( rapw_thread ); <nl> if ( cb && ! called_back ) { <nl> - cb ( RA_REGISTER , " Registration finished ", cb_data ); <nl> + cb ( RA_REGISTER , " finished ", cb_data ); <nl> } <nl> } <nl>  <nl> register_all_protocol_handoffs ( register_cb cb , gpointer cb_data ) <nl> } <nl> g_thread_join ( raphw_thread ); <nl> if ( cb && ! called_back ) { <nl> - cb ( RA_HANDOFF , " Registration finished ", cb_data ); <nl> + cb ( RA_HANDOFF , " finished ", cb_data ); <nl> } <nl> g_async_queue_unref ( register_cb_done_q ); <nl> 
mmm epan / wslua / wslua_pinfo . c <nl> ppp epan / wslua / wslua_pinfo . c <nl> WSLUA_METAMETHOD Columns__newindex ( lua_State * L ) { <nl>  <nl> for ( cn = colnames ; cn -> name ; cn ++) { <nl> if ( g_str_equal ( cn -> name , colname ) ) { <nl> - col_set_str ( cols -> cinfo , cn -> id , text ); <nl> + col_add_str ( cols -> cinfo , cn -> id , text ); <nl> return 0 ; <nl> } <nl> }
mmm asn1 / tcap / packet - tcap - template . c <nl> ppp asn1 / tcap / packet - tcap - template . c <nl> proto_register_tcap ( void ) <nl> /* we will fake a ssn subfield which has the same value obtained from sccp */ <nl> tcap_itu_ssn_dissector_table = register_dissector_table (" tcap . itu_ssn ", " ITU TCAP SSN ", FT_UINT8 , BASE_DEC ); <nl> tcap_ansi_ssn_dissector_table = register_dissector_table (" tcap . ansi_ssn ", " ANSI TCAP SSN ", FT_UINT8 , BASE_DEC ); <nl> + <nl> + /* ' globally ' register dissector */ <nl> + register_dissector (" tcap ", dissect_tcap , proto_tcap ); <nl> + <nl> } <nl>  <nl> 
mmm epan / dissectors / packet - gprs - llc . c <nl> ppp epan / dissectors / packet - gprs - llc . c <nl> proto_register_llcgprs ( void ) <nl> void <nl> proto_reg_handoff_llcgprs ( void ) <nl> { <nl> - dissector_handle_t llcgprs_handle ; <nl> - <nl> - llcgprs_handle = create_dissector_handle ( dissect_llcgprs , <nl> - proto_llcgprs ); <nl> -/* dissector_add (" PARENT_SUBFIELD ", ID_VALUE , llcgprs_handle ); <nl> -*/ <nl> data_handle = find_dissector (" data "); <nl> }
mmm ui / qt / utils / field_information . cpp <nl> ppp ui / qt / utils / field_information . cpp <nl> field_info * FieldInformation :: fieldInfo () const <nl> FieldInformation :: HeaderInfo FieldInformation :: headerInfo () const <nl> { <nl> HeaderInfo header ; <nl> - header . isValid = false ; <nl>  <nl> if ( fi_ && fi_ -> hfinfo ) <nl> { <nl> - header . isValid = true ; <nl> header . name = fi_ -> hfinfo -> name ; <nl> header . description = fi_ -> hfinfo -> blurb ; <nl> header . abbreviation = fi_ -> hfinfo -> abbrev ; <nl> + header . isValid = true ; <nl> header . type = fi_ -> hfinfo -> type ; <nl> header . parent = fi_ -> hfinfo -> parent ; <nl> header . id = fi_ -> hfinfo -> id ; <nl> } <nl> + else <nl> + { <nl> + header . name = ""; <nl> + header . description = ""; <nl> + header . abbreviation = ""; <nl> + header . isValid = false ; <nl> + header . type = FT_NONE ; <nl> + header . parent = 0 ; <nl> + header . id = 0 ; <nl> + } <nl>  <nl> return header ; <nl> }
mmm epan / dissectors / packet - wsp . c <nl> ppp epan / dissectors / packet - wsp . c <nl> add_content_type ( proto_tree * tree , tvbuff_t * tvb , guint32 val_start , <nl> So we have to disable that one and become " slow " by pretending that <nl> the tree is " visible ". <nl> */ <nl> - PTREE_DATA ( tree )-> visible = 1 ; <nl> + if ( tree ) <nl> + PTREE_DATA ( tree )-> visible = 1 ; <nl>  <nl> * textual_content = NULL ; <nl> * well_known_content = 0 ;
mmm wiretap / netscreen . c <nl> ppp wiretap / netscreen . c <nl> static gboolean <nl> parse_netscreen_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl> char * line , int * err , gchar ** err_info ) <nl> { <nl> + int pkt_len ; <nl> int sec ; <nl> int dsec ; <nl> char cap_int [ NETSCREEN_MAX_INT_NAME_LENGTH ]; <nl> char direction [ 2 ]; <nl> - guint pkt_len ; <nl> char cap_src [ 13 ]; <nl> char cap_dst [ 13 ]; <nl> guint8 * pd ; <nl> gchar * p ; <nl> int n , i = 0 ; <nl> - guint offset = 0 ; <nl> + int offset = 0 ; <nl> gchar dststr [ 13 ]; <nl>  <nl> phdr -> rec_type = REC_TYPE_PACKET ; <nl> phdr -> presence_flags = WTAP_HAS_TS | WTAP_HAS_CAP_LEN ; <nl>  <nl> - if ( sscanf ( line , "% 9d .% 9d : % 15 [ a - z0 - 9 /:.-](% 1 [ io ]) len =% 9u :% 12s ->% 12s /", <nl> + if ( sscanf ( line , "% 9d .% 9d : % 15 [ a - z0 - 9 /:.-](% 1 [ io ]) len =% 9d :% 12s ->% 12s /", <nl> & sec , & dsec , cap_int , direction , & pkt_len , cap_src , cap_dst ) < 5 ) { <nl> * err = WTAP_ERR_BAD_FILE ; <nl> * err_info = g_strdup (" netscreen : Can ' t parse packet - header "); <nl> return - 1 ; <nl> } <nl> + if ( pkt_len < 0 ) { <nl> + * err = WTAP_ERR_BAD_FILE ; <nl> + * err_info = g_strdup (" netscreen : packet header has a negative packet length "); <nl> + return FALSE ; <nl> + } <nl> if ( pkt_len > WTAP_MAX_PACKET_SIZE ) { <nl> /* <nl> * Probably a corrupt capture file ; don ' t blow up trying
mmm epan / dissectors / packet - gsm_a_bssmap . c <nl> ppp epan / dissectors / packet - gsm_a_bssmap . c <nl> static const value_string bssap_speech_codec_values [] = { <nl> static guint8 <nl> be_speech_codec_lst ( tvbuff_t * tvb , proto_tree * tree , guint32 offset , guint len _U_ , gchar * add_string _U_ , int string_len _U_ ) <nl> { <nl> - guint32 curr_offset , consumed ; <nl> + guint32 curr_offset , consumed = 0 ; <nl> guint8 codec ; <nl> guint8 number = 0 ; <nl> proto_item * item = NULL ;
mmm epan / dissectors / packet - sbus . c <nl> ppp epan / dissectors / packet - sbus . c <nl> dissect_sbus ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data _U_ <nl> } <nl> offset += 2 ; /* now at the end of the telegram */ <nl> } <nl> - return tvb_length ( tvb ); <nl> + return offset ; <nl> /* End of dissect_sbus */ <nl> } <nl> 
mmm capture . c <nl> ppp capture . c <nl> /* capture . c <nl> * Routines for packet capture windows <nl> * <nl> - * $ Id : capture . c , v 1 . 102 2000 / 05 / 18 09 : 05 : 25 guy Exp $ <nl> + * $ Id : capture . c , v 1 . 103 2000 / 05 / 19 19 : 53 : 48 gram Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ zing . org > <nl> do_capture ( char * capfile_name ) <nl> error = errno ; <nl> close ( sync_pipe [ 1 ]); <nl> close ( sync_pipe [ 0 ]); <nl> + close ( cf . save_file_fd ); <nl> unlink ( cf . save_file ); <nl> g_free ( cf . save_file ); <nl> cf . save_file = NULL ; <nl> do_capture ( char * capfile_name ) <nl> return ; <nl> } <nl>  <nl> + close ( cf . save_file_fd ); <nl> + <nl> /* Parent process - read messages from the child process over the <nl> sync pipe . */ <nl> close ( sync_pipe [ 1 ]); <nl> do_capture ( char * capfile_name ) <nl> } else { <nl> /* Not sync mode . */ <nl> capture_succeeded = capture (); <nl> + close ( cf . save_file_fd ); <nl> if ( quit_after_cap ) { <nl> /* DON ' T unlink the save file . Presumably someone wants it . */ <nl> gtk_exit ( 0 );
mmm epan / wslua / wslua_dumper . c <nl> ppp epan / wslua / wslua_dumper . c <nl> WSLUA_METHOD Dumper_dump ( lua_State * L ) { <nl> pkthdr . len = ba -> len ; <nl> pkthdr . caplen = ba -> len ; <nl> pkthdr . pkt_encap = DUMPER_ENCAP ( d ); <nl> - pkthdr . pseudo_header = * ph -> wph ; <nl> + if ( ph -> wph ) { <nl> + pkthdr . pseudo_header = * ph -> wph ; <nl> + } <nl>  <nl> /* TODO : Can we get access to pinfo -> pkt_comment here somehow ? We <nl> * should be copying it to pkthdr . opt_comment if we can . */
mmm epan / prefs . c <nl> ppp epan / prefs . c <nl> prefs_get_string_list ( const gchar * str ) <nl> slstr [ j ] = '\ 0 '; <nl> if ( j > 0 ) <nl> sl = g_list_append ( sl , slstr ); <nl> + else <nl> + g_free ( slstr ); <nl> break ; <nl> } <nl> if ( cur_c == '"' && ! backslash ) { <nl> prefs_get_string_list ( const gchar * str ) <nl> and it wasn ' t preceded by a backslash ; it ' s the end of <nl> the string we were working on ... */ <nl> slstr [ j ] = '\ 0 '; <nl> - if ( j > 0 ) <nl> + if ( j > 0 ) { <nl> sl = g_list_append ( sl , slstr ); <nl> + slstr = ( gchar *) g_malloc ( sizeof ( gchar ) * COL_MAX_LEN ); <nl> + } <nl>  <nl> /* ... and the beginning of a new string . */ <nl> state = PRE_STRING ; <nl> - slstr = ( gchar *) g_malloc ( sizeof ( gchar ) * COL_MAX_LEN ); <nl> j = 0 ; <nl> } else if (! g_ascii_isspace ( cur_c ) || state != PRE_STRING ) { <nl> /* Either this isn ' t a white - space character , or we ' ve started a
mmm ui / gtk / stats_tree_stat . c <nl> ppp ui / gtk / stats_tree_stat . c <nl> clear_node_pr ( stat_node * n ) <nl> clear_node_pr ( c ); <nl> } <nl>  <nl> - if ( n -> pr -> iter ) { <nl> + if ( n -> pr && n -> pr -> iter ) { <nl> gtk_tree_store_remove ( n -> st -> pr -> store , n -> pr -> iter ); <nl> n -> pr -> iter = NULL ; <nl> }
mmm epan / dissectors / packet - mac - lte . c <nl> ppp epan / dissectors / packet - mac - lte . c <nl> static void dissect_ulsch_or_dlsch ( tvbuff_t * tvb , packet_info * pinfo , proto_tree <nl> case PADDING_LCID : <nl> /* No payload , unless its the last subheader , in which case <nl> it extends to the end of the PDU */ <nl> - if ( n == ( number_of_headers - 1 )) { <nl> + if ( n == ( number_of_headers - 1 ) && ( tvb_length_remaining ( tvb , offset ) > 0 )) { <nl> proto_tree_add_item ( tree , hf_mac_lte_padding_data , <nl> tvb , offset , - 1 , FALSE ); <nl> }
mmm ui / gtk / main . c <nl> ppp ui / gtk / main . c <nl> print_usage ( gboolean print_ver ) { <nl> fprintf ( output , " - I capture in monitor mode , if available \ n "); <nl> # endif <nl> # if defined ( _WIN32 ) || defined ( HAVE_PCAP_CREATE ) <nl> - fprintf ( output , " - B < buffer size > size of kernel buffer ( def : 2MB )\ n "); <nl> + fprintf ( output , " - B < buffer size > size of kernel buffer ( def : % dMB )\ n ", DEFAULT_CAPTURE_BUFFER_SIZE ); <nl> # endif <nl> fprintf ( output , " - y < link type > link layer type ( def : first appropriate )\ n "); <nl> fprintf ( output , " - D print list of interfaces and exit \ n ");mmm ui / qt / main . cpp <nl> ppp ui / qt / main . cpp <nl> print_usage ( gboolean print_ver ) { <nl> fprintf ( output , " - I capture in monitor mode , if available \ n "); <nl> # endif <nl> # if defined ( _WIN32 ) || defined ( HAVE_PCAP_CREATE ) <nl> - fprintf ( output , " - B < buffer size > size of kernel buffer ( def : 2MB )\ n "); <nl> + fprintf ( output , " - B < buffer size > size of kernel buffer ( def : % dMB )\ n ", DEFAULT_CAPTURE_BUFFER_SIZE ); <nl> # endif <nl> fprintf ( output , " - y < link type > link layer type ( def : first appropriate )\ n "); <nl> fprintf ( output , " - D print list of interfaces and exit \ n "); <nl> print_usage ( gboolean print_ver ) { <nl> fprintf ( output , " - S update packet display when new packets are captured \ n "); <nl> fprintf ( output , " - l turn on automatic scrolling while - S is in use \ n "); <nl> # if defined ( _WIN32 ) || defined ( HAVE_PCAP_CREATE ) <nl> - fprintf ( output , " - B < buffer size > size of kernel buffer ( def : 2MB )\ n "); <nl> + fprintf ( output , " - B < buffer size > size of kernel buffer ( def : % dMB )\ n ", DEFAULT_CAPTURE_BUFFER_SIZE ); <nl> # endif <nl> fprintf ( output , " - y < link type > link layer type ( def : first appropriate )\ n "); <nl> fprintf ( output , " - D print list of interfaces and exit \ n ");mmm tshark . c <nl> ppp tshark . c <nl> print_usage ( gboolean print_ver ) { <nl> fprintf ( output , " - I capture in monitor mode , if available \ n "); <nl> # endif <nl> # if defined ( _WIN32 ) || defined ( HAVE_PCAP_CREATE ) <nl> - fprintf ( output , " - B < buffer size > size of kernel buffer ( def : 2MB )\ n "); <nl> + fprintf ( output , " - B < buffer size > size of kernel buffer ( def : % dMB )\ n ", DEFAULT_CAPTURE_BUFFER_SIZE ); <nl> # endif <nl> fprintf ( output , " - y < link type > link layer type ( def : first appropriate )\ n "); <nl> fprintf ( output , " - D print list of interfaces and exit \ n "); <nl> print_usage ( gboolean print_ver ) { <nl> fprintf ( output , " - S update packet display when new packets are captured \ n "); <nl> fprintf ( output , " - l turn on automatic scrolling while - S is in use \ n "); <nl> # if defined ( _WIN32 ) || defined ( HAVE_PCAP_CREATE ) <nl> - fprintf ( output , " - B < buffer size > size of kernel buffer ( def : 2MB )\ n "); <nl> + fprintf ( output , " - B < buffer size > size of kernel buffer ( def : % dMB )\ n ", DEFAULT_CAPTURE_BUFFER_SIZE ); <nl> # endif <nl> fprintf ( output , " - y < link type > link layer type ( def : first appropriate )\ n "); <nl> fprintf ( output , " - D print list of interfaces and exit \ n "); <nl> print_usage ( gboolean print_ver ) <nl> fprintf ( output , " - I capture in monitor mode , if available \ n "); <nl> # endif <nl> # if defined ( _WIN32 ) || defined ( HAVE_PCAP_CREATE ) <nl> - fprintf ( output , " - B < buffer size > size of kernel buffer ( def : 2MB )\ n "); <nl> + fprintf ( output , " - B < buffer size > size of kernel buffer ( def : % dMB )\ n ", DEFAULT_CAPTURE_BUFFER_SIZE ); <nl> # endif <nl> fprintf ( output , " - y < link type > link layer type ( def : first appropriate )\ n "); <nl> fprintf ( output , " - D print list of interfaces and exit \ n ");
mmm epan / dissectors / packet - rtp . c <nl> ppp epan / dissectors / packet - rtp . c <nl> dissect_rtp_hext_rfc5215_onebyte ( tvbuff_t * tvb , packet_info * pinfo , <nl> return ; <nl>  <nl> ext_length = ( ext_hdr_hdr & 0x0F ) + 1 ; <nl> + <nl> + /* Exit on malformed extension headers */ <nl> + if ( ext_offset + ext_length + 1 > tvb_captured_length ( tvb )) { <nl> + return ; <nl> + } <nl> + <nl> if ( rtp_hext_tree ) { <nl> rtp_hext_rfc5285_tree = proto_tree_add_subtree ( rtp_hext_tree , tvb , ext_offset , ext_length + 1 , <nl> ett_hdr_ext_rfc5285 , NULL , " RFC 5285 Header Extension ( One - Byte Header )");
mmm epan / tvbuff . c <nl> ppp epan / tvbuff . c <nl> tvb_captured_length ( const tvbuff_t * tvb ) <nl> static inline gint <nl> _tvb_captured_length_remaining ( const tvbuff_t * tvb , const gint offset ) <nl> { <nl> - guint abs_offset , rem_length ; <nl> + guint abs_offset = 0 , rem_length ; <nl> int exception ; <nl>  <nl> exception = compute_offset_and_remaining ( tvb , offset , & abs_offset , & rem_length );
mmm epan / column - utils . c <nl> ppp epan / column - utils . c <nl> col_set_addr ( packet_info * pinfo , int col , address * addr , gboolean is_res , <nl> g_strlcpy ( pinfo -> cinfo -> col_expr . col_expr [ col ], " ipv6 . src ", COL_MAX_LEN ); <nl> else <nl> g_strlcpy ( pinfo -> cinfo -> col_expr . col_expr [ col ], " ipv6 . dst ", COL_MAX_LEN ); <nl> + memcpy (& ipv6_addr . bytes , addr -> data , sizeof ipv6_addr . bytes ); <nl> g_strlcpy ( pinfo -> cinfo -> col_expr . col_expr_val [ col ], ip6_to_str (& ipv6_addr ), COL_MAX_LEN ); <nl> break ; <nl> 
mmm gtk / main . c <nl> ppp gtk / main . c <nl> /* main . c <nl> * <nl> - * $ Id : main . c , v 1 . 315 2003 / 09 / 15 23 : 20 : 34 guy Exp $ <nl> + * $ Id : main . c , v 1 . 316 2003 / 09 / 23 06 : 25 : 10 oabad Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> packet_list_button_pressed_cb ( GtkWidget * w , GdkEvent * event , gpointer data _U_ ) <nl> set_frame_mark (! fdata -> flags . marked , fdata , row ); <nl> return TRUE ; <nl> } <nl> - else if ( event_button -> button == 1 ) { <nl> - gtk_clist_select_row ( GTK_CLIST ( w ), row , column ); <nl> - return TRUE ; <nl> - } <nl> } <nl> return FALSE ; <nl> }
mmm epan / dissectors / packet - kafka . c <nl> ppp epan / dissectors / packet - kafka . c <nl> dissect_kafka_message ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , int s <nl> offset += 4 ; <nl>  <nl> if ( raw ) { <nl> - payload = tvb_child_uncompress ( tvb , raw , 0 , tvb_length ( raw )); <nl> + payload = tvb_child_uncompress ( tvb , raw , 0 , tvb_captured_length ( raw )); <nl> if ( payload ) { <nl> add_new_data_source ( pinfo , payload , " Uncompressed Message "); <nl> dissect_kafka_message_set ( payload , pinfo , subtree , 0 , FALSE ); <nl> dissect_kafka_message ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , int s <nl> decrypt_item = proto_tree_add_item ( subtree , hf_kafka_message_value , raw , 0 , - 1 , ENC_NA ); <nl> expert_add_info ( pinfo , decrypt_item , & ei_kafka_message_decompress ); <nl> } <nl> - offset += tvb_length ( raw ); <nl> + offset += tvb_captured_length ( raw ); <nl> } <nl> else { <nl> proto_tree_add_bytes ( subtree , hf_kafka_message_value , tvb , offset , 0 , NULL ); <nl> dissect_kafka ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data _U <nl> if ( matcher == NULL || matcher -> request_frame >= PINFO_FD_NUM ( pinfo )) { <nl> col_set_str ( pinfo -> cinfo , COL_INFO , " Kafka Response ( Unknown API , Missing Request )"); <nl> /* TODO : expert info , don ' t have request , can ' t dissect */ <nl> - return tvb_length ( tvb ); <nl> + return tvb_captured_length ( tvb ); <nl> } <nl>  <nl> wmem_queue_pop ( match_queue ); <nl> dissect_kafka ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data _U <nl>  <nl> } <nl>  <nl> - return tvb_length ( tvb ); <nl> + return tvb_captured_length ( tvb ); <nl> } <nl>  <nl> static int <nl> dissect_kafka_tcp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> tcp_dissect_pdus ( tvb , pinfo , tree , TRUE , 4 , <nl> get_kafka_pdu_len , dissect_kafka , data ); <nl>  <nl> - return tvb_length ( tvb ); <nl> + return tvb_captured_length ( tvb ); <nl> } <nl>  <nl> void <nl> proto_register_kafka ( void ) <nl> }, <nl> { & hf_kafka_request_frame , <nl> { " Request Frame ", " kafka . request_frame ", <nl> - FT_FRAMENUM , BASE_NONE , 0 , 0 , <nl> + FT_FRAMENUM , BASE_NONE , FRAMENUM_TYPE ( FT_FRAMENUM_REQUEST ), 0 , <nl> NULL , HFILL } <nl> }, <nl> { & hf_kafka_broker_nodeid , <nl> proto_register_kafka ( void ) <nl> }, <nl> { & hf_kafka_response_frame , <nl> { " Response Frame ", " kafka . reponse_frame ", <nl> - FT_FRAMENUM , BASE_NONE , 0 , 0 , <nl> + FT_FRAMENUM , BASE_NONE , FRAMENUM_TYPE ( FT_FRAMENUM_RESPONSE ), 0 , <nl> NULL , HFILL } <nl> } <nl> };
mmm gtk / recent . c <nl> ppp gtk / recent . c <nl> write_recent ( void ) <nl> g_free ( rf_path ); <nl> return FALSE ; <nl> } <nl> + g_free ( rf_path ); <nl>  <nl> fputs ("# Recent settings file for Wireshark " VERSION ".\ n " <nl> "#\ n "
mmm epan / dissectors / packet - bgp . c <nl> ppp epan / dissectors / packet - bgp . c <nl> static int decode_bgp_link_nlri_prefix_descriptors ( tvbuff_t * tvb , <nl> break ; <nl>  <nl> case BGP_NLRI_TLV_IP_REACHABILITY_INFORMATION : <nl> - decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> - tvb , offset + 4 , 0 , " Reachability "); <nl> + if ( decode_prefix4 ( tlv_sub_tree , pinfo , tlv_sub_item , hf_bgp_ls_nlri_ip_reachability_prefix_ip , <nl> + tvb , offset + 4 , 0 , " Reachability ") == - 1 ) <nl> + return diss_length ; <nl> break ; <nl> } <nl> 
mmm epan / dissectors / packet - capwap . c <nl> ppp epan / dissectors / packet - capwap . c <nl> dissect_capwap_data ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl>  <nl> if ( global_capwap_reassemble && fragment_is ) <nl> { <nl> + gint len_rem = tvb_length_remaining ( tvb , offset ); <nl> + if ( len_rem <= 0 ) <nl> + return ; <nl> + <nl> pinfo -> fragmented = TRUE ; <nl>  <nl> frag_msg = fragment_add_check ( tvb , offset , pinfo , fragment_id , <nl> capwap_fragment_table , <nl> capwap_reassembled_table , <nl> fragment_offset , <nl> - tvb_length_remaining ( tvb , offset ), <nl> + len_rem , <nl> fragment_more ); <nl>  <nl> next_tvb = process_reassembled_data ( tvb , offset , pinfo ,
mmm epan / dissectors / packet - smb - mailslot . c <nl> ppp epan / dissectors / packet - smb - mailslot . c <nl> dissect_mailslot_smb ( tvbuff_t * mshdr_tvb , tvbuff_t * setup_tvb , <nl> } <nl>  <nl> smb_info = pinfo -> private_data ; <nl> - if ( smb_info -> sip != NULL && smb_info -> sip -> extra_info_type == SMB_EI_TRI ) <nl> + if ( smb_info != NULL && smb_info -> sip != NULL && smb_info -> sip -> extra_info_type == SMB_EI_TRI ) <nl> tri = smb_info -> sip -> extra_info ; <nl> else <nl> tri = NULL ;
mmm epan / dissectors / packet - mle . c <nl> ppp epan / dissectors / packet - mle . c <nl> dissect_mle_decrypt ( tvbuff_t * tvb , <nl>  <nl> DISSECTOR_ASSERT ( pinfo -> src . len == 16 ); <nl> DISSECTOR_ASSERT ( pinfo -> dst . len == 16 ); <nl> - memcpy ( d_a , ( guint8 *) pinfo -> src . data , pinfo -> src . len ); <nl> - memcpy ( d_a + 16 , ( guint8 *) pinfo -> dst . data , pinfo -> dst . len ); <nl> + memcpy ( d_a , ( const guint8 *) pinfo -> src . data , pinfo -> src . len ); <nl> + memcpy ( d_a + 16 , ( const guint8 *) pinfo -> dst . data , pinfo -> dst . len ); <nl>  <nl> tvb_memcpy ( tvb , d_a + 32 , payload_info -> aux_offset , payload_info -> aux_length ); <nl> l_a = 32 + payload_info -> aux_length ;
mmm ui / qt / capture_interfaces_dialog . cpp <nl> ppp ui / qt / capture_interfaces_dialog . cpp <nl> bool InterfaceTreeWidgetItem :: operator < ( const QTreeWidgetItem & other ) const { <nl> # include < QComboBox > <nl>  <nl> InterfaceTreeDelegate :: InterfaceTreeDelegate ( QObject * parent ) <nl> - : QStyledItemDelegate ( parent ) <nl> + : QStyledItemDelegate ( parent ), tree_ ( NULL ) <nl> { <nl> } <nl> 
mmm epan / dissectors / packet - icmpv6 . c <nl> ppp epan / dissectors / packet - icmpv6 . c <nl> dissect_icmpv6_nd_opt ( tvbuff_t * tvb , int offset , packet_info * pinfo , proto_tree <nl>  <nl> /* 6LBR Address */ <nl> proto_tree_add_item ( icmp6opt_tree , hf_icmpv6_opt_abro_6lbr_address , tvb , opt_offset , 16 , ENC_NA ); <nl> - proto_item_append_text ( ti , " : Version % d .% d , Valid Lifetime : % d , 6LBR : % s ", version_high , version_low , valid_lifetime , tvb_ip6_to_str ( tvb , opt_offset )); <nl> + proto_item_append_text ( ti , " : Version % d .% d , Valid Lifetime : % d , 6LBR : % s ", version_high , version_low , valid_lifetime , tvb_ip6_to_str ( tvb , opt_offset )); <nl> opt_offset += 16 ; <nl>  <nl> }
mmm dumpcap . c <nl> ppp dumpcap . c <nl> capture_loop_init_output ( capture_options * capture_opts , loop_data * ld , char * err <nl> - 1 , /* section_length */ <nl> & ld -> bytes_written , <nl> & err ); <nl> + g_string_free ( cpu_info_str , TRUE ); <nl> g_free ( appname ); <nl>  <nl> for ( i = 0 ; successful && ( i < capture_opts -> ifaces -> len ); i ++) { <nl> do_file_switch_or_stop ( capture_options * capture_opts , <nl> - 1 , /* section_length */ <nl> &( global_ld . bytes_written ), <nl> & global_ld . err ); <nl> + g_string_free ( cpu_info_str , TRUE ); <nl> g_free ( appname ); <nl>  <nl> for ( i = 0 ; successful && ( i < capture_opts -> ifaces -> len ); i ++) {
mmm epan / dissectors / packet - mpeg - descriptor . c <nl> ppp epan / dissectors / packet - mpeg - descriptor . c <nl> proto_mpeg_descriptor_dissect_extension ( tvbuff_t * tvb , guint offset , guint8 len , <nl> proto_tree_add_text ( tree , tvb , offset , len - already_dissected , " Private data "); <nl> break ; <nl> default : <nl> - proto_tree_add_item ( tree , hf_mpeg_descr_extension_data , tvb , offset , len , ENC_NA ); <nl> + already_dissected = offset - offset_start ; <nl> + if ( already_dissected < len ) <nl> + proto_tree_add_item ( tree , hf_mpeg_descr_extension_data , tvb , offset , len - already_dissected , ENC_NA ); <nl> break ; <nl> } <nl> 
mmm print . c <nl> ppp print . c <nl> proto_tree_write_node_pdml ( proto_node * node , gpointer data ) <nl> label_ptr = ""; <nl> } <nl>  <nl> - fputs ("< field show =\"", pdata -> fh ); <nl> + /* Show empty name since it is a required field */ <nl> + fputs ("< field name =\"", pdata -> fh ); <nl> + fputs ("\" show =\"", pdata -> fh ); <nl> print_escaped_xml ( pdata -> fh , label_ptr ); <nl>  <nl> fprintf ( pdata -> fh , "\" size =\"% d ", fi -> length );
mmm epan / dissectors / packet - e100 . c <nl> ppp epan / dissectors / packet - e100 . c <nl> proto_reg_handoff_e100 ( void ) <nl> /* Check all UDP traffic , as the specific UDP port is configurable */ <nl> heur_dissector_add (" udp ", dissect_e100 , " E100 over UDP ", " e100_udp ", proto_e100 , HEURISTIC_ENABLE ); <nl> /* e100 traffic encapsulates traffic from the ethernet frame on */ <nl> - eth_handle = find_dissector (" eth "); <nl> + eth_handle = find_dissector (" eth_withoutfcs "); <nl> } <nl>  <nl> /*
mmm epan / dissectors / packet - dcerpc . c <nl> ppp epan / dissectors / packet - dcerpc . c <nl> dissect_dcerpc_cn_bs_body ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> * it was just too short to tell and ask the TCP layer for more <nl> * data . */ <nl> pinfo -> desegment_offset = offset ; <nl> - pinfo -> desegment_len = sizeof ( e_dce_cn_common_hdr_t ) - tvb_length_remaining ( tvb , offset ); <nl> + pinfo -> desegment_len = ( guint32 )( sizeof ( e_dce_cn_common_hdr_t ) - tvb_length_remaining ( tvb , offset )); <nl> } else { <nl> /* Really not DCE - RPC */ <nl> break ;
mmm gtk / packet_list_store . c <nl> ppp gtk / packet_list_store . c <nl> packet_list_sortable_set_sort_column_id ( GtkTreeSortable * sortable , <nl> packet_list -> sort_order == order ) <nl> return ; <nl>  <nl> + if (! col_based_on_frame_data (& cfile . cinfo , sort_col_id )) { <nl> + g_warning (" Sorting on column % u not supported ", sort_col_id ); <nl> + return ; <nl> + } <nl> + <nl> packet_list -> sort_id = sort_col_id ; <nl> packet_list -> sort_order = order ; <nl> 
mmm epan / dissectors / packet - ieee80211 . c <nl> ppp epan / dissectors / packet - ieee80211 . c <nl> static void init_wepkeys ( void ) { <nl>  <nl> # ifdef USE_ENV <nl> buf = ep_alloc ( 128 ); <nl> - sprintf ( buf , 128 , " ETHEREAL_WEPKEY % d ", i + 1 ); <nl> + g_snprintf ( buf , 128 , " ETHEREAL_WEPKEY % d ", i + 1 ); <nl> tmp = getenv ( buf ); <nl> # else <nl> tmp = wep_keystr [ i ];
mmm asn1 / gsmmap / packet - gsm_map - template . c <nl> ppp asn1 / gsmmap / packet - gsm_map - template . c <nl> static int dissect_returnResultData ( packet_info * pinfo , proto_tree * tree , tvbuff <nl> break ; <nl> case 56 : <nl> offset = dissect_gsm_map_SendAuthenticationInfoRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> + break ; <nl> case 57 : /* restoreData */ <nl> offset = dissect_gsm_map_RestoreDataRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> break ;mmm epan / dissectors / packet - gsm_map . c <nl> ppp epan / dissectors / packet - gsm_map . c <nl> static int dissect_returnResultData ( packet_info * pinfo , proto_tree * tree , tvbuff <nl> break ; <nl> case 56 : <nl> offset = dissect_gsm_map_SendAuthenticationInfoRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> + break ; <nl> case 57 : /* restoreData */ <nl> offset = dissect_gsm_map_RestoreDataRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> break ; <nl> /* Do not modify this file . */ <nl> /* It is created automatically by the ASN . 1 to Ethereal dissector compiler */ <nl> -/* .\ packet - gsm_map . c */ <nl> +/* ./ packet - gsm_map . c */ <nl> /* ../../ tools / asn2eth . py - X - b - e - p gsm_map - c gsmmap . cnf - s packet - gsm_map - template GSMMAP . asn */ <nl>  <nl> /* Input file : packet - gsm_map - template . c */ <nl> static int dissect_returnResultData ( packet_info * pinfo , proto_tree * tree , tvbuff <nl> break ; <nl> case 56 : <nl> offset = dissect_gsm_map_SendAuthenticationInfoRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> + break ; <nl> case 57 : /* restoreData */ <nl> offset = dissect_gsm_map_RestoreDataRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> break ;mmm epan / dissectors / packet - gsm_map . h <nl> ppp epan / dissectors / packet - gsm_map . h <nl> static int dissect_returnResultData ( packet_info * pinfo , proto_tree * tree , tvbuff <nl> break ; <nl> case 56 : <nl> offset = dissect_gsm_map_SendAuthenticationInfoRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> + break ; <nl> case 57 : /* restoreData */ <nl> offset = dissect_gsm_map_RestoreDataRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> break ; <nl> /* Do not modify this file . */ <nl> /* It is created automatically by the ASN . 1 to Ethereal dissector compiler */ <nl> -/* .\ packet - gsm_map . c */ <nl> +/* ./ packet - gsm_map . c */ <nl> /* ../../ tools / asn2eth . py - X - b - e - p gsm_map - c gsmmap . cnf - s packet - gsm_map - template GSMMAP . asn */ <nl>  <nl> /* Input file : packet - gsm_map - template . c */ <nl> static int dissect_returnResultData ( packet_info * pinfo , proto_tree * tree , tvbuff <nl> break ; <nl> case 56 : <nl> offset = dissect_gsm_map_SendAuthenticationInfoRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> + break ; <nl> case 57 : /* restoreData */ <nl> offset = dissect_gsm_map_RestoreDataRes ( FALSE , tvb , offset , pinfo , tree , - 1 ); <nl> break ; <nl> /* Do not modify this file . */ <nl> /* It is created automatically by the ASN . 1 to Ethereal dissector compiler */ <nl> -/* .\ packet - gsm_map . h */ <nl> +/* ./ packet - gsm_map . h */ <nl> /* ../../ tools / asn2eth . py - X - b - e - p gsm_map - c gsmmap . cnf - s packet - gsm_map - template GSMMAP . asn */ <nl>  <nl> /* Input file : packet - gsm_map - template . h */
mmm ui / win32 / file_dlg_win32 . c <nl> ppp ui / win32 / file_dlg_win32 . c <nl> build_file_save_type_list ( GArray * savable_file_types ) { <nl> ft = g_array_index ( savable_file_types , int , i ); <nl> append_file_type ( sa , ft ); <nl> } <nl> - g_array_free ( savable_file_types , TRUE ); <nl> } <nl>  <nl> /* terminate the array */
mmm wiretap / toshiba . c <nl> ppp wiretap / toshiba . c <nl> parse_toshiba_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl> union wtap_pseudo_header * pseudo_header = & phdr -> pseudo_header ; <nl> char line [ TOSHIBA_LINE_LENGTH ]; <nl> int num_items_scanned ; <nl> - guint pkt_len ; <nl> - int pktnum , hr , min , sec , csec ; <nl> + int pkt_len , pktnum , hr , min , sec , csec ; <nl> char channel [ 10 ], direction [ 10 ]; <nl> int i , hex_lines ; <nl> guint8 * pd ; <nl> parse_toshiba_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl>  <nl> } while ( strcmp ( line , " OFFSET 0001 - 0203 ") != 0 ); <nl>  <nl> - num_items_scanned = sscanf ( line + 64 , " LEN =% 9u ", & pkt_len ); <nl> + num_items_scanned = sscanf ( line + 64 , " LEN =% 9d ", & pkt_len ); <nl> if ( num_items_scanned != 1 ) { <nl> * err = WTAP_ERR_BAD_FILE ; <nl> * err_info = g_strdup (" toshiba : OFFSET line doesn ' t have valid LEN item "); <nl> return FALSE ; <nl> } <nl> + if ( pkt_len < 0 ) { <nl> + * err = WTAP_ERR_BAD_FILE ; <nl> + * err_info = g_strdup (" toshiba : packet header has a negative packet length "); <nl> + return FALSE ; <nl> + } <nl> if ( pkt_len > WTAP_MAX_PACKET_SIZE ) { <nl> /* <nl> * Probably a corrupt capture file ; don ' t blow up trying
mmm epan / dissectors / packet - sigcomp . c <nl> ppp epan / dissectors / packet - sigcomp . c <nl> dissect_sigcomp_tcp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * _ <nl>  <nl> col_clear ( pinfo -> cinfo , COL_INFO ); <nl>  <nl> - length = tvb_captured_length_remaining ( tvb , offset ); <nl> + length = tvb_reported_length ( tvb ); <nl>  <nl> try_again : <nl> /* create display subtree for the protocol */
mmm file . c <nl> ppp file . c <nl> cf_print_packets ( capture_file * cf , print_args_t * print_args , <nl> } <nl>  <nl> /* if num_visible_col is 0 , we are done */ <nl> - if ( num_visible_col == 0 ) <nl> + if ( num_visible_col == 0 ) { <nl> + g_free ( callback_args . header_line_buf ); <nl> return CF_PRINT_OK ; <nl> + } <nl>  <nl> /* Find the widths for each of the columns - maximum of the <nl> width of the title and the width of the data - and construct
mmm capture_sync . c <nl> ppp capture_sync . c <nl> sync_pipe_start ( capture_options * capture_opts ) { <nl> execv ( argv [ 0 ], ( gpointer ) argv ); <nl> g_snprintf ( errmsg , sizeof errmsg , " Couldn ' t run % s in child process : % s ", <nl> argv [ 0 ], strerror ( errno )); <nl> - sync_pipe_errmsg_to_parent ( errmsg , ""); <nl> + sync_pipe_errmsg_to_parent ( 1 , errmsg , ""); <nl>  <nl> /* Exit with " _exit ()", so that we don ' t close the connection <nl> to the X server ( and cause stuff buffered up by our parent but <nl> sync_interface_stats_open ( int * read_fd , int * fork_child , gchar ** msg ) { <nl>  <nl> /* Close down the stats process */ <nl> int <nl> - sync_interface_stats_close ( int * read_fd , int * fork_child , gchar ** msg ) { <nl> + sync_interface_stats_close ( int * read_fd , int * fork_child <nl> +# ifndef _WIN32 <nl> + _U_ <nl> +# endif <nl> +, gchar ** msg ) { <nl> # ifdef _WIN32 <nl> return sync_pipe_close_command ( read_fd , fork_child , msg ); <nl> # else
mmm epan / dissectors / packet - kerberos . c <nl> ppp epan / dissectors / packet - kerberos . c <nl> static int wrap_dissect_gss_kerb ( tvbuff_t * tvb , int offset , packet_info * pinfo , <nl>  <nl> auth_tvb = tvb_new_subset ( <nl> tvb , offset , tvb_length_remaining ( tvb , offset ), <nl> - tvb_length_remaining ( tvb , offset )); <nl> + tvb_reported_length_remaining ( tvb , offset )); <nl>  <nl> dissect_kerberos_main ( auth_tvb , pinfo , tree , FALSE , NULL ); <nl> mmm epan / dissectors / packet - ber . c <nl> ppp epan / dissectors / packet - ber . c <nl> static int wrap_dissect_gss_kerb ( tvbuff_t * tvb , int offset , packet_info * pinfo , <nl>  <nl> auth_tvb = tvb_new_subset ( <nl> tvb , offset , tvb_length_remaining ( tvb , offset ), <nl> - tvb_length_remaining ( tvb , offset )); <nl> + tvb_reported_length_remaining ( tvb , offset )); <nl>  <nl> dissect_kerberos_main ( auth_tvb , pinfo , tree , FALSE , NULL ); <nl>  <nl> call_ber_oid_callback ( char * oid , tvbuff_t * tvb , int offset , packet_info * pinfo , <nl> { <nl> tvbuff_t * next_tvb ; <nl>  <nl> - next_tvb = tvb_new_subset ( tvb , offset , tvb_length_remaining ( tvb , offset ), tvb_length_remaining ( tvb , offset )); <nl> + next_tvb = tvb_new_subset ( tvb , offset , tvb_length_remaining ( tvb , offset ), tvb_reported_length_remaining ( tvb , offset )); <nl> if (! dissector_try_string ( ber_oid_dissector_table , oid , next_tvb , pinfo , tree )){ <nl> proto_item * item = NULL ; <nl> proto_tree * next_tree = NULL ; <nl> printf (" OCTET STRING dissect_ber_octet_string (% s ) entered \ n ", name ); <nl> if ( len <=( guint32 ) tvb_length_remaining ( tvb , offset )){ <nl> * out_tvb = tvb_new_subset ( tvb , offset , len , len ); <nl> } else { <nl> - * out_tvb = tvb_new_subset ( tvb , offset , tvb_length_remaining ( tvb , offset ), tvb_length_remaining ( tvb , offset )); <nl> + * out_tvb = tvb_new_subset ( tvb , offset , tvb_length_remaining ( tvb , offset ), tvb_reported_length_remaining ( tvb , offset )); <nl> } <nl> } <nl> } <nl> ber_sequence_try_again : <nl> * length ) of if the tvb is short , then just <nl> * give it all of the tvb and hope for the best . <nl> */ <nl> - next_tvb = tvb_new_subset ( tvb , hoffset , tvb_length_remaining ( tvb , hoffset ), tvb_length_remaining ( tvb , hoffset )); <nl> + next_tvb = tvb_new_subset ( tvb , hoffset , tvb_length_remaining ( tvb , hoffset ), tvb_reported_length_remaining ( tvb , hoffset )); <nl> } else { <nl> next_tvb = tvb_new_subset ( tvb , hoffset , eoffset - hoffset , eoffset - hoffset ); <nl> } <nl> int dissect_ber_bitstring ( gboolean implicit_tag , packet_info * pinfo , proto_tree <nl> if ( len <=( guint32 ) tvb_length_remaining ( tvb , offset )){ <nl> * out_tvb = tvb_new_subset ( tvb , offset , len , len ); <nl> } else { <nl> - * out_tvb = tvb_new_subset ( tvb , offset , tvb_length_remaining ( tvb , offset ), tvb_length_remaining ( tvb , offset )); <nl> + * out_tvb = tvb_new_subset ( tvb , offset , tvb_length_remaining ( tvb , offset ), tvb_reported_length_remaining ( tvb , offset )); <nl> } <nl> } <nl> }
mmm epan / dissectors / packet - nsip . c <nl> ppp epan / dissectors / packet - nsip . c <nl> decode_pdu_sns_delete ( build_info_t * bi ) { <nl> { NSIP_IE_IP4_ELEMENTS , NSIP_IE_PRESENCE_C , NSIP_IE_FORMAT_TLV , 0 , 0 }, <nl> { NSIP_IE_IP6_ELEMENTS , NSIP_IE_PRESENCE_C , NSIP_IE_FORMAT_TLV , 0 , 0 }, <nl> }; <nl> - decode_iei_transaction_id ( ies , bi , bi -> offset ); <nl> - decode_pdu_general (& ies [ 1 ], 3 , bi ); <nl> + decode_pdu_general ( ies , 1 , bi ); <nl> + decode_iei_transaction_id (& ies [ 1 ], bi , bi -> offset ); <nl> + decode_pdu_general (& ies [ 2 ], 3 , bi ); <nl> } <nl>  <nl> static void
mmm epan / dissectors / packet - cip . c <nl> ppp epan / dissectors / packet - cip . c <nl> static int dissect_cip_attribute ( packet_info * pinfo , proto_tree * tree , proto_ite <nl> /* Convert to nstime epoch */ <nl> computed_time = CIP_TIMEBASE +( temp_data * 60 * 60 * 24 ); <nl> date = gmtime (& computed_time ); <nl> - strftime ( date_str , 20 , "% b % d , % Y ", date ); <nl> + if ( date != NULL ) <nl> + strftime ( date_str , 20 , "% b % d , % Y ", date ); <nl> + else <nl> + g_strlcpy ( date_str , " Not representable ", sizeof date_str ); <nl> proto_tree_add_uint_format_value ( tree , *( attr -> phf ), tvb , offset , 2 , temp_data , "% s ", date_str ); <nl> consumed = 2 ; <nl> break ;
mmm epan / dissectors / packet - quic . c <nl> ppp epan / dissectors / packet - quic . c <nl> dissect_quic_common ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl>  <nl> /* Diversification Nonce */ <nl> if ( puflags & PUFLAGS_DNONCE && quic_info -> version >= 33 ){ <nl> - proto_tree_add_item ( quic_tree , hf_quic_diversification_nonce , tvb , offset , 32 , ENC_NA ); <nl> - offset += 32 ; <nl> + if ( pinfo -> srcport == 443 ){ /* Diversification nonce is only present from server to client */ <nl> + proto_tree_add_item ( quic_tree , hf_quic_diversification_nonce , tvb , offset , 32 , ENC_NA ); <nl> + offset += 32 ; <nl> + } <nl> } <nl>  <nl> - <nl> /* Packet Number */ <nl>  <nl> /* Get len of packet number ( and packet number ), may be a more easy function to get the length ... */
mmm epan / dissectors / packet - imf . c <nl> ppp epan / dissectors / packet - imf . c <nl> header_fields_initialize_cb ( void ) <nl> guint i ; <nl> gchar * header_name ; <nl>  <nl> - if ( custom_field_table ) { <nl> + if ( custom_field_table && hf ) { <nl> guint hf_size = g_hash_table_size ( custom_field_table ); <nl> /* Unregister all fields */ <nl> for ( i = 0 ; i < hf_size ; i ++) {mmm epan / dissectors / packet - http . c <nl> ppp epan / dissectors / packet - http . c <nl> header_fields_initialize_cb ( void ) <nl> guint i ; <nl> gchar * header_name ; <nl>  <nl> - if ( custom_field_table ) { <nl> + if ( custom_field_table && hf ) { <nl> guint hf_size = g_hash_table_size ( custom_field_table ); <nl> /* Unregister all fields */ <nl> for ( i = 0 ; i < hf_size ; i ++) { <nl> header_fields_initialize_cb ( void ) <nl> guint i ; <nl> gchar * header_name ; <nl>  <nl> - if ( header_fields_hash ) { <nl> + if ( header_fields_hash && hf ) { <nl> guint hf_size = g_hash_table_size ( header_fields_hash ); <nl> /* Unregister all fields */ <nl> for ( i = 0 ; i < hf_size ; i ++) {
mmm epan / prefs . c <nl> ppp epan / prefs . c <nl> set_pref ( gchar * pref_name , gchar * value , void * private_data _U_ , <nl> find_index_from_string_array ( value , gui_layout_content_text , 0 ); <nl> } else if ( strcmp ( pref_name , PRS_CONSOLE_LOG_LEVEL ) == 0 ) { <nl> prefs . console_log_level = strtoul ( value , NULL , 10 ); <nl> - if ( prefs . console_log_level & G_LOG_LEVEL_INFO | G_LOG_LEVEL_DEBUG ) { <nl> + if ( prefs . console_log_level & ( G_LOG_LEVEL_INFO | G_LOG_LEVEL_DEBUG )) { <nl> /* <nl> * GLib >= 2 . 32 drops INFO and DEBUG messages by default . Tell <nl> * it not to do that .
mmm epan / dissectors / packet - usb . c <nl> ppp epan / dissectors / packet - usb . c <nl> try_dissect_next_protocol ( proto_tree * parent , tvbuff_t * next_tvb , gint offset , p <nl>  <nl> if ( try_heuristics && dissector_try_heuristic ( heur_subdissector_list , next_tvb , pinfo , parent , usb_conv_info )) { <nl> offset += tvb_length ( next_tvb ); <nl> - } else if ( dissector_try_uint_new ( usb_dissector_table , usb_conv_info -> interfaceClass , next_tvb , pinfo , parent , TRUE , usb_conv_info )) { <nl> + } else if ( usb_dissector_table && <nl> + dissector_try_uint_new ( usb_dissector_table , usb_conv_info -> interfaceClass , next_tvb , pinfo , parent , TRUE , usb_conv_info )) { <nl> offset += tvb_length ( next_tvb ); <nl> } <nl> }
mmm epan / dissectors / packet - gtpv2 . c <nl> ppp epan / dissectors / packet - gtpv2 . c <nl> dissect_gtpv2_pres_rep_area_action ( tvbuff_t * tvb , packet_info * pinfo , proto_tree <nl> if ( length == 1 ) <nl> return ; <nl> /* Octet 6 to 8 Presence Reporting Area Identifier */ <nl> - proto_tree_add_item ( tree , hf_gtpv2_pres_rep_area_id , tvb , offset , 2 , ENC_BIG_ENDIAN ); <nl> - offset += 2 ; <nl> - if ( length == 3 ) <nl> + proto_tree_add_item ( tree , hf_gtpv2_pres_rep_area_id , tvb , offset , 3 , ENC_BIG_ENDIAN ); <nl> + offset += 3 ; <nl> + if ( length == 4 ) <nl> return ; <nl>  <nl> - new_tvb = tvb_new_subset_length ( tvb , offset , length - 3 ); <nl> + new_tvb = tvb_new_subset_length ( tvb , offset , length - 4 ); <nl>  <nl> /* Share the rest of the dissection with the AVP dissector */ <nl> dissect_diameter_3gpp_presence_reporting_area_elements_list ( new_tvb , pinfo , tree , NULL ); <nl> void proto_register_gtpv2 ( void ) <nl> }, <nl> { & hf_gtpv2_pres_rep_area_id , <nl> {" Presence Reporting Area Identifier ", " gtpv2 . pres_rep_area_action . pres_rep_area_id ", <nl> - FT_UINT16 , BASE_HEX , NULL , 0x0 , <nl> + FT_UINT24 , BASE_HEX , NULL , 0x0 , <nl> NULL , HFILL } <nl> }, <nl> { & hf_gtpv2_pres_rep_area_act_no_tai ,
mmm ui / gtk / profile_dlg . c <nl> ppp ui / gtk / profile_dlg . c <nl> fill_list ( GtkWidget * main_w ) <nl> * and use it later without any crashes . This may not be a <nl> * valid assumption . <nl> */ <nl> + g_free ( l_select ); <nl> l_select = ( GtkTreeIter *) g_memdup (& iter , sizeof ( iter )); <nl> } <nl> fl_entry = g_list_next ( fl_entry );
mmm epan / dissectors / packet - catapult - dct2000 . c <nl> ppp epan / dissectors / packet - catapult - dct2000 . c <nl> static void parse_outhdr_string ( const guchar * outhdr_string , gint outhdr_string_ <nl> guint d ; <nl>  <nl> /* Find digits */ <nl> - for ( ; n < outhdr_string_len ; n ++) { <nl> + for ( ; ( n < outhdr_string_len ) && ( number_digits < MAX_OUTHDR_VALUES ); n ++) { <nl> if (! g_ascii_isdigit ( outhdr_string [ n ])) { <nl> break ; <nl> }
mmm plugins / mgcp / packet - mgcp . c <nl> ppp plugins / mgcp / packet - mgcp . c <nl> * PKT - SP - EC - MGCP - I09 - 040113 , January 13 , 2004 , Cable Television <nl> * Laboratories , Inc ., http :// www . PacketCable . com / <nl> * <nl> - * $ Id : packet - mgcp . c , v 1 . 46 2004 / 05 / 30 17 : 58 : 35 etxrab Exp $ <nl> + * $ Id : packet - mgcp . c , v 1 . 47 2004 / 05 / 31 19 : 31 : 14 etxrab Exp $ <nl> * <nl> * Copyright ( c ) 2000 by Ed Warnicke < hagbard @ physics . rutgers . edu > <nl> * Copyright ( c ) 2004 by Thomas Anders < thomas . anders [ AT ] blue - cable . de > <nl> dissect_mgcp_connectionparams ( proto_tree * parent_tree , tvbuff_t * tvb , gint offse <nl> } <nl> offset += tokenlen + 1 ; /* 1 extra for the delimiter */ <nl> } <nl> + g_strfreev ( typval ); <nl> + g_strfreev ( tokens ); <nl> + <nl> } <nl>  <nl> /*
mmm epan / dissectors / packet - corosync - totemnet . c <nl> ppp epan / dissectors / packet - corosync - totemnet . c <nl> dissect_corosynec_totemnet ( tvbuff_t * tvb , <nl> return call_dissector ( corosync_totemsrp_handle , tvb , pinfo , parent_tree ); <nl> } <nl>  <nl> + static void <nl> + corosync_totemnet_shutdown ( void ) <nl> +{ <nl> + g_strfreev ( corosync_totemnet_private_keys_list ); <nl> +} <nl>  <nl> void <nl> proto_register_corosync_totemnet ( void ) <nl> proto_register_corosync_totemnet ( void ) <nl> prefs_register_string_preference ( corosync_totemnet_module , " private_keys ", " Private keys ", <nl> " Semicolon - separated list of keys for decryption ( e . g . key1 ; key2 ;..." , <nl> ( const gchar **)& corosync_totemnet_private_keys ); <nl> + <nl> + register_shutdown_routine ( corosync_totemnet_shutdown ); <nl> } <nl>  <nl> void
mmm epan / dissectors / packet - gsm_a_bssmap . c <nl> ppp epan / dissectors / packet - gsm_a_bssmap . c <nl> be_field_element_dissect ( tvbuff_t * tvb , proto_tree * tree , guint32 offset , guint <nl> str = match_strval_idx (( guint32 ) oct , bssmap_field_element_ids , & idx ); <nl> ie_len = tvb_get_guint8 ( tvb , curr_offset ++); <nl>  <nl> + if (! str ) <nl> + str = " Unknown "; <nl> + <nl> /* <nl> * add Field Element name <nl> */
mmm packet - dcerpc - mapi . c <nl> ppp packet - dcerpc - mapi . c <nl> * Routines for MS Exchange MAPI <nl> * Copyright 2002 , Ronnie Sahlberg <nl> * <nl> - * $ Id : packet - dcerpc - mapi . c , v 1 . 5 2002 / 05 / 25 09 : 19 : 45 sahlberg Exp $ <nl> + * $ Id : packet - dcerpc - mapi . c , v 1 . 6 2002 / 05 / 25 10 : 25 : 27 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> mapi_decrypt_pdu ( tvbuff_t * tvb , int offset , <nl> proto_tree_add_item ( tr , hf_mapi_decrypted_data , mmd -> tvb , 2 , pdu_len , FALSE ); <nl>  <nl> proto_tree_add_item ( tr , hf_mapi_pdu_trailer , mmd -> tvb , pdu_len , 4 , FALSE ); <nl> - if ( len >( pdu_len + 4 )){ <nl> + if ( len >(( guint32 ) pdu_len + 4 )){ <nl> proto_tree_add_item ( tr , hf_mapi_pdu_extra_trailer , mmd -> tvb , pdu_len + 4 , len -( pdu_len + 4 ), FALSE ); <nl> } <nl> 
mmm epan / dissectors / packet - sua . c <nl> ppp epan / dissectors / packet - sua . c <nl> proto_register_sua ( void ) <nl> " This may affect TCAP ' s ability to recognize which messages belong to which TCAP session .", & set_addresses ); <nl>  <nl> heur_subdissector_list = register_heur_dissector_list (" sua "); <nl> - sua_parameter_table = register_dissector_table (" sua . prop . tags ", " SUA Proprietary Tags ", FT_UINT16 , BASE_DEC , DISSECTOR_TABLE_NOT_ALLOW_DUPLICATE ); <nl> + sua_parameter_table = register_dissector_table (" sua . prop . tags ", " SUA Proprietary Tags ", FT_UINT16 , BASE_DEC , DISSECTOR_TABLE_ALLOW_DUPLICATE ); <nl> sua_tap = register_tap (" sua "); <nl>  <nl> assocs = wmem_tree_new_autoreset ( wmem_epan_scope (), wmem_file_scope ());
mmm wiretap / netmon . c <nl> ppp wiretap / netmon . c <nl> netmon_process_record ( wtap * wth , FILE_T fh , struct wtap_pkthdr * phdr , <nl> switch ( network ) <nl> { <nl> case 0xE080 : // " WiFi Message " <nl> + pkt_encap = WTAP_ENCAP_IEEE_802_11 ; <nl> + break ; <nl> case 0xE081 : // " Ndis Etw WiFi Channel Message " <nl> case 0xE082 : // " Fiddler Netmon Message " <nl> case 0xE089 : // " Pef Ndis Msg ";
mmm epan / dissectors / packet - ssl - utils . c <nl> ppp epan / dissectors / packet - ssl - utils . c <nl> ssl_find_private_key ( SslDecryptSession * ssl_session , GHashTable * key_hash , GTree <nl> ssl_debug_printf (" ssl_find_private_key server % s :% u \ n ", <nl> ep_address_to_str (& dummy . addr ), dummy . port ); <nl>  <nl> + if ( g_hash_table_size ( key_hash ) == 0 ) { <nl> + ssl_debug_printf (" ssl_find_private_key : no keys found \ n "); <nl> + return ; <nl> + } else { <nl> + ssl_debug_printf (" ssl_find_private_key : testing % i keys \ n ", <nl> + g_hash_table_size ( key_hash )); <nl> + } <nl> + <nl> /* try to retrieve private key for this service . Do it now ' cause pinfo <nl> * is not always available <nl> * Note that with HAVE_LIBGNUTLS undefined private_key is allways 0
mmm epan / wslua / wslua_internals . c <nl> ppp epan / wslua / wslua_internals . c <nl> WSLUA_API void wslua_setfuncs ( lua_State * L , const luaL_Reg * l , int nup ) { <nl> } <nl>  <nl> /* identical to lua_getfield but without triggering metamethods */ <nl> - WSLUA_API void lua_rawgetfield ( lua_State * L , int index , const char * k ) { <nl> + WSLUA_API void lua_rawgetfield ( lua_State * L , int idx , const char * k ) { <nl> lua_pushstring ( L , k ); <nl> - lua_rawget ( L , index ); <nl> + lua_rawget ( L , idx ); <nl> } <nl>  <nl> /* identical to lua_setfield but without triggering metamethods */ <nl> - WSLUA_API void lua_rawsetfield ( lua_State * L , int index , const char * k ) { <nl> + WSLUA_API void lua_rawsetfield ( lua_State * L , int idx , const char * k ) { <nl> lua_pushstring ( L , k ); <nl> lua_insert ( L , - 2 ); <nl> - lua_rawset ( L , index ); <nl> + lua_rawset ( L , idx ); <nl> } <nl>  <nl> WSLUA_API void wslua_print_stack ( char * s , lua_State * L ) {
mmm epan / dissectors / packet - gtp . c <nl> ppp epan / dissectors / packet - gtp . c <nl> * Copyright 2011 , Grzegorz Szczytowski < grzegorz . szczytowski @ gmail . com > <nl> * <nl> * Updates and corrections : <nl> - * Copyright 2011 - 2012 , Anders Broman < anders . broman @ ericsson . com > <nl> + * Copyright 2011 - 2013 , Anders Broman < anders . broman @ ericsson . com > <nl> * <nl> * PDCP PDU number extension header support added by Martin Isaksson < martin . isaksson @ ericsson . com > <nl> * <nl> decode_gtp_priv_ext ( tvbuff_t * tvb , int offset , packet_info * pinfo _U_ , proto_t <nl> offset = offset + 2 ; <nl>  <nl> if ( length > 2 ) { <nl> - next_tvb = tvb_new_subset_remaining ( tvb , offset ); <nl> + next_tvb = tvb_new_subset ( tvb , offset , length - 2 , length - 2 ); <nl> if (! dissector_try_uint ( gtp_priv_ext_dissector_table , ext_id , next_tvb , pinfo , ext_tree_priv_ext )){ <nl> proto_tree_add_item ( ext_tree_priv_ext , hf_gtp_ext_val , tvb , offset , length - 2 , ENC_NA ); <nl> }
mmm epan / dissectors / packet - xot . c <nl> ppp epan / dissectors / packet - xot . c <nl> proto_register_xot ( void ) <nl> proto_xot = proto_register_protocol (" X . 25 over TCP ", " XOT ", " xot "); <nl> proto_register_field_array ( proto_xot , hf , array_length ( hf )); <nl> proto_register_subtree_array ( ett , array_length ( ett )); <nl> + register_dissector (" xot ", dissect_xot , proto_xot ); <nl>  <nl> xot_module = prefs_register_protocol ( proto_xot , NULL ); <nl> prefs_register_bool_preference ( xot_module , " desegment ",
mmm epan / crypt / airpdcap . c <nl> ppp epan / crypt / airpdcap . c <nl> AirPDcapDecryptWPABroadcastKey ( const EAPOL_RSN_KEY * pEAPKey , guint8 * decryption <nl> } else if ( key_version == AIRPDCAP_WPA_KEY_VER_AES_CCMP ){ <nl> /* AES */ <nl> key_bytes_len = pntoh16 ( pEAPKey -> key_data_len ); <nl> + <nl> + /* AES keys must be at least 128 bits = 16 bytes . */ <nl> + if ( key_bytes_len < 16 ) { <nl> + return ; <nl> + } <nl> } <nl>  <nl> if ( key_bytes_len > TKIP_GROUP_KEYBYTES_LEN_MAX || key_bytes_len == 0 ) { /* Don ' t read past the end of pEAPKey -> ie */
mmm epan / dissectors / packet - ssl - utils . c <nl> ppp epan / dissectors / packet - ssl - utils . c <nl> ssl_association_remove ( GTree * associations , SslAssociation * assoc ) <nl> if ( assoc -> handle ) <nl> dissector_delete (( assoc -> tcp )?" tcp . port ":" udp . port ", assoc -> ssl_port , assoc -> handle ); <nl>  <nl> + g_free ( assoc -> info ); <nl> + <nl> g_tree_remove ( associations , assoc ); <nl> g_free ( assoc ); <nl> }
mmm wiretap / netscreen . c <nl> ppp wiretap / netscreen . c <nl> parse_netscreen_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl>  <nl> phdr -> rec_type = REC_TYPE_PACKET ; <nl> phdr -> presence_flags = WTAP_HAS_TS | WTAP_HAS_CAP_LEN ; <nl> + /* Suppress compiler warnings */ <nl> + memset ( cap_int , 0 , sizeof ( cap_int )); <nl> + memset ( cap_dst , 0 , sizeof ( cap_dst )); <nl>  <nl> if ( sscanf ( line , "% 9d .% 9d : % 15 [ a - z0 - 9 /:.-](% 1 [ io ]) len =% 9d :% 12s ->% 12s /", <nl> & sec , & dsec , cap_int , direction , & pkt_len , cap_src , cap_dst ) < 5 ) {
mmm epan / tvbuff . c <nl> ppp epan / tvbuff . c <nl> * the data of a backing tvbuff , or can be a composite of <nl> * other tvbuffs . <nl> * <nl> - * $ Id : tvbuff . c , v 1 . 63 2004 / 05 / 06 17 : 40 : 52 obiot Exp $ <nl> + * $ Id : tvbuff . c , v 1 . 64 2004 / 05 / 07 18 : 15 : 24 obiot Exp $ <nl> * <nl> * Copyright ( c ) 2000 by Gilbert Ramirez < gram @ alumni . rice . edu > <nl> * <nl> tvb_uncompress ( tvbuff_t * tvb , int offset , int comprlen ) <nl> if ( uncompr != NULL ) { <nl> uncompr_tvb = tvb_new_real_data (( guint8 *) uncompr , bytes_out , <nl> bytes_out ); <nl> + tvb_set_free_cb ( uncompr_tvb , g_free ); <nl> } <nl> g_free ( compr ); <nl> return uncompr_tvb ;
mmm epan / emem . c <nl> ppp epan / emem . c <nl> emem_scrub_memory ( char * buf , size_t size , gboolean alloc ) <nl>  <nl> } <nl>  <nl> - static void <nl> - emem_create_chunk ( emem_chunk_t ** free_list , gboolean use_canary ) { <nl> + static emem_chunk_t * <nl> + emem_create_chunk ( gboolean use_canary ) { <nl> # if defined ( _WIN32 ) <nl> BOOL ret ; <nl> char * buf_end , * prot1 , * prot2 ; <nl> emem_create_chunk ( emem_chunk_t ** free_list , gboolean use_canary ) { <nl> # endif /* _WIN32 / USE_GUARD_PAGES */ <nl> emem_chunk_t * npc ; <nl>  <nl> - /* we dont have any free data , so we must allocate a new one */ <nl> - DISSECTOR_ASSERT (!* free_list ); <nl> - <nl> npc = g_new ( emem_chunk_t , 1 ); <nl> npc -> next = NULL ; <nl> if ( use_canary ) { <nl> emem_create_chunk ( emem_chunk_t ** free_list , gboolean use_canary ) { <nl> else <nl> npc -> canary_info = NULL ; <nl>  <nl> - * free_list = npc ; <nl> - <nl> # if defined ( _WIN32 ) <nl> /* <nl> * MSDN documents VirtualAlloc / VirtualProtect at <nl> emem_create_chunk ( emem_chunk_t ** free_list , gboolean use_canary ) { <nl> # endif <nl>  <nl> if ( npc -> buf == NULL ) { <nl> + g_free ( npc ); <nl> THROW ( OutOfMemoryError ); <nl> } <nl>  <nl> emem_create_chunk ( emem_chunk_t ** free_list , gboolean use_canary ) { <nl>  <nl> npc -> amount_free = npc -> amount_free_init ; <nl> npc -> free_offset = npc -> free_offset_init ; <nl> + return npc ; <nl> } <nl>  <nl> static void * <nl> emem_alloc_chunk ( size_t size , emem_header_t * mem ) <nl> DISSECTOR_ASSERT ( size <( EMEM_PACKET_CHUNK_SIZE >> 2 )); <nl>  <nl> if (! mem -> free_list ) <nl> - emem_create_chunk (& mem -> free_list , use_canary ); <nl> + mem -> free_list = emem_create_chunk ( use_canary ); <nl>  <nl> /* oops , we need to allocate more memory to serve this request <nl> * than we have free . move this node to the used list and try again <nl> emem_alloc_chunk ( size_t size , emem_header_t * mem ) <nl> mem -> used_list = npc ; <nl>  <nl> if (! mem -> free_list ) <nl> - emem_create_chunk (& mem -> free_list , use_canary ); <nl> + mem -> free_list = emem_create_chunk ( use_canary ); <nl> } <nl>  <nl> free_list = mem -> free_list ;
mmm epan / dissectors / packet - etsi_card_app_toolkit . c <nl> ppp epan / dissectors / packet - etsi_card_app_toolkit . c <nl> dissect_cat ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> break ; <nl> case 0x05 : /* alpha identifier */ <nl> break ; <nl> + case 0x06 : /* address */ <nl> + de_cld_party_bcd_num ( tvb , elem_tree , pinfo , pos , len , NULL , 0 ); <nl> + break ; <nl> case 0x0b : /* sms tpdu */ <nl> new_tvb = tvb_new_subset ( tvb , pos , len , len ); <nl> if ( new_tvb ) {
mmm epan / dissectors / packet - zebra . c <nl> ppp epan / dissectors / packet - zebra . c <nl> static const value_string families [] = { <nl>  <nl> # define PSIZE ( a ) ((( a ) + 7 ) / ( 8 )) <nl>  <nl> - static void <nl> + static int <nl> dissect_zebra_request ( proto_tree * tree , gboolean request , tvbuff_t * tvb , <nl> int offset , guint16 len , guint8 command ) <nl> { <nl> dissect_zebra_request ( proto_tree * tree , gboolean request , tvbuff_t * tvb , <nl> /* Not yet implemeted in ZEBRA */ <nl> break ; <nl> } <nl> + return offset ; <nl> } <nl>  <nl> static void
mmm epan / wslua / wslua_util . c <nl> ppp epan / wslua / wslua_util . c <nl> WSLUA_METAMETHOD Dir__call ( lua_State * L ) { <nl> const gchar * filename ; <nl> const char * ext ; <nl>  <nl> - if (! dir ) <nl> + if (! dir ) { <nl> luaL_argerror ( L , 1 ," must be a Dir "); <nl> + return 0 ; <nl> + } <nl>  <nl> if (! dir -> dir ) { <nl> return 0 ;
mmm epan / dissectors / packet - mongo . c <nl> ppp epan / dissectors / packet - mongo . c <nl> dissect_bson_document ( tvbuff_t * tvb , packet_info * pinfo , guint offset , proto_tre <nl>  <nl> return document_length ; <nl> } <nl> + <nl> static int <nl> dissect_mongo_reply ( tvbuff_t * tvb , packet_info * pinfo , guint offset , proto_tree * tree ) <nl> { <nl> dissect_mongo_reply ( tvbuff_t * tvb , packet_info * pinfo , guint offset , proto_tree <nl> } <nl> return offset ; <nl> } <nl> + <nl> static int <nl> dissect_mongo_msg ( tvbuff_t * tvb , guint offset , proto_tree * tree ) <nl> { <nl> dissect_mongo_kill_cursors ( tvbuff_t * tvb , guint offset , proto_tree * tree ) <nl> } <nl> return offset ; <nl> } <nl> + <nl> static int <nl> dissect_mongo_pdu ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data _U_ ) <nl> { <nl> proto_register_mongo ( void ) <nl>  <nl> proto_mongo = proto_register_protocol (" Mongo Wire Protocol ", " MONGO ", " mongo "); <nl>  <nl> + /* Allow dissector to find be found by name . */ <nl> + new_register_dissector (" mongo ", dissect_mongo , proto_mongo ); <nl> + <nl> proto_register_field_array ( proto_mongo , hf , array_length ( hf )); <nl> proto_register_subtree_array ( ett , array_length ( ett )); <nl> expert_mongo = expert_register_protocol ( proto_mongo );
mmm plugins / megaco / packet - megaco . c <nl> ppp plugins / megaco / packet - megaco . c <nl> static gint tvb_skip_wsp ( tvbuff_t * tvb , gint offset ){ <nl>  <nl> for ( counter = offset ; counter < end && <nl> (( tempchar = tvb_get_guint8 ( tvb , counter )) == ' ' || <nl> - tempchar == '\ t '|| tempchar == '\ n '); counter ++); <nl> + tempchar == '\ t ' || tempchar == '\ n ' || tempchar == '\ r '); counter ++); <nl> return ( counter ); <nl> } <nl> static gint tvb_skip_wsp_return ( tvbuff_t * tvb , gint offset ){ <nl> static gint tvb_skip_wsp_return ( tvbuff_t * tvb , gint offset ){ <nl>  <nl> for ( counter = offset ; counter > end && <nl> (( tempchar = tvb_get_guint8 ( tvb , counter )) == ' ' || <nl> - tempchar == '\ t '|| tempchar == '\ n '); counter --); <nl> + tempchar == '\ t ' || tempchar == '\ n ' || tempchar == '\ r '); counter --); <nl> counter ++; <nl> return ( counter ); <nl> }
mmm epan / dissectors / packet - websocket . c <nl> ppp epan / dissectors / packet - websocket . c <nl> dissect_websocket_frame ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , voi <nl> if ( http_conv ) { <nl> websocket_conv -> subprotocol = http_conv -> websocket_protocol ; <nl> websocket_conv -> server_port = http_conv -> server_port ; <nl> - websocket_parse_extensions ( websocket_conv , http_conv -> websocket_extensions ); <nl> + if ( http_conv -> websocket_extensions ) { <nl> + websocket_parse_extensions ( websocket_conv , http_conv -> websocket_extensions ); <nl> + } <nl> } <nl>  <nl> conversation_add_proto_data ( conv , proto_websocket , websocket_conv );
mmm epan / dissectors / packet - manolito . c <nl> ppp epan / dissectors / packet - manolito . c <nl> dissect_manolito ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * diss <nl> 4 + length , str , "% s (% s ): % s ", ( char *) field_name_str , longname , str ); <nl> offset += length ; <nl> } else if ( dtype == MANOLITO_INTEGER ) { <nl> + gboolean len_ok = TRUE ; <nl> guint64 n = 0 ; <nl>  <nl> /* integers can be up to 5 bytes */ <nl> dissect_manolito ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * diss <nl> case 1 : <nl> n = tvb_get_guint8 ( tvb , offset ); <nl> break ; <nl> + <nl> + default : <nl> + len_ok = FALSE ; <nl> } <nl>  <nl> - ti = proto_tree_add_uint64_format ( manolito_tree , hf_manolito_integer , tvb , start , <nl> - 4 + length , n , "% s (% s ): %" G_GINT64_MODIFIER " u ", <nl> - ( char *) field_name_str , longname , n ); <nl> + if ( len_ok ) { <nl> + ti = proto_tree_add_uint64_format ( manolito_tree , hf_manolito_integer , tvb , start , <nl> + 4 + length , n , "% s (% s ): %" G_GINT64_MODIFIER " u ", <nl> + ( char *) field_name_str , longname , n ); <nl> + } <nl> + else { <nl> + /* XXX - expert info */ <nl> + } <nl> offset += length ; <nl> } else { <nl> proto_tree_add_expert_format ( manolito_tree , pinfo , & ei_manolito_type ,
mmm packet - zebra . c <nl> ppp packet - zebra . c <nl> * <nl> * Jochen Friedrich < jochen @ scram . de > <nl> * <nl> - * $ Id : packet - zebra . c , v 1 . 20 2002 / 01 / 24 09 : 20 : 54 guy Exp $ <nl> + * $ Id : packet - zebra . c , v 1 . 21 2002 / 04 / 02 01 : 32 : 11 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> dissect_zebra_request ( proto_tree * tree , gboolean request , tvbuff_t * tvb , <nl> guint32 prefix4 ; <nl> guint16 i ; <nl> guint8 buffer6 [ 16 ], prefixlen , message ; <nl> - const guint8 * prefix ; <nl> proto_item * ti ; <nl> proto_tree * msg_tree ; <nl>  <nl> dissect_zebra_request ( proto_tree * tree , gboolean request , tvbuff_t * tvb , <nl> offset , 1 , prefixlen ); <nl> offset += 1 ; <nl>  <nl> - prefix = tvb_get_ptr ( tvb , offset , PSIZE ( prefixlen )); <nl> prefix4 = 0 ; <nl> - memcpy (& prefix4 , prefix , <nl> + tvb_memcpy ( tvb , ( guint8 *)& prefix4 , offset , <nl> MIN (( unsigned ) PSIZE ( prefixlen ), sizeof prefix4 )); <nl> proto_tree_add_ipv4 ( tree , hf_zebra_prefix4 , <nl> tvb , offset , PSIZE ( prefixlen ), prefix4 ); <nl> dissect_zebra_request ( proto_tree * tree , gboolean request , tvbuff_t * tvb , <nl> offset , 1 , prefixlen ); <nl> offset += 1 ; <nl>  <nl> - prefix = tvb_get_ptr ( tvb , offset , PSIZE ( prefixlen )); <nl> memset ( buffer6 , '\ 0 ', sizeof buffer6 ); <nl> - memcpy ( buffer6 , prefix , <nl> + tvb_memcpy ( tvb , buffer6 , offset , <nl> MIN (( unsigned ) PSIZE ( prefixlen ), sizeof buffer6 )); <nl> proto_tree_add_ipv6 ( tree , hf_zebra_prefix6 , <nl> tvb , offset , PSIZE ( prefixlen ), buffer6 );
mmm epan / dissectors / packet - ieee80211 - radiotap - iter . c <nl> ppp epan / dissectors / packet - ieee80211 - radiotap - iter . c <nl> int ieee80211_radiotap_iterator_init ( <nl>  <nl> /* find payload start allowing for extended bitmap ( s ) */ <nl> if ( iterator -> _bitmap_shifter & ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> + if (! ITERATOR_VALID ( iterator , sizeof ( guint32 ))) <nl> + return - EINVAL ; <nl> while ( get_unaligned_le32 ( iterator -> _arg ) & <nl> ( 1 << IEEE80211_RADIOTAP_EXT )) { <nl> iterator -> _arg += sizeof ( guint32 );
mmm ui / win32 / file_dlg_win32 . c <nl> ppp ui / win32 / file_dlg_win32 . c <nl> gboolean win32_save_as_statstree ( HWND h_wnd , GString * file_name , int * file_type ) <nl> return gsfn_ok ; <nl> } <nl>  <nl> - <nl> + <nl> gboolean <nl> win32_export_specified_packets_file ( HWND h_wnd , capture_file * cf , <nl> GString * file_name , <nl> win32_export_sslkeys_file ( HWND h_wnd ) { <nl> OPENFILENAME * ofn ; <nl> TCHAR file_name [ MAX_PATH ] = _T (""); <nl> char * dirname ; <nl> - gchar * keylist ; <nl> + gchar * keylist = NULL ; <nl> char * file_name8 ; <nl> int fd ; <nl> int ofnsize ;
mmm epan / dissectors / packet - mim . c <nl> ppp epan / dissectors / packet - mim . c <nl> proto_reg_handoff_fabricpath ( void ) <nl> /* <nl> dissector_handle_t fp_handle ; <nl> fp_handle = new_create_dissector_handle ( dissect_fp , proto_fp ); <nl> - dissector_add (" ethertype ", ETHERTYPE_DCE , fp_handle ); <nl> + dissector_add_uint (" ethertype ", ETHERTYPE_DCE , fp_handle ); <nl> */ <nl> static gboolean prefs_initialized = FALSE ; <nl> 
mmm ui / gtk / simple_dialog . c <nl> ppp ui / gtk / simple_dialog . c <nl> do_simple_message_box ( ESD_TYPE_E type , gboolean * notagain , <nl> if ( notagain != NULL ) { <nl> checkbox = gtk_check_button_new_with_label (" Don ' t show this message again ."); <nl> gtk_container_set_border_width ( GTK_CONTAINER ( checkbox ), 12 ); <nl> - gtk_box_pack_start ( GTK_BOX ( gtk_message_dialog_get_message_area ( GTK_MESSAGE_DIALOG ( msg_dialog ))), checkbox , <nl> - TRUE , TRUE , 0 ); <nl> + gtk_box_pack_start ( GTK_BOX ( gtk_dialog_get_content_area ( GTK_DIALOG ( msg_dialog ))), <nl> + checkbox , TRUE , TRUE , 0 ); <nl> gtk_widget_show ( checkbox ); <nl> } <nl> 
mmm epan / epan . h <nl> ppp epan / epan . h <nl> typedef struct _epan_dissect_t epan_dissect_t ; <nl> # include " dfilter / dfilter . h " <nl>  <nl> /* init the whole epan module , this is used to be called only once in a program */ <nl> - void epan_init ( void (* register_all_protocols )( register_cb cb , gpointer client_data ), <nl> - void (* register_all_handoffs )( register_cb cb , gpointer client_data ), <nl> + void epan_init ( void (* register_all_protocols_func )( register_cb cb , gpointer client_data ), <nl> + void (* register_all_handoffs_func )( register_cb cb , gpointer client_data ), <nl> register_cb cb , <nl> void * client_data , <nl> void (* report_failure )( const char *, va_list ),mmm epan / epan . c <nl> ppp epan / epan . c <nl> typedef struct _epan_dissect_t epan_dissect_t ; <nl> # include " dfilter / dfilter . h " <nl>  <nl> /* init the whole epan module , this is used to be called only once in a program */ <nl> - void epan_init ( void (* register_all_protocols )( register_cb cb , gpointer client_data ), <nl> - void (* register_all_handoffs )( register_cb cb , gpointer client_data ), <nl> + void epan_init ( void (* register_all_protocols_func )( register_cb cb , gpointer client_data ), <nl> + void (* register_all_handoffs_func )( register_cb cb , gpointer client_data ), <nl> register_cb cb , <nl> void * client_data , <nl> void (* report_failure )( const char *, va_list ), <nl> epan_get_version ( void ) { <nl> } <nl>  <nl> void <nl> - epan_init ( void (* register_all_protocols )( register_cb cb , gpointer client_data ), <nl> - void (* register_all_handoffs )( register_cb cb , gpointer client_data ), <nl> + epan_init ( void (* register_all_protocols_func )( register_cb cb , gpointer client_data ), <nl> + void (* register_all_handoffs_func )( register_cb cb , gpointer client_data ), <nl> register_cb cb , <nl> gpointer client_data , <nl> void (* report_failure )( const char *, va_list ), <nl> epan_init ( void (* register_all_protocols )( register_cb cb , gpointer client_data ), <nl> tvbuff_init (); <nl> tap_init (); <nl> prefs_init (); <nl> - proto_init ( register_all_protocols , register_all_handoffs , cb , client_data ); <nl> + proto_init ( register_all_protocols_func , register_all_handoffs_func , <nl> + cb , client_data ); <nl> packet_init (); <nl> dfilter_init (); <nl> final_registration_all_protocols ();
mmm ui / gtk / capture_dlg . c <nl> ppp ui / gtk / capture_dlg . c <nl> capture_all_filter_check_syntax_cb ( GtkWidget * w _U_ , gpointer user_data _U_ ) <nl> } <nl> # ifdef HAVE_EXTCAP <nl> /* Can ' t verify extcap capture filters */ <nl> - if ( device . if_info . extcap != NULL ) <nl> + if ( device . if_info . extcap != NULL && strlen ( device . if_info . extcap ) > 0 ) <nl> continue ; <nl> # endif <nl> filter_text = gtk_combo_box_text_get_active_text ( GTK_COMBO_BOX_TEXT ( filter_cm ));
mmm plugins / profinet / packet - dcerpc - pn - io . c <nl> ppp plugins / profinet / packet - dcerpc - pn - io . c <nl> static expert_field ei_pn_io_error_code2 = EI_INIT ; <nl> static expert_field ei_pn_io_ar_info_not_found = EI_INIT ; <nl> static expert_field ei_pn_io_iocr_type = EI_INIT ; <nl> static expert_field ei_pn_io_frame_id = EI_INIT ; <nl> + static expert_field ei_pn_io_nr_of_tx_port_groups = EI_INIT ; <nl>  <nl> static e_uuid_t uuid_pn_io_device = { 0xDEA00001 , 0x6C97 , 0x11D1 , { 0x82 , 0x71 , 0x00 , 0xA0 , 0x24 , 0x42 , 0xDF , 0x7D } }; <nl> static guint16 ver_pn_io_device = 1 ; <nl> dissect_PDIRFrameData_block ( tvbuff_t * tvb , int offset , <nl> offset = dissect_dcerpc_uint8 ( tvb , offset , pinfo , sub_tree , drep , <nl> hf_pn_io_frame_details_reserved , & u8FrameDetails ); <nl> /* TxPortGroup */ <nl> - offset = dissect_dcerpc_uint8 ( tvb , offset , pinfo , ir_frame_data_tree , drep , <nl> - hf_pn_io_nr_of_tx_port_groups , & u8NumberOfTxPortGroups ); <nl> + u8NumberOfTxPortGroups = tvb_get_guint8 ( tvb , offset ); <nl> + sub_item = proto_tree_add_uint ( ir_frame_data_tree , hf_pn_io_nr_of_tx_port_groups , <nl> + tvb , offset , 1 , u8NumberOfTxPortGroups ); <nl> + offset ++; <nl> if (( u8NumberOfTxPortGroups > 21 ) || (( u8NumberOfTxPortGroups & 0x1 ) != 1 )) { <nl> - proto_tree_add_text ( ir_frame_data_tree , tvb , offset - 1 , 1 , " Not allowed value of NumberOfTxPortGroups "); <nl> + expert_add_info ( pinfo , sub_item , & ei_pn_io_nr_of_tx_port_groups ); <nl> } <nl>  <nl> /* TxPortArray */ <nl> proto_register_pn_io ( void ) <nl> { & ei_pn_io_frame_id , { " pn_io . frame_id . changed ", PI_UNDECODED , PI_WARN , " FrameID changed ", EXPFILL }}, <nl> { & ei_pn_io_iocr_type , { " pn_io . iocr_type . unknown ", PI_UNDECODED , PI_WARN , " IOCRType undecoded !", EXPFILL }}, <nl> { & ei_pn_io_localalarmref , { " pn_io . localalarmref . changed ", PI_UNDECODED , PI_WARN , " AlarmCRBlockReq : local alarm ref changed ", EXPFILL }}, <nl> + { & ei_pn_io_nr_of_tx_port_groups , { " pn_io . nr_of_tx_port_groups . not_allowed ", PI_PROTOCOL , PI_WARN , " Not allowed value of NumberOfTxPortGroups ", EXPFILL }}, <nl> }; <nl>  <nl> expert_module_t * expert_pn_io ;
mmm epan / dissectors / packet - snmp . c <nl> ppp epan / dissectors / packet - snmp . c <nl> extern int dissect_snmp_VarBind ( gboolean implicit_tag _U_ , <nl>  <nl> add_oid_debug_subtree ( oid_info , pt_name ); <nl>  <nl> - if ( subids && oid_matched + oid_left ) { <nl> + if (! subids ) { <nl> + proto_item * pi = proto_tree_add_text ( pt_name , tvb , 0 , 0 , " invalid oid : % s ", oid_bytes ); <nl> + pt = proto_item_add_subtree ( pi , ett_decoding_error ); <nl> + expert_add_info_format ( actx -> pinfo , pi , PI_MALFORMED , PI_WARN , " invalid oid : % s ", oid_bytes ); <nl> + return dissect_unknown_ber ( actx -> pinfo , tvb , name_offset , pt ); <nl> + } <nl> + <nl> + if ( oid_matched + oid_left ) { <nl> oid_string = oid_subid2string ( subids , oid_matched + oid_left ); <nl> } <nl> 
mmm ui / qt / packet_list . cpp <nl> ppp ui / qt / packet_list . cpp <nl> void PacketList :: columnsChanged () <nl> setColumnVisibility (); <nl> create_far_overlay_ = true ; <nl> packet_list_model_ -> resetColumns (); <nl> + applyRecentColumnWidths (); <nl> columns_changed_ = false ; <nl> } <nl>  <nl> void PacketList :: setCaptureFile ( capture_file * cf ) <nl> cap_file_ = cf ; <nl> if ( cap_file_ && columns_changed_ ) { <nl> columnsChanged (); <nl> - applyRecentColumnWidths (); <nl> } <nl> packet_list_model_ -> setCaptureFile ( cf ); <nl> create_near_overlay_ = true ;
mmm gtk / gui_utils . c <nl> ppp gtk / gui_utils . c <nl> str_ptr_data_func ( GtkTreeViewColumn * column _U_ , <nl> GtkTreeModel * model , <nl> GtkTreeIter * iter , <nl> gpointer user_data ) <nl> - { <nl> + { <nl> const gchar * str = NULL ; <nl>  <nl> /* The col to get data from is in userdata */ <nl> str_ptr_sort_func ( GtkTreeModel * model , <nl> gtk_tree_model_get ( model , a , data_column , & str_a , - 1 ); <nl> gtk_tree_model_get ( model , b , data_column , & str_b , - 1 ); <nl>  <nl> - if ( str_a == NULL || str_b == NULL ){ <nl> - if ( str_a == NULL && str_b == NULL ) <nl> - return 0 ; <nl> + if ( str_a == str_b ) { <nl> + /* it ' s worth testing because a lot of row point to <nl> + the same data */ <nl> + return 0 ; <nl> + } <nl> + else if ( str_a == NULL || str_b == NULL ) { <nl> ret = ( str_a == NULL ) ? - 1 : 1 ; <nl> - } else { <nl> + } <nl> + else { <nl> ret = g_ascii_strcasecmp ( str_a , str_b ); <nl> } <nl> return ret ;
mmm epan / tvbuff . c <nl> ppp epan / tvbuff . c <nl> tvb_uncompress ( tvbuff_t * tvb , int offset , int comprlen ) <nl> compr = tvb_memdup ( tvb , offset , comprlen ); <nl>  <nl> if (! compr ) { <nl> + g_free ( strm ); <nl> return NULL ; <nl> } <nl>  <nl> tvb_uncompress ( tvbuff_t * tvb , int offset , int comprlen ) <nl>  <nl> if ( strmbuf == NULL ) { <nl> g_free ( compr ); <nl> + g_free ( strm ); <nl> return NULL ; <nl> } <nl> 
mmm epan / dissectors / packet - usb . c <nl> ppp epan / dissectors / packet - usb . c <nl> dissect_usb_common ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * parent , <nl> guint8 header_info ) <nl> { <nl> gint offset = 0 ; <nl> - gint new_offset ; <nl> int endpoint ; <nl> gint type_2 = 0 ; <nl> guint8 urb_type ; <nl> dissect_usb_common ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * parent , <nl> default : <nl> /* Try to find a non - standard specific dissector */ <nl> if ( tvb_reported_length_remaining ( tvb , offset ) != 0 ) { <nl> - next_tvb = tvb_new_subset_remaining ( tvb , offset ); <nl> - new_offset = try_dissect_next_protocol ( tree , parent , next_tvb , offset , pinfo , usb_conv_info , type_2 , urb_type , NULL , NULL ); <nl> - if ( new_offset > offset ) <nl> - offset = new_offset ; <nl> + gint new_offset ; <nl> + next_tvb = tvb_new_subset_remaining ( tvb , offset ); <nl> + new_offset = try_dissect_next_protocol ( tree , parent , next_tvb , offset , pinfo , usb_conv_info , type_2 , urb_type , NULL , NULL ); <nl> + if ( new_offset > offset ) <nl> + offset = new_offset ; <nl> } <nl>  <nl> if ( tvb_reported_length_remaining ( tvb , offset ) != 0 ) {
mmm capture_loop . c <nl> ppp capture_loop . c <nl> capture_loop_start ( capture_options * capture_opts , gboolean * stats_known , struct <nl> /* We haven ' t yet gotten the capture statistics . */ <nl> * stats_known = FALSE ; <nl>  <nl> -# ifndef _WIN32 <nl> +# ifdef _WIN32 <nl> + /* get the initial state of the signal pipe */ <nl> + /* ( if it ' s already stopped here , ignore it later ) */ <nl> + signal_pipe_enabled = ! signal_pipe_stopped (); <nl> +# else <nl> /* <nl> * Catch SIGUSR1 , so that we exit cleanly if the parent process <nl> * kills us with it due to the user selecting " Capture -> Stop ". <nl> capture_loop_start ( capture_options * capture_opts , gboolean * stats_known , struct <nl> signal ( SIGUSR1 , capture_loop_stop_signal_handler ); <nl> # endif <nl>  <nl> - /* get the initial state of the signal pipe */ <nl> - /* ( if it ' s already stopped here , ignore it later ) */ <nl> - signal_pipe_enabled = ! signal_pipe_stopped (); <nl> - <nl> g_log ( LOG_DOMAIN_CAPTURE_CHILD , G_LOG_LEVEL_INFO , " Capture child starting ..."); <nl> capture_opts_log ( LOG_DOMAIN_CAPTURE_CHILD , G_LOG_LEVEL_DEBUG , capture_opts ); <nl> 
mmm proto . c <nl> ppp proto . c <nl> /* proto . c <nl> * Routines for protocol tree <nl> * <nl> - * $ Id : proto . c , v 1 . 59 2000 / 04 / 04 06 : 17 : 29 guy Exp $ <nl> + * $ Id : proto . c , v 1 . 60 2000 / 04 / 04 17 : 07 : 07 gram Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ zing . org > <nl> proto_tree_add_bytes_format ( proto_tree * tree , int hfindex , gint start , gint leng <nl> static void <nl> proto_tree_set_bytes ( field_info * fi , const guint8 * start_ptr , gint length ) <nl> { <nl> - <nl> + g_assert ( start_ptr != NULL ); <nl> + g_assert ( length > 0 ); <nl> /* This g_malloc ' ed memory is freed in <nl> proto_tree_free_node () */ <nl> fi -> value . bytes = g_malloc ( length ); <nl> proto_tree_add_field_info ( int hfindex , gint start , gint length , int visible ) <nl>  <nl> fi = g_mem_chunk_alloc ( gmc_field_info ); <nl>  <nl> + g_assert ( hfindex >= 0 && hfindex < gpa_hfinfo -> len ); <nl> fi -> hfinfo = proto_registrar_get_nth ( hfindex ); <nl> g_assert ( fi -> hfinfo != NULL ); <nl> fi -> start = start ;
mmm epan / dissectors / packet - telnet . c <nl> ppp epan / dissectors / packet - telnet . c <nl> unescape_and_tvbuffify_telnet_option ( packet_info * pinfo , tvbuff_t * tvb , int offs <nl> return NULL ; <nl>  <nl> spos = tvb_get_ptr ( tvb , offset , len ); <nl> - /* XXX we never g_free () this one . This is done automagically <nl> - when the parent tvb is destroyed ? <nl> - */ <nl> buf = g_malloc ( len ); <nl> dpos = buf ; <nl> skip = 0 ; <nl> unescape_and_tvbuffify_telnet_option ( packet_info * pinfo , tvbuff_t * tvb , int offs <nl> l --; <nl> } <nl> krb5_tvb = tvb_new_real_data ( buf , len - skip , len - skip ); <nl> + tvb_set_free_cb ( krb5_tvb , g_free ); <nl> tvb_set_child_real_data_tvbuff ( tvb , krb5_tvb ); <nl> add_new_data_source ( pinfo , krb5_tvb , " Unpacked Telnet Uption "); <nl> 
mmm epan / dissectors / packet - umts_fp . c <nl> ppp epan / dissectors / packet - umts_fp . c <nl> check_payload_crc_for_heur ( tvbuff_t * tvb , guint16 header_length ) <nl> static guint32 <nl> generate_ue_id_for_heur ( packet_info * pinfo ) <nl> { <nl> - if ( pinfo -> ptype != PT_UDP && pinfo -> src . type == AT_IPv4 && pinfo -> dst . type == AT_IPv4 ) { <nl> + if ( pinfo -> ptype == PT_UDP && pinfo -> src . type == AT_IPv4 && pinfo -> dst . type == AT_IPv4 ) { <nl> /* This logic assumes FP is delivered over IP / UDP */ <nl> /* Will return the same ID even if the address and ports are reversed */ <nl> 
mmm epan / crypt / dot11decrypt . c <nl> ppp epan / crypt / dot11decrypt . c <nl> Dot11DecryptDecryptWPABroadcastKey ( const EAPOL_RSN_KEY * pEAPKey , guint8 * decrypt <nl> DEBUG_DUMP (" FullDecrKey :", new_key , 32 ); <nl>  <nl> if ( gcry_cipher_open (& rc4_handle , GCRY_CIPHER_ARCFOUR , GCRY_CIPHER_MODE_STREAM , 0 )) { <nl> + g_free ( szEncryptedKey ); <nl> return DOT11DECRYPT_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> if ( gcry_cipher_setkey ( rc4_handle , new_key , sizeof ( new_key ))) { <nl> gcry_cipher_close ( rc4_handle ); <nl> + g_free ( szEncryptedKey ); <nl> return DOT11DECRYPT_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> 
mmm epan / dissectors / packet - ssl - utils . c <nl> ppp epan / dissectors / packet - ssl - utils . c <nl> static const char * ciphers []={ <nl> " RC2 ", <nl> " IDEA ", <nl> " AES ", <nl> - " AES256 " <nl> + " AES256 ", <nl> + "* UNKNOWN *" <nl> }; <nl>  <nl> /* look in openssl / ssl / ssl_lib . c for a complete list of available cipersuite */ <nl> ssl_create_decoder ( SslDecoder * dec , SslCipherSuite * cipher_suite , <nl> } <nl> if ( ciph == 0 ) { <nl> ssl_debug_printf (" ssl_create_decoder can ' t find cipher % s \ n ", <nl> - ciphers [ cipher_suite -> enc - 0x30 ]); <nl> + ciphers [( cipher_suite -> enc - 0x30 ) > 7 ? 7 : ( cipher_suite -> enc - 0x30 )]); <nl> return - 1 ; <nl> } <nl> 
mmm epan / dissectors / packet - ntlmssp . c <nl> ppp epan / dissectors / packet - ntlmssp . c <nl> dissect_ntlmssp_payload ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , voi <nl> /* Encrypted body */ <nl> proto_tree_add_item ( ntlmssp_tree , hf_ntlmssp_verf_body , <nl> tvb , offset , ntlm_signature_size + ntlm_seq_size , ENC_NA ); <nl> + memset ( key , 0 , sizeof ( key )); <nl> tvb_memcpy ( tvb , key , offset , ntlm_signature_size + ntlm_seq_size ); <nl> /* Try to decrypt */ <nl> decrypt_data_payload ( tvb , offset +( ntlm_signature_size + ntlm_seq_size ), encrypted_block_length -( ntlm_signature_size + ntlm_seq_size ), pinfo , ntlmssp_tree , key );
mmm packet - icmpv6 . c <nl> ppp packet - icmpv6 . c <nl> /* packet - icmpv6 . c <nl> * Routines for ICMPv6 packet disassembly <nl> * <nl> - * $ Id : packet - icmpv6 . c , v 1 . 57 2002 / 01 / 09 19 : 13 : 03 guy Exp $ <nl> + * $ Id : packet - icmpv6 . c , v 1 . 58 2002 / 01 / 10 09 : 49 : 35 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> again : <nl> ND_OPT_MAP_FLAG_P , 8 , " P ", " No P ")); <nl> proto_tree_add_text ( icmp6opt_tree , tvb , <nl> offset + offsetof ( struct nd_opt_map_info , nd_opt_map_lifetime ), <nl> - 4 , " Lifetime : % d ", pntohl (& map -> nd_opt_map_lifetime )); <nl> + 4 , " Lifetime : % u ", pntohl (& map -> nd_opt_map_lifetime )); <nl>  <nl> proto_tree_add_text ( icmp6opt_tree , tvb , <nl> offset + offsetof ( struct nd_opt_map_info , nd_opt_map_address ), 16 ,
mmm ui / qt / uat_dialog . cpp <nl> ppp ui / qt / uat_dialog . cpp <nl> void UatDialog :: on_uatTreeWidget_itemActivated ( QTreeWidgetItem * item , int column <nl> case PT_TXTMOD_FILENAME : <nl> { <nl> QString cur_path = fieldString ( row , column ); <nl> - const QByteArray & new_path = QFileDialog :: getSaveFileName ( this , <nl> + const QByteArray & new_path = QFileDialog :: getOpenFileName ( this , <nl> field -> title , cur_path , QString (), NULL , fd_opt ). toUtf8 (); <nl> field -> cb . set ( rec , new_path . constData (), ( unsigned ) new_path . size (), field -> cbdata . set , field -> fld_data ); <nl> updateItem (* item );mmm epan / uat . h <nl> ppp epan / uat . h <nl> void UatDialog :: on_uatTreeWidget_itemActivated ( QTreeWidgetItem * item , int column <nl> case PT_TXTMOD_FILENAME : <nl> { <nl> QString cur_path = fieldString ( row , column ); <nl> - const QByteArray & new_path = QFileDialog :: getSaveFileName ( this , <nl> + const QByteArray & new_path = QFileDialog :: getOpenFileName ( this , <nl> field -> title , cur_path , QString (), NULL , fd_opt ). toUtf8 (); <nl> field -> cb . set ( rec , new_path . constData (), ( unsigned ) new_path . size (), field -> cbdata . set , field -> fld_data ); <nl> updateItem (* item ); <nl> static void basename ## _ ## field_name ## _tostr_cb ( void * rec , const char ** out <nl> */ <nl> # define UAT_FILENAME_CB_DEF ( basename , field_name , rec_t ) UAT_CSTRING_CB_DEF ( basename , field_name , rec_t ) <nl>  <nl> +/* XXX UAT_FLD_FILENAME is currently unused . */ <nl> # define UAT_FLD_FILENAME ( basename , field_name , title , desc ) \ <nl> {# field_name , title , PT_TXTMOD_FILENAME ,{ uat_fld_chk_str , basename ## _ ## field_name ## _set_cb , basename ## _ ## field_name ## _tostr_cb },{ 0 , 0 , 0 }, 0 , desc , FLDFILL } <nl>  <nl> +/* <nl> + * Both the Qt and GTK + UIs assume that we ' re opening a preexisting <nl> + * file . We might want to split the ... _FILENAME defines into <nl> + * ... _FILE_OPEN and ... _FILE_SAVE if we ever need to specify a <nl> + * file that we ' re creating . <nl> + */ <nl> # define UAT_FLD_FILENAME_OTHER ( basename , field_name , title , chk , desc ) \ <nl> {# field_name , title , PT_TXTMOD_FILENAME ,{ chk , basename ## _ ## field_name ## _set_cb , basename ## _ ## field_name ## _tostr_cb },{ 0 , 0 , 0 }, 0 , desc , FLDFILL } <nl> 
mmm gtk / main . c <nl> ppp gtk / main . c <nl> main_filter_packets ( capture_file * cf , const gchar * dftext , gboolean force ) <nl> char * s ; <nl> cf_status_t cf_status ; <nl>  <nl> + /* we ' ll crash later on if dftext is NULL */ <nl> + g_assert ( dftext != NULL ); <nl> + <nl> s = g_strdup ( dftext ); <nl>  <nl> /* GtkCombos don ' t let us get at their list contents easily , so we maintain
mmm text2pcap . c <nl> ppp text2pcap . c <nl> parse_options ( int argc , char * argv []) <nl> {" version ", no_argument , NULL , ' v '}, <nl> { 0 , 0 , 0 , 0 } <nl> }; <nl> + struct tm * now_tm ; <nl>  <nl> # ifdef _WIN32 <nl> arg_list_utf_16to8 ( argc , argv ); <nl> parse_options ( int argc , char * argv []) <nl> } <nl>  <nl> ts_sec = time ( 0 ); /* initialize to current time */ <nl> - /* We trust the OS to return a time after the Epoch . */ <nl> - timecode_default = * localtime (& ts_sec ); <nl> + now_tm = localtime (& ts_sec ); <nl> + if ( now_tm == NULL ) { <nl> + /* <nl> + * This shouldn ' t happen - on UN * X , this should Just Work , and <nl> + * on Windows , it won ' t work if ts_sec is before the Epoch , <nl> + * but it ' s long after 1970 , so .... <nl> + */ <nl> + fprintf ( stderr , " localtime ( right now ) failed \ n "); <nl> + return EXIT_FAILURE ; <nl> + } <nl> + timecode_default = * now_tm ; <nl> timecode_default . tm_isdst = - 1 ; /* Unknown for now , depends on time given to the strptime () function */ <nl>  <nl> /* Display summary of our state */
mmm ui / qt / extcap_argument . h <nl> ppp ui / qt / extcap_argument . h <nl> private Q_SLOTS : <nl> }; <nl>  <nl> Q_DECLARE_METATYPE ( ExtcapArgument ) <nl> + Q_DECLARE_METATYPE ( ExtcapArgument *) <nl>  <nl> class ExtArgText : public ExtcapArgument <nl> {
mmm packet - dcerpc . c <nl> ppp packet - dcerpc . c <nl> * Copyright 2001 , Todd Sabin < tas @ webspan . net > <nl> * Copyright 2003 , Tim Potter < tpot @ samba . org > <nl> * <nl> - * $ Id : packet - dcerpc . c , v 1 . 151 2003 / 11 / 06 07 : 44 : 13 sahlberg Exp $ <nl> + * $ Id : packet - dcerpc . c , v 1 . 152 2003 / 11 / 06 09 : 13 : 26 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> dcerpc_try_handoff ( packet_info * pinfo , proto_tree * tree , <nl> pinfo -> current_proto = saved_proto ; <nl> pinfo -> private_data = saved_private_data ; <nl> } else { <nl> - /* No subdissector - show it as * decrypted * stub data . */ <nl> + /* No subdissector - show it as stub data . */ <nl> if ( decrypted_tvb ){ <nl> show_stub_data ( decrypted_tvb , 0 , tree , auth_info , FALSE ); <nl> } else { <nl> dissect_dcerpc_cn_stub ( tvbuff_t * tvb , int offset , packet_info * pinfo , <nl> thus we must reassemble it . <nl> */ <nl>  <nl> + /* Do we have any non - encrypted data to reassemble ? */ <nl> + if ( decrypted_tvb == NULL ) { <nl> + /* No . We can ' t even try to reassemble . */ <nl> + goto end_cn_stub ; <nl> + } <nl> + <nl> /* if this is the first fragment we need to start reassembly <nl> */ <nl> if ( hdr -> flags & PFC_FIRST_FRAG ){
mmm epan / dissectors / packet - slowprotocols . c <nl> ppp epan / dissectors / packet - slowprotocols . c <nl> dissect_oampdu_event_notification ( tvbuff_t * tvb , proto_tree * tree ) <nl>  <nl> offset += OAMPDU_EVENT_LENGTH_SZ ; <nl>  <nl> - offset += ( raw_word - 2 ); <nl> + offset += ( raw_octet - 2 ); <nl> break ; <nl> } <nl> default :
mmm epan / proto . c <nl> ppp epan / proto . c <nl> proto_tree_add_subtree_format ( proto_tree * tree , tvbuff_t * tvb , gint start , gint <nl> va_list ap ; <nl> header_field_info * hfinfo ; <nl>  <nl> + /* Make sure pi is initialized in case TRY_TO_FAKE_THIS_ITEM bails */ <nl> + if ( tree_item != NULL ) <nl> + * tree_item = NULL ; <nl> + <nl> TRY_TO_FAKE_THIS_ITEM ( tree , hf_text_only , hfinfo ); <nl>  <nl> pi = proto_tree_add_text_node ( tree , tvb , start , length );
mmm epan / dissectors / packet - ber . c <nl> ppp epan / dissectors / packet - ber . c <nl> reassemble_octet_string ( asn1_ctx_t * actx , proto_tree * tree , gint hf_id , tvbuff_t <nl>  <nl> /* so we need to consume octet strings for the given length */ <nl>  <nl> - /* not sure we need this */ <nl> - actx -> pinfo -> fragmented = TRUE ; <nl> - <nl> if ( out_tvb ) <nl> * out_tvb = NULL ; <nl>  <nl> + if ( con_len == 0 ) /* Zero encodings ( 8 . 7 . 3 ) */ <nl> + return offset ; <nl> + <nl> + /* not sure we need this */ <nl> + actx -> pinfo -> fragmented = TRUE ; <nl> + <nl> while (! fd_head ) { <nl>  <nl> offset = dissect_ber_octet_string ( FALSE , actx , NULL , tvb , offset , hf_id , & next_tvb );
mmm epan / dissectors / packet - rtps . c <nl> ppp epan / dissectors / packet - rtps . c <nl> static void dissect_HEARTBEAT_VIRTUAL ( tvbuff_t * tvb , packet_info * pinfo _U_ , gin <nl> if (!( flags & FLAG_VIRTUAL_HEARTBEAT_N )) { <nl> proto_tree_add_item ( sil_tree_writer , hf_rtps_virtual_heartbeat_num_virtual_guids , tvb , <nl> offset , 4 , little_endian ? ENC_LITTLE_ENDIAN : ENC_BIG_ENDIAN ); <nl> + num_virtual_guids = NEXT_guint32 ( tvb , offset , little_endian ); <nl> offset += 4 ; <nl> } else { <nl> num_virtual_guids = 0 ;
mmm wiretap / netscaler . c <nl> ppp wiretap / netscaler . c <nl> static gboolean nstrace_read_v20 ( wtap * wth , int * err , gchar ** err_info , gint64 * <nl> if (( nstrace -> nstrace_buflen - nstrace_buf_offset ) < sizeof * fp ) {\ <nl> * err = WTAP_ERR_BAD_FILE ;\ <nl> * err_info = g_strdup (" nstrace : record header crosses page boundary ");\ <nl> + g_free ( nstrace_tmpbuff );\ <nl> return FALSE ;\ <nl> }\ <nl> ( phdr )-> rec_type = REC_TYPE_PACKET ;\ <nl> static gboolean nstrace_read_v20 ( wtap * wth , int * err , gchar ** err_info , gint64 * <nl> if (( phdr )-> caplen < sizeof * fp ) {\ <nl> * err = WTAP_ERR_BAD_FILE ;\ <nl> * err_info = g_strdup (" nstrace : record size is less than record header size ");\ <nl> + g_free ( nstrace_tmpbuff );\ <nl> return FALSE ;\ <nl> }\ <nl> ws_buffer_assure_space ( wth -> frame_buffer , ( phdr )-> caplen );\ <nl> static gboolean nstrace_read_v20 ( wtap * wth , int * err , gchar ** err_info , gint64 * <nl> /* Read the next page */\ <nl> bytes_read = file_read ( nstrace_buf , NSPR_PAGESIZE_TRACE , wth -> fh );\ <nl> if ( ! file_eof ( wth -> fh ) && bytes_read != NSPR_PAGESIZE_TRACE ) {\ <nl> + g_free ( nstrace_tmpbuff );\ <nl> return FALSE ;\ <nl> } else {\ <nl> nstrace_buf_offset = 0 ;\ <nl> static gboolean nstrace_read_v20 ( wtap * wth , int * err , gchar ** err_info , gint64 * <nl> nstrace -> nstrace_buf_offset = nstrace_buf_offset ;\ <nl> nstrace -> nstrace_buflen = nstrace_buflen ;\ <nl> nstrace -> nsg_creltime = nsg_creltime ;\ <nl> + g_free ( nstrace_tmpbuff );\ <nl> return TRUE ;\ <nl> } while ( 0 ) <nl>  <nl> static gboolean nstrace_read_v30 ( wtap * wth , int * err , gchar ** err_info , gint64 * <nl> gchar * nstrace_buf = nstrace -> pnstrace_buf ; <nl> guint32 nstrace_buf_offset = nstrace -> nstrace_buf_offset ; <nl> guint32 nstrace_buflen = nstrace -> nstrace_buflen ; <nl> - guint8 nstrace_tmpbuff [ 65536 ]; <nl> + guint8 * nstrace_tmpbuff ; <nl> guint32 nstrace_tmpbuff_off = 0 , nst_dataSize = 0 , rec_size = 0 , nsg_nextPageOffset = 0 ; <nl> nspr_hd_v20_t * hdp ; <nl> int bytes_read = 0 ; <nl> static gboolean nstrace_read_v30 ( wtap * wth , int * err , gchar ** err_info , gint64 * <nl> return FALSE ; /* Reached End Of File */ <nl> } <nl>  <nl> + nstrace_tmpbuff = ( guint8 *) g_malloc ( 65536 ); <nl> + <nl> do <nl> { <nl> if (! nstrace_buf [ nstrace_buf_offset ] && nstrace_buf_offset <= NSPR_PAGESIZE_TRACE ){ <nl> static gboolean nstrace_read_v30 ( wtap * wth , int * err , gchar ** err_info , gint64 * <nl> if ( nspr_getv20recordsize ( hdp ) == 0 ) { <nl> * err = WTAP_ERR_BAD_FILE ; <nl> * err_info = g_strdup (" nstrace : zero size record found "); <nl> + g_free ( nstrace_tmpbuff ); <nl> return FALSE ; <nl> } <nl> switch ( hdp -> phd_RecordType ) <nl> static gboolean nstrace_read_v30 ( wtap * wth , int * err , gchar ** err_info , gint64 * <nl> nstrace_buflen = NSPR_PAGESIZE_TRACE ; <nl> } while (( nstrace_buflen > 0 ) && ( nstrace_read_buf ( wth -> fh , nstrace_buf , nstrace_buflen , err , err_info ))); <nl>  <nl> + g_free ( nstrace_tmpbuff ); <nl> return FALSE ; <nl> } <nl> 
mmm wiretap / pppdump . c <nl> ppp wiretap / pppdump . c <nl> /* pppdump . c <nl> * <nl> - * $ Id : pppdump . c , v 1 . 5 2000 / 11 / 19 03 : 47 : 36 guy Exp $ <nl> + * $ Id : pppdump . c , v 1 . 6 2000 / 11 / 19 20 : 56 : 17 gerald Exp $ <nl> * <nl> * Copyright ( c ) 2000 by Gilbert Ramirez < gram @ xiexie . org > <nl> * <nl> process_data ( pppdump_t * state , FILE_T fh , pkt_t * pkt , int n , guint8 * pd , int * er <nl> return 0 ; <nl> } <nl>  <nl> + if ( num_written > sizeof ( pd )) { <nl> + * err = WTAP_ERR_UNC_OVERFLOW ; <nl> + return - 1 ; <nl> + } <nl> + <nl> memcpy ( pd , pkt -> buf , num_written ); <nl>  <nl> num_bytes --;
mmm wiretap / erf . c <nl> ppp wiretap / erf . c <nl> static void erf_write_wtap_option_to_capture_tag ( wtap_block_t block _U_ , <nl> break ; <nl> default : <nl> erf_meta_tag_free ( tag_ptr ); <nl> - return ; <nl> + tag_ptr = NULL ; <nl> + break ; <nl> } <nl>  <nl> - g_ptr_array_add ( section_ptr -> tags , tag_ptr ); <nl> + if ( tag_ptr ) <nl> + g_ptr_array_add ( section_ptr -> tags , tag_ptr ); <nl> } <nl>  <nl> static void erf_write_wtap_option_to_host_tag ( wtap_block_t block _U_ , <nl> static void erf_write_wtap_option_to_host_tag ( wtap_block_t block _U_ , <nl> break ; <nl> default : <nl> erf_meta_tag_free ( tag_ptr ); <nl> - return ; <nl> + tag_ptr = NULL ; <nl> + break ; <nl> } <nl>  <nl> - g_ptr_array_add ( section_ptr -> tags , tag_ptr ); <nl> + if ( tag_ptr ) <nl> + g_ptr_array_add ( section_ptr -> tags , tag_ptr ); <nl> } <nl>  <nl> static void erf_write_wtap_option_to_interface_tag ( wtap_block_t block _U_ ,
mmm epan / dissectors / packet - ssl - utils . c <nl> ppp epan / dissectors / packet - ssl - utils . c <nl> ssl_keylog_lookup ( SslDecryptSession * ssl_session , <nl> line [ bytes_read - 1 ] = 0 ; <nl> bytes_read --; <nl> } <nl> + if ( bytes_read > 0 && line [ bytes_read - 1 ] == '\ r ') { <nl> + line [ bytes_read - 1 ] = 0 ; <nl> + bytes_read --; <nl> + } <nl>  <nl> ssl_debug_printf (" checking keylog line : % s \ n ", line ); <nl> 
mmm epan / dissectors / packet - tcp . c <nl> ppp epan / dissectors / packet - tcp . c <nl> dissect_tcpopt_mptcp ( const ip_tcp_opt * optp _U_ , tvbuff_t * tvb , <nl> guint8 indx ; <nl> guint8 flags ; <nl> guint8 ipver ; <nl> + int start_offset = offset ; <nl>  <nl> mptcp_tree = proto_tree_add_subtree ( opt_tree , tvb , offset , optlen , ett_tcp_option_mptcp , & ti , " Multipath TCP "); <nl>  <nl> dissect_tcpopt_mptcp ( const ip_tcp_opt * optp _U_ , tvbuff_t * tvb , <nl> 2 , ENC_BIG_ENDIAN ); <nl> offset += 2 ; <nl>  <nl> - proto_tree_add_item ( mptcp_tree , <nl> - hf_tcp_option_mptcp_checksum , tvb , offset , <nl> - 2 , ENC_BIG_ENDIAN ); <nl> + if (( int ) optlen >= offset - start_offset + 4 ) <nl> + { <nl> + proto_tree_add_item ( mptcp_tree , <nl> + hf_tcp_option_mptcp_checksum , tvb , offset , <nl> + 2 , ENC_BIG_ENDIAN ); <nl> + } <nl> } <nl> break ; <nl> 
mmm ui / gtk / main_menubar . c <nl> ppp ui / gtk / main_menubar . c <nl> set_menus_for_selected_packet ( capture_file * cf ) <nl>  <nl> set_menu_sensitivity ( ui_manager_main_menubar , path , <nl> menu_dissector_filter_spe_cb (/* frame_data * fd _U_ */ NULL , cf -> edt , filter_entry )); <nl> + g_free ( path ); <nl> i ++; <nl> list_entry = g_list_next ( list_entry ); <nl> }
mmm ui / cli / tap - rtp . c <nl> ppp ui / cli / tap - rtp . c <nl> rtp_streams_stat_draw ( void * arg _U_ ) <nl>  <nl> list = g_list_next ( list ); <nl>  <nl> - g_free ( payload_type ); <nl> wmem_free ( NULL , src_addr ); <nl> wmem_free ( NULL , dst_addr ); <nl> wmem_free ( NULL , payload_type );
mmm randpkt . c <nl> ppp randpkt . c <nl> main ( int argc , char ** argv ) <nl> pkthdr . ts . tv_sec = i ; /* just for variety */ <nl>  <nl> for ( j = example -> sample_length ; j < len_random ; j ++) { <nl> - buffer [ j ] = ( rand () % 0x100 ); <nl> + /* Add format strings here and there */ <nl> + if (( int ) ( 100 . 0 * rand ()/( RAND_MAX + 1 . 0 )) < 3 && j < ( len_random - 3 )) { <nl> + memcpy (& buffer [ j ], "% s ", 3 ); <nl> + j += 2 ; <nl> + } else { <nl> + buffer [ j ] = ( rand () % 0x100 ); <nl> + } <nl> } <nl>  <nl> wtap_dump ( dump , & pkthdr , & ps_header , & buffer [ 0 ], & err );
mmm gtk / capture_file_dlg . c <nl> ppp gtk / capture_file_dlg . c <nl> color_global_cb ( GtkWidget * widget _U_ , gpointer data ) <nl>  <nl> gtk_file_chooser_select_filename ( GTK_FILE_CHOOSER ( fs_widget ), path ); <nl>  <nl> - g_free (( gchar *) path ); <nl> + g_free ( path ); <nl> } <nl>  <nl> /* Import color filters */
mmm epan / dissectors / packet - sdp . c <nl> ppp epan / dissectors / packet - sdp . c <nl> static void dissect_sdp_media_attribute ( tvbuff_t * tvb , packet_info * pinfo , proto <nl> if ( port_offset != - 1 ) { <nl> /* Port ends with '/' */ <nl> port_end_offset = tvb_find_guint8 ( tvb , port_offset , - 1 , '/'); <nl> - <nl> + if ( port_end_offset == - 1 ) { <nl> + /* No "/" look for the ";" */ <nl> + port_end_offset = tvb_find_guint8 ( tvb , port_offset , - 1 , ';');; <nl> + } <nl> /* Attempt to convert address */ <nl> if ( inet_pton ( AF_INET , <nl> ( char *) tvb_get_ephemeral_string ( tvb , address_offset , port_offset - address_offset ),
mmm ui / qt / packet_list . cpp <nl> ppp ui / qt / packet_list . cpp <nl> PacketList :: PacketList ( QWidget * parent ) : <nl> decode_as_ ( NULL ), <nl> ctx_column_ (- 1 ), <nl> capture_in_progress_ ( false ), <nl> - tail_timer_id_ ( 0 ) <nl> + tail_timer_id_ ( 0 ), <nl> + rows_inserted_ ( false ) <nl> { <nl> QMenu * submenu , * subsubmenu ; <nl> QAction * action ;
mmm epan / dissectors / packet - uts . c <nl> ppp epan / dissectors / packet - uts . c <nl> dissect_uts ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tree * tree ) <nl> else <nl> proto_tree_add_uint_format ( uts_header_tree , hf_sid , tvb , 2 , 1 , sid , " SID (% 02X )", sid ); <nl>  <nl> - if ( sid == GDID ) <nl> + if ( did == GDID ) <nl> proto_tree_add_uint_format ( uts_header_tree , hf_did , tvb , 3 , 1 , did , " DID (% 02X ) ( General )", did ); <nl> else <nl> proto_tree_add_uint_format ( uts_header_tree , hf_did , tvb , 3 , 1 , did , " DID (% 02X )", did );
mmm tshark . c <nl> ppp tshark . c <nl> load_cap_file ( capture_file * cf , char * save_file , int out_file_type , <nl> snapshot_length = WTAP_MAX_PACKET_SIZE ; <nl> } <nl> /* If we don ' t have an application name add Tshark */ <nl> - if ( shb_hdr -> shb_user_appl == NULL ) <nl> + if ( shb_hdr -> shb_user_appl == NULL ) { <nl> g_snprintf ( appname , sizeof ( appname ), " TShark " VERSION "% s ", wireshark_svnversion ); <nl> + shb_hdr -> shb_user_appl = appname ; <nl> + } <nl>  <nl> pdh = wtap_dump_open_ng ( save_file , out_file_type , linktype , snapshot_length , <nl> FALSE /* compressed */, shb_hdr , idb_inf , & err );mmm editcap . c <nl> ppp editcap . c <nl> load_cap_file ( capture_file * cf , char * save_file , int out_file_type , <nl> snapshot_length = WTAP_MAX_PACKET_SIZE ; <nl> } <nl> /* If we don ' t have an application name add Tshark */ <nl> - if ( shb_hdr -> shb_user_appl == NULL ) <nl> + if ( shb_hdr -> shb_user_appl == NULL ) { <nl> g_snprintf ( appname , sizeof ( appname ), " TShark " VERSION "% s ", wireshark_svnversion ); <nl> + shb_hdr -> shb_user_appl = appname ; <nl> + } <nl>  <nl> pdh = wtap_dump_open_ng ( save_file , out_file_type , linktype , snapshot_length , <nl> FALSE /* compressed */, shb_hdr , idb_inf , & err ); <nl> main ( int argc , char * argv []) <nl> filename = g_strdup ( argv [ optind + 1 ]); <nl>  <nl> /* If we don ' t have an application name add Editcap */ <nl> - if ( shb_hdr -> shb_user_appl == NULL ) <nl> + if ( shb_hdr -> shb_user_appl == NULL ) { <nl> g_snprintf ( appname , sizeof ( appname ), " Editcap " VERSION ); <nl> + shb_hdr -> shb_user_appl = appname ; <nl> + } <nl>  <nl> pdh = wtap_dump_open_ng ( filename , out_file_type , out_frame_type , <nl> snaplen ? MIN ( snaplen , wtap_snapshot_length ( wth )) : wtap_snapshot_length ( wth ),
mmm epan / dissectors / packet - pdcp - lte . c <nl> ppp epan / dissectors / packet - pdcp - lte . c <nl> UAT_CSTRING_CB_DEF ( uat_ue_keys_records , rrcIntegrityKeyString , uat_ue_keys_reco <nl>  <nl> static gboolean global_pdcp_decipher_signalling = FALSE ; <nl> static gboolean global_pdcp_decipher_userplane = FALSE ; <nl> - static gboolean global_pdcp_check_integrity = FALSE ; <nl> # endif <nl> + static gboolean global_pdcp_check_integrity = FALSE ; <nl>  <nl> static const value_string direction_vals [] = <nl> {
mmm epan / dissectors / packet - nfs . c <nl> ppp epan / dissectors / packet - nfs . c <nl> dissect_nfs_acemask4 ( tvbuff_t * tvb , int offset , proto_tree * tree ) <nl>  <nl> if ( acemask & ACE4_APPEND_DATA ) <nl> proto_tree_add_text ( acemask_tree , tvb , offset , 4 , <nl> - " ACE4_ADD_FILE / ACE4_ADD_SUBDIRECTORY ( 0x % 08x )", <nl> + " ACE4_APPEND_DATA / ACE4_ADD_SUBDIRECTORY ( 0x % 08x )", <nl> ACE4_APPEND_DATA ); <nl>  <nl> if ( acemask & ACE4_READ_NAMED_ATTRS )
mmm ui / tap - sequence - analysis . c <nl> ppp ui / tap - sequence - analysis . c <nl> sequence_analysis_list_free ( seq_analysis_info_t * sainfo ) <nl> while ( list ) <nl> { <nl> sequence_analysis_item_free ( list -> data ); <nl> + list = g_list_next ( list ); <nl> } <nl> g_queue_free ( sainfo -> items ); <nl> }
mmm epan / dissectors / packet - isup_thin . c <nl> ppp epan / dissectors / packet - isup_thin . c <nl> # include < epan / packet . h > <nl> # include " prefs . h " <nl>  <nl> - static int ISUP_thinTCPPort = 0 ; <nl> + static guint ISUP_thinTCPPort = 0 ; <nl>  <nl> /* Initialize the protocol and registered fields */ <nl> static int proto_isup_thin = - 1 ; <nl> dissect_isup_thin ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> void <nl> proto_reg_handoff_isup_thin ( void ) <nl> { <nl> - static int Initialized = FALSE ; <nl> + static gboolean Initialized = FALSE ; <nl> static dissector_handle_t isup_thin_handle ; <nl> - static int saved_tcp_port ; <nl> + static guint saved_tcp_port ; <nl>  <nl> if (! Initialized ) { <nl> isup_thin_handle = find_dissector (" isup_thin ");
mmm epan / dissectors / packet - ieee80211 . c <nl> ppp epan / dissectors / packet - ieee80211 . c <nl> dissect_rm_enabled_capabilities_ie ( packet_info * pinfo , proto_tree * tree , <nl>  <nl> if ( tag_len != 5 ) <nl> { <nl> - expert_add_info_format ( pinfo , ti_len , & ei_ieee80211_tag_length , " RM Enabled Capabilities length % u wrong , must = 4 ", tag_len ); <nl> + expert_add_info_format ( pinfo , ti_len , & ei_ieee80211_tag_length , " RM Enabled Capabilities length % u wrong , must = 5 ", tag_len ); <nl> return offset ; <nl> } <nl> proto_item_append_text ( ti , " (% d octets )", tag_len );mmm epan / dissectors / packet - gsm_a_dtap . c <nl> ppp epan / dissectors / packet - gsm_a_dtap . c <nl> dissect_rm_enabled_capabilities_ie ( packet_info * pinfo , proto_tree * tree , <nl>  <nl> if ( tag_len != 5 ) <nl> { <nl> - expert_add_info_format ( pinfo , ti_len , & ei_ieee80211_tag_length , " RM Enabled Capabilities length % u wrong , must = 4 ", tag_len ); <nl> + expert_add_info_format ( pinfo , ti_len , & ei_ieee80211_tag_length , " RM Enabled Capabilities length % u wrong , must = 5 ", tag_len ); <nl> return offset ; <nl> } <nl> proto_item_append_text ( ti , " (% d octets )", tag_len ); <nl> proto_register_gsm_a_dtap ( void ) <nl> NULL , HFILL } <nl> }, <nl> { & hf_gsm_a_dtap_bearer_cap_coding_standard , <nl> - { " Coding standard ", " gsm_a . dtap . coding_standard ", <nl> + { " Coding standard ", " gsm_a . dtap . cap_coding_standard ", <nl> FT_BOOLEAN , 8 , TFS (& tfs_bearer_cap_coding_standard ), 0x10 , <nl> NULL , HFILL } <nl> },
mmm epan / dissectors / packet - smb2 . c <nl> ppp epan / dissectors / packet - smb2 . c <nl> dissect_smb2_read_request ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , i <nl>  <nl> /* minimum count */ <nl> proto_tree_add_item ( tree , hf_smb2_min_count , tvb , offset , 4 , ENC_LITTLE_ENDIAN ); <nl> + offset += 4 ; <nl>  <nl> /* channel */ <nl> proto_tree_add_item ( tree , hf_smb2_channel , tvb , offset , 4 , ENC_LITTLE_ENDIAN );
mmm wiretap / catapult_dct2000 . c <nl> ppp wiretap / catapult_dct2000 . c <nl> gboolean catapult_dct2000_dump ( wtap_dumper * wdh , const struct wtap_pkthdr * phdr , <nl> gboolean read_new_line ( FILE_T fh , gint64 * offset , gint * length , <nl> gchar * linebuff , size_t linebuffsize ) <nl> { <nl> - char * result ; <nl> - <nl> /* Read in a line */ <nl> - result = file_gets ( linebuff , ( int ) linebuffsize - 1 , fh ); <nl> + char * result = file_gets ( linebuff , ( int ) linebuffsize - 1 , fh ); <nl> if ( result == NULL ) { <nl> /* No characters found , or error */ <nl> return FALSE ; <nl> static gboolean parse_line ( gchar * linebuff , gint line_length , <nl> return FALSE ; <nl> } <nl>  <nl> - /* Reset strings ( that won ' t be set be comments ) */ <nl> - g_strlcpy ( variant_name , " 0 ", MAX_VARIANT_DIGITS ); <nl> - g_strlcpy ( outhdr_name , "", MAX_OUTHDR_NAME ); <nl> - g_strlcpy ( port_number_string , " 0 ", MAX_PORT_DIGITS ); <nl> + /* Reset strings ( that won ' t be set by comments ) */ <nl> + variant_name [ 0 ] = '\ 0 '; <nl> + outhdr_name [ 0 ] = '\ 0 '; <nl> + port_number_string [ 0 ] = '\ 0 '; <nl>  <nl> if (!(* is_comment )) { <nl> /* '.' must follow context name */ <nl> static gboolean parse_line ( gchar * linebuff , gint line_length , <nl> ( strcmp ( protocol_name , " fp_r4 ") == 0 ) || <nl> ( strcmp ( protocol_name , " fp_r5 ") == 0 ) || <nl> ( strcmp ( protocol_name , " fp_r6 ") == 0 ) || <nl> - ( strcmp ( protocol_name , " fp_r7 ") == 0 )) { <nl> + ( strcmp ( protocol_name , " fp_r7 ") == 0 ) || <nl> + ( strcmp ( protocol_name , " fp_r8 ") == 0 )) { <nl>  <nl> if (( variant > 256 ) && ( variant % 256 == 3 )) { <nl> /* FP over udp is contained in IPPrim ... */
mmm ui / qt / overlay_scroll_bar . cpp <nl> ppp ui / qt / overlay_scroll_bar . cpp <nl> void OverlayScrollBar :: paintEvent ( QPaintEvent * event ) <nl> pm_painter . setPen ( border_color ); <nl> pm_painter . drawLine ( near_dest . topLeft (), near_dest . bottomLeft ()); <nl> pm_painter . drawLine ( near_dest . topRight (), near_dest . bottomRight ()); <nl> + pm_painter . drawLine ( near_dest . bottomLeft (), near_dest . bottomRight ()); <nl> pm_painter . restore (); <nl>  <nl> // Draw the map .
mmm epan / dissectors / packet - zbee - nwk . c <nl> ppp epan / dissectors / packet - zbee - nwk . c <nl> dissect_zbee_nwk_heur ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void <nl> ieee802154_packet * packet = ( ieee802154_packet *) data ; <nl>  <nl> /* All ZigBee frames must always have a 16 - bit source address . */ <nl> - if ( packet -> src_addr_mode != IEEE802154_FCF_ADDR_SHORT ) { <nl> + if ( ( packet == NULL ) || <nl> + ( packet -> src_addr_mode != IEEE802154_FCF_ADDR_SHORT ) ) { <nl> return FALSE ; <nl> } <nl> /* ZigBee MAC frames must always contain a 16 - bit destination address . */
mmm packet - lpd . c <nl> ppp packet - lpd . c <nl> * Routines for LPR and LPRng packet disassembly <nl> * Gilbert Ramirez < gram @ xiexie . org > <nl> * <nl> - * $ Id : packet - lpd . c , v 1 . 17 2000 / 04 / 08 07 : 07 : 28 guy Exp $ <nl> + * $ Id : packet - lpd . c , v 1 . 18 2000 / 04 / 20 15 : 24 : 41 gram Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ zing . org > <nl> dissect_lpd ( const u_char * pd , int offset , frame_data * fd , proto_tree * tree ) <nl> RFC 1179 . http :// www . astart . com / lprng / LPRng - HOWTO . html */ <nl> char * lpd_client_code [] = { <nl> " Unknown command ", <nl> - " LPC : start print ", <nl> - " LPR : transfer a printer job ", <nl> - " LPQ : print short form of queue status ", <nl> + " LPC : start print / jobcmd : abort ", <nl> + " LPR : transfer a printer job / jobcmd : receive control file ", <nl> + " LPQ : print short form of queue status / jobcmd : receive data file ", <nl> " LPQ : print long form of queue status ", <nl> " LPRM : remove jobs ", <nl> " LPRng lpc : do control operation ", <nl> dissect_lpd ( const u_char * pd , int offset , frame_data * fd , proto_tree * tree ) <nl> " Bad job format , do not retry " <nl> }; <nl>  <nl> - <nl> - if ( pd [ offset + 1 ] == '\ n ') { <nl> + /* rfc1179 states that all responses are 1 byte long */ <nl> + if ( END_OF_FRAME == 1 ) { <nl> lpr_packet_type = response ; <nl> } <nl> else if ( pd [ offset ] <= 9 ) { <nl> dissect_lpd ( const u_char * pd , int offset , frame_data * fd , proto_tree * tree ) <nl> int response = pd [ offset ]; <nl>  <nl> if ( response <= 3 ) { <nl> - proto_tree_add_text ( lpd_tree , offset , 2 , " Response : % s ", <nl> + proto_tree_add_text ( lpd_tree , offset , 1 , " Response : % s ", <nl> lpd_server_code [ response ]); <nl> } <nl> else {
mmm gtk / main . c <nl> ppp gtk / main . c <nl> /* main . c <nl> * <nl> - * $ Id : main . c , v 1 . 386 2004 / 02 / 01 20 : 28 : 11 ulfl Exp $ <nl> + * $ Id : main . c , v 1 . 387 2004 / 02 / 01 22 : 43 : 34 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> main_window_delete_event_cb ( GtkWidget * widget _U_ , GdkEvent * event _U_ , gpointer <nl> } <nl>  <nl> static void <nl> - main_load_window_geometry ( GtkWidget * widget ) <nl> + main_load_window_geometry ( GtkWidget * widget <nl> +# if GTK_MAJOR_VERSION < 2 <nl> + _U_ <nl> +# endif <nl> +) <nl> { <nl> /* as we now have the geometry from the recent file , set it */ <nl> if ( prefs . gui_geometry_save_position ) {
mmm epan / dissectors / packet - mq - pcf . c <nl> ppp epan / dissectors / packet - mq - pcf . c <nl> static void dissect_mqpcf ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , m <nl>  <nl> static gboolean dissect_mqpcf_heur ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data ) <nl> { <nl> - if ( tvb_length ( tvb ) >= 36 ) <nl> + if ( data && tvb_length ( tvb ) >= 36 ) <nl> { <nl> mq_parm_t * p_mq_parm = ( mq_parm_t *) data ; <nl> if ( strncmp (( const char *) p_mq_parm -> mq_format , MQ_MQFMT_ADMIN , 8 ) == 0
mmm epan / dissectors / packet - dcerpc - mgmt . c <nl> ppp epan / dissectors / packet - dcerpc - mgmt . c <nl> static e_guid_t uuid_mgmt = { 0xafa8bd80 , 0x7d8a , 0x11c9 , { 0xbe , 0xf4 , 0x08 , 0x <nl> static guint16 ver_mgmt = 1 ; <nl>  <nl> static int <nl> - mgmtrpc_dissect_inq_princ_name_response ( tvbuff_t * tvb _U_ , int offset _U_ , packet_info * pinfo _U_ , proto_tree * tree _U_ , dcerpc_info * di _U_ , guint8 * drep _U_ ) <nl> + mgmtrpc_dissect_inq_princ_name_response ( tvbuff_t * tvb , int offset , packet_info * pinfo , proto_tree * tree , dcerpc_info * di , guint8 * drep ) <nl> { <nl>  <nl> offset = dissect_ndr_cvstring ( tvb , offset , pinfo , tree , di , drep , <nl> mgmtrpc_dissect_inq_princ_name_response ( tvbuff_t * tvb _U_ , int offset _U_ , packe <nl> return offset ; <nl> } <nl> static int <nl> - mgmtrpc_dissect_inq_princ_name_request ( tvbuff_t * tvb _U_ , int offset _U_ , packet_info * pinfo _U_ , proto_tree * tree _U_ , dcerpc_info * di _U_ , guint8 * drep _U_ ) <nl> + mgmtrpc_dissect_inq_princ_name_request ( tvbuff_t * tvb , int offset , packet_info * pinfo , proto_tree * tree , dcerpc_info * di , guint8 * drep ) <nl> { <nl> offset = dissect_ndr_uint32 ( tvb , offset , pinfo , tree , di , drep , hf_mgmt_proto , NULL ); <nl> offset = dissect_ndr_uint32 ( tvb , offset , pinfo , tree , di , drep , hf_mgmt_princ_size , NULL );
mmm epan / dissectors / packet - umts_fp . c <nl> ppp epan / dissectors / packet - umts_fp . c <nl> fp_set_per_packet_inf_from_conv ( umts_fp_conversation_info_t * p_conv_data , <nl>  <nl> /* Peek at C / T , different RLC params for different logical channels */ <nl> /* C / T is 4 bits according to 3GPP TS 25 . 321 , paragraph 9 . 2 . 1 , from MAC header ( not FP )*/ <nl> - c_t = tvb_get_bits8 ( tvb , tb_bit_off /*( 2 + p_conv_data -> num_dch_in_flow )* 8 */, 4 ); /* c_t = tvb_get_guint8 ( tvb , offset );*/ <nl> - macinf -> lchid [ j + chan ] = c_t + 1 ; <nl> + c_t = ( tvb_get_bits8 ( tvb , tb_bit_off /*( 2 + p_conv_data -> num_dch_in_flow )* 8 */, 4 ) + 1 ) % 0xf ; /* c_t = tvb_get_guint8 ( tvb , offset );*/ <nl> + macinf -> lchid [ j + chan ] = c_t ; <nl>  <nl> - macinf -> content [ j + chan ] = lchId_type_table [ c_t + 1 ]; /* Base MAC content on logical channel id ( Table is in packet - nbap . h )*/ <nl> - rlcinf -> mode [ j + chan ] = lchId_rlc_map [ c_t + 1 ]; /* Based RLC mode on logical channel id */ <nl> + macinf -> content [ j + chan ] = lchId_type_table [ c_t ]; /* Base MAC content on logical channel id ( Table is in packet - nbap . h )*/ <nl> + rlcinf -> mode [ j + chan ] = lchId_rlc_map [ c_t ]; /* Based RLC mode on logical channel id */ <nl> } <nl> } else { <nl> fake_lchid = make_fake_lchid ( pinfo , p_conv_data -> dchs_in_flow_list [ chan ]);
mmm epan / dissectors / packet - h264 . c <nl> ppp epan / dissectors / packet - h264 . c <nl> dissect_h264_exp_golomb_code ( proto_tree * tree , int hf_index , tvbuff_t * tvb , gint <nl> codenum = 1 ; <nl> codenum = codenum << leading_zero_bits ; <nl> mask = codenum >> 1 ; <nl> - if ( leading_zero_bits > 8 ) <nl> + if ( leading_zero_bits > 16 ) <nl> + value = tvb_get_bits32 ( tvb , bit_offset , leading_zero_bits , FALSE ); <nl> + else if ( leading_zero_bits > 8 ) <nl> value = tvb_get_bits16 ( tvb , bit_offset , leading_zero_bits , FALSE ); <nl> else <nl> value = tvb_get_bits8 ( tvb , bit_offset , leading_zero_bits );
mmm file . c <nl> ppp file . c <nl> /* file . c <nl> * File I / O routines <nl> * <nl> - * $ Id : file . c , v 1 . 59 1999 / 08 / 10 04 : 13 : 36 guy Exp $ <nl> + * $ Id : file . c , v 1 . 60 1999 / 08 / 10 06 : 54 : 12 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ zing . org > <nl> wtap_dispatch_cb ( u_char * user , const struct wtap_pkthdr * phdr , int offset , <nl> /* Allocate the next list entry , and add it to the list . */ <nl> fdata = ( frame_data *) g_malloc ( sizeof ( frame_data )); <nl>  <nl> + fdata -> next = NULL ; <nl> fdata -> pkt_len = phdr -> len ; <nl> fdata -> cap_len = phdr -> caplen ; <nl> fdata -> file_off = offset ;
mmm ui / qt / column_preferences_frame . cpp <nl> ppp ui / qt / column_preferences_frame . cpp <nl> const int custom_occurrence_col_ = 4 ; <nl> ColumnPreferencesFrame :: ColumnPreferencesFrame ( QWidget * parent ) : <nl> QFrame ( parent ), <nl> ui ( new Ui :: ColumnPreferencesFrame ), <nl> + cur_column_ ( 0 ), <nl> cur_line_edit_ ( NULL ), <nl> cur_combo_box_ ( NULL ), <nl> + saved_combo_idx_ ( 0 ), <nl> saved_custom_combo_idx_ (- 1 ) <nl> { <nl> ui -> setupUi ( this );
mmm epan / plugin_if . c <nl> ppp epan / plugin_if . c <nl> typedef struct _ext_toolbar_update_list_t <nl> GList * entries ; <nl> } ext_toolbar_update_list_t ; <nl>  <nl> - extern gint <nl> + static gint <nl> ext_toolbar_find_item ( gconstpointer a , gconstpointer b ) <nl> { <nl> if ( a == 0 || b == 0 )
mmm plugins / mate / mate_util . c <nl> ppp plugins / mate / mate_util . c <nl> extern void avp_init ( void ) { <nl> extern AVP * new_avp_from_finfo ( const gchar * name , field_info * finfo ) { <nl> AVP * new = ( AVP *) g_slice_new ( any_avp_type ); <nl> gchar * value ; <nl> + gchar * repr = NULL ; <nl>  <nl> new -> n = scs_subscribe ( avp_strings , name ); <nl>  <nl> if ( finfo -> value . ftype -> val_to_string_repr ) { <nl> - value = scs_subscribe ( avp_strings , fvalue_to_string_repr (& finfo -> value , FTREPR_DISPLAY , NULL )); <nl> + repr = fvalue_to_string_repr (& finfo -> value , FTREPR_DISPLAY , NULL ); <nl> + } <nl> + <nl> + if ( repr ) { <nl> + value = scs_subscribe ( avp_strings , repr ); <nl> # ifdef _AVP_DEBUGGING <nl> dbg_print ( dbg_avp , 2 , dbg_fp ," new_avp_from_finfo : from string : % s ", value ); <nl> # endif
mmm epan / dissectors / packet - rtacser . c <nl> ppp epan / dissectors / packet - rtacser . c <nl> dissect_rtacser_data ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl>  <nl> p_add_proto_data ( pinfo -> pool , pinfo , proto_rtacser , 0 , GUINT_TO_POINTER ( global_rtacser_payload_proto )); <nl>  <nl> - if ( tvb_reported_length_remaining ( tvb , RTACSER_HEADER_LEN ) > 0 ) { <nl> + if ( tvb_reported_length_remaining ( tvb , offset ) > 0 ) { <nl> payload_tvb = tvb_new_subset_remaining ( tvb , RTACSER_HEADER_LEN ); <nl> if (! dissector_try_uint ( subdissector_table , global_rtacser_payload_proto , payload_tvb , pinfo , tree )){ <nl> call_dissector ( data_handle , payload_tvb , pinfo , tree );
mmm gtk / proto_draw . c <nl> ppp gtk / proto_draw . c <nl> /* proto_draw . c <nl> * Routines for GTK + packet display <nl> * <nl> - * $ Id : proto_draw . c , v 1 . 24 2001 / 01 / 02 01 : 32 : 21 guy Exp $ <nl> + * $ Id : proto_draw . c , v 1 . 25 2001 / 01 / 11 05 : 51 : 10 gram Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ zing . org > <nl> packet_hex_print ( GtkText * bv , guint8 * pd , frame_data * fd , field_info * finfo ) <nl> if ( bstart > 0 ) { <nl> int lineheight , linenum ; <nl> float scrollval ; <nl> - linenum = bstart / BYTE_VIEW_WIDTH ; <nl>  <nl> - /* need to change to some way of getting that offset instead of + 4 */ <nl> - lineheight = gdk_string_height ( m_b_font , " 0 ") + 4 ; <nl> + linenum = bstart / BYTE_VIEW_WIDTH ; <nl> + /* This is the lineheight that the GtkText widget uses when drawing text . */ <nl> + lineheight = m_b_font -> ascent + m_b_font -> descent ; <nl> scrollval = MIN ( linenum * lineheight , bv -> vadj -> upper - bv -> vadj -> page_size ); <nl>  <nl> gtk_adjustment_set_value ( bv -> vadj , scrollval );
mmm epan / dissectors / packet - iax2 . c <nl> ppp epan / dissectors / packet - iax2 . c <nl> static guint iax_circuit_lookup ( const address * address , <nl> new_key -> addr . type = address -> type ; <nl> new_key -> addr . len = MIN ( address -> len , MAX_ADDRESS ); <nl> new_key -> addr . data = new_key -> address_data ; <nl> - memmove ( new_key -> address_data , address -> data , new_key -> addr . len ); <nl> + memcpy ( new_key -> address_data , address -> data , new_key -> addr . len ); <nl> new_key -> ptype = ptype ; <nl> new_key -> port = port ; <nl> new_key -> callno = callno ;
mmm epan / decode_as . c <nl> ppp epan / decode_as . c <nl> save_decode_as_entries ( gchar ** err ) <nl>  <nl> dissector_all_tables_foreach_changed ( decode_as_write_entry , da_file ); <nl> fclose ( da_file ); <nl> + g_free ( daf_path ); <nl> return 0 ; <nl> } <nl> 
mmm epan / dissectors / packet - gsm_bssmap_le . c <nl> ppp epan / dissectors / packet - gsm_bssmap_le . c <nl> bssmap_le_perf_loc_resp ( tvbuff_t * tvb , proto_tree * tree , guint32 offset , guint l <nl> /* Deciphering Keys 9 . 2 . 3 C ( note 2 ) 3 - n */ <nl> ELEM_OPT_TLV ( gsm_bssmap_le_elem_strings [ DE_BMAPLE_DECIPH_KEYS ]. value , GSM_PDU_TYPE_BSSMAP_LE , DE_BMAPLE_DECIPH_KEYS , ""); <nl> /* LCS Cause 9 . 2 . 4 C ( note 3 ) 3 - n */ <nl> - ELEM_OPT_TLV ( gsm_bssmap_le_elem_strings [ DE_BMAPLE_LCS_CAUSE ]. value , GSM_PDU_TYPE_BSSMAP_LE , BSSMAP_LE_LCS_CAUSE , ""); <nl> + ELEM_OPT_TLV ( gsm_bssmap_le_elem_strings [ DE_BMAPLE_LCS_CAUSE ]. value , GSM_PDU_TYPE_BSSMAP_LE , DE_BMAPLE_LCS_CAUSE , ""); <nl> /* Velocity Estimate 9 . 2 . 5 O 3 - n */ <nl> ELEM_OPT_TLV ( gsm_bssmap_elem_strings [ BE_VEL_EST ]. value , BSSAP_PDU_TYPE_BSSMAP , BE_VEL_EST , ""); <nl> /* GANSS Positioning Data 9 . 2 . 6 O 3 - n */
mmm epan / wslua / wslua_byte_array . c <nl> ppp epan / wslua / wslua_byte_array . c <nl> WSLUA_CONSTRUCTOR ByteArray_tvb ( lua_State * L ) { <nl> data = ( guint8 *) g_memdup ( ba -> data , ba -> len ); <nl>  <nl> tvb = ( Tvb ) g_malloc ( sizeof ( struct _wslua_tvb )); <nl> - tvb -> ws_tvb = tvb_new_real_data ( data , ba -> len , ba -> len ); <nl> + tvb -> ws_tvb = tvb_new_child_real_data ( lua_tvb , data , ba -> len , ba -> len ); <nl> tvb -> expired = FALSE ; <nl> - tvb -> need_free = TRUE ; <nl> + tvb -> need_free = FALSE ; <nl> tvb_set_free_cb ( tvb -> ws_tvb , g_free ); <nl>  <nl> add_new_data_source ( lua_pinfo , tvb -> ws_tvb , name );
mmm gtk / gui_utils . c <nl> ppp gtk / gui_utils . c <nl> window_new_with_geom ( GtkWindowType type , const gchar * title , const gchar * geom_n <nl> } <nl>  <nl>  <nl> -/* Create a new window for a splash screen ; it ' s a main window , with no title , <nl> +/* Create a new window for a splash screen ; it ' s a main window , without decoration , <nl> positioned in the center of the screen . */ <nl> GtkWidget * <nl> splash_window_new ( void ) <nl> { <nl> GtkWidget * win ; <nl>  <nl> - win = gtk_window_new ( GTK_WINDOW_TOPLEVEL ); <nl> + win = window_new ( GTK_WINDOW_TOPLEVEL , " Wireshark "); <nl> gtk_window_set_decorated ( GTK_WINDOW ( win ), FALSE ); <nl>  <nl> /* set the initial position ( must be done , before show is called !) */
mmm wiretap / snoop . c <nl> ppp wiretap / snoop . c <nl> /* snoop . c <nl> * <nl> - * $ Id : snoop . c , v 1 . 65 2003 / 11 / 11 20 : 49 : 46 guy Exp $ <nl> + * $ Id : snoop . c , v 1 . 66 2003 / 12 / 19 22 : 23 : 05 guy Exp $ <nl> * <nl> * Wiretap Library <nl> * Copyright ( c ) 1998 by Gilbert Ramirez < gram @ alumni . rice . edu > <nl> static gboolean snoop_read ( wtap * wth , int * err , long * data_offset ) <nl> * err = WTAP_ERR_BAD_RECORD ; <nl> return FALSE ; <nl> } <nl> + if ( packet_size > rec_size ) { <nl> + /* <nl> + * Probably a corrupt capture file . <nl> + */ <nl> + g_message (" snoop : File has % u - byte packet , bigger than record size % u ", <nl> + packet_size , rec_size ); <nl> + * err = WTAP_ERR_BAD_RECORD ; <nl> + return FALSE ; <nl> + } <nl>  <nl> * data_offset = wth -> data_offset ; <nl> 
mmm gtk / wlan_stat_dlg . c <nl> ppp gtk / wlan_stat_dlg . c <nl> csv_handle ( GtkTreeModel * model , GtkTreePath * path _U_ , GtkTreeIter * iter , <nl> i == PERCENT_COLUMN || i == PROTECTION_COLUMN ) { <nl> gtk_tree_model_get ( model , iter , i , & table_text , - 1 ); <nl> g_string_append ( CSV_str , table_text ); <nl> + g_free ( table_text ); <nl> } else { <nl> gtk_tree_model_get ( model , iter , i , & table_value , - 1 ); <nl> g_string_append_printf ( CSV_str , "% u ", table_value );
mmm epan / prefs . c <nl> ppp epan / prefs . c <nl> prefs_register_protocol_subtree ( const char * subtree , int id , void (* apply_cb )( vo <nl>  <nl> } <nl>  <nl> - /* g_free ( csubtree ); */ <nl> + g_free ( csubtree ); <nl>  <nl> } <nl> 
mmm epan / dissectors / packet - rtp . h <nl> ppp epan / dissectors / packet - rtp . h <nl> struct _rtp_conversation_info <nl> { <nl> gchar method [ MAX_RTP_SETUP_METHOD_SIZE + 1 ]; <nl> guint32 frame_number ; <nl> - guint32 rtp_event_pt ; /* this is payload type for dynamic RTP events ( RFC2833 ) */ <nl> + GHashTable * rtp_dyn_payload ; /* a hash table with the dynamic RTP payload */ <nl> }; <nl>  <nl> /* Add an RTP conversation with the given details */ <nl> void rtp_add_address ( packet_info * pinfo , <nl> int other_port , <nl> gchar * setup_method , <nl> guint32 setup_frame_number , <nl> - int rtp_event_pt ); <nl> + GHashTable * rtp_dyn_payload ); <nl> + <nl> +/* Free and destroy the dyn_payload hash table */ <nl> + void rtp_free_hash_dyn_payload ( GHashTable * rtp_dyn_payload ); <nl> +
mmm gtk / gui_utils . c <nl> ppp gtk / gui_utils . c <nl> GtkWidget * xpm_to_widget_from_parent ( GtkWidget * parent , const char ** xpm ) { <nl> } <nl>  <nl>  <nl> -/* convert an xpm to a GtkWidget , using the top_level window settings */ <nl> -/* ( be sure that the top_level window is already being displayed ) */ <nl> +/* convert an xpm to a GtkWidget */ <nl> GtkWidget * xpm_to_widget ( const char ** xpm ) { <nl> - return xpm_to_widget_from_parent ( top_level , xpm ); <nl> + GdkPixbuf * pixbuf ; <nl> + <nl> + pixbuf = gdk_pixbuf_new_from_xpm_data ( xpm ); <nl> + return gtk_image_new_from_pixbuf ( pixbuf ); <nl> } <nl>  <nl> /* Convert an pixbuf data to a GtkWidget */
mmm epan / emem . c <nl> ppp epan / emem . c <nl> emem_alloc ( size_t size , emem_header_t * mem , gboolean use_chunks , guint8 * canary ) <nl> /* There ' s no padding / alignment involved ( from our point of view ) when <nl> * we fetch the memory directly from the system pool , so WYSIWYG */ <nl> npc -> free_offset = npc -> free_offset_init = 0 ; <nl> - npc -> amount_free = npc -> amount_free_init = size ; <nl> + npc -> amount_free = npc -> amount_free_init = ( unsigned int ) size ; <nl> } <nl>  <nl> return buf ;
mmm gtk / packet_win . c <nl> ppp gtk / packet_win . c <nl> finfo_integer_changed ( GtkSpinButton * spinbutton , gpointer user_data ) <nl>  <nl> else if ( finfo_type == FT_UINT8 || finfo_type == FT_UINT16 || finfo_type == FT_UINT24 || finfo_type == FT_UINT32 || finfo_type == FT_UINT64 ) <nl> u_val = ( guint64 ) val ; <nl> + else { <nl> + g_assert_not_reached (); <nl> + return ; <nl> + } <nl>  <nl> if ( FI_GET_FLAG ( finfo , FI_LITTLE_ENDIAN )) { <nl> while ( finfo_length ) {
mmm epan / dissectors / packet - dcerpc - netlogon . c <nl> ppp epan / dissectors / packet - dcerpc - netlogon . c <nl> static const true_false_string get_dcname_request_flags_force_rediscovery = { <nl> " You may return cached data " <nl> }; <nl> static const true_false_string get_dcname_request_flags_directory_service_required = { <nl> - " DIRECRTORY SERVICE is REQUIRED on the server ", <nl> + " DIRECTORY SERVICE is REQUIRED on the server ", <nl> " We do NOT require directory service servers " <nl> }; <nl> static const true_false_string get_dcname_request_flags_directory_service_preferred = { <nl> static const true_false_string get_dcname_request_flags_timeserv_required = { <nl> " timeserv service is NOT required " <nl> }; <nl> static const true_false_string get_dcname_request_flags_writable_required = { <nl> - " the requrned dc MUST be WRITEABLE ", <nl> + " the returned dc MUST be WRITEABLE ", <nl> " a read - only dc may be returned " <nl> }; <nl> static const true_false_string get_dcname_request_flags_good_timeserv_preferred = {
mmm text2pcap . c <nl> ppp text2pcap . c <nl> write_current_packet ( void ) <nl> ( guint32 ) ts_sec , ts_usec , <nl> length , length , <nl> 0 , <nl> - 6 , <nl> + 1000000 , <nl> packet_buf , 0 , <nl> & bytes_written , & err ); <nl> } else { <nl> write_file_header ( void ) <nl> 102400 , <nl> & bytes_written , <nl> 0 , <nl> - 6 , <nl> + 0 , <nl> & err ); <nl> } <nl> } else {
mmm packet - ip . c <nl> ppp packet - ip . c <nl> /* packet - ip . c <nl> * Routines for IP and miscellaneous IP protocol packet disassembly <nl> * <nl> - * $ Id : packet - ip . c , v 1 . 66 1999 / 12 / 09 21 : 58 : 04 guy Exp $ <nl> + * $ Id : packet - ip . c , v 1 . 67 1999 / 12 / 13 05 : 09 : 05 gram Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ zing . org > <nl> dissect_ip_tcp_options ( const u_char * opd , int offset , guint length , <nl> option length . */ <nl> proto_tree_add_text ( opt_tree , offset , 2 , <nl> "% s ( with too - short option length = % u byte % s )", name , <nl> - plurality ( len , "", " s ")); <nl> + len , plurality ( len , "", " s ")); <nl> return ; <nl> } else if ( len - 2 > length ) { <nl> /* Bogus - option goes past the end of the header . */
mmm wsutil / strtoi . c <nl> ppp wsutil / strtoi . c <nl> gboolean ws_strtoi64 ( const gchar * str , const gchar ** endptr , gint64 * cint ) <nl> gchar * end ; <nl> gint64 val ; <nl>  <nl> + g_assert ( cint ); <nl> + <nl> + if (! str ) { <nl> + errno = EINVAL ; <nl> + return FALSE ; <nl> + } <nl> + <nl> errno = 0 ; <nl> val = g_ascii_strtoll ( str , & end , 10 ); <nl> if (( val == 0 && end == str ) || ( endptr == NULL && * end != '\ 0 ')) { <nl> static gboolean ws_basestrtou64 ( const gchar * str , const gchar ** endptr , guint64 * <nl> gchar * end ; <nl> guint64 val ; <nl>  <nl> + g_assert ( cint ); <nl> + <nl> + if (! str ) { <nl> + errno = EINVAL ; <nl> + return FALSE ; <nl> + } <nl> + <nl> if ( str [ 0 ] == '-' || str [ 0 ] == '+') { <nl> /* <nl> * Unsigned numbers don ' t have a sign .
mmm epan / dissectors / packet - erf . c <nl> ppp epan / dissectors / packet - erf . c <nl> dissect_erf ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data _U_ ) <nl> atm_info . vpi = (( atm_hdr & 0x0ff00000 ) >> 20 ); <nl> atm_info . vci = (( atm_hdr & 0x000ffff0 ) >> 4 ); <nl> atm_info . channel = ( flags & 0x03 ); <nl> + atm_info . aal2_cid = aal2_cid ; <nl> atm_info . type = TRAF_UNKNOWN ; <nl> atm_info . subtype = TRAF_ST_UNKNOWN ; <nl> 
mmm epan / dissectors / packet - pppoe . c <nl> ppp epan / dissectors / packet - pppoe . c <nl> void proto_register_pppoed ( void ) <nl> } <nl> }, <nl> { & hf_pppoed_tag_host_uniq , <nl> - { " Host - Uniq ", " pppoed . tags . host_uniq ", FT_STRING , BASE_NONE , <nl> + { " Host - Uniq ", " pppoed . tags . host_uniq ", FT_BYTES , BASE_NONE , <nl> NULL , 0x0 , "", HFILL <nl> } <nl> }, <nl> { & hf_pppoed_tag_ac_cookie , <nl> - { " AC - Cookie ", " pppoed . tags . ac_cookie ", FT_BYTES , BASE_HEX , <nl> + { " AC - Cookie ", " pppoed . tags . ac_cookie ", FT_BYTES , BASE_NONE , <nl> NULL , 0x0 , "", HFILL <nl> } <nl> }, <nl> void proto_register_pppoed ( void ) <nl> } <nl> }, <nl> { & hf_pppoed_tag_relay_session_id , <nl> - { " Relay - Session - Id ", " pppoed . tags . relay_session_id ", FT_BYTES , BASE_HEX , <nl> + { " Relay - Session - Id ", " pppoed . tags . relay_session_id ", FT_BYTES , BASE_NONE , <nl> NULL , 0x0 , "", HFILL <nl> } <nl> },
mmm wsutil / strtoi . c <nl> ppp wsutil / strtoi . c <nl> gboolean ws_strtou64 ( const gchar * str , guint64 * cint ) <nl> gchar * endptr ; <nl> guint64 val ; <nl>  <nl> + if ( str [ 0 ] == '-' || str [ 0 ] == '+') { <nl> + /* <nl> + * Unsigned numbers don ' t have a sign . <nl> + */ <nl> + errno = EINVAL ; <nl> + return FALSE ; <nl> + } <nl> errno = 0 ; <nl> val = g_ascii_strtoull ( str , & endptr , 10 ); <nl> if (( val == 0 && endptr == str ) || (* endptr != 0 )) {
mmm epan / dissectors / packet - slowprotocols . c <nl> ppp epan / dissectors / packet - slowprotocols . c <nl> dissect_oampdu_information ( tvbuff_t * tvb , proto_tree * tree ) <nl> else <nl> state_tree = proto_item_add_subtree ( state_item , ett_oampdu_remote_info_state ); <nl>  <nl> - proto_tree_add_boolean ( state_tree , hf_oampdu_info_state_parser , <nl> + proto_tree_add_uint ( state_tree , hf_oampdu_info_state_parser , <nl> tvb , offset , 1 , raw_octet ); <nl>  <nl> proto_tree_add_boolean ( state_tree , hf_oampdu_info_state_mux , <nl> proto_register_slow_protocols ( void ) <nl>  <nl> { & hf_oampdu_info_state_parser , <nl> { " Parser Action ", " oam . info . state . parser ", <nl> - FT_BOOLEAN , 8 , VALS (& parser_vals ), 0x03 , <nl> + FT_UINT8 , 8 , VALS (& parser_vals ), 0x03 , <nl> " Parser Action ", HFILL }}, <nl>  <nl> { & hf_oampdu_info_state_mux ,
mmm dumpcap . c <nl> ppp dumpcap . c <nl> capture_loop_dispatch ( capture_options * capture_opts _U_ , loop_data * ld , <nl> inpkts = pcap_dispatch ( ld -> pcap_h , 1 , capture_loop_packet_cb , <nl> ( u_char *) ld ); <nl> if ( inpkts < 0 ) { <nl> - ld -> pcap_err = TRUE ; <nl> + if ( inpkts == - 1 ) { <nl> + /* Error , rather than pcap_breakloop (). */ <nl> + ld -> pcap_err = TRUE ; <nl> + } <nl> ld -> go = FALSE ; /* error or pcap_breakloop () - stop capturing */ <nl> } <nl> } else {
mmm epan / dissectors / packet - infiniband . c <nl> ppp epan / dissectors / packet - infiniband . c <nl> create_conv_and_add_proto_data ( packet_info * pinfo , guint64 service_id , <nl> conversation_add_proto_data ( conv , proto_infiniband , proto_data ); <nl>  <nl> /* next , register the conversation using the LIDs */ <nl> - set_address ( addr , AT_IB , sizeof ( guint16 ), & lid ); <nl> + set_address ( addr , AT_IB , sizeof ( guint16 ), wmem_memdup ( pinfo -> pool , & lid , sizeof lid )); <nl> conv = conversation_new ( pinfo -> num , addr , addr , <nl> PT_IBQP , port , port , options ); <nl> conversation_add_proto_data ( conv , proto_infiniband , proto_data );
mmm epan / oids . c <nl> ppp epan / oids . c <nl> static void register_mibs ( void ) { <nl> hf_register_info hf ; <nl>  <nl> hf . p_id = &( oid_data -> value_hfid ); <nl> - hf . hfinfo . name = oid_data -> name ; <nl> + hf . hfinfo . name = g_strdup ( oid_data -> name ); <nl> hf . hfinfo . abbrev = alnumerize ( oid_data -> name ); <nl> hf . hfinfo . type = typedata -> ft_type ; <nl> hf . hfinfo . display = typedata -> display ;
mmm epan / tvbuff . c <nl> ppp epan / tvbuff . c <nl> mktime_utc ( struct tm * tm ) <nl> { <nl> 0 , 31 , 59 , 90 , 120 , 151 , 181 , 212 , 243 , 273 , 304 , 334 <nl> }; <nl> + <nl> + int yr ; <nl> # endif <nl>  <nl> # ifndef HAVE_TIMEGM <nl> mktime_utc ( struct tm * tm ) <nl> return ( time_t ) - 1 ; <nl>  <nl> retval = ( tm -> tm_year - 70 ) * 365 ; <nl> - retval += ( tm -> tm_year - 68 ) / 4 ; <nl> - retval += days_before [ tm -> tm_mon ] + tm -> tm_mday - 1 ; <nl>  <nl> - if ( tm -> tm_year % 4 == 0 && tm -> tm_mon < 2 ) <nl> - retval -= 1 ; <nl> + /* count number of leap years */ <nl> + yr = tm -> tm_year + 1900 ; <nl> + if ( tm -> tm_mon + 1 < 3 && ( yr % 4 ) == 0 && (( yr % 100 ) != 0 || ( yr % 400 ) == 0 )) <nl> + yr --; <nl> + retval += ((( yr / 4 ) - ( yr / 100 ) + ( yr / 400 )) - 477 ); /* 477 = (( 1970 / 4 ) - ( 1970 / 100 ) + ( 1970 / 400 )) */ <nl> + <nl> + retval += days_before [ tm -> tm_mon ] + tm -> tm_mday - 1 ; <nl>  <nl> retval = (((( retval * 24 ) + tm -> tm_hour ) * 60 ) + tm -> tm_min ) * 60 + tm -> tm_sec ; <nl> # else
mmm gtk / main . c <nl> ppp gtk / main . c <nl> main_widgets_show_or_hide ( void ) <nl> } else { <nl> gtk_widget_hide ( welcome_pane ); <nl> } <nl> + <nl> + /* workaround for bug in GtkCList to ensure packet list scrollbar is updated */ <nl> + packet_list_freeze (); <nl> + packet_list_thaw (); <nl> } <nl>  <nl> 
mmm epan / dissectors / packet - gsm_a . c <nl> ppp epan / dissectors / packet - gsm_a . c <nl> be_cell_id_aux ( tvbuff_t * tvb , proto_tree * tree , guint32 offset , guint len , gchar <nl>  <nl> case 0x01 : <nl> case 0x05 : <nl> - case 0x0a : /* For intersystem handover from GSM to UMTS or cdma2000 : */ <nl> + case 0x0a : /* For intersystem handover from GSM to UMTS or cdma2000 : */ <nl>  <nl> /* LAC */ <nl>  <nl> be_cell_id_aux ( tvbuff_t * tvb , proto_tree * tree , guint32 offset , guint len , gchar <nl> if ( add_string ) <nl> g_snprintf ( add_string , string_len , " - LAC ( 0x % 04x )", value ); <nl>  <nl> - case 0x09 : /* For intersystem handover from GSM to UMTS or cdma2000 : */ <nl> + /* FALLTHRU */ <nl> + <nl> + case 0x09 : /* For intersystem handover from GSM to UMTS or cdma2000 : */ <nl>  <nl> if (( disc == 0x08 ) ||( disc == 0x09 ) || ( disc == 0x0a )){ <nl> /* RNC - ID */
mmm pcapio . c <nl> ppp pcapio . c <nl> pcapng_write_enhanced_packet_block ( FILE * pfile , <nl> sizeof ( guint32 )); <nl> } <nl> /* If we have options add size of end - of - options */ <nl> - /* If we have options add size of end - of - options */ <nl> if ( options_length != 0 ) { <nl> options_length += ( guint32 ) sizeof ( struct option ); <nl> } <nl> pcapng_write_enhanced_packet_block ( FILE * pfile , <nl> option . value_length = 0 ; <nl> if (! write_to_file ( pfile , ( const guint8 *)& option , sizeof ( struct option ), bytes_written , err )) <nl> return FALSE ; <nl> - } <nl> + } <nl> + if ( options_length != 0 ) { <nl> + /* write end of options */ <nl> + option . type = OPT_ENDOFOPT ; <nl> + option . value_length = 0 ; <nl> + if (! write_to_file ( pfile , ( const guint8 *)& option , sizeof ( struct option ), bytes_written , err )) <nl> + return FALSE ; <nl> + } <nl>  <nl> return write_to_file ( pfile , ( const guint8 *)& block_total_length , sizeof ( guint32 ), bytes_written , err ); <nl> }
mmm packet - tcp . c <nl> ppp packet - tcp . c <nl> /* packet - tcp . c <nl> * Routines for TCP packet disassembly <nl> * <nl> - * $ Id : packet - tcp . c , v 1 . 184 2003 / 03 / 03 23 : 20 : 57 sahlberg Exp $ <nl> + * $ Id : packet - tcp . c , v 1 . 185 2003 / 03 / 04 04 : 36 : 44 sharpe Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> dissect_tcpopt_sack ( const ip_tcp_opt * optp , tvbuff_t * tvb , <nl> if ( field_tree == NULL ) { <nl> /* Haven ' t yet made a subtree out of this option . Do so . */ <nl> field_tree = proto_item_add_subtree ( tf , * optp -> subtree_index ); <nl> + proto_tree_add_boolean_hidden ( field_tree , hf_tcp_option_sack , tvb , <nl> + offset , optlen , TRUE ); <nl> } <nl> if ( optlen < 4 ) { <nl> proto_tree_add_text ( field_tree , tvb , offset , optlen , <nl> dissect_tcpopt_sack ( const ip_tcp_opt * optp , tvbuff_t * tvb , <nl> break ; <nl> } <nl> leftedge = tvb_get_ntohl ( tvb , offset ); <nl> + proto_tree_add_uint_format ( field_tree , hf_tcp_option_sack_sle , tvb , <nl> + offset , 4 , leftedge , <nl> + " left edge = % u ", leftedge ); <nl> optlen -= 4 ; <nl> if ( optlen < 4 ) { <nl> proto_tree_add_text ( field_tree , tvb , offset , optlen , <nl> dissect_tcpopt_sack ( const ip_tcp_opt * optp , tvbuff_t * tvb , <nl> /* XXX - check whether it goes past end of packet */ <nl> rightedge = tvb_get_ntohl ( tvb , offset + 4 ); <nl> optlen -= 4 ; <nl> - proto_tree_add_boolean_hidden ( field_tree , hf_tcp_option_sack , tvb , <nl> - offset , 8 , TRUE ); <nl> - proto_tree_add_uint_format ( field_tree , hf_tcp_option_sack_sle , tvb , <nl> - offset , 8 , leftedge , <nl> - " left edge = % u ", leftedge ); <nl> proto_tree_add_uint_format ( field_tree , hf_tcp_option_sack_sre , tvb , <nl> - offset , 8 , rightedge , <nl> + offset + 4 , 4 , rightedge , <nl> " right edge = % u ", rightedge ); <nl> tcp_info_append_uint ( pinfo , " SLE ", leftedge ); <nl> tcp_info_append_uint ( pinfo , " SRE ", rightedge );
mmm packet - nbns . c <nl> ppp packet - nbns . c <nl> * Gilbert Ramirez < gram @ xiexie . org > <nl> * Much stuff added by Guy Harris < guy @ alum . mit . edu > <nl> * <nl> - * $ Id : packet - nbns . c , v 1 . 61 2001 / 09 / 30 23 : 14 : 43 guy Exp $ <nl> + * $ Id : packet - nbns . c , v 1 . 62 2001 / 10 / 12 01 : 41 : 03 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> dissect_nbss ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> * there should be no compression ), and we <nl> * shouldn ' t have more than 128 bytes ( actually , <nl> * we shouldn ' t have that many ). <nl> + * <nl> + * XXX - actually , MacOS X 10 . 1 ( yes , that ' s <nl> + * redundant , but that ' s what Apple calls it , <nl> + * not MacOS X . 1 ) puts names longer than 16 <nl> + * characters into session request messages , <nl> + * so we can have more than 32 bytes of <nl> + * name value , so we can have more than 128 <nl> + * bytes of data . <nl> */ <nl> - if ( length < 2 || length > 128 ) <nl> + if ( length < 2 || length > 256 ) <nl> goto continuation ; <nl> break ; <nl> 
mmm epan / crypt / airpdcap . c <nl> ppp epan / crypt / airpdcap . c <nl> static INT AirPDcapScanForKeys ( <nl>  <nl> /* get and check the body length ( IEEE 802 . 1X - 2004 , pg . 25 ) */ <nl> bodyLength = pntoh16 ( data + offset + 2 ); <nl> - if (( tot_len - offset - 4 ) < bodyLength ) { /* Only check if frame is long enough for eapol header , ignore tailing garbage , see bug 9065 */ <nl> + if ((( tot_len - offset - 4 ) < bodyLength ) || ( bodyLength < sizeof ( EAPOL_RSN_KEY ))) { /* Only check if frame is long enough for eapol header , ignore tailing garbage , see bug 9065 */ <nl> AIRPDCAP_DEBUG_PRINT_LINE (" AirPDcapScanForKeys ", " EAPOL body too short ", AIRPDCAP_DEBUG_LEVEL_3 ); <nl> return AIRPDCAP_RET_NO_VALID_HANDSHAKE ; <nl> }
mmm epan / epan . c <nl> ppp epan / epan . c <nl> epan_init ( void (* register_all_protocols_func )( register_cb cb , gpointer client_da <nl> register_cb cb , <nl> gpointer client_data ) <nl> { <nl> - gboolean status = TRUE ; <nl> + volatile gboolean status = TRUE ; <nl>  <nl> /* initialize memory allocation subsystem */ <nl> wmem_init ();
mmm epan / dissectors / packet - spnego . c <nl> ppp epan / dissectors / packet - spnego . c <nl> dissect_spnego_supportedMech ( tvbuff_t * tvb , int offset , packet_info * pinfo _U_ , <nl> proto_tree_add_text ( tree , tvb , offset , nbytes , " supportedMech : % s ", <nl> oid_string ); <nl>  <nl> - g_free ( oid_string ); <nl> - <nl> offset += nbytes ; <nl>  <nl> /* Should check for an unrecognized OID ... */
mmm plugins / megaco / packet - megaco . c <nl> ppp plugins / megaco / packet - megaco . c <nl> * Routines for megaco packet disassembly <nl> * RFC 3015 <nl> * <nl> -* $ Id : packet - megaco . c , v 1 . 15 2004 / 04 / 21 19 : 58 : 14 etxrab Exp $ <nl> +* $ Id : packet - megaco . c , v 1 . 16 2004 / 04 / 23 03 : 20 : 58 guy Exp $ <nl> * <nl> * Christian Falckenberg , 2002 / 10 / 17 <nl> * Copyright ( c ) 2002 by Christian Falckenberg <nl> dissect_megaco_text ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> if ( tree ) <nl> len = tvb_len - tvb_previous_offset ; <nl> proto_tree_add_text ( megaco_tree , tvb , tvb_previous_offset , - 1 , <nl> - "% s ", tvb_format_text ( tvb , tvb_previous_offset , len ), tvb_len , <nl> - tvb_previous_offset ); <nl> + "% s ", tvb_format_text ( tvb , tvb_previous_offset , len )); <nl> if ( global_megaco_raw_text ){ <nl> tvb_raw_text_add ( tvb , megaco_tree ); <nl> }
mmm wiretap / libpcap . c <nl> ppp wiretap / libpcap . c <nl> /* libpcap . c <nl> * <nl> - * $ Id : libpcap . c , v 1 . 16 1999 / 08 / 28 01 : 19 : 44 guy Exp $ <nl> + * $ Id : libpcap . c , v 1 . 17 1999 / 08 / 31 22 : 36 : 20 guy Exp $ <nl> * <nl> * Wiretap Library <nl> * Copyright ( c ) 1998 by Gilbert Ramirez < gram @ verdict . uthscsa . edu > <nl> ((( x )& 0x00FF )<< 8 )) <nl>  <nl> /* On some systems , the FDDI MAC addresses are bit - swapped . */ <nl> -# if ! defined ( ultrix ) && ! defined ( __alpha ) && ! defined ( __bsdi ) <nl> +# if ! defined ( ultrix ) && ! defined ( __alpha ) && ! defined ( __bsdi__ ) <nl> # define BIT_SWAPPED_MAC_ADDRS <nl> # endif <nl> 
mmm epan / dissectors / packet - adwin - config . c <nl> ppp epan / dissectors / packet - adwin - config . c <nl> dissect_TCPFlashUpdate ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tree * adwin <nl> } <nl>  <nl> /* 00 : 50 : c2 : 0a : 2 *:** */ <nl> - static char mac_iab_start [] = { 0x00 , 0x50 , 0xc2 , 0x0a , 0x20 , 0x00 }; <nl> - static char mac_iab_end [] = { 0x00 , 0x50 , 0xc2 , 0x0a , 0x2f , 0xff }; <nl> + static const unsigned char mac_iab_start [] = { 0x00 , 0x50 , 0xc2 , 0x0a , 0x20 , 0x00 }; <nl> + static const unsigned char mac_iab_end [] = { 0x00 , 0x50 , 0xc2 , 0x0a , 0x2f , 0xff }; <nl>  <nl> /* 00 : 22 : 71 :**:**:** */ <nl> - static char mac_oui_start [] = { 0x00 , 0x22 , 0x71 , 0x00 , 0x00 , 0x00 }; <nl> - static char mac_oui_end [] = { 0x00 , 0x22 , 0x71 , 0xff , 0xff , 0xff }; <nl> + static const unsigned char mac_oui_start [] = { 0x00 , 0x22 , 0x71 , 0x00 , 0x00 , 0x00 }; <nl> + static const unsigned char mac_oui_end [] = { 0x00 , 0x22 , 0x71 , 0xff , 0xff , 0xff }; <nl>  <nl> /* ff : ff : ff : ff : ff : ff */ <nl> - static char mac_broadcast [] = { 0xff , 0xff , 0xff , 0xff , 0xff , 0xff }; <nl> + static const unsigned char mac_broadcast [] = { 0xff , 0xff , 0xff , 0xff , 0xff , 0xff }; <nl>  <nl> /* return TRUE if mac is in mac address range assigned to ADwin or if <nl> * mac is broadcast */mmm ui / gtk / wlan_stat_dlg . c <nl> ppp ui / gtk / wlan_stat_dlg . c <nl> dissect_TCPFlashUpdate ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tree * adwin <nl> } <nl>  <nl> /* 00 : 50 : c2 : 0a : 2 *:** */ <nl> - static char mac_iab_start [] = { 0x00 , 0x50 , 0xc2 , 0x0a , 0x20 , 0x00 }; <nl> - static char mac_iab_end [] = { 0x00 , 0x50 , 0xc2 , 0x0a , 0x2f , 0xff }; <nl> + static const unsigned char mac_iab_start [] = { 0x00 , 0x50 , 0xc2 , 0x0a , 0x20 , 0x00 }; <nl> + static const unsigned char mac_iab_end [] = { 0x00 , 0x50 , 0xc2 , 0x0a , 0x2f , 0xff }; <nl>  <nl> /* 00 : 22 : 71 :**:**:** */ <nl> - static char mac_oui_start [] = { 0x00 , 0x22 , 0x71 , 0x00 , 0x00 , 0x00 }; <nl> - static char mac_oui_end [] = { 0x00 , 0x22 , 0x71 , 0xff , 0xff , 0xff }; <nl> + static const unsigned char mac_oui_start [] = { 0x00 , 0x22 , 0x71 , 0x00 , 0x00 , 0x00 }; <nl> + static const unsigned char mac_oui_end [] = { 0x00 , 0x22 , 0x71 , 0xff , 0xff , 0xff }; <nl>  <nl> /* ff : ff : ff : ff : ff : ff */ <nl> - static char mac_broadcast [] = { 0xff , 0xff , 0xff , 0xff , 0xff , 0xff }; <nl> + static const unsigned char mac_broadcast [] = { 0xff , 0xff , 0xff , 0xff , 0xff , 0xff }; <nl>  <nl> /* return TRUE if mac is in mac address range assigned to ADwin or if <nl> * mac is broadcast */ <nl> wlanstat_launch ( GtkAction * action _U_ , gpointer user_data _U_ ) <nl> void <nl> register_tap_listener_wlanstat ( void ) <nl> { <nl> - static const char src [ 6 ] = { 0xff , 0xff , 0xff , 0xff , 0xff , 0xff }; <nl> + static const unsigned char src [ 6 ] = { 0xff , 0xff , 0xff , 0xff , 0xff , 0xff }; <nl>  <nl> SET_ADDRESS (& broadcast , AT_ETHER , 6 , src ); <nl> }
mmm epan / dissectors / packet - vmlab . c <nl> ppp epan / dissectors / packet - vmlab . c <nl> dissect_vmlab ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> guint8 attributes ; <nl> guint8 portgroup ; <nl>  <nl> - guint16 encap_proto ; <nl> + volatile guint16 encap_proto ; <nl>  <nl> col_set_str ( pinfo -> cinfo , COL_PROTOCOL , " VMLAB "); <nl> col_clear ( pinfo -> cinfo , COL_INFO );
mmm epan / dissectors / packet - isakmp . c <nl> ppp epan / dissectors / packet - isakmp . c <nl> static struct strfunc { <nl> {" Delete ", dissect_delete }, <nl> {" Vendor ID ", dissect_vid }, <nl> {" Attrib ", dissect_config }, <nl> - {" NAT - Discovery ", dissect_nat_discovery }, /* draft - ietf - ipsec - nat - t - ike */ <nl> + {" NAT - Discovery ", dissect_nat_discovery }, /* draft - ietf - ipsec - nat - t - ike - 04 */ <nl> {" NAT - Original Address ", dissect_nat_original_address } /* draft - ietf - ipsec - nat - t - ike */ <nl> }; <nl>  <nl> payloadtype2str ( guint8 type ) { <nl> if ( type < 128 ) <nl> return " RESERVED "; <nl> if ( type == 130 ) <nl> - return " NAT - D ( draft - ietf - ipsec - nat - t - ike - 01 to 04 )"; <nl> + return " NAT - D ( draft - ietf - ipsec - nat - t - ike - 01 to 03 )"; <nl> if ( type == 131 ) <nl> return " NAT - OA ( draft - ietf - ipsec - nat - t - ike - 01 to 04 )"; <nl> return " Private USE ";
mmm packet - smb . c <nl> ppp packet - smb . c <nl> * Copyright 1999 , Richard Sharpe < rsharpe @ ns . aus . com > <nl> * 2001 Rewrite by Ronnie Sahlberg and Guy Harris <nl> * <nl> - * $ Id : packet - smb . c , v 1 . 327 2003 / 04 / 14 17 : 38 : 49 guy Exp $ <nl> + * $ Id : packet - smb . c , v 1 . 328 2003 / 04 / 17 00 : 13 : 26 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> proto_reg_handoff_smb ( void ) <nl> ntlmssp_handle = find_dissector (" ntlmssp "); <nl>  <nl> heur_dissector_add (" netbios ", dissect_smb_heur , proto_smb ); <nl> + heur_dissector_add (" cotp ", dissect_smb_heur , proto_smb ); <nl> smb_handle = create_dissector_handle ( dissect_smb , proto_smb ); <nl> dissector_add (" ipx . socket ", IPX_SOCKET_NWLINK_SMB_SERVER , smb_handle ); <nl> dissector_add (" ipx . socket ", IPX_SOCKET_NWLINK_SMB_REDIR , smb_handle );
mmm epan / dissectors / packet - nfs . c <nl> ppp epan / dissectors / packet - nfs . c <nl> dissect_nfs_argop4 ( tvbuff_t * tvb , int offset , packet_info * pinfo , <nl> } <nl> } <nl>  <nl> + for ( ops_counter = 0 ; ops_counter < ops ; ops_counter ++) <nl> + { <nl> + g_string_free ( op_summary [ ops_counter ]. optext , TRUE ); <nl> + } <nl> + <nl> + g_free ( op_summary ); <nl>  <nl> return offset ; <nl> } <nl> dissect_nfs_resop4 ( tvbuff_t * tvb , int offset , packet_info * pinfo , <nl> } <nl> } <nl>  <nl> + for ( ops_counter = 0 ; ops_counter < ops ; ops_counter ++) <nl> + { <nl> + g_string_free ( op_summary [ ops_counter ]. optext , TRUE ); <nl> + } <nl> + <nl> + g_free ( op_summary ); <nl>  <nl> return offset ; <nl> }
mmm simple_dialog . h <nl> ppp simple_dialog . h <nl> * Definitions for alert box routines with toolkit - independent APIs but <nl> * toolkit - dependent implementations . <nl> * <nl> - * $ Id : simple_dialog . h , v 1 . 14 2004 / 06 / 04 20 : 04 : 34 ulfl Exp $ <nl> + * $ Id : simple_dialog . h , v 1 . 15 2004 / 06 / 04 21 : 12 : 01 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> extern gpointer simple_dialog ( ESD_TYPE_E type , gint btn_mask , <nl> * @ param ap parameters <nl> * @ return the newly created dialog <nl> */ <nl> - extern gpointer vsimple_dialog ( gint type , gint btn_mask , <nl> + extern gpointer vsimple_dialog ( ESD_TYPE_E type , gint btn_mask , <nl> const gchar * msg_format , va_list ap ); <nl> # else <nl> /** Create and show a simple dialog .mmm gtk / simple_dialog . c <nl> ppp gtk / simple_dialog . c <nl> * Definitions for alert box routines with toolkit - independent APIs but <nl> * toolkit - dependent implementations . <nl> * <nl> - * $ Id : simple_dialog . h , v 1 . 14 2004 / 06 / 04 20 : 04 : 34 ulfl Exp $ <nl> + * $ Id : simple_dialog . h , v 1 . 15 2004 / 06 / 04 21 : 12 : 01 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> extern gpointer simple_dialog ( ESD_TYPE_E type , gint btn_mask , <nl> * @ param ap parameters <nl> * @ return the newly created dialog <nl> */ <nl> - extern gpointer vsimple_dialog ( gint type , gint btn_mask , <nl> + extern gpointer vsimple_dialog ( ESD_TYPE_E type , gint btn_mask , <nl> const gchar * msg_format , va_list ap ); <nl> # else <nl> /** Create and show a simple dialog . <nl> /* simple_dialog . c <nl> * Simple message dialog box routines . <nl> * <nl> - * $ Id : simple_dialog . c , v 1 . 35 2004 / 05 / 26 03 : 49 : 24 ulfl Exp $ <nl> + * $ Id : simple_dialog . c , v 1 . 36 2004 / 06 / 04 21 : 12 : 01 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> display_queued_messages ( void ) <nl> */ <nl>  <nl> gpointer <nl> - vsimple_dialog ( gint type , gint btn_mask , const gchar * msg_format , va_list ap ) <nl> + vsimple_dialog ( ESD_TYPE_E type , gint btn_mask , const gchar * msg_format , va_list ap ) <nl> { <nl> gchar * message ; <nl> queued_message_t * queued_message ; <nl> vsimple_dialog ( gint type , gint btn_mask , const gchar * msg_format , va_list ap ) <nl> } <nl>  <nl> gpointer <nl> - simple_dialog ( gint type , gint btn_mask , const gchar * msg_format , ...) <nl> + simple_dialog ( ESD_TYPE_E type , gint btn_mask , const gchar * msg_format , ...) <nl> { <nl> va_list ap ; <nl> gpointer ret ;
mmm epan / dissectors / packet - rsvd . c <nl> ppp epan / dissectors / packet - rsvd . c <nl> static const value_string rsvd_disk_type_vals [] = { <nl>  <nl> static const value_string rsvd_disk_format_vals [] = { <nl> { 0x03 , " VIRTUAL_STORAGE_TYPE_DEVICE_VHDX " }, <nl> + { 0x04 , " VIRTUAL_STORAGE_TYPE_DEVICE_VHDSET " }, <nl> { 0 , NULL } <nl> }; <nl>  <nl> dissect_RSVD_GET_INITIAL_INFO ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tree <nl> return offset ; <nl> } <nl>  <nl> + static const value_string rsvd_data_in_vals [] = { <nl> + { 0x00 , " Client is requesting data from the server " }, <nl> + { 0x01 , " Client is sending data to the server " }, <nl> + { 0x02 , " Client is neither sending nor requesting an additional data buffer " }, <nl> + { 0 , NULL } <nl> +}; <nl> + <nl> /* <nl> * Dissect a tunnelled SCSI request and call the SCSI dissector where <nl> * needed . <nl> proto_register_rsvd ( void ) <nl> NULL , 0 , NULL , HFILL }}, <nl>  <nl> { & hf_svhdx_tunnel_scsi_data_in , <nl> - { " DataIn ", " rsvd . svhdx_scsi_data_in ", FT_BOOLEAN , 8 , <nl> - NULL , 0 , NULL , HFILL }}, <nl> + { " DataIn ", " rsvd . svhdx_scsi_data_in ", FT_UINT8 , BASE_HEX , <nl> + VALS ( rsvd_data_in_vals ), 0 , " SCSI CDB transfer type ", HFILL }}, <nl>  <nl> { & hf_svhdx_tunnel_scsi_reserved2 , <nl> { " Reserved2 ", " rsvd . svhdx_scsi_reserved2 ", FT_UINT8 , BASE_HEX ,mmm epan / dissectors / packet - smb2 . c <nl> ppp epan / dissectors / packet - smb2 . c <nl> static const value_string rsvd_disk_type_vals [] = { <nl>  <nl> static const value_string rsvd_disk_format_vals [] = { <nl> { 0x03 , " VIRTUAL_STORAGE_TYPE_DEVICE_VHDX " }, <nl> + { 0x04 , " VIRTUAL_STORAGE_TYPE_DEVICE_VHDSET " }, <nl> { 0 , NULL } <nl> }; <nl>  <nl> dissect_RSVD_GET_INITIAL_INFO ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tree <nl> return offset ; <nl> } <nl>  <nl> + static const value_string rsvd_data_in_vals [] = { <nl> + { 0x00 , " Client is requesting data from the server " }, <nl> + { 0x01 , " Client is sending data to the server " }, <nl> + { 0x02 , " Client is neither sending nor requesting an additional data buffer " }, <nl> + { 0 , NULL } <nl> +}; <nl> + <nl> /* <nl> * Dissect a tunnelled SCSI request and call the SCSI dissector where <nl> * needed . <nl> proto_register_rsvd ( void ) <nl> NULL , 0 , NULL , HFILL }}, <nl>  <nl> { & hf_svhdx_tunnel_scsi_data_in , <nl> - { " DataIn ", " rsvd . svhdx_scsi_data_in ", FT_BOOLEAN , 8 , <nl> - NULL , 0 , NULL , HFILL }}, <nl> + { " DataIn ", " rsvd . svhdx_scsi_data_in ", FT_UINT8 , BASE_HEX , <nl> + VALS ( rsvd_data_in_vals ), 0 , " SCSI CDB transfer type ", HFILL }}, <nl>  <nl> { & hf_svhdx_tunnel_scsi_reserved2 , <nl> { " Reserved2 ", " rsvd . svhdx_scsi_reserved2 ", FT_UINT8 , BASE_HEX , <nl> static const value_string smb2_ioctl_shared_virtual_disk_vals [] = { <nl> static const value_string smb2_ioctl_shared_virtual_disk_hstate_vals [] = { <nl> { 0x00 , " HandleStateNone " }, <nl> { 0x01 , " HandleStateFileShared " }, <nl> - { 0x02 , " HandleStateShared " }, <nl> + { 0x03 , " HandleStateShared " }, <nl> { 0 , NULL } <nl> }; <nl> 
mmm epan / dissectors / packet - infiniband . c <nl> ppp epan / dissectors / packet - infiniband . c <nl> dissect_infiniband ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> proto_tree_add_item ( base_transport_header_tree , hf_infiniband_transport_header_version , tvb , offset , 1 , FALSE ); offset += 1 ; <nl> proto_tree_add_item ( base_transport_header_tree , hf_infiniband_partition_key , tvb , offset , 2 , FALSE ); offset += 2 ; <nl> proto_tree_add_item ( base_transport_header_tree , hf_infiniband_reserved8 , tvb , offset , 1 , FALSE ); offset += 1 ; <nl> - proto_tree_add_item ( base_transport_header_tree , hf_infiniband_destination_qp , tvb , offset , 3 , FALSE ); offset += 3 ; <nl> + proto_tree_add_item ( base_transport_header_tree , hf_infiniband_destination_qp , tvb , offset , 3 , FALSE ); <nl> + dst_qp = tvb_get_ntoh24 ( tvb , offset ); offset += 3 ; <nl> proto_tree_add_item ( base_transport_header_tree , hf_infiniband_acknowledge_request , tvb , offset , 1 , FALSE ); <nl> proto_tree_add_item ( base_transport_header_tree , hf_infiniband_reserved7 , tvb , offset , 1 , FALSE ); offset += 1 ; <nl> proto_tree_add_item ( base_transport_header_tree , hf_infiniband_packet_sequence_number , tvb , offset , 3 , FALSE ); offset += 3 ;
mmm gtk / gtkvumeter . c <nl> ppp gtk / gtkvumeter . c <nl> static void gtk_vumeter_size_calculate ( GtkWidget * widget , GtkRequisition * requi <nl> PangoLayout * layout = gtk_widget_create_pango_layout ( widget , item -> label ); <nl> pango_layout_get_pixel_size ( layout , & layout_width , & layout_height ); <nl> /* XXX - memleak */ <nl> + } else { <nl> + layout_width = 0 ; <nl> + layout_height = 0 ; <nl> } <nl>  <nl> if ( vumeter -> vertical == TRUE ) {
mmm gtk / dlg_utils . c <nl> ppp gtk / dlg_utils . c <nl> dlg_button_row_new ( const gchar * stock_id_first , ...) <nl> gtk_box_pack_end ( GTK_BOX ( hbox ), button_hbox , TRUE , TRUE , 0 ); <nl> g_object_set_data ( G_OBJECT ( hbox ), BUTTON_HBOX_KEY , button_hbox ); <nl> gtk_widget_show ( button_hbox ); <nl> + gtk_button_box_set_spacing ( GTK_BUTTON_BOX ( button_hbox ), 5 ); <nl>  <nl> help_hbox = gtk_hbutton_box_new (); <nl> gtk_box_pack_end ( GTK_BOX ( hbox ), help_hbox , FALSE , FALSE , 0 ); <nl> gtk_widget_show ( help_hbox ); <nl> + gtk_button_box_set_spacing ( GTK_BUTTON_BOX ( help_hbox ), 5 ); <nl>  <nl> if ( buttons == 0 ) { <nl> /* if no buttons wanted , simply do nothing */ <nl> dlg_button_row_new ( const gchar * stock_id_first , ...) <nl> /* if more than one button , sort buttons from left to right */ <nl> /* ( the whole button cluster will then be right aligned ) */ <nl> gtk_button_box_set_layout ( GTK_BUTTON_BOX ( button_hbox ), GTK_BUTTONBOX_END ); <nl> - gtk_button_box_set_spacing ( GTK_BUTTON_BOX ( button_hbox ), 5 ); <nl>  <nl> /* GTK + 1 . 3 and later - on Win32 , we use 1 . 3 [. x ] or 2 . x , not 1 . 2 [. x ] */ <nl> # if ! defined ( _WIN32 ) <nl> dlg_button_row_new ( const gchar * stock_id_first , ...) <nl> if ( close != NULL ) dlg_button_new ( hbox , button_hbox , close ); <nl> if ( cancel != NULL ) dlg_button_new ( hbox , button_hbox , cancel ); <nl>  <nl> - /* GTK2 : we don ' t know that button combination , add it to the above list ! */ <nl> - /* g_assert_not_reached (); */ <nl> return hbox ; <nl> } <nl> 
mmm ui / qt / io_graph_dialog . cpp <nl> ppp ui / qt / io_graph_dialog . cpp <nl> static uat_field_t io_graph_fields [] = { <nl> UAT_END_FIELDS <nl> }; <nl>  <nl> - static void * io_graph_copy_cb ( void * dst_ptr , const void * src_ptr , size_t len _U_ ) { <nl> + static void * io_graph_copy_cb ( void * dst_ptr , const void * src_ptr , size_t len ) { <nl> + Q_UNUSED ( len ); <nl> io_graph_settings_t * dst = ( io_graph_settings_t *) dst_ptr ; <nl> const io_graph_settings_t * src = ( const io_graph_settings_t *) src_ptr ; <nl> 
mmm epan / reassemble . c <nl> ppp epan / reassemble . c <nl> fragment_add_work ( fragment_data * fd_head , tvbuff_t * tvb , const int offset , <nl> fragment_data * fd_i ; <nl> guint32 max , dfpos , fraglen ; <nl> tvbuff_t * old_tvb_data ; <nl> - char * data ; <nl> + guint8 * data ; <nl>  <nl> /* create new fd describing this fragment */ <nl> fd = g_slice_new ( fragment_data ); <nl> fragment_add_work ( fragment_data * fd_head , tvbuff_t * tvb , const int offset , <nl> */ <nl> /* store old data just in case */ <nl> old_tvb_data = fd_head -> tvb_data ; <nl> - data = g_malloc ( fd_head -> datalen ); <nl> + data = ( guint8 *) g_malloc ( fd_head -> datalen ); <nl> fd_head -> tvb_data = tvb_new_real_data ( data , fd_head -> datalen , fd_head -> datalen ); <nl> tvb_set_free_cb ( fd_head -> tvb_data , g_free ); <nl>  <nl> fragment_defragment_and_free ( fragment_data * fd_head , const packet_info * pinfo ) <nl> fragment_data * last_fd = NULL ; <nl> guint32 dfpos = 0 , size = 0 ; <nl> tvbuff_t * old_tvb_data = NULL ; <nl> - char * data ; <nl> + guint8 * data ; <nl>  <nl> for ( fd_i = fd_head -> next ; fd_i ; fd_i = fd_i -> next ) { <nl> if (! last_fd || last_fd -> offset != fd_i -> offset ){ <nl> fragment_defragment_and_free ( fragment_data * fd_head , const packet_info * pinfo ) <nl>  <nl> /* store old data in case the fd_i -> data pointers refer to it */ <nl> old_tvb_data = fd_head -> tvb_data ; <nl> - data = g_malloc ( size ); <nl> + data = ( guint8 *) g_malloc ( size ); <nl> fd_head -> tvb_data = tvb_new_real_data ( data , size , size ); <nl> tvb_set_free_cb ( fd_head -> tvb_data , g_free ); <nl> fd_head -> len = size ; /* record size for caller */
mmm tshark . c <nl> ppp tshark . c <nl> print_usage ( gboolean print_ver ) <nl>  <nl> /* fprintf ( output , "\ n ");*/ <nl> fprintf ( output , " Output :\ n "); <nl> - fprintf ( output , " - w < outfile |-> set the output filename ( or '-' for stdout )\ n "); <nl> + fprintf ( output , " - w < outfile |-> write packets to a pcap - format file named \" outfile \"\ n "); <nl> + fprintf ( output , " ( or to the standard output for \"-\")\ n "); <nl> fprintf ( output , " - C < config profile > start with specified configuration profile \ n "); <nl> fprintf ( output , " - F < output file type > set the output file type , default is libpcap \ n "); <nl> fprintf ( output , " an empty \"- F \" option will list the file types \ n ");
mmm epan / dissectors / packet - icq . c <nl> ppp epan / dissectors / packet - icq . c <nl> dissect_icqv5Client ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> * bytes in the buffer . <nl> */ <nl> rounded_size = (((( capturedsize - ICQ5_CL_SESSIONID ) + 3 )/ 4 )* 4 ) + ICQ5_CL_SESSIONID ; <nl> - decr_pd = tvb_memdup ( tvb , 0 , capturedsize ); <nl> + /* rounded_size might exceed the tvb bounds so we can ' t just use tvb_memdup here . */ <nl> + decr_pd = g_malloc ( rounded_size ); <nl> + tvb_memcpy ( tvb , decr_pd , 0 , capturedsize ); <nl> decrypt_v5 ( decr_pd , rounded_size , key ); <nl>  <nl> /* Allocate a new tvbuff , referring to the decrypted data . */
mmm epan / dissectors / packet - prp . c <nl> ppp epan / dissectors / packet - prp . c <nl> dissect_prp_redundancy_control_trailer ( tvbuff_t * tvb , packet_info * pinfo _U_ , pr <nl> if ( length < 14 ) <nl> return 0 ; <nl>  <nl> + /* <nl> + * This is horribly broken . It assumes the frame is an Ethernet <nl> + * frame , with a type field at an offset of 12 bytes from the header . <nl> + * That is not guaranteed to be true . <nl> + */ <nl> + if (! tvb_bytes_exist ( tvb , 12 , 2 )) <nl> + return 0 ; <nl> if ( ETHERTYPE_VLAN == tvb_get_ntohs ( tvb , 12 )) /* tagged frame */ <nl> { <nl> offset = 18 ; <nl> dissect_prp_redundancy_control_trailer ( tvbuff_t * tvb , packet_info * pinfo _U_ , pr <nl> if (! tree ) <nl> return tvb_captured_length ( tvb ); <nl>  <nl> + /* <nl> + * Is there enough data in the packet to every try to search for a <nl> + * trailer ? <nl> + */ <nl> + if (! tvb_bytes_exist ( tvb , ( length - 4 )+ 2 , 2 )) <nl> + return 0 ; /* no */ <nl> + <nl> /* search for PRP - 0 trailer */ <nl> /* If the frame is > 64 bytes , the PRP - 0 trailer is always at the end . */ <nl> /* If the frame is <= 64 bytes , the PRP - 0 trailer may be anywhere ( before the padding ) */
mmm epan / dissectors / packet - sip . c <nl> ppp epan / dissectors / packet - sip . c <nl> dissect_sip_contact_item ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , gi <nl> gint current_offset ; <nl> gint queried_offset ; <nl> gint contact_params_start_offset = - 1 ; <nl> - gint contact_param_end_offset = - 1 ; <nl> + /* gint contact_param_end_offset = - 1 ;*/ <nl> uri_offset_info uri_offsets ; <nl>  <nl> /* skip Spaces and Tabs */
mmm wiretap / cosine . c <nl> ppp wiretap / cosine . c <nl> parse_cosine_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl> { <nl> union wtap_pseudo_header * pseudo_header = & phdr -> pseudo_header ; <nl> int num_items_scanned ; <nl> - int yy , mm , dd , hr , min , sec , csec ; <nl> - guint pkt_len ; <nl> + int yy , mm , dd , hr , min , sec , csec , pkt_len ; <nl> int pro , off , pri , rm , error ; <nl> guint code1 , code2 ; <nl> char if_name [ COSINE_MAX_IF_NAME_LEN ] = "", direction [ 6 ] = ""; <nl> parse_cosine_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl> & yy , & mm , & dd , & hr , & min , & sec , & csec ) == 7 ) { <nl> /* appears to be output to a control blade */ <nl> num_items_scanned = sscanf ( line , <nl> - "% 4d -% 2d -% 2d ,% 2d :% 2d :% 2d .% 9d : % 5s (% 127 [ A - Za - z0 - 9 /:]), Length :% 9u , Pro :% 9d , Off :% 9d , Pri :% 9d , RM :% 9d , Err :% 9d [% 8x , % 8x ]", <nl> + "% 4d -% 2d -% 2d ,% 2d :% 2d :% 2d .% 9d : % 5s (% 127 [ A - Za - z0 - 9 /:]), Length :% 9d , Pro :% 9d , Off :% 9d , Pri :% 9d , RM :% 9d , Err :% 9d [% 8x , % 8x ]", <nl> & yy , & mm , & dd , & hr , & min , & sec , & csec , <nl> direction , if_name , & pkt_len , <nl> & pro , & off , & pri , & rm , & error , <nl> parse_cosine_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl> } else { <nl> /* appears to be output to PE */ <nl> num_items_scanned = sscanf ( line , <nl> - "% 5s (% 127 [ A - Za - z0 - 9 /:]), Length :% 9u , Pro :% 9d , Off :% 9d , Pri :% 9d , RM :% 9d , Err :% 9d [% 8x , % 8x ]", <nl> + "% 5s (% 127 [ A - Za - z0 - 9 /:]), Length :% 9d , Pro :% 9d , Off :% 9d , Pri :% 9d , RM :% 9d , Err :% 9d [% 8x , % 8x ]", <nl> direction , if_name , & pkt_len , <nl> & pro , & off , & pri , & rm , & error , <nl> & code1 , & code2 ); <nl> parse_cosine_packet ( FILE_T fh , struct wtap_pkthdr * phdr , Buffer * buf , <nl> } <nl> yy = mm = dd = hr = min = sec = csec = 0 ; <nl> } <nl> + if ( pkt_len < 0 ) { <nl> + * err = WTAP_ERR_BAD_FILE ; <nl> + * err_info = g_strdup (" cosine : packet header has a negative packet length "); <nl> + return FALSE ; <nl> + } <nl> if ( pkt_len > WTAP_MAX_PACKET_SIZE ) { <nl> /* <nl> * Probably a corrupt capture file ; don ' t blow up trying
mmm epan / filesystem . c <nl> ppp epan / filesystem . c <nl> file_open_error_message ( int err , gboolean for_writing ) <nl> break ; <nl> # endif <nl>  <nl> + case EINVAL : <nl> + errmsg = " The file \"% s \" could not be created because an invalid filename was specified ."; <nl> + break ; <nl> + <nl> default : <nl> g_snprintf ( errmsg_errno , sizeof ( errmsg_errno ), <nl> " The file \"%% s \" could not be % s : % s .",
mmm epan / crypt / airpdcap . c <nl> ppp epan / crypt / airpdcap . c <nl> AirPDcapGetStaAddress ( <nl> switch ( AIRPDCAP_DS_BITS ( frame -> fc [ 1 ])) { /* Bit 1 = FromDS , bit 0 = ToDS */ <nl> case 0 : <nl> case 1 : <nl> - case 3 : <nl> return frame -> addr2 ; <nl> case 2 : <nl> return frame -> addr1 ; <nl> + case 3 : <nl> + if ( memcmp ( frame -> addr1 , frame -> addr2 , AIRPDCAP_MAC_LEN ) < 0 ) <nl> + return frame -> addr1 ; <nl> + else <nl> + return frame -> addr2 ; <nl> + <nl> default : <nl> return NULL ; <nl> } <nl> AirPDcapGetBssidAddress ( <nl> case 0 : <nl> return frame -> addr3 ; <nl> case 1 : <nl> - case 3 : <nl> return frame -> addr1 ; <nl> case 2 : <nl> return frame -> addr2 ; <nl> + case 3 : <nl> + if ( memcmp ( frame -> addr1 , frame -> addr2 , AIRPDCAP_MAC_LEN ) > 0 ) <nl> + return frame -> addr1 ; <nl> + else <nl> + return frame -> addr2 ; <nl> + <nl> default : <nl> return NULL ; <nl> }
mmm epan / frame_data . c <nl> ppp epan / frame_data . c <nl> frame_data_init ( frame_data * fdata , guint32 num , <nl> fdata -> cum_bytes = cum_bytes + phdr -> len ; <nl> fdata -> cap_len = phdr -> caplen ; <nl> fdata -> file_off = offset ; <nl> - /* To save some memory , we coarcese it into a gint8 */ <nl> - g_assert ( phdr -> pkt_encap <= G_MAXINT8 ); <nl> - fdata -> lnk_t = ( gint8 ) phdr -> pkt_encap ; <nl> + /* To save some memory , we coerce it into a gint16 */ <nl> + g_assert ( phdr -> pkt_encap <= G_MAXINT16 ); <nl> + fdata -> lnk_t = ( gint16 ) phdr -> pkt_encap ; <nl> fdata -> abs_ts . secs = phdr -> ts . secs ; <nl> fdata -> abs_ts . nsecs = phdr -> ts . nsecs ; <nl> fdata -> flags . passed_dfilter = 0 ;mmm epan / frame_data . h <nl> ppp epan / frame_data . h <nl> frame_data_init ( frame_data * fdata , guint32 num , <nl> fdata -> cum_bytes = cum_bytes + phdr -> len ; <nl> fdata -> cap_len = phdr -> caplen ; <nl> fdata -> file_off = offset ; <nl> - /* To save some memory , we coarcese it into a gint8 */ <nl> - g_assert ( phdr -> pkt_encap <= G_MAXINT8 ); <nl> - fdata -> lnk_t = ( gint8 ) phdr -> pkt_encap ; <nl> + /* To save some memory , we coerce it into a gint16 */ <nl> + g_assert ( phdr -> pkt_encap <= G_MAXINT16 ); <nl> + fdata -> lnk_t = ( gint16 ) phdr -> pkt_encap ; <nl> fdata -> abs_ts . secs = phdr -> ts . secs ; <nl> fdata -> abs_ts . nsecs = phdr -> ts . nsecs ; <nl> fdata -> flags . passed_dfilter = 0 ; <nl> typedef struct _frame_data { <nl> guint32 pkt_len ; /* Packet length */ <nl> guint32 cap_len ; /* Amount actually captured */ <nl> guint32 cum_bytes ; /* Cumulative bytes into the capture */ <nl> - guint16 subnum ; /* subframe number , for protocols that require this */ <nl> gint64 file_off ; /* File offset */ <nl> - gint8 lnk_t ; /* Per - packet encapsulation / data - link type */ <nl> + guint16 subnum ; /* subframe number , for protocols that require this */ <nl> + gint16 lnk_t ; /* Per - packet encapsulation / data - link type */ <nl> struct { <nl> unsigned int passed_dfilter : 1 ; /* 1 = display , 0 = no display */ <nl> unsigned int encoding : 2 ; /* Character encoding ( ASCII , EBCDIC ...) */
mmm ui / gtk / rlc_lte_graph . c <nl> ppp ui / gtk / rlc_lte_graph . c <nl> # define MOUSE_BUTTON_MIDDLE 2 <nl> # define MOUSE_BUTTON_RIGHT 3 <nl>  <nl> -# define MAX_PIXELS_PER_SN 90 <nl> +# define MAX_PIXELS_PER_SN 90 <nl> +# define MAX_PIXELS_PER_SECOND 50000 <nl>  <nl> extern int proto_rlc_lte ; <nl>  <nl> static void do_zoom_common ( struct graph * g , GdkEventButton * event , <nl> } <nl> } else { <nl> /* Zoom in */ <nl> - if ( lock_horizontal ) { <nl> + if (( lock_horizontal ) || ( g -> geom . width >= ( g -> bounds . width * MAX_PIXELS_PER_SECOND ))) { <nl> factor . x = 1 . 0 ; <nl> } <nl> else { <nl> static void do_zoom_common ( struct graph * g , GdkEventButton * event , <nl> } <nl>  <nl> /* Don ' t zoom in too far vertically */ <nl> - if (( g -> geom . height >= ( g -> bounds . height * MAX_PIXELS_PER_SN )) || lock_vertical ) { <nl> + if ( lock_vertical || ( g -> geom . height >= ( g -> bounds . height * MAX_PIXELS_PER_SN ))) { <nl> factor . y = 1 . 0 ; <nl> } <nl> else {
mmm epan / dissectors / packet - rohc . c <nl> ppp epan / dissectors / packet - rohc . c <nl> start_over : <nl> } <nl> col_prepend_fstr ( pinfo -> cinfo , COL_PROTOCOL , " ROHC <"); <nl> col_append_str ( pinfo -> cinfo , COL_PROTOCOL , ">"); <nl> + pinfo -> private_data = save_private_data ; <nl> + return ; <nl> } <nl> else if ((( oct & 0x80 )== 0x00 ) && ( rohc_cid_context -> profile == ROHC_PROFILE_RTP )) { <nl> /* 5 . 7 . 1 . Packet type 0 : UO - 0 , R - 0 , R - 0 - CRC */ <nl> start_over : <nl> } <nl>  <nl> payload_tvb = tvb_new_subset_remaining ( tvb , offset ); <nl> - call_dissector_only ( data_handle , payload_tvb , pinfo , rohc_tree , NULL ); <nl> + call_dissector_only ( data_handle , payload_tvb , pinfo , tree , NULL ); <nl>  <nl> pinfo -> private_data = save_private_data ; <nl> }
mmm gtk / file_dlg . c <nl> ppp gtk / file_dlg . c <nl> /* file_dlg . c <nl> * Dialog boxes for handling files <nl> * <nl> - * $ Id : file_dlg . c , v 1 . 41 2001 / 08 / 21 06 : 39 : 18 guy Exp $ <nl> + * $ Id : file_dlg . c , v 1 . 42 2001 / 09 / 10 08 : 49 : 11 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> select_file_type_cb ( GtkWidget * w , gpointer data ) <nl> /* We can select only the filtered or marked packets to be saved if we can <nl> use Wiretap to save the file . */ <nl> gtk_widget_set_sensitive ( filter_cb , can_save_with_wiretap ( new_filetype )); <nl> + gtk_widget_set_sensitive ( mark_cb , can_save_with_wiretap ( new_filetype )); <nl> filetype = new_filetype ; <nl> } <nl> } <nl> file_save_as_cmd_cb ( GtkWidget * w , gpointer data ) <nl> main_vb , FALSE , FALSE , 0 ); <nl> gtk_widget_show ( main_vb ); <nl>  <nl> + /* <nl> + * XXX - should this be sensitive only if the current display filter <nl> + * has rejected some packets , so that not all packets are currently <nl> + * being displayed , and if it has accepted some packets , so that some <nl> + * packets are currently being displayed ? <nl> + */ <nl> filter_cb = gtk_check_button_new_with_label (" Save only packets currently being displayed "); <nl> gtk_container_add ( GTK_CONTAINER ( main_vb ), filter_cb ); <nl> gtk_toggle_button_set_state ( GTK_TOGGLE_BUTTON ( filter_cb ), FALSE ); <nl> file_save_as_cmd_cb ( GtkWidget * w , gpointer data ) <nl> gtk_widget_set_sensitive ( filter_cb , can_save_with_wiretap ( filetype )); <nl> gtk_widget_show ( filter_cb ); <nl>  <nl> + /* <nl> + * XXX - should this be sensitive only if at least one packet is <nl> + * marked , so that there are marked packets to save , and if not <nl> + * all packets are marked , so that " only marked packets " is different <nl> + * from " all packets "? <nl> + */ <nl> mark_cb = gtk_check_button_new_with_label (" Save only marked packets "); <nl> gtk_container_add ( GTK_CONTAINER ( main_vb ), mark_cb ); <nl> gtk_toggle_button_set_state ( GTK_TOGGLE_BUTTON ( mark_cb ), FALSE );
mmm epan / proto . c <nl> ppp epan / proto . c <nl> proto_item_set_len ( proto_item * pi , const gint length ) <nl> DISSECTOR_ASSERT ( length >= 0 ); <nl> fi -> length = length ; <nl>  <nl> - if ( fi -> value . ftype -> ftype == FT_BYTES ) <nl> + /* <nl> + * You cannot just make the " len " field of a GByteArray <nl> + * larger , if there ' s no data to back that length ; <nl> + * you can only make it smaller . <nl> + */ <nl> + if ( fi -> value . ftype -> ftype == FT_BYTES && length <= fi -> length ) <nl> fi -> value . value . bytes -> len = length ; <nl> } <nl> 
mmm ui / qt / multicast_statistics_dialog . cpp <nl> ppp ui / qt / multicast_statistics_dialog . cpp <nl> void MulticastStatisticsDialog :: updateMulticastParameters () <nl>  <nl> param = buffer_alarm_threshold_le_ -> text (). toInt (& ok ); <nl> if ( ok && param > 0 ) { <nl> - mcast_stream_trigger = param ; <nl> + mcast_stream_bufferalarm = param ; <nl> } <nl>  <nl> param = stream_empty_speed_le_ -> text (). toInt (& ok );
mmm epan / nghttp2 / nghttp2_hd . c <nl> ppp epan / nghttp2 / nghttp2_hd . c <nl> static nghttp2_hd_entry * add_hd_table_incremental ( nghttp2_hd_context * context , <nl>  <nl> if ( rv != 0 ) { <nl> -- new_ent -> ref ; <nl> + <nl> + /* nv -> name and nv -> value are managed by caller . */ <nl> + new_ent -> nv . name = NULL ; <nl> + new_ent -> nv . namelen = 0 ; <nl> + new_ent -> nv . value = NULL ; <nl> + new_ent -> nv . valuelen = 0 ; <nl> + <nl> nghttp2_hd_entry_free ( new_ent ); <nl> free ( new_ent ); <nl> 
mmm epan / dissectors / packet - pdcp - lte . c <nl> ppp epan / dissectors / packet - pdcp - lte . c <nl> static gint pdcp_channel_equal ( gconstpointer v , gconstpointer v2 ) <nl> static guint pdcp_channel_hash_func ( gconstpointer v ) <nl> { <nl> /* Just use pointer , as the fields are all in this value */ <nl> - return ( guint ) v ; <nl> + return GPOINTER_TO_UINT ( v ); <nl> } <nl>  <nl> 
mmm gtk / main . c <nl> ppp gtk / main . c <nl> create_main_window ( gint pl_size , gint tv_size , gint bv_size , e_prefs * prefs ) <nl> channel_list = g_list_append ( channel_list , ieee80211_mhz_to_str ( airpcap_if_active -> pSupportedChannels [ i ]. Frequency )); <nl> } <nl> gtk_combo_set_popdown_strings ( GTK_COMBO ( channel_cm ), channel_list ); <nl> + g_list_free ( channel_list ); <nl> } <nl>  <nl> gtk_tooltips_set_tip ( airpcap_tooltips , GTK_WIDGET ( GTK_COMBO ( channel_cm )-> entry ), <nl> create_main_window ( gint pl_size , gint tv_size , gint bv_size , e_prefs * prefs ) <nl> linktype_list = g_list_append ( linktype_list , AIRPCAP_VALIDATION_TYPE_NAME_CORRUPT ); <nl>  <nl> gtk_combo_set_popdown_strings ( GTK_COMBO ( wrong_crc_cm ), linktype_list ) ; <nl> + g_list_free ( linktype_list ); <nl> gtk_tooltips_set_tip ( airpcap_tooltips , GTK_WIDGET ( GTK_COMBO ( wrong_crc_cm )-> entry ), <nl> " Select the 802 . 11 FCS filter that the wireless adapter will apply .", <nl> NULL );
mmm epan / dissectors / packet - sdp . c <nl> ppp epan / dissectors / packet - sdp . c <nl> dissect_sdp_media ( tvbuff_t * tvb , proto_item * ti , <nl> proto_tree_add_string ( sdp_media_tree , hf_media_format , tvb , offset , <nl> tokenlen , val_to_str ( atol ( media_format ), rtp_payload_type_vals , "% u ")); <nl> transport_info -> media_pt [ transport_info -> media_pt_count ] = atol ( media_format ); <nl> - if ( transport_info -> media_pt_count < SDP_MAX_RTP_PAYLOAD_TYPES ) <nl> + if ( transport_info -> media_pt_count < SDP_MAX_RTP_PAYLOAD_TYPES - 1 ) <nl> transport_info -> media_pt_count ++; <nl> g_free ( media_format ); <nl> } else {
mmm packet - dns . c <nl> ppp packet - dns . c <nl> /* packet - dns . c <nl> * Routines for DNS packet disassembly <nl> * <nl> - * $ Id : packet - dns . c , v 1 . 99 2003 / 01 / 31 08 : 29 : 09 guy Exp $ <nl> + * $ Id : packet - dns . c , v 1 . 100 2003 / 05 / 05 08 : 14 : 31 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> add_opt_rr_to_tree ( proto_item * trr , int rr_type , tvbuff_t * tvb , int offset , <nl> const char * name , int namelen , const char * type_name , int class , <nl> guint ttl , gushort data_len ) <nl> { <nl> - proto_tree * rr_tree ; <nl> + proto_tree * rr_tree , * Z_tree ; <nl> + proto_item * Z_item = NULL ; <nl>  <nl> rr_tree = proto_item_add_subtree ( trr , rr_type ); <nl> proto_tree_add_text ( rr_tree , tvb , offset , namelen , " Name : % s ", name ); <nl> add_opt_rr_to_tree ( proto_item * trr , int rr_type , tvbuff_t * tvb , int offset , <nl> proto_tree_add_text ( rr_tree , tvb , offset , 1 , " EDNS0 version : % u ", <nl> ( ttl >> 16 ) & 0xff ); <nl> offset ++; <nl> - proto_tree_add_text ( rr_tree , tvb , offset , 2 , " Must be zero : 0x % x ", ttl & 0xffff ); <nl> + Z_item = proto_tree_add_text ( rr_tree , tvb , offset , 2 , " Z : 0x % x ", ttl & 0xffff ); <nl> + if ( ttl & 0x8000 ) { <nl> + Z_tree = proto_item_add_subtree ( Z_item , rr_type ); <nl> + proto_tree_add_text ( Z_tree , tvb , offset , 2 , " Bit 0 ( DO bit ): 1 ( Accepts DNSSEC security RRs )"); <nl> + proto_tree_add_text ( Z_tree , tvb , offset , 2 , " Bits 1 - 15 : 0x % x ( reserved )", ( ttl >> 17 ) & 0xff ); <nl> + } <nl> offset += 2 ; <nl> proto_tree_add_text ( rr_tree , tvb , offset , 2 , " Data length : % u ", data_len ); <nl> return rr_tree ;
mmm plugins / ethercat / packet - esl . c <nl> ppp plugins / ethercat / packet - esl . c <nl> typedef union _EslFlagsUnion <nl> guint16 extended : 1 ; <nl> guint16 port11 : 1 ; <nl> guint16 port10 : 1 ; <nl> - guint16 crcError : 1 ; <nl> guint16 alignError : 1 ; <nl> + guint16 crcError : 1 ; <nl> guint16 timeStampEna : 1 ; <nl> guint16 port9 : 1 ; <nl> guint16 port8 : 1 ; <nl> typedef union _EslFlagsUnion <nl> # define esl_extended_bitmask 0x0100 <nl> # define esl_port11_bitmask 0x0200 <nl> # define esl_port10_bitmask 0x0400 <nl> -# define esl_crcError_bitmask 0x0800 <nl> -# define esl_alignError_bitmask 0x1000 <nl> +# define esl_alignError_bitmask 0x0800 <nl> +# define esl_crcError_bitmask 0x1000 <nl> # define esl_timeStampEna_bitmask 0x2000 <nl> # define esl_port9_bitmask 0x4000 <nl> # define esl_port8_bitmask 0x8000 <nl> dissect_esl_header ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tree * tree , void <nl> flags = tvb_get_letohs ( tvb , offset ); <nl> proto_tree_add_uint ( esl_header_tree , hf_esl_port , tvb , offset , 2 , flags_to_port ( flags )); <nl>  <nl> - proto_tree_add_item ( esl_header_tree , hf_esl_crcerror , tvb , offset , 2 , ENC_LITTLE_ENDIAN ); <nl> proto_tree_add_item ( esl_header_tree , hf_esl_alignerror , tvb , offset , 2 , ENC_LITTLE_ENDIAN ); <nl> + proto_tree_add_item ( esl_header_tree , hf_esl_crcerror , tvb , offset , 2 , ENC_LITTLE_ENDIAN ); <nl> + <nl> offset += 2 ; <nl>  <nl> proto_tree_add_item ( esl_header_tree , hf_esl_timestamp , tvb , offset , 8 , ENC_LITTLE_ENDIAN );
mmm epan / dissectors / packet - tipc . c <nl> ppp epan / dissectors / packet - tipc . c <nl> void proto_register_tipc ( void ); <nl>  <nl> static int proto_tipc = - 1 ; <nl>  <nl> -/* dissector handles */ <nl> - static dissector_handle_t ip_handle ; <nl> - <nl> static int hf_tipc_msg_fragments = - 1 ; <nl> static int hf_tipc_msg_fragment = - 1 ; <nl> static int hf_tipc_msg_fragment_overlap = - 1 ; <nl> proto_reg_handoff_tipc ( void ) <nl> dissector_handle_t tipc_tcp_handle ; <nl>  <nl> tipc_tcp_handle = create_dissector_handle ( dissect_tipc_tcp , proto_tipc ); <nl> - ip_handle = find_dissector (" ip "); <nl>  <nl> dissector_add_uint (" ethertype ", ETHERTYPE_TIPC , tipc_handle ); <nl> dissector_add_for_decode_as_with_preference (" tcp . port ", tipc_tcp_handle );
mmm packet - dcerpc - spoolss . c <nl> ppp packet - dcerpc - spoolss . c <nl> * Routines for SMB \ PIPE \ spoolss packet disassembly <nl> * Copyright 2001 , Tim Potter < tpot @ samba . org > <nl> * <nl> - * $ Id : packet - dcerpc - spoolss . c , v 1 . 5 2002 / 03 / 19 22 : 09 : 23 guy Exp $ <nl> + * $ Id : packet - dcerpc - spoolss . c , v 1 . 6 2002 / 03 / 20 09 : 09 : 07 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> static int prs_uint16uni ( tvbuff_t * tvb , int offset , packet_info * pinfo , <nl>  <nl> /* Get remaining data in buffer as a string */ <nl>  <nl> - remaining = tvb_length_remaining ( tvb , offset ); <nl> + remaining = tvb_length_remaining ( tvb , offset )/ 2 ; <nl> text = fake_unicode ( tvb , offset , remaining ); <nl> len = strlen ( text ); <nl> 
mmm epan / dissectors / packet - rtps . c <nl> ppp epan / dissectors / packet - rtps . c <nl> static const struct Flag_definition DATA_FRAG_FLAGS [] = { <nl> { ' Q ', " Inline QoS " }, /* Bit 1 */ <nl> { ' E ', " Endianness bit " } /* Bit 0 */ <nl> }; <nl> +# if 0 <nl> /* Vendor specific : RTI */ <nl> static const struct Flag_definition NACK_FLAGS [] = { <nl> { RESERVEDFLAG_CHAR , RESERVEDFLAG_STRING }, /* Bit 7 */ <nl> static const struct Flag_definition NACK_FLAGS [] = { <nl> { ' F ', " Final flag " }, /* Bit 1 */ <nl> { ' E ', " Endianness bit " } /* Bit 0 */ <nl> }; <nl> +# endif <nl>  <nl>  <nl> /***************************************************************************/ <nl> static void dissect_DATA_FRAG ( tvbuff_t * tvb , packet_info * pinfo , gint offset , gu <nl> proto_item * octet_item ; <nl> guint32 wid ; <nl> gboolean from_builtin_writer ; <nl> - rtps_util_decode_flags ( tree , tvb , offset + 1 , flags , NOKEY_DATA_FRAG_FLAGS ); <nl> + rtps_util_decode_flags ( tree , tvb , offset + 1 , flags , DATA_FRAG_FLAGS ); <nl>  <nl> octet_item = proto_tree_add_item ( tree , hf_rtps_sm_octets_to_next_header , tvb , <nl> offset + 2 , 2 , little_endian ? ENC_LITTLE_ENDIAN : ENC_BIG_ENDIAN );
mmm plugins / profinet / packet - dcom - cba . c <nl> ppp plugins / profinet / packet - dcom - cba . c <nl> dissect_ICBAPhysicalDevice_get_LogicalDevice_rqst ( tvbuff_t * tvb , int offset , <nl>  <nl> offset = dissect_dcom_dcerpc_pointer ( tvb , offset , pinfo , tree , di , drep , <nl> & u32Pointer ); <nl> + <nl> + szStr [ 0 ] ='\ 0 '; <nl> + <nl> if ( u32Pointer ) { <nl> offset = dissect_dcom_BSTR ( tvb , offset , pinfo , tree , di , drep , <nl> hf_cba_name , szStr , u32MaxStr );
mmm epan / dissectors / packet - dcerpc - spoolss . c <nl> ppp epan / dissectors / packet - dcerpc - spoolss . c <nl> dissect_spoolss_uint16uni ( tvbuff_t * tvb , int offset , packet_info * pinfo _U_ , <nl>  <nl> /* Get remaining data in buffer as a string */ <nl>  <nl> - remaining = tvb_captured_length_remaining ( tvb , offset ); <nl> + remaining = tvb_reported_length_remaining ( tvb , offset ); <nl> if ( remaining <= 0 ) { <nl> if ( data ) <nl> * data = g_strdup (""); <nl> dissect_spoolss_keybuffer ( tvbuff_t * tvb , int offset , packet_info * pinfo , <nl> end_offset = tvb_reported_length_remaining ( tvb , offset ) + 1 ; <nl> } <nl>  <nl> - while ( offset < end_offset ) <nl> + while ( offset > 0 && offset < end_offset ) { <nl> offset = dissect_spoolss_uint16uni ( <nl> tvb , offset , pinfo , tree , drep , NULL , hf_keybuffer ); <nl> + } <nl>  <nl> return offset ; <nl> }
mmm epan / crypt / airpdcap . c <nl> ppp epan / crypt / airpdcap . c <nl> AirPDcapDecryptWPABroadcastKey ( const EAPOL_RSN_KEY * pEAPKey , guint8 * decryption_ <nl> } <nl> } <nl>  <nl> - if ( key_bytes_len < GROUP_KEY_MIN_LEN || key_bytes_len > eapol_len - sizeof ( EAPOL_RSN_KEY )) { <nl> + if (( key_bytes_len < GROUP_KEY_MIN_LEN ) || <nl> + ( eapol_len < sizeof ( EAPOL_RSN_KEY )) || <nl> + ( key_bytes_len > eapol_len - sizeof ( EAPOL_RSN_KEY ))) { <nl> return AIRPDCAP_RET_NO_VALID_HANDSHAKE ; <nl> } <nl> 
mmm plugins / megaco / packet - megaco . c <nl> ppp plugins / megaco / packet - megaco . c <nl> nextcontext : <nl> tvb_previous_offset = tvb_find_guint8 ( tvb , tvb_current_offset , <nl> tvb_len , '=')+ 1 ; <nl> tvb_previous_offset = tvb_skip_wsp ( tvb , tvb_previous_offset ); <nl> - tvb_current_offset = tvb_find_guint8 ( tvb , tvb_previous_offset , <nl> + tvb_next_offset = tvb_find_guint8 ( tvb , tvb_previous_offset , <nl> tvb_len , '{'); <nl> + if ( tvb_current_offset >= tvb_next_offset ) { <nl> + proto_tree_add_text ( megaco_tree , tvb , 0 , 0 , "[ Parse error : Invalid offset ]"); <nl> + return ; <nl> + } <nl> + tvb_current_offset = tvb_next_offset ; <nl>  <nl>  <nl> tokenlen = tvb_current_offset - tvb_previous_offset ;
mmm epan / dissectors / packet - netflow . c <nl> ppp epan / dissectors / packet - netflow . c <nl> v9_template_add ( struct v9_template * template ) <nl> template -> length += template -> entries [ i ]. length ; <nl> } <nl>  <nl> - memmove (& v9_template_cache [ v9_template_hash ( template -> id , <nl> + memcpy (& v9_template_cache [ v9_template_hash ( template -> id , <nl> & template -> source_addr , template -> source_id )], <nl> template , sizeof (* template )); <nl> }
mmm epan / dissectors / packet - dnp . c <nl> ppp epan / dissectors / packet - dnp . c <nl> static int <nl> dissect_dnp3_al ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> { <nl> guint8 al_ctl , al_seq , al_func , al_class = 0 , i ; <nl> - guint16 bytes , obj_type ; <nl> + guint16 bytes , obj_type = 0 ; <nl> guint data_len = 0 , offset = 0 ; <nl> proto_item * ti , * tc , * t_robj ; <nl> proto_tree * al_tree , * field_tree , * robj_tree ;
mmm epan / tvbuff . c <nl> ppp epan / tvbuff . c <nl> tvb_get_letohguid ( tvbuff_t * tvb , const gint offset , e_guid_t * guid ) <nl> guid -> data1 = pletohl ( ptr + 0 ); <nl> guid -> data2 = pletohs ( ptr + 4 ); <nl> guid -> data3 = pletohs ( ptr + 6 ); <nl> - memcpy ( guid -> data4 , offset + 8 , sizeof guid -> data4 ); <nl> + memcpy ( guid -> data4 , ptr + 8 , sizeof guid -> data4 ); <nl> } <nl>  <nl> /*
mmm epan / dissectors / packet - tds . c <nl> ppp epan / dissectors / packet - tds . c <nl> dissect_tds_resp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl>  <nl> length_remaining = tvb_ensure_length_remaining ( tvb , pos ); <nl>  <nl> + if (( int ) token_sz < 0 ) { <nl> + proto_tree_add_text ( tree , tvb , pos , 0 , " Bogus token size : % u ", <nl> + token_sz ); <nl> + break ; <nl> + } <nl> + if (( int ) token_len_field_size < 0 ) { <nl> + proto_tree_add_text ( tree , tvb , pos , 0 , " Bogus token length field size : % u ", <nl> + token_len_field_size ); <nl> + break ; <nl> + } <nl> token_item = proto_tree_add_text ( tree , tvb , pos , token_sz , <nl> " Token 0x % 02x % s ", token , <nl> val_to_str ( token , token_names , " Unknown Token Type "));
mmm extcap . c <nl> ppp extcap . c <nl> extcap_register_preferences_callback ( gpointer key , gpointer value _U_ , gpointer <nl>  <nl> void extcap_register_preferences ( void ) <nl> { <nl> + if ( prefs . capture_no_extcap ) <nl> + return ; <nl> + <nl> module_t * dev_module = prefs_find_module (" extcap "); <nl>  <nl> if (! dev_module ) <nl> extcap_load_interface_list ( void ) <nl> gchar * argv ; <nl> gchar * error ; <nl>  <nl> + if ( prefs . capture_no_extcap ) <nl> + return ; <nl> + <nl> if ( _toolbars ) <nl> { <nl> // Remove existing interface toolbars here instead of in extcap_clear_interfaces ()
mmm follow . c <nl> ppp follow . c <nl> /* follow . c <nl> * <nl> - * $ Id : follow . c , v 1 . 3 1998 / 10 / 10 03 : 32 : 09 gerald Exp $ <nl> + * $ Id : follow . c , v 1 . 4 1998 / 10 / 28 01 : 29 : 16 guy Exp $ <nl> * <nl> * Copyright 1998 Mike Hall < mlh @ io . com > <nl> * <nl> reassemble_tcp ( u_long sequence , u_long length , const char * data , int synflag , u <nl> tmp_frag -> data = ( u_char *) malloc ( length ); <nl> tmp_frag -> seq = sequence ; <nl> tmp_frag -> len = length ; <nl> - bcopy ( data , tmp_frag -> data , length ); <nl> + memcpy ( tmp_frag -> data , data , length ); <nl> if ( frags [ src_index ] ) { <nl> tmp_frag -> next = frags [ src_index ]; <nl> } else {
mmm packet - isis - clv . c <nl> ppp packet - isis - clv . c <nl> /* packet - isis - clv . c <nl> * Common CLV decode routines . <nl> * <nl> - * $ Id : packet - isis - clv . c , v 1 . 6 2000 / 06 / 19 08 : 33 : 47 guy Exp $ <nl> + * $ Id : packet - isis - clv . c , v 1 . 7 2000 / 08 / 10 14 : 21 : 09 deniel Exp $ <nl> * Stuart Stanley < stuarts @ mxmail . net > <nl> * <nl> * Ethereal - Network traffic analyzer <nl> isis_dissect_clvs ( const isis_clv_handle_t * opts , int len , int id_length , <nl> length = pd [ offset ++]; <nl> adj = ( sizeof ( code ) + sizeof ( length ) + length ); <nl> len -= adj ; <nl> - if ( len < 0 ) { <nl> + if ( len < 0 || ! BYTES_ARE_IN_FRAME ( offset , length ) ) { <nl> isis_dissect_unknown ( offset , adj , tree , fd , <nl> " Short CLV header (% d vs % d )", <nl> adj , len + adj );
mmm epan / dissectors / packet - mp2t . c <nl> ppp epan / dissectors / packet - mp2t . c <nl> typedef struct frame_analysis_data { <nl> static mp2t_analysis_data_t * <nl> init_mp2t_conversation_data ( void ) <nl> { <nl> - mp2t_analysis_data_t * mp2t_data = NULL ; <nl> + mp2t_analysis_data_t * mp2t_data ; <nl>  <nl> mp2t_data = wmem_new0 ( wmem_file_scope (), struct mp2t_analysis_data ); <nl>  <nl> init_mp2t_conversation_data ( void ) <nl> static mp2t_analysis_data_t * <nl> get_mp2t_conversation_data ( conversation_t * conv ) <nl> { <nl> - mp2t_analysis_data_t * mp2t_data = NULL ; <nl> + mp2t_analysis_data_t * mp2t_data ; <nl>  <nl> mp2t_data = ( mp2t_analysis_data_t *) conversation_get_proto_data ( conv , proto_mp2t ); <nl> if (! mp2t_data ) { <nl> get_mp2t_conversation_data ( conversation_t * conv ) <nl> static frame_analysis_data_t * <nl> init_frame_analysis_data ( mp2t_analysis_data_t * mp2t_data , packet_info * pinfo ) <nl> { <nl> - frame_analysis_data_t * frame_analysis_data_p = NULL ; <nl> + frame_analysis_data_t * frame_analysis_data_p ; <nl>  <nl> frame_analysis_data_p = wmem_new0 ( wmem_file_scope (), struct frame_analysis_data ); <nl> frame_analysis_data_p -> ts_table = wmem_tree_new ( wmem_file_scope ()); <nl> init_frame_analysis_data ( mp2t_analysis_data_t * mp2t_data , packet_info * pinfo ) <nl> static frame_analysis_data_t * <nl> get_frame_analysis_data ( mp2t_analysis_data_t * mp2t_data , packet_info * pinfo ) <nl> { <nl> - frame_analysis_data_t * frame_analysis_data_p = NULL ; <nl> + frame_analysis_data_t * frame_analysis_data_p ; <nl> frame_analysis_data_p = ( frame_analysis_data_t *) wmem_tree_lookup32 ( mp2t_data -> frame_table , pinfo -> fd -> num ); <nl> return frame_analysis_data_p ; <nl> } <nl> get_frame_analysis_data ( mp2t_analysis_data_t * mp2t_data , packet_info * pinfo ) <nl> static pid_analysis_data_t * <nl> get_pid_analysis ( mp2t_analysis_data_t * mp2t_data , guint32 pid ) <nl> { <nl> - pid_analysis_data_t * pid_data = NULL ; <nl> + pid_analysis_data_t * pid_data ; <nl>  <nl> pid_data = ( pid_analysis_data_t *) wmem_tree_lookup32 ( mp2t_data -> pid_table , pid ); <nl> if (! pid_data ) { <nl> static guint <nl> mp2t_get_packet_length ( tvbuff_t * tvb , guint offset , packet_info * pinfo , <nl> guint32 frag_id , enum pid_payload_type pload_type ) <nl> { <nl> - fragment_head * frag = NULL ; <nl> + fragment_head * frag ; <nl> tvbuff_t * len_tvb = NULL , * frag_tvb = NULL , * data_tvb = NULL ; <nl> gint pkt_len = 0 ; <nl> guint remaining_len ; <nl> save_state : <nl> static guint32 <nl> calc_skips ( gint32 curr , gint32 prev ) <nl> { <nl> - int res = 0 ; <nl> + int res ; <nl>  <nl> /* Only count the missing TS frames in between prev and curr . <nl> * The " prev " frame CC number seen is confirmed received , it ' s
mmm epan / dissectors / packet - smpp . c <nl> ppp epan / dissectors / packet - smpp . c <nl> dissect_smpp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> guint32 offset = 0 ; <nl> while ( tvb_reported_length_remaining ( tvb , offset ) > 0 ) { <nl> guint16 pdu_len = tvb_get_ntohl ( tvb , offset ); <nl> + if ( pdu_len < 1 ) <nl> + THROW ( ReportedBoundsError ); <nl> gint pdu_real_len = tvb_length_remaining ( tvb , offset ); <nl> tvbuff_t * pdu_tvb ; <nl> 
mmm epan / dissectors / packet - smb . c <nl> ppp epan / dissectors / packet - smb . c <nl> dissect_transaction_request ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl>  <nl> WORD_COUNT ; <nl>  <nl> - if ( wc == 8 ) { <nl> + if ( wc == 8 || ( wc == 9 && si -> cmd == SMB_COM_TRANSACTION2_SECONDARY )) { <nl> /* secondary client request */ <nl>  <nl> /* total param count , only a 16bit integer here */ <nl> dissect_transaction_request ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> proto_tree_add_uint ( tree , hf_smb_data_disp16 , tvb , offset , 2 , dd ); <nl> offset += 2 ; <nl>  <nl> - if ( si -> cmd == SMB_COM_TRANSACTION2 ) { <nl> + if ( si -> cmd == SMB_COM_TRANSACTION2 || si -> cmd == SMB_COM_TRANSACTION2_SECONDARY ) { <nl> guint16 fid ; <nl>  <nl> /* fid */
mmm epan / dissectors / packet - epon . c <nl> ppp epan / dissectors / packet - epon . c <nl> dissect_epon ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> guint dpoe_sec_byte ; <nl> gboolean dpoe_encrypted = FALSE ; <nl>  <nl> - /* Start_of_Packet delimiter (/ S /) can either happen in byte 1 or byte 2 , <nl> - * making the captured preamble either 7 or 6 bytes in length . If the <nl> + /* Start_of_Packet delimiter (/ S /) can happen in byte 1 , 2 or 3 , <nl> + * making the captured preamble 8 , 7 or 6 bytes in length . If the <nl> * preamble starts with 0x55 , then / S / happened in byte 1 , making the <nl> * captured preamble 7 bytes in length . <nl> */ <nl> - if ( tvb_get_ntoh24 ( tvb , 0 ) == 0x55D555 ) { <nl> + if ( tvb_get_ntohl ( tvb , 0 ) == 0x5555D555 ) { <nl> + offset += 2 ; <nl> + } else if ( tvb_get_ntoh24 ( tvb , 0 ) == 0x55D555 ) { <nl> offset += 1 ; <nl> } else if ( tvb_get_ntohs ( tvb , 0 ) == 0xD555 ) { <nl> offset += 0 ;
mmm epan / dissectors / packet - btobex . c <nl> ppp epan / dissectors / packet - btobex . c <nl> dissect_headers ( proto_tree * tree , tvbuff_t * tvb , int offset , packet_info * pinfo , <nl> proto_item_append_text ( hdr_tree , " (\"% s \")", str ); <nl>  <nl> col_append_fstr ( pinfo -> cinfo , COL_INFO , " \"% s \"", str ); <nl> + offset += item_length - 3 ; <nl> } <nl> else { <nl> col_append_str ( pinfo -> cinfo , COL_INFO , " \"\""); <nl> } <nl> - <nl> - offset += item_length - 3 ; <nl> } <nl> break ; <nl> case 0x40 : /* byte sequence */ <nl> dissect_headers ( proto_tree * tree , tvbuff_t * tvb , int offset , packet_info * pinfo , <nl> col_append_fstr ( pinfo -> cinfo , COL_INFO , " \"% s \"", tvb_get_ephemeral_string ( tvb , offset , item_length - 3 )); <nl> } <nl>  <nl> - offset += item_length - 3 ; <nl> + if ( item_length >= 3 ) /* prevent infinite loops */ <nl> + offset += item_length - 3 ; <nl> break ; <nl> case 0x80 : /* 1 byte */ <nl> proto_item_append_text ( hdr_tree , " (% i )", tvb_get_ntohl ( tvb , offset ));
mmm epan / dissectors / packet - bacapp . c <nl> ppp epan / dissectors / packet - bacapp . c <nl> fAbstractSyntaxNType ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , guint <nl> offset = fReadAccessResult ( tvb , pinfo , tree , offset ); <nl> break ; <nl> } <nl> - /* intentially fall through here so don ' t reorder this case statement */ <nl> + /* intentionally fall through */ /* here so don ' t reorder this case statement */ <nl> default : <nl> if ( tag_info ) { <nl> if ( tag_is_opening ( tag_info )) { <nl> fPropertyReference ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , guint of <nl> case 1 : /* propertyArrayIndex */ <nl> offset = fPropertyArrayIndex ( tvb , pinfo , tree , offset ); <nl> if ( list != 0 ) break ; /* Continue decoding if this may be a list */ <nl> + break ; <nl> default : <nl> lastoffset = offset ; /* Set loop end condition */ <nl> break ; <nl> fBACnetObjectPropertyReference ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tr <nl> case 1 : /* PropertyIdentifier and propertyArrayIndex */ <nl> offset = fPropertyReference ( tvb , pinfo , tree , offset , 1 , 0 ); <nl> col_set_writable ( pinfo -> cinfo , COL_INFO , FALSE ); /* don ' t set all infos into INFO column */ <nl> + break ; <nl> default : <nl> lastoffset = offset ; /* Set loop end condition */ <nl> break ;
mmm epan / dissectors / packet - sctp . c <nl> ppp epan / dissectors / packet - sctp . c <nl> dissect_data_chunk ( tvbuff_t * chunk_tvb , <nl> */ <nl> if ( b_bit ) <nl> { <nl> - gboolean retval ; <nl> + gboolean retval = FALSE ; <nl>  <nl> /* <nl> * If this particular fragment happens to get a ReportedBoundsError
mmm gtk / uat_gui . c <nl> ppp gtk / uat_gui . c <nl> static gboolean uat_cancel_dlg_cb ( GtkWidget * win _U_ , gpointer user_data ) { <nl> if ( dd -> is_new ) g_free ( dd -> rec ); <nl> g_ptr_array_free ( dd -> entries , TRUE ); <nl> window_destroy ( GTK_WIDGET ( dd -> win )); <nl> - g_free ( dd ); <nl>  <nl> while ( dd -> tobe_freed -> len ) g_free ( g_ptr_array_remove_index_fast ( dd -> tobe_freed , dd -> tobe_freed -> len - 1 ) ); <nl>  <nl> + g_free ( dd ); <nl> + <nl> return TRUE ; <nl> } <nl> 
mmm epan / dissectors / packet - ltp . c <nl> ppp epan / dissectors / packet - ltp . c <nl> dissect_data_segment ( proto_tree * ltp_tree , tvbuff_t * tvb , packet_info * pinfo , int <nl> } <nl> } <nl> /* Adding size of the data */ <nl> + if (( segment_offset + ( int ) length < segment_offset ) || ( segment_offset + ( int ) length < ( int ) length )) { <nl> + /* Addition result has wrapped */ <nl> + return 0 ; <nl> + } <nl> segment_offset += ( int ) length ; <nl> + <nl> + if (( segment_offset + frame_offset < segment_offset ) || ( segment_offset + frame_offset < frame_offset )) { <nl> + /* Addition result has wrapped */ <nl> + return 0 ; <nl> + } <nl> if (( unsigned )( frame_offset + segment_offset ) > tvb_length ( tvb )){ <nl> /* This would mean the data segment is incomplete */ <nl> return 0 ;
mmm epan / dissectors / packet - bthci_evt . c <nl> ppp epan / dissectors / packet - bthci_evt . c <nl> dissect_bthci_evt_inq_result_with_rssi ( tvbuff_t * tvb , int offset , packet_info * p <nl> static int <nl> dissect_bthci_evt_eir_ad_data ( tvbuff_t * tvb , int offset , packet_info * pinfo _U_ , proto_tree * tree , guint8 size ) <nl> { <nl> - guint16 i ; <nl> - guint8 j , length , type ; <nl> + guint16 i , j ; <nl> + guint8 length , type ; <nl> proto_item * ti_eir = NULL ; <nl> proto_item * ti_eir_subtree = NULL ; <nl> 
mmm epan / tvbuff . c <nl> ppp epan / tvbuff . c <nl> tvb_uncompress ( tvbuff_t * tvb , int offset , int comprlen ) <nl> inflateReset ( strm ); <nl> next = c ; <nl> strm -> next_in = next ; <nl> + if ( c - compr > comprlen ) { <nl> + g_free ( strm ); <nl> + g_free ( compr ); <nl> + g_free ( strmbuf ); <nl> + return NULL ; <nl> + } <nl> comprlen -= ( c - compr ); <nl>  <nl> err = inflateInit2 ( strm , wbits );
mmm plugins / profinet / packet - dcerpc - pn - io . c <nl> ppp plugins / profinet / packet - dcerpc - pn - io . c <nl> dissect_ExpectedSubmoduleBlockReq_block ( tvbuff_t * tvb , int offset , <nl> /* Initial */ <nl> io_data_object = wmem_new0 ( wmem_file_scope (), ioDataObject ); <nl> io_data_object -> profisafeSupported = FALSE ; <nl> - io_data_object -> moduleNameStr = wmem_strdup ( wmem_file_scope (), " Unknown "); <nl> + io_data_object -> moduleNameStr = ( gchar *) wmem_alloc ( wmem_file_scope (), MAX_NAMELENGTH ); <nl> + g_strlcpy ( io_data_object -> moduleNameStr , " Unknown ", MAX_NAMELENGTH ); <nl> vendorMatch = FALSE ; <nl> deviceMatch = FALSE ; <nl> gsdmlFoundFlag = FALSE ; <nl> dissect_ExpectedSubmoduleBlockReq_block ( tvbuff_t * tvb , int offset , <nl> /* Find a String with the saved TextID and with a fitting value for it in the same line . This value is the name of the Module ! */ <nl> if ((( strstr ( temp , tmp_moduletext )) != NULL ) && (( strstr ( temp , moduleValueInfo )) != NULL )) { <nl> pch = strstr ( temp , moduleValueInfo ); <nl> - if ( pch != NULL && sscanf ( pch , " Value =\"%[^\"]", io_data_object -> moduleNameStr ) == 1 ) <nl> + if ( pch != NULL && sscanf ( pch , " Value =\"% 199 [^\"]", io_data_object -> moduleNameStr ) == 1 ) <nl> break ; /* Found the name of the module */ <nl> } <nl> }
mmm capture_loop . c <nl> ppp capture_loop . c <nl> capture_loop_packet_cb ( guchar * user , const struct pcap_pkthdr * phdr , <nl> int err ; <nl>  <nl> /* if the user told us to stop after x packets , do we have enough ? */ <nl> - if (( ld -> packets_max > 0 ) && (++ ld -> counts . total >= ld -> packets_max )) <nl> + ld -> counts . total ++; <nl> + if (( ld -> packets_max > 0 ) && ( ld -> counts . total >= ld -> packets_max )) <nl> { <nl> ld -> go = FALSE ; <nl> }
mmm epan / dissectors / packet - dcm . c <nl> ppp epan / dissectors / packet - dcm . c <nl> dissect_dcm_pdv_fragmented ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> pdv_body_len , <nl> !( pdv -> is_last_fragment )); <nl>  <nl> - if (( head && ( head -> next == NULL )) || pdv -> is_last_fragment ) { <nl> + if ( head && ( head -> next == NULL )) { <nl> /* Was not really fragmented , therefore use ' conventional ' decoding <nl> fragment_add_seq_next () won ' t add any items to the list , when last fragment only <nl> */
mmm epan / dissectors / packet - geneve . c <nl> ppp epan / dissectors / packet - geneve . c <nl> /* packet - geneve . c <nl> * Routines for Geneve - Generic Network Virtualization Encapsulation <nl> - * http :// tools . ietf . org / html / draft - gross - geneve - 00 <nl> + * http :// tools . ietf . org / html / draft - ietf - nvo3 - geneve <nl> * <nl> * Copyright ( c ) 2014 VMware , Inc . All Rights Reserved . <nl> * Author : Jesse Gross < jesse @ nicira . com > <nl>  <nl> static const range_string class_id_names [] = { <nl> { 0 , 0xFF , " Standard " }, <nl> - { 0xFFFF , 0xFFFF , " Experimental " }, <nl> + { 0x0100 , 0x0100 , " Linux " }, <nl> + { 0x0101 , 0x0101 , " Open vSwitch " }, <nl> + { 0x0102 , 0x0102 , " Open Virtual Networking ( OVN )" }, <nl> + { 0x0103 , 0x0103 , " In - band Network Telemetry ( INT )" }, <nl> + { 0x0104 , 0x0104 , " VMware " }, <nl> + { 0xFFF0 , 0xFFFF , " Experimental " }, <nl> { 0 , 0 , NULL } <nl> }; <nl> 
mmm wiretap / pppdump . c <nl> ppp wiretap / pppdump . c <nl> /* pppdump . c <nl> * <nl> - * $ Id : pppdump . c , v 1 . 11 2001 / 12 / 13 05 : 49 : 12 gram Exp $ <nl> + * $ Id : pppdump . c , v 1 . 12 2001 / 12 / 13 05 : 50 : 51 gram Exp $ <nl> * <nl> * Copyright ( c ) 2000 by Gilbert Ramirez < gram @ alumni . rice . edu > <nl> * <nl> pppdump_close ( wtap * wth ) <nl> } <nl>  <nl> if ( state -> pids ) { /* should always be TRUE */ <nl> - int i ; <nl> + unsigned int i ; <nl> for ( i = 0 ; i < g_ptr_array_len ( state -> pids ); i ++) { <nl> g_free ( g_ptr_array_index ( state -> pids , i )); <nl> }
mmm epan / frame_data . c <nl> ppp epan / frame_data . c <nl> void <nl> frame_data_reset ( frame_data * fdata ) <nl> { <nl> fdata -> flags . visited = 0 ; <nl> + fdata -> subnum = 0 ; <nl>  <nl> if ( fdata -> pfd ) { <nl> g_slist_free ( fdata -> pfd );
mmm wiretap / pcapng . c <nl> ppp wiretap / pcapng . c <nl> pcapng_read_section_header_block ( FILE_T fh , pcapng_block_header_t * bh , <nl> bytes_read = pcapng_read_option ( fh , pn , & oh , option_content , opt_cont_buf_len , to_read , err , err_info , " section_header "); <nl> if ( bytes_read <= 0 ) { <nl> pcapng_debug (" pcapng_read_section_header_block : failed to read option "); <nl> + g_free ( option_content ); <nl> return PCAPNG_BLOCK_ERROR ; <nl> } <nl> to_read -= bytes_read ;
mmm wiretap / peektagged . c <nl> ppp wiretap / peektagged . c <nl> peektagged_read_packet ( wtap * wth , FILE_T fh , struct wtap_pkthdr * phdr , <nl>  <nl> case TAG_PEEKTAGGED_CENTER_FREQUENCY : <nl> /* XXX - also seen in an EtherPeek capture ; value unknown */ <nl> + ieee_802_11 . presence_flags |= PHDR_802_11_HAS_FREQUENCY ; <nl> + ieee_802_11 . frequency = pletoh32 (& tag_value [ 2 ]); <nl> break ; <nl>  <nl> case TAG_PEEKTAGGED_UNKNOWN_0x000E :
mmm epan / proto . c <nl> ppp epan / proto . c <nl> test_length ( header_field_info * hfinfo , proto_tree * tree , tvbuff_t * tvb , <nl> else { <nl> size += n ; <nl> } <nl> + } else if ( hfinfo -> type == FT_STRINGZ ) { <nl> + /* If we ' re fetching until the end of the TVB , only validate <nl> + * that the offset is within range . <nl> + */ <nl> + if ( length == - 1 ) <nl> + size = 0 ; <nl> } <nl> + <nl> tvb_ensure_bytes_exist ( tvb , start , size ); <nl> } <nl> 
mmm epan / dissectors / packet - ntlmssp . c <nl> ppp epan / dissectors / packet - ntlmssp . c <nl> dissect_ntlmssp ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> col_append_sep_fstr ( pinfo -> cinfo , COL_INFO , ", ","% s ", <nl> val_to_str ( ntlmssph -> type , <nl> ntlmssp_message_types , <nl> - " Unknown message type ")); <nl> + " Unknown NTLMSSP message type ")); <nl>  <nl> /* Call the appropriate dissector based on the Message Type */ <nl> switch ( ntlmssph -> type ) {
mmm epan / dissectors / packet - rtcp . c <nl> ppp epan / dissectors / packet - rtcp . c <nl> static void calculate_roundtrip_delay ( tvbuff_t * tvb , packet_info * pinfo , <nl> p_add_proto_data ( pinfo -> fd , proto_rtcp , p_packet_data ); <nl> } <nl>  <nl> + /* Don ' t allow match seemingly calculated from same frame ! */ <nl> + if ( pinfo -> fd -> num == p_conv_data -> last_received_frame_number ) <nl> + { <nl> + return ; <nl> + } <nl> + <nl> /* Any previous report must match the lsr given here */ <nl> if ( p_conv_data -> last_received_ts == lsr ) <nl> { <nl> static void calculate_roundtrip_delay ( tvbuff_t * tvb , packet_info * pinfo , <nl> gint nseconds_between_packets = <nl> pinfo -> fd -> abs_ts . nsecs - p_conv_data -> last_received_timestamp . nsecs ; <nl>  <nl> - <nl> - gint total_gap = (( seconds_between_packets * 1000 ) + <nl> - nseconds_between_packets ) / 1000000 ; <nl> + gint total_gap = ( seconds_between_packets * 1000 ) + <nl> + ( nseconds_between_packets / 1000000 ); <nl> gint delay = total_gap - ( int )((( double ) dlsr /( double ) 65536 ) * 1000 . 0 ); <nl>  <nl> /* No useful calculation can be done if dlsr not set ... */
mmm gtk / follow_udp . c <nl> ppp gtk / follow_udp . c <nl> follow_udp_stream_cb ( GtkWidget * w , gpointer data _U_ ) <nl> simple_dialog ( ESD_TYPE_ERROR , ESD_BTN_OK , <nl> " Error creating filter for this stream .\ n " <nl> " A network layer header is needed "); <nl> + g_free ( follow_info ); <nl> return ; <nl> } <nl>  <nl> follow_udp_stream_cb ( GtkWidget * w , gpointer data _U_ ) <nl> simple_dialog ( ESD_TYPE_ERROR , ESD_BTN_OK , <nl> " Can ' t register udp_follow tap : % s \ n ", <nl> msg -> str ); <nl> + g_free ( follow_info ); <nl> return ; <nl> } <nl> 
mmm capture_opts . h <nl> ppp capture_opts . h <nl> typedef struct capture_options_tag { <nl>  <nl> /* GUI related */ <nl> gboolean real_time_mode ; /**< Update list of packets in real time */ <nl> - gboolean show_info ; /**< show the info dialog . GTK + only . */ <nl> + gboolean show_info ; /**< show the info dialog . */ <nl> gboolean restart ; /**< restart after closing is done */ <nl> gchar * orig_save_file ; /**< the original capture file name ( saved for a restart ) */ <nl> 
mmm wiretap / ngsniffer . c <nl> ppp wiretap / ngsniffer . c <nl> ng_file_seek_rand ( wtap * wth , long offset , int whence , int * err ) <nl> the uncompressed byte stream , starting with the blob <nl> following the current blob . */ <nl> new = g_list_next ( ngsniffer -> current_blob ); <nl> - for (;;) { <nl> + while ( new ) { <nl> next = g_list_next ( new ); <nl> if ( next == NULL ) { <nl> /* No more blobs ; the current one is it . */ <nl> ng_file_seek_rand ( wtap * wth , long offset , int whence , int * err ) <nl> the uncompressed byte stream , starting with the blob <nl> preceding the current blob . */ <nl> new = g_list_previous ( ngsniffer -> current_blob ); <nl> - for (;;) { <nl> + while ( new ) { <nl> /* Does this blob start at or before the target offset ? <nl> If so , the current blob is the one we want . */ <nl> new_blob = new -> data ;
mmm epan / dissectors / packet - hip . c <nl> ppp epan / dissectors / packet - hip . c <nl> dissect_hip_tlv ( tvbuff_t * tvb , packet_info * pinfo , int offset , proto_item * ti , i <nl> newoffset += ( 1 + tvb_get_guint8 ( tvb , newoffset + 2 )); <nl> tlv_len -= ( 1 + tvb_get_guint8 ( tvb , newoffset + 2 )); <nl> } <nl> - if ( ti_loc ) { <nl> + if ( locator_type <= 2 ) { <nl> ti_loc = proto_item_add_subtree ( ti_loc , ett_hip_locator_data ); <nl> /* Traffic type */ <nl> proto_tree_add_item ( ti_loc , hf_hip_tlv_locator_traffic_type , tvb ,
mmm packet - icq . c <nl> ppp packet - icq . c <nl> /* packet - icq . c <nl> * Routines for ICQ packet disassembly <nl> * <nl> - * $ Id : packet - icq . c , v 1 . 22 2000 / 11 / 19 08 : 53 : 58 guy Exp $ <nl> + * $ Id : packet - icq . c , v 1 . 23 2000 / 11 / 19 19 : 23 : 54 gerald Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Johan Feyaerts <nl> dissect_icqv5Client ( const u_char * pd , <nl> guint16 seqnum1 = 0 , seqnum2 = 0 ; <nl> guint32 uin = - 1 , sessionid = - 1 ; <nl> guint32 key = - 1 ; <nl> - guint16 pktsize = - 1 ; /* The size of the ICQ content */ <nl> - u_char decr_pd [ 1600 ]; /* Decrypted content , size should be dynamic */ <nl> + guint16 pktsize = - 1 ; /* The size of the ICQ content */ <nl> + static u_char * decr_pd = NULL ; /* Decrypted content */ <nl>  <nl> pktsize = END_OF_FRAME ; <nl> + <nl> + if ( decr_pd == NULL ) <nl> + decr_pd = ( u_char *) g_malloc ( sizeof ( u_char ) * 128 ); <nl> + <nl> + while ( sizeof ( decr_pd ) < pktsize + 3 ) <nl> + decr_pd = ( u_char *) g_realloc ( decr_pd , sizeof ( decr_pd ) * 2 ); <nl> + <nl> /* First copy the memory , we don ' t want to overwrite the old content */ <nl> memcpy ( decr_pd , & pd [ offset ], pktsize ); <nl> if ( pktsize > 0x14 ) {
mmm epan / dissectors / packet - ansi_a . c <nl> ppp epan / dissectors / packet - ansi_a . c <nl> elem_signal ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tree * tree , guint32 off <nl> break ; <nl> } <nl>  <nl> + other_decode_bitfield_value ( a_bigbuf , oct , 0x03 , 8 ); <nl> proto_tree_add_text ( tree , <nl> tvb , curr_offset , 1 , <nl> "% s : Alert Pitch : % s ", <nl> elem_fwd_ms_info_recs ( tvbuff_t * tvb , packet_info * pinfo _U_ , proto_tree * tree , g <nl> curr_offset ++; <nl> break ; <nl>  <nl> + case ANSI_FWD_MS_INFO_REC_SIGNAL : <nl> + curr_offset += elem_signal ( tvb , pinfo , subtree , curr_offset , len , add_string , string_len ); <nl> + break ; <nl> + <nl> default : <nl> proto_tree_add_text ( subtree , <nl> tvb , curr_offset , oct_len ,
mmm epan / dissectors / packet - radius . c <nl> ppp epan / dissectors / packet - radius . c <nl> static void dissect_attribute_value_pairs ( proto_tree * tree , packet_info * pinfo , <nl> last_eap = TRUE ; <nl> } <nl>  <nl> - if ( last_eap ) { <nl> + if ( last_eap && eap_buffer ) { <nl> gboolean save_writable ; <nl>  <nl> proto_item_append_text ( avp_item ,
mmm ui / qt / tcp_stream_dialog . cpp <nl> ppp ui / qt / tcp_stream_dialog . cpp <nl> TCPStreamDialog :: TCPStreamDialog ( QWidget * parent , capture_file * cf , tcp_graph_ty <nl> graph_ . type = graph_type ; <nl>  <nl> QCustomPlot * sp = ui -> streamPlot ; <nl> + QCPPlotTitle * file_title = new QCPPlotTitle ( sp , cf_get_display_name ( cap_file_ )); <nl> + file_title -> setFont ( sp -> xAxis -> labelFont ()); <nl> title_ = new QCPPlotTitle ( sp ); <nl> tracer_ = new QCPItemTracer ( sp ); <nl> sp -> plotLayout ()-> insertRow ( 0 ); <nl> + sp -> plotLayout ()-> addElement ( 0 , 0 , file_title ); <nl> + sp -> plotLayout ()-> insertRow ( 0 ); <nl> sp -> plotLayout ()-> addElement ( 0 , 0 , title_ ); <nl> sp -> addGraph (); // 0 - All : Selectable segments <nl> sp -> addGraph ( sp -> xAxis , sp -> yAxis2 ); // 1 - Throughput : Moving average <nl> TCPStreamDialog :: TCPStreamDialog ( QWidget * parent , capture_file * cf , tcp_graph_ty <nl> sp -> graph ( 0 )-> setScatterStyle ( QCPScatterStyle ( QCPScatterStyle :: ssDisc , 5 )); <nl>  <nl> sp -> xAxis -> setLabel ( tr (" Time ( s )")); <nl> + sp -> yAxis -> setLabelColor ( QColor ( graph_color_1 )); <nl> sp -> yAxis -> setTickLabelColor ( QColor ( graph_color_1 )); <nl>  <nl> tracer_ -> setVisible ( false ); <nl> void TCPStreamDialog :: resetAxes () <nl>  <nl> void TCPStreamDialog :: initializeStevens () <nl> { <nl> - QString dlg_title = QString ( tr (" TCP Graph ")) + streamDescription (); <nl> + QString dlg_title = QString ( tr (" Sequence numbers ")) + streamDescription (); <nl> setWindowTitle ( dlg_title ); <nl> title_ -> setText ( dlg_title ); <nl>  <nl> void TCPStreamDialog :: initializeStevens () <nl>  <nl> void TCPStreamDialog :: initializeThroughput () <nl> { <nl> - QString dlg_title = QString ( tr (" Throughput ")) <nl> + QString dlg_title = QString ( tr (" Throughput ")) <nl> + streamDescription () <nl> + QString ( tr (" (% 1 segment MA )")). arg ( moving_avg_period_ ); <nl> setWindowTitle ( dlg_title ); <nl> void TCPStreamDialog :: initializeThroughput () <nl> sp -> yAxis -> setLabel ( tr (" Segment length ( B )")); <nl>  <nl> sp -> yAxis2 -> setLabel ( tr (" Avg througput ( bits / s )")); <nl> + sp -> yAxis2 -> setLabelColor ( QColor ( graph_color_2 )); <nl> sp -> yAxis2 -> setTickLabelColor ( QColor ( graph_color_2 )); <nl> sp -> yAxis2 -> setVisible ( true ); <nl> } <nl>  <nl> QString TCPStreamDialog :: streamDescription () <nl> { <nl> - return QString ( tr ("% 1 % 2 :% 3 % 4 % 5 :% 6 ")) <nl> - . arg ( cf_get_display_name ( cap_file_ )) <nl> + return QString ( tr (" for % 1 :% 2 % 3 % 4 :% 5 ")) <nl> . arg ( ep_address_to_str (& graph_ . src_address )) <nl> . arg ( graph_ . src_port ) <nl> . arg ( UTF8_RIGHTWARDS_ARROW )
mmm gtk / main . c <nl> ppp gtk / main . c <nl> main_cf_cb_file_closing ( capture_file * cf ) <nl> will there ever be more than one on the stack ? */ <nl> statusbar_pop_file_msg (); <nl>  <nl> - /* go back to " No packets " */ <nl> - packets_bar_update (); <nl> - <nl> /* Restore the standard title bar message . */ <nl> set_main_window_name (" The Ethereal Network Analyzer "); <nl>  <nl> main_cf_cb_file_closed ( capture_file * cf _U_ ) <nl> splash_destroy ( close_dlg ); <nl> close_dlg = NULL ; <nl> } <nl> + <nl> + /* go back to " No packets " */ <nl> + packets_bar_update (); <nl> } <nl>  <nl> static void
mmm wsutil / filesystem . c <nl> ppp wsutil / filesystem . c <nl> gboolean <nl> profile_exists ( const gchar * profilename , gboolean global ) <nl> { <nl> gchar * path = NULL , * global_path ; <nl> + if (! profilename ) <nl> + return FALSE ; <nl> if ( global ) { <nl> global_path = get_global_profiles_dir (); <nl> path = g_strdup_printf ("% s % s % s ", global_path ,
mmm epan / dissectors / packet - smb2 . c <nl> ppp epan / dissectors / packet - smb2 . c <nl> static const value_string smb2_class_vals [] = { <nl> { SMB2_CLASS_FILE_INFO , " FILE_INFO "}, <nl> { SMB2_CLASS_FS_INFO , " FS_INFO "}, <nl> { SMB2_CLASS_SEC_INFO , " SEC_INFO "}, <nl> + { SMB2_CLASS_QUOTA_INFO , " QUOTA_INFO "}, <nl> { SMB2_CLASS_POSIX_INFO , " POSIX_INFO "}, <nl> { 0 , NULL } <nl> }; <nl> dissect_smb2_class_infolevel ( packet_info * pinfo , tvbuff_t * tvb , int offset , prot <nl> hfindex = hf_smb2_infolevel_sec_info ; <nl> vsx = & smb2_sec_info_levels_ext ; <nl> break ; <nl> + case SMB2_CLASS_QUOTA_INFO : <nl> + /* infolevel is not being used for quota */ <nl> + hfindex = hf_smb2_infolevel ; <nl> + vsx = NULL ; <nl> + break ; <nl> case SMB2_CLASS_POSIX_INFO : <nl> hfindex = hf_smb2_infolevel_posix_info ; <nl> vsx = & smb2_posix_info_levels_ext ;
mmm epan / dissectors / packet - zbee - zcl - general . c <nl> ppp epan / dissectors / packet - zbee - zcl - general . c <nl> dissect_zcl_pwr_prof_enphsschednotif ( tvbuff_t * tvb , proto_tree * tree , guint * off <nl> * offset += 1 ; <nl>  <nl> /* Scheduled Energy Phases decoding */ <nl> - for ( i = 0 ; i < num_of_sched_phases ; i ++) { <nl> + for ( i = 0 ; ( i < num_of_sched_phases && i < ZBEE_ZCL_PWR_PROF_NUM_EN_PHS_ETT ); i ++) { <nl> /* Create subtree */ <nl> sub_tree = proto_tree_add_subtree_format ( tree , tvb , * offset , 1 , <nl> ett_zbee_zcl_pwr_prof_enphases [ i ], NULL , " Energy Phase #% u ", i ); <nl> dissect_zcl_pwr_prof_pwrprofnotif ( tvbuff_t * tvb , proto_tree * tree , guint * offset <nl> * offset += 1 ; <nl>  <nl> /* Energy Phases decoding */ <nl> - for ( i = 0 ; i < num_of_transferred_phases ; i ++) { <nl> + for ( i = 0 ; ( i < num_of_transferred_phases && i < ZBEE_ZCL_PWR_PROF_NUM_EN_PHS_ETT ); i ++) { <nl> /* Create subtree */ <nl> sub_tree = proto_tree_add_subtree_format ( tree , tvb , * offset , 1 , <nl> ett_zbee_zcl_pwr_prof_enphases [ i ], NULL , " Energy Phase #% u ", i );
mmm epan / wmem / wmem_allocator_block . c <nl> ppp epan / wmem / wmem_allocator_block . c <nl> wmem_block_split_used_chunk ( wmem_block_allocator_t * allocator , <nl> extra -> prev = ( guint32 ) ( aligned_size + sizeof ( wmem_block_chunk_t )); <nl> extra -> used = FALSE ; <nl>  <nl> + /* merge it to its right if possible ( it can ' t be merged left , obviously ) */ <nl> + wmem_block_merge_free ( allocator , extra ); <nl> + <nl> /* add it to the free list */ <nl> wmem_block_add_to_free_list ( allocator , extra ); <nl> }
mmm epan / tvbuff . c <nl> ppp epan / tvbuff . c <nl> tvb_skip_wsp ( tvbuff_t * tvb , const gint offset , const gint maxlength ) <nl> } <nl>  <nl> gint <nl> - tvb_skip_wsp_return ( tvbuff_t * tvb , const gint offset ) { <nl> + tvb_skip_wsp_return ( tvbuff_t * tvb , const gint offset ) <nl> +{ <nl> gint counter = offset ; <nl> guint8 tempchar ; <nl>  <nl> - for ( counter = offset ; counter > 0 && <nl> + DISSECTOR_ASSERT ( tvb && tvb -> initialized ); <nl> + <nl> + for ( counter = offset ; counter > 0 && <nl> (( tempchar = tvb_get_guint8 ( tvb , counter )) == ' ' || <nl> tempchar == '\ t ' || tempchar == '\ n ' || tempchar == '\ r '); counter --); <nl> counter ++; <nl> + <nl> return ( counter ); <nl> } <nl> 
mmm epan / dissectors / packet - ipsec . c <nl> ppp epan / dissectors / packet - ipsec . c <nl> static int <nl> dissect_ah ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data ) <nl> { <nl> proto_tree * ah_tree , * root_tree ; <nl> - proto_item * pi ; <nl> + proto_item * pi , * ti ; <nl> guint ah_nxt ; /* Next header */ <nl> guint8 ah_len ; /* Length of header in 32bit words minus 2 */ <nl> guint ah_hdr_len ; /* Length of header in octets */ <nl> dissect_ah ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data ) <nl> ah_tree = proto_item_add_subtree ( pi , ett_ah ); <nl>  <nl> proto_tree_add_item ( ah_tree , hf_ah_next_header , tvb , 0 , 1 , ENC_BIG_ENDIAN ); <nl> - proto_tree_add_item ( ah_tree , hf_ah_length , tvb , 1 , 1 , ENC_BIG_ENDIAN ); <nl> + ti = proto_tree_add_item ( ah_tree , hf_ah_length , tvb , 1 , 1 , ENC_BIG_ENDIAN ); <nl> + proto_item_append_text ( ti , " (% u bytes )", ah_hdr_len ); <nl> proto_tree_add_item ( ah_tree , hf_ah_reserved , tvb , 2 , 2 , ENC_NA ); <nl> proto_tree_add_item_ret_uint ( ah_tree , hf_ah_spi , tvb , 4 , 4 , ENC_BIG_ENDIAN , & ah_spi ); <nl>  <nl> proto_register_ipsec ( void ) <nl> { " Next header ", " ah . next_header ", FT_UINT8 , BASE_DEC | BASE_EXT_STRING , & ipproto_val_ext , 0x0 , <nl> NULL , HFILL }}, <nl> { & hf_ah_length , <nl> - { " Length ", " ah . length ", FT_UINT8 , BASE_DEC | BASE_UNIT_STRING , & units_byte_bytes , 0x0 , <nl> + { " Length ", " ah . length ", FT_UINT8 , BASE_DEC , NULL , 0x0 , <nl> NULL , HFILL }}, <nl> { & hf_ah_reserved , <nl> { " Reserved ", " ah . reserved ", FT_BYTES , BASE_NONE , NULL , 0x0 ,
mmm gtk / display_opts . c <nl> ppp gtk / display_opts . c <nl> /* display_opts . c <nl> * Routines for packet display windows <nl> * <nl> - * $ Id : display_opts . c , v 1 . 3 2000 / 05 / 08 01 : 11 : 46 guy Exp $ <nl> + * $ Id : display_opts . c , v 1 . 4 2000 / 05 / 08 01 : 23 : 16 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ zing . org > <nl> # include " packet . h " <nl> # include " file . h " <nl> # include " display_opts . h " <nl> +# include " dlg_utils . h " <nl>  <nl> extern capture_file cf ; <nl> extern GtkWidget * packet_list ; <nl> display_opt_cb ( GtkWidget * w , gpointer d ) { <nl> gtk_box_pack_start ( GTK_BOX ( bbox ), cancel_bt , TRUE , TRUE , 0 ); <nl> gtk_widget_show ( cancel_bt ); <nl>  <nl> + /* Catch the " key_press_event " signal in the window , so that we can catch <nl> + the ESC key being pressed and act as if the " Cancel " button had <nl> + been selected . */ <nl> + dlg_set_cancel ( display_opt_w , cancel_bt ); <nl> + <nl> display_opt_window_active = TRUE ; <nl> gtk_widget_show ( display_opt_w ); <nl> }
mmm gtk / gtkclist . c <nl> ppp gtk / gtkclist . c <nl> * Copyright ( C ) 1995 - 1997 Peter Mattis , Spencer Kimball , Josh MacDonald , <nl> * Copyright ( C ) 1997 - 1998 Jay Painter < jpaint @ serv . net >< jpaint @ gimp . org > <nl> * <nl> - * $ Id : gtkclist . c , v 1 . 13 2002 / 09 / 09 20 : 32 : 30 jmayer Exp $ <nl> + * $ Id : gtkclist . c , v 1 . 14 2003 / 06 / 28 21 : 46 : 08 sahlberg Exp $ <nl> * <nl> * This library is free software ; you can redistribute it and / or <nl> * modify it under the terms of the GNU Library General Public <nl> * GTK + at ftp :// ftp . gtk . org / pub / gtk /. <nl> */ <nl>  <nl> +/* TODO : <nl> + * get rid of autoresize of the columns completely and just use some <nl> + * sane default widths instead <nl> + */ <nl> + <nl> # include " config . h " <nl> # include < stdlib . h > <nl> # include < string . h > <nl> LIST_WIDTH ( GtkCList * clist ) <nl> } G_STMT_END <nl>  <nl>  <nl> +/* maximum size in pxels that columns will be autosized to */ <nl> +# define MAX_COLUMN_AUTOSIZE_WIDTH 600 <nl> + <nl> /* Signals */ <nl> enum { <nl> SELECT_ROW , <nl> gtk_clist_set_column_auto_resize ( GtkCList * clist , <nl> { <nl> gint width ; <nl>  <nl> - width = gtk_clist_optimal_column_width ( clist , column ); <nl> + /* cap the auto - rezised width to something reasonable */ <nl> + width = MIN ( gtk_clist_optimal_column_width ( clist , column ), MAX_COLUMN_AUTOSIZE_WIDTH ); <nl> gtk_clist_set_column_width ( clist , column , width ); <nl> } <nl> }
mmm ui / gtk / iax2_analysis . c <nl> ppp ui / gtk / iax2_analysis . c <nl> void iax2_analysis ( <nl> simple_dialog ( ESD_TYPE_ERROR , ESD_BTN_OK , <nl> " Can ' t create temporary file for IAX2 analysis :\ n % s .", <nl> g_strerror ( errno )); <nl> + g_free ( user_data ); <nl> return ; <nl> } <nl> user_data -> f_tempname = g_strdup ( tempname ); <nl> void iax2_analysis ( <nl> simple_dialog ( ESD_TYPE_ERROR , ESD_BTN_OK , <nl> " Can ' t create temporary file for IAX2 analysis :\ n % s .", <nl> g_strerror ( errno )); <nl> + g_free ( user_data -> f_tempname ); <nl> + g_free ( user_data ); <nl> return ; <nl> } <nl> user_data -> r_tempname = g_strdup ( tempname );mmm ui / gtk / rtp_analysis . c <nl> ppp ui / gtk / rtp_analysis . c <nl> void iax2_analysis ( <nl> simple_dialog ( ESD_TYPE_ERROR , ESD_BTN_OK , <nl> " Can ' t create temporary file for IAX2 analysis :\ n % s .", <nl> g_strerror ( errno )); <nl> + g_free ( user_data ); <nl> return ; <nl> } <nl> user_data -> f_tempname = g_strdup ( tempname ); <nl> void iax2_analysis ( <nl> simple_dialog ( ESD_TYPE_ERROR , ESD_BTN_OK , <nl> " Can ' t create temporary file for IAX2 analysis :\ n % s .", <nl> g_strerror ( errno )); <nl> + g_free ( user_data -> f_tempname ); <nl> + g_free ( user_data ); <nl> return ; <nl> } <nl> user_data -> r_tempname = g_strdup ( tempname ); <nl> void rtp_analysis ( <nl> simple_dialog ( ESD_TYPE_ERROR , ESD_BTN_OK , <nl> " Can ' t create temporary file for RTP analysis :\ n % s .", <nl> g_strerror ( errno )); <nl> + g_free ( user_data ); <nl> return ; <nl> } <nl> user_data -> f_tempname = g_strdup ( tempname ); <nl> void rtp_analysis ( <nl> simple_dialog ( ESD_TYPE_ERROR , ESD_BTN_OK , <nl> " Can ' t create temporary file for RTP analysis :\ n % s .", <nl> g_strerror ( errno )); <nl> + g_free ( user_data -> f_tempname ); <nl> + g_free ( user_data ); <nl> return ; <nl> } <nl> user_data -> r_tempname = g_strdup ( tempname );
mmm epan / wmem / wmem_allocator_block . c <nl> ppp epan / wmem / wmem_allocator_block . c <nl> wmem_block_split_free_chunk ( wmem_block_allocator_t * allocator , <nl> available = chunk -> len ; <nl>  <nl> /* set new values for chunk */ <nl> - chunk -> len = aligned_size + sizeof ( wmem_block_chunk_t ); <nl> + chunk -> len = ( guint32 ) ( aligned_size + sizeof ( wmem_block_chunk_t )); <nl> chunk -> last = FALSE ; <nl>  <nl> /* with chunk ' s values set , we can use the standard macro to calculate <nl> wmem_block_split_free_chunk ( wmem_block_allocator_t * allocator , <nl>  <nl> /* Now that we ' ve copied over the free - list stuff ( which may have overlapped <nl> * with our new chunk header ) we can safely write our new chunk header . */ <nl> - extra -> len = available ; <nl> + extra -> len = ( guint32 ) available ; <nl> extra -> last = last ; <nl> - extra -> prev = aligned_size + sizeof ( wmem_block_chunk_t ); <nl> + extra -> prev = ( guint32 ) ( aligned_size + sizeof ( wmem_block_chunk_t )); <nl> extra -> used = FALSE ; <nl> } <nl>  <nl> wmem_block_split_used_chunk ( wmem_block_allocator_t * allocator , <nl> available = chunk -> len ; <nl>  <nl> /* set new values for chunk */ <nl> - chunk -> len = aligned_size + sizeof ( wmem_block_chunk_t ); <nl> + chunk -> len = ( guint32 ) ( aligned_size + sizeof ( wmem_block_chunk_t )); <nl> chunk -> last = FALSE ; <nl>  <nl> /* with chunk ' s values set , we can use the standard macro to calculate <nl> wmem_block_split_used_chunk ( wmem_block_allocator_t * allocator , <nl> available -= ( aligned_size + sizeof ( wmem_block_chunk_t )); <nl>  <nl> /* set the new values for the chunk */ <nl> - extra -> len = available ; <nl> + extra -> len = ( guint32 ) available ; <nl> extra -> last = last ; <nl> - extra -> prev = aligned_size + sizeof ( wmem_block_chunk_t ); <nl> + extra -> prev = ( guint32 ) ( aligned_size + sizeof ( wmem_block_chunk_t )); <nl> extra -> used = FALSE ; <nl>  <nl> /* add it to the free list */
mmm epan / dissectors / packet - user_encap . c <nl> ppp epan / dissectors / packet - user_encap . c <nl> static void dissect_user ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , guint <nl> call_dissector ( encap -> payload_handle , payload_tvb , pinfo , tree ); <nl>  <nl> if ( encap -> trailer_size ) { <nl> - tvbuff_t * trailer_tvb = tvb_new_subset ( tvb , 0 , encap -> trailer_size , encap -> trailer_size ); <nl> + tvbuff_t * trailer_tvb = tvb_new_subset ( tvb , encap -> header_size + len , encap -> trailer_size , encap -> trailer_size ); <nl> call_dissector ( encap -> trailer_handle , trailer_tvb , pinfo , tree ); <nl> offset = encap -> trailer_size ; <nl> } <nl> static void dissect_user ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , guint <nl>  <nl> void proto_reg_handoff_user_encap ( void ) <nl> { <nl> - int i = 0 ; <nl> + int i ; <nl> static dissector_handle_t data_handle ; <nl>  <nl> data_handle = find_dissector (" data "); <nl>  <nl> - do { <nl> + for ( i = 0 ; i < 16 ; i ++) { <nl> encaps [ i ]. handle = find_dissector ( encaps [ i ]. abbr ); <nl> dissector_add (" wtap_encap ", WTAP_ENCAP_USER0 + i , encaps [ i ]. handle ); <nl>  <nl> void proto_reg_handoff_user_encap ( void ) <nl> } <nl>  <nl> encaps [ i ]. encap_dissector = encap_dissectors [ encaps [ i ]. encap ]; <nl> - <nl> - i ++; <nl> - } while ( i < 16 ); <nl> - <nl> + } <nl> } <nl>  <nl> void proto_register_user_encap ( void )
mmm epan / dissectors / packet - q931 . c <nl> ppp epan / dissectors / packet - q931 . c <nl> dissect_q931_pdu ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> offset += call_ref_len ; <nl> } <nl> message_type = tvb_get_guint8 ( tvb , offset ); <nl> - if ( have_valid_q931_pi ) { <nl> + if ( have_valid_q931_pi && q931_pi ) { <nl> q931_pi -> message_type = message_type ; <nl> } <nl> col_add_str ( pinfo -> cinfo , COL_INFO , get_message_name ( prot_discr , message_type ));
mmm epan / dissectors / packet - rdt . c <nl> ppp epan / dissectors / packet - rdt . c <nl> # include < epan / packet . h > <nl> # include < epan / conversation . h > <nl> # include < epan / prefs . h > <nl> -# include < epan / emem . h > <nl> # include < epan / strutil . h > <nl> +# include < epan / wmem / wmem . h > <nl>  <nl> # include " packet - rdt . h " <nl>  <nl> void rdt_add_address ( packet_info * pinfo , <nl> if (! p_conv_data ) <nl> { <nl> /* Create conversation data */ <nl> - p_conv_data = se_new ( struct _rdt_conversation_info ); <nl> + p_conv_data = wmem_new ( wmem_file_scope (), struct _rdt_conversation_info ); <nl> conversation_add_proto_data ( p_conv , proto_rdt , p_conv_data ); <nl> } <nl>  <nl> static void show_setup_info ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree ) <nl> if ( p_conv_data ) <nl> { <nl> /* Save this conversation info into packet info */ <nl> - p_conv_packet_data = se_new ( struct _rdt_conversation_info ); <nl> + p_conv_packet_data = wmem_new ( wmem_file_scope (), struct _rdt_conversation_info ); <nl> g_strlcpy ( p_conv_packet_data -> method , p_conv_data -> method , MAX_RDT_SETUP_METHOD_SIZE ); <nl> p_conv_packet_data -> frame_number = p_conv_data -> frame_number ; <nl> p_conv_packet_data -> feature_level = p_conv_data -> feature_level ;
mmm epan / oids . c <nl> ppp epan / oids . c <nl> static oid_info_t * add_oid ( const char * name , oid_kind_t kind , const oid_value_ty <nl> if (! g_str_equal ( n -> name , name )) { <nl> D ( 2 ,(" Renaming Oid from : % s -> % s , this means the same oid is registered more than once ", n -> name , name )); <nl> } <nl> - /* XXX - Don ' t free n -> name here . It may be part of an hf_register_info <nl> - * struct that has been appended to the hfa GArray . */ <nl> + /* There used to be a comment here that claimed we couldn ' t free <nl> + * n -> name since it may be part of an hf_register_info struct <nl> + * that has been appended to the hfa GArray . I think that comment <nl> + * was wrong , because we only ever create oid_info_t ' s in this <nl> + * function , and we are always careful here to g_strdup the name . <nl> + * All that to justify freeing n -> name in the next line , since <nl> + * doing so fixes some memory leaks . */ <nl> + g_free ( n -> name ); <nl> } <nl>  <nl> n -> name = g_strdup ( name );
mmm plugins / gryphon / packet - gryphon . c <nl> ppp plugins / gryphon / packet - gryphon . c <nl> dissect_gryphon_message ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , <nl> /* <nl> * Indicate what kind of message this is . <nl> */ <nl> - col_set_str ( pinfo -> cinfo , COL_INFO , val_to_str ( frmtyp , frame_type , "- Invalid -")); <nl> + col_set_str ( pinfo -> cinfo , COL_INFO , val_to_str_const ( frmtyp , frame_type , "- Invalid -")); <nl> } <nl>  <nl> if ( tree == NULL )
mmm epan / tvbuff . c <nl> ppp epan / tvbuff . c <nl> tvb_get_ephemeral_unicode_stringz ( tvbuff_t * tvb , const gint offset , gint * length <nl>  <nl> size = tvb_unicode_strsize ( tvb , offset ); <nl>  <nl> - for ( i = 0 ; i < size ; i += 2 ) { /* XXX - make <= ??? */ <nl> + for ( i = 0 ; i < size ; i += 2 ) { <nl>  <nl> if ( encoding == ENC_BIG_ENDIAN ) <nl> uchar = tvb_get_ntohs ( tvb , offset + i ); <nl> else <nl> uchar = tvb_get_letohs ( tvb , offset + i ); <nl>  <nl> - /* Calculate how much space is needed to store UTF - 16 character in UTF - 8 */ <nl> + /* Calculate how much space is needed to store UTF - 16 character <nl> + * in UTF - 8 */ <nl> tmpbuf_len = g_unichar_to_utf8 ( uchar , NULL ); <nl>  <nl> - tmpbuf = g_malloc ( tmpbuf_len + 1 ); /* + 1 to make room for null terminator */ <nl> + tmpbuf = g_malloc ( tmpbuf_len + 1 ); /* + 1 to make room for null <nl> + * terminator */ <nl>  <nl> g_unichar_to_utf8 ( uchar , tmpbuf ); <nl>  <nl> - /* NULL terminate the tmpbuf so ep_strbuf_append knows where to stop */ <nl> + /* NULL terminate the tmpbuf so ep_strbuf_append knows where <nl> + * to stop */ <nl> tmpbuf [ tmpbuf_len ] = '\ 0 '; <nl>  <nl> ep_strbuf_append ( strbuf , tmpbuf );
mmm epan / dissectors / packet - mqtt . c <nl> ppp epan / dissectors / packet - mqtt . c <nl> static void * mqtt_message_decode_copy_cb ( void * dest , const void * orig , size_t le <nl> const mqtt_message_decode_t * o = ( const mqtt_message_decode_t *) orig ; <nl> mqtt_message_decode_t * d = ( mqtt_message_decode_t *) dest ; <nl>  <nl> + d -> match_criteria = o -> match_criteria ; <nl> d -> topic_pattern = g_strdup ( o -> topic_pattern ); <nl> d -> payload_proto_name = g_strdup ( o -> payload_proto_name ); <nl> 
mmm epan / dissectors / packet - sflow . c <nl> ppp epan / dissectors / packet - sflow . c <nl> static const value_string sflow_245_ipv4_precedence_types [] = { <nl> { SFLOW_245_IPV4_PRECEDENCE_CRITIC_ECP , " CRITIC / ECP "}, <nl> { SFLOW_245_IPV4_PRECEDENCE_INTERNETWORK_CONTROL , " Internetwork Control "}, <nl> { SFLOW_245_IPV4_PRECEDENCE_NETWORK_CONTROL , " Network Control "}, <nl> + { 0 , NULL } <nl> }; <nl>  <nl> /* sFlow v5 flow record formats */ <nl> dissect_sflow_5_extended_mpls_lvp_fec ( tvbuff_t * tvb , proto_tree * tree , gint offs <nl> guint32 length ; <nl>  <nl> length = tvb_get_ntohl ( tvb , offset ); <nl> - proto_tree_add_text ( tree , tvb , offset , 4 , " MPLS FEC Address Preﬁx Length : % u bytes ", length ); <nl> + proto_tree_add_text ( tree , tvb , offset , 4 , " MPLS FEC Address Prefix Length : % u bytes ", length ); <nl> offset += 4 ; <nl> return offset ; <nl> }
mmm gtk / toolbar . c <nl> ppp gtk / toolbar . c <nl> static void toolbar_append_separator ( GtkWidget * toolbar ) { <nl>  <nl>  <nl> # define toolbar_icon ( new_icon , window , xpm ) { \ <nl> - icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , xpm ); \ <nl> + icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , ( gchar **) xpm ); \ <nl> new_icon = gtk_pixmap_new ( icon , mask ); \ <nl> } <nl> mmm gtk / ui_util . c <nl> ppp gtk / ui_util . c <nl> static void toolbar_append_separator ( GtkWidget * toolbar ) { <nl>  <nl>  <nl> # define toolbar_icon ( new_icon , window , xpm ) { \ <nl> - icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , xpm ); \ <nl> + icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , ( gchar **) xpm ); \ <nl> new_icon = gtk_pixmap_new ( icon , mask ); \ <nl> } <nl>  <nl> tree_view_new ( GtkTreeModel * model ) <nl>  <nl> # if GTK_MAJOR_VERSION < 2 <nl> GtkWidget * <nl> - ctree_new_with_titles ( gint columns , gint tree_column , gchar * titles []) <nl> + ctree_new_with_titles ( gint columns , gint tree_column , const gchar * titles []) <nl> { <nl> GtkWidget * tree ; <nl> mmm gtk / tcp_graph . c <nl> ppp gtk / tcp_graph . c <nl> static void toolbar_append_separator ( GtkWidget * toolbar ) { <nl>  <nl>  <nl> # define toolbar_icon ( new_icon , window , xpm ) { \ <nl> - icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , xpm ); \ <nl> + icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , ( gchar **) xpm ); \ <nl> new_icon = gtk_pixmap_new ( icon , mask ); \ <nl> } <nl>  <nl> tree_view_new ( GtkTreeModel * model ) <nl>  <nl> # if GTK_MAJOR_VERSION < 2 <nl> GtkWidget * <nl> - ctree_new_with_titles ( gint columns , gint tree_column , gchar * titles []) <nl> + ctree_new_with_titles ( gint columns , gint tree_column , const gchar * titles []) <nl> { <nl> GtkWidget * tree ; <nl>  <nl> static void graph_destroy ( struct graph * g ) <nl> gdk_pixmap_unref ( g -> pixmap [ 1 ]); <nl> g_free ( g -> x_axis ); <nl> g_free ( g -> y_axis ); <nl> - g_free ( g -> title ); <nl> + g_free ( ( gpointer ) ( g -> title ) ); <nl> graph_segment_list_free ( g ); <nl> graph_element_lists_free ( g ); <nl> # if 0 <nl> static void axis_destroy ( struct axis * axis ) <nl> { <nl> gdk_pixmap_unref ( axis -> pixmap [ 0 ]); <nl> gdk_pixmap_unref ( axis -> pixmap [ 1 ]); <nl> - g_free ( axis -> label ); <nl> + g_free ( ( gpointer ) ( axis -> label ) ); <nl> } <nl>  <nl> static void axis_display ( struct axis * axis )mmm gtk / ui_util . h <nl> ppp gtk / ui_util . h <nl> static void toolbar_append_separator ( GtkWidget * toolbar ) { <nl>  <nl>  <nl> # define toolbar_icon ( new_icon , window , xpm ) { \ <nl> - icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , xpm ); \ <nl> + icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , ( gchar **) xpm ); \ <nl> new_icon = gtk_pixmap_new ( icon , mask ); \ <nl> } <nl>  <nl> tree_view_new ( GtkTreeModel * model ) <nl>  <nl> # if GTK_MAJOR_VERSION < 2 <nl> GtkWidget * <nl> - ctree_new_with_titles ( gint columns , gint tree_column , gchar * titles []) <nl> + ctree_new_with_titles ( gint columns , gint tree_column , const gchar * titles []) <nl> { <nl> GtkWidget * tree ; <nl>  <nl> static void graph_destroy ( struct graph * g ) <nl> gdk_pixmap_unref ( g -> pixmap [ 1 ]); <nl> g_free ( g -> x_axis ); <nl> g_free ( g -> y_axis ); <nl> - g_free ( g -> title ); <nl> + g_free ( ( gpointer ) ( g -> title ) ); <nl> graph_segment_list_free ( g ); <nl> graph_element_lists_free ( g ); <nl> # if 0 <nl> static void axis_destroy ( struct axis * axis ) <nl> { <nl> gdk_pixmap_unref ( axis -> pixmap [ 0 ]); <nl> gdk_pixmap_unref ( axis -> pixmap [ 1 ]); <nl> - g_free ( axis -> label ); <nl> + g_free ( ( gpointer ) ( axis -> label ) ); <nl> } <nl>  <nl> static void axis_display ( struct axis * axis ) <nl> extern GtkWidget * ctree_new ( gint columns , gint tree_column ); <nl> * @ return the newly created GtkCTree <nl> */ <nl> extern GtkWidget * ctree_new_with_titles ( gint columns , gint tree_column , <nl> - gchar * titles []); <nl> + const gchar * titles []); <nl> # else <nl> /** Create a GtkTreeView , give it the right styles , and remember it . <nl> *mmm gtk / simple_dialog . c <nl> ppp gtk / simple_dialog . c <nl> static void toolbar_append_separator ( GtkWidget * toolbar ) { <nl>  <nl>  <nl> # define toolbar_icon ( new_icon , window , xpm ) { \ <nl> - icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , xpm ); \ <nl> + icon = gdk_pixmap_create_from_xpm_d ( window -> window , & mask , & window -> style -> white , ( gchar **) xpm ); \ <nl> new_icon = gtk_pixmap_new ( icon , mask ); \ <nl> } <nl>  <nl> tree_view_new ( GtkTreeModel * model ) <nl>  <nl> # if GTK_MAJOR_VERSION < 2 <nl> GtkWidget * <nl> - ctree_new_with_titles ( gint columns , gint tree_column , gchar * titles []) <nl> + ctree_new_with_titles ( gint columns , gint tree_column , const gchar * titles []) <nl> { <nl> GtkWidget * tree ; <nl>  <nl> static void graph_destroy ( struct graph * g ) <nl> gdk_pixmap_unref ( g -> pixmap [ 1 ]); <nl> g_free ( g -> x_axis ); <nl> g_free ( g -> y_axis ); <nl> - g_free ( g -> title ); <nl> + g_free ( ( gpointer ) ( g -> title ) ); <nl> graph_segment_list_free ( g ); <nl> graph_element_lists_free ( g ); <nl> # if 0 <nl> static void axis_destroy ( struct axis * axis ) <nl> { <nl> gdk_pixmap_unref ( axis -> pixmap [ 0 ]); <nl> gdk_pixmap_unref ( axis -> pixmap [ 1 ]); <nl> - g_free ( axis -> label ); <nl> + g_free ( ( gpointer ) ( axis -> label ) ); <nl> } <nl>  <nl> static void axis_display ( struct axis * axis ) <nl> extern GtkWidget * ctree_new ( gint columns , gint tree_column ); <nl> * @ return the newly created GtkCTree <nl> */ <nl> extern GtkWidget * ctree_new_with_titles ( gint columns , gint tree_column , <nl> - gchar * titles []); <nl> + const gchar * titles []); <nl> # else <nl> /** Create a GtkTreeView , give it the right styles , and remember it . <nl> * <nl> display_simple_dialog ( gint type , gint btn_mask , char * message ) <nl> GdkBitmap * mask ; <nl> GtkStyle * style ; <nl> GdkColormap * cmap ; <nl> - gchar ** icon ; <nl> + const gchar ** icon ; <nl>  <nl> /* Main window */ <nl> switch ( type ) { <nl> display_simple_dialog ( gint type , gint btn_mask , char * message ) <nl> style = gtk_widget_get_style ( win ); <nl> cmap = gdk_colormap_get_system (); <nl> pixmap = gdk_pixmap_colormap_create_from_xpm_d ( NULL , cmap , & mask , <nl> - & style -> bg [ GTK_STATE_NORMAL ], icon ); <nl> + & style -> bg [ GTK_STATE_NORMAL ], ( gchar **) icon ); <nl> type_pm = gtk_pixmap_new ( pixmap , mask ); <nl> gtk_misc_set_alignment ( GTK_MISC ( type_pm ), 0 . 5 , 0 . 0 ); <nl> gtk_container_add ( GTK_CONTAINER ( top_hb ), type_pm );
mmm epan / dissectors / packet - xml . c <nl> ppp epan / dissectors / packet - xml . c <nl> next_attribute : <nl> g_ptr_array_free ( new -> element_names , TRUE ); <nl>  <nl> g_hash_table_insert ( root_element -> elements , new -> name , new ); <nl> - <nl> - g_free ( curr_name ); <nl> } <nl> } <nl> 
mmm ui / qt / wireless_frame . cpp <nl> ppp ui / qt / wireless_frame . cpp <nl> WirelessFrame :: WirelessFrame ( QWidget * parent ) : <nl>  <nl> WirelessFrame ::~ WirelessFrame () <nl> { <nl> + ws80211_free_interfaces ( interfaces_ ); <nl> delete ui ; <nl> } <nl> 
mmm epan / oids . c <nl> ppp epan / oids . c <nl> guint oid_string2subid ( wmem_allocator_t * scope , const char * str , guint32 ** subid <nl> subid += * r - ' 0 '; <nl>  <nl> if ( subids >= subids_overflow || subid > 0xffffffff ) { <nl> + wmem_free ( scope , * subids_p ); <nl> * subids_p = NULL ; <nl> return 0 ; <nl> }
mmm epan / dissectors / packet - qnet6 . c <nl> ppp epan / dissectors / packet - qnet6 . c <nl> dissect_qnet6 ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * dat <nl> /* <nl> * data after header <nl> */ <nl> - if ( cklen != 0 ) <nl> + if ( cklen > 0 ) <nl> { <nl> crc = crc32_mpeg2_seed ( tvb_get_ptr ( tvb , 36 + 2 , cklen ), cklen , ~ crc ); <nl> crc = ~ crc ;
mmm epan / dissectors / packet - radius . c <nl> ppp epan / dissectors / packet - radius . c <nl> dissect_attribute_value_pairs ( proto_tree * tree , packet_info * pinfo , tvbuff_t * tv <nl>  <nl> avp_vsa_len -= avp_vsa_header_len ; <nl>  <nl> + memset (& vendor_type , 0 , sizeof ( vendor_type )); <nl> if ( avp_is_extended ) { <nl> vendor_type . u8_code [ 0 ] = avp_type . u8_code [ 0 ]; <nl> vendor_type . u8_code [ 1 ] = avp_vsa_type ;
mmm tools / lemon / lemon . c <nl> ppp tools / lemon / lemon . c <nl> PRIVATE void tplt_xfer ( const char * name , FILE * in , FILE * out , int * lineno ) <nl> PRIVATE FILE * tplt_open ( struct lemon * lemp ) <nl> { <nl> static char templatename [] = " lempar . c "; <nl> - char * buf ; <nl> FILE * in ; <nl> char * tpltname = NULL ; <nl> char * cp ; <nl>  <nl> if ( lemp -> templatename ) { <nl> tpltname = strdup ( lemp -> templatename ); <nl> - } <nl> - else { <nl> + } else { <nl> + char * buf ; <nl> + <nl> cp = strrchr ( lemp -> filename ,'.'); <nl> buf = malloc ( 1000 ); <nl> if ( cp ){ <nl> PRIVATE FILE * tplt_open ( struct lemon * lemp ) <nl> sprintf ( buf ,"% s . lt ", lemp -> filename ); <nl> } <nl> if ( access ( buf , 004 )== 0 ){ <nl> - tpltname = buf ; <nl> + tpltname = strdup ( buf ); <nl> } else if ( access ( templatename , 004 )== 0 ){ <nl> tpltname = strdup ( templatename ); <nl> } else { <nl> tpltname = pathsearch ( lemp -> argv0 , templatename , 0 ); <nl> - free ( buf ); <nl> } <nl> + free ( buf ); <nl> } <nl> if ( tpltname == 0 ){ <nl> fprintf ( stderr ," Can ' t find the parser driver template file \"% s \".\ n ",
mmm epan / dissectors / packet - sip . c <nl> ppp epan / dissectors / packet - sip . c <nl> dissect_sip_common ( tvbuff_t * tvb , int offset , packet_info * pinfo , proto_tree * tr <nl> } <nl> } <nl>  <nl> - if ( sub_value_offset == linelen ) <nl> + if ( sub_value_offset == value_len ) <nl> { <nl> /* Didn ' t find method name */ <nl> THROW ( ReportedBoundsError ); <nl> dissect_sip_common ( tvbuff_t * tvb , int offset , packet_info * pinfo , proto_tree * tr <nl> } <nl>  <nl> /* Extract method name from value */ <nl> - strlen_to_copy = ( int ) linelen - sub_value_offset ; <nl> + strlen_to_copy = ( int ) value_len - sub_value_offset ; <nl> if ( strlen_to_copy > MAX_CSEQ_METHOD_SIZE ) { <nl> /* Note the error in the protocol tree */ <nl> if ( hdr_tree ) {
mmm epan / dissectors / packet - rtpproxy . c <nl> ppp epan / dissectors / packet - rtpproxy . c <nl> static int <nl> dissect_rtpproxy ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data _U_ ) <nl> { <nl> gboolean has_lf = FALSE ; <nl> - guint offset = 0 ; <nl> + gint offset = 0 ; <nl> gint new_offset = 0 ; <nl> guint tmp ; <nl> - guint realsize = 0 ; <nl> + gint realsize = 0 ; <nl> guint8 * rawstr ; <nl> proto_item * ti ; <nl> proto_tree * rtpproxy_tree ; <nl> dissect_rtpproxy ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * tree , void * data <nl>  <nl> /* Extract parameters */ <nl> /* Parameters should be right after the command and before EOL ( in case of Info command ) or before whitespace */ <nl> - new_offset = ( tmp == ' i ' ? ( gint )( realsize - 1 > offset ? offset + strlen (" Ib ") : offset + strlen (" I ")) : tvb_find_guint8 ( tvb , offset , - 1 , ' ')); <nl> + new_offset = ( tmp == ' i ' ? ( realsize - 1 > offset ? offset + ( gint ) strlen (" Ib ") : offset + ( gint ) strlen (" I ")) : tvb_find_guint8 ( tvb , offset , - 1 , ' ')); <nl>  <nl> - if ( new_offset != ( gint ) offset + 1 ){ <nl> + if ( new_offset != offset + 1 ){ <nl> rtpproxy_tree = proto_item_add_subtree ( ti , ett_rtpproxy_command ); <nl> proto_tree_add_item ( rtpproxy_tree , hf_rtpproxy_command_parameters , tvb , offset + 1 , new_offset - ( offset + 1 ), ENC_ASCII | ENC_NA ); <nl> rtpproxy_tree = proto_item_get_parent ( ti );
mmm ui / win32 / console_win32 . c <nl> ppp ui / win32 / console_win32 . c <nl> /* console_win32 . c <nl> * Console support for MSWindows <nl> * <nl> - * $ Id : console_win32 . c 45689 2012 - 10 - 21 15 : 04 : 50Z alagoutte $ <nl> + * $ Id $ <nl> * <nl> * Wireshark - Network traffic analyzer <nl> * By Gerald Combs < gerald @ wireshark . org > <nl> * <nl> */ <nl>  <nl> +# ifdef _WIN32 <nl> + <nl> # include < string . h > <nl> # include < stdio . h > <nl> # include < stdlib . h > <nl> # include " console_win32 . h " <nl> # include "../../ console_io . h " <nl>  <nl> -# ifdef _WIN32 /* Needed for console I / O */ <nl> # if _MSC_VER < 1500 <nl> /* AttachConsole () needs this # define ! */ <nl> # define _WIN32_WINNT 0x0501 <nl> # include < conio . h > <nl> # include < windows . h > <nl> # include < tchar . h > <nl> -# endif <nl>  <nl> -# ifdef _WIN32 <nl> static gboolean has_console ; /* TRUE if app has console */ <nl> static gboolean console_wait ; /* " Press any key ..." */ <nl> static gboolean stdin_capture = FALSE ; /* Don ' t grab stdin & stdout if TRUE */ <nl> -# endif <nl>  <nl> -# ifdef _WIN32 <nl> /* The code to create and desstroy console windows should not be necessary , <nl> at least as I read the GLib source code , as it looks as if GLib is , on <nl> Win32 , * supposed * to create a console window into which to display its <nl> set_console_wait ( gboolean set_console_wait ) <nl> { <nl> console_wait = set_console_wait ; <nl> } <nl> + <nl> gboolean <nl> get_console_wait ( void ) <nl> { <nl> get_stdin_capture ( void ) <nl> { <nl> return stdin_capture ; <nl> } <nl> + <nl> # endif /* _WIN32 */ <nl>  <nl> /*
mmm epan / dissectors / packet - smb . c <nl> ppp epan / dissectors / packet - smb . c <nl> dissect_smb_command ( tvbuff_t * tvb , packet_info * pinfo , int offset , proto_tree * s <nl> smb_dissector [ cmd ]. request : smb_dissector [ cmd ]. response ; <nl>  <nl> offset = (* dissector )( tvb , pinfo , cmd_tree , offset , smb_tree ); <nl> + <nl> + if (! tvb_offset_exists ( tvb , offset - 1 )) { <nl> + THROW ( ReportedBoundsError ); <nl> + } <nl> proto_item_set_end ( cmd_item , tvb , offset ); <nl> } <nl> return offset ;
mmm epan / dissectors / packet - ieee80211 . c <nl> ppp epan / dissectors / packet - ieee80211 . c <nl> dissect_extended_capabilities_ie ( packet_info * pinfo , proto_tree * tree , <nl> expert_add_info_format ( pinfo , ti_len , PI_MALFORMED , PI_ERROR , " Tag length % u too short , must be greater than 0 ", tag_len ); <nl> return offset ; <nl> } <nl> - proto_item_append_text ( ti , " (% d octets )", tag_len ); <nl> + proto_item_append_text ( ti , " (% u octet % s )", tag_len , plurality ( tag_len , "", " s ")); <nl>  <nl> /* Extended Capability octet 1 */ <nl> ti_ex_cap = proto_tree_add_item ( tree , hf_ieee80211_tag_extended_capabilities , tvb , offset , 1 , ENC_NA ); <nl> add_tagged_field ( packet_info * pinfo , proto_tree * tree , tvbuff_t * tvb , int offset <nl> " (% s ) code not implemented , Contact " <nl> " Wireshark developers if you want this supported ", val_to_str_ext ( tag_no , <nl> & tag_num_vals_ext , "(% d )")); <nl> - proto_item_append_text ( ti , ": Tag % u Len % u ", tag_no , tag_len ); <nl> + proto_item_append_text ( ti , ": Undecoded "); <nl> break ; <nl> } <nl> if ( offset < tag_end ) {
mmm epan / wslua / wslua_field . c <nl> ppp epan / wslua / wslua_field . c <nl> static int FieldInfo_get_range ( lua_State * L ) { <nl> r -> tvb = ep_new ( struct _wslua_tvb ); <nl>  <nl> r -> tvb -> ws_tvb = fi -> ds_tvb ; <nl> + r -> tvb -> expired = FALSE ; <nl> + r -> tvb -> need_free = FALSE ; <nl> r -> offset = fi -> start ; <nl> r -> len = fi -> length ; <nl> 
mmm epan / dissectors / packet - umts_mac . c <nl> ppp epan / dissectors / packet - umts_mac . c <nl> static void dissect_mac_fdd_dch ( tvbuff_t * tvb , packet_info * pinfo , proto_tree * t <nl>  <nl> macinf = p_get_proto_data ( pinfo -> fd , proto_umts_mac ); <nl> fpinf = p_get_proto_data ( pinfo -> fd , proto_fp ); <nl> - pos = fpinf -> cur_tb ; <nl> if (! macinf || ! fpinf ) { <nl> proto_tree_add_text ( dch_tree , tvb , 0 , - 1 , <nl> " Cannot dissect MAC frame because per - frame info is missing "); <nl> return ; <nl> } <nl> + pos = fpinf -> cur_tb ; <nl> if ( macinf -> ctmux [ pos ]) { <nl> proto_tree_add_bits_item ( dch_tree , hf_mac_ct , tvb , 0 , 4 , FALSE ); <nl> bitoffs = 4 ;
mmm capture . c <nl> ppp capture . c <nl> /* capture . c <nl> * Routines for packet capture windows <nl> * <nl> - * $ Id : capture . c , v 1 . 197 2002 / 12 / 18 06 : 44 : 50 guy Exp $ <nl> + * $ Id : capture . c , v 1 . 198 2002 / 12 / 29 01 : 19 : 08 guy Exp $ <nl> * <nl> * Ethereal - Network traffic analyzer <nl> * By Gerald Combs < gerald @ ethereal . com > <nl> pipe_open_live ( char * pipename , struct pcap_hdr * hdr , loop_data * ld , <nl> " Unexpected error from select : % s ", strerror ( errno )); <nl> goto error ; <nl> } else if ( sel_ret > 0 ) { <nl> - b = read ( fd , & magic + bytes_read , sizeof magic - bytes_read ); <nl> + b = read ( fd , (( char *)& magic )+ bytes_read , sizeof magic - bytes_read ); <nl> if ( b <= 0 ) { <nl> if ( b == 0 ) <nl> snprintf ( errmsg , errmsgl , " End of file on pipe during open ");
mmm epan / dissectors / packet - cops . c <nl> ppp epan / dissectors / packet - cops . c <nl> * Based on PKT - SP - DQOS - I09 - 040402 ( April 2 , 2004 ) <nl> * <nl> * PacketCable Multimedia Specification <nl> - * Based on PKT - SP - MM - I02 - 040930 <nl> + * Based on PKT - SP - MM - I04 - 080522 <nl> * <nl> * www . packetcable . com <nl> * <nl> cops_classifier ( tvbuff_t * tvb , proto_tree * st , guint n , guint32 offset , gboolean <nl> offset += 2 ; <nl> } <nl>  <nl> - /* Priority */ <nl> - info_to_display ( tvb , stt , offset , 1 ," Priority ", NULL , FMT_HEX ,& hf_cops_pcmm_classifier_priority ); <nl> - offset += 1 ; <nl> - <nl> if ( extended ) { <nl> /* ClassifierID */ <nl> info_to_display ( tvb , stt , offset , 2 ," ClassifierID ", NULL , FMT_HEX ,& hf_cops_pcmm_classifier_classifier_id ); <nl> offset += 2 ; <nl> + } <nl> + <nl> + /* Priority */ <nl> + info_to_display ( tvb , stt , offset , 1 ," Priority ", NULL , FMT_HEX ,& hf_cops_pcmm_classifier_priority ); <nl> + offset += 1 ; <nl>  <nl> + if ( extended ) { <nl> /* Activation State */ <nl> info_to_display ( tvb , stt , offset , 1 ," Activation State ", NULL , FMT_HEX ,& hf_cops_pcmm_classifier_activation_state ); <nl> offset += 1 ;
mmm wsutil / ws_pipe . c <nl> ppp wsutil / ws_pipe . c <nl> ws_pipe_wait_for_pipe ( HANDLE * pipe_handles , int num_pipe_handles , HANDLE pid ) <nl> int num_waiting_to_connect = 0 ; <nl> int num_handles = num_pipe_handles + 1 ; // PID handle is also added to list of handles . <nl>  <nl> + SecureZeroMemory ( pipeinsts , sizeof ( pipeinsts )); <nl> + <nl> if ( num_pipe_handles == 0 || num_pipe_handles > 3 ) <nl> { <nl> g_log ( LOG_DOMAIN_CAPTURE , G_LOG_LEVEL_DEBUG , " Invalid number of pipes given as argument .");
mmm src / mqtt_client . c <nl> ppp src / mqtt_client . c <nl> wait_again : <nl> /* Determine if we received data for this request */ <nl> if (( wait_type == MQTT_PACKET_TYPE_ANY || <nl> wait_type == packet_type || <nl> - MqttIsPubRespPacket ( packet_type ) == MqttIsPubRespPacket ( wait_type )) && <nl> - ( wait_packet_id == 0 || wait_packet_id == packet_id )) <nl> + ( MqttIsPubRespPacket ( packet_type ) && <nl> + MqttIsPubRespPacket ( wait_type ))) && <nl> + ( wait_packet_id == 0 || wait_packet_id == packet_id )) <nl> { <nl> use_packet_obj = packet_obj ; <nl> waitMatchFound = 1 ;
mmm examples / client / client . c <nl> ppp examples / client / client . c <nl> static int ClientBenchmarkThroughput ( WOLFSSL_CTX * ctx , char * host , word16 port , <nl>  <nl> /* Compare TX and RX buffers */ <nl> if ( XMEMCMP ( tx_buffer , rx_buffer , len ) != 0 ) { <nl> + free ( tx_buffer ); <nl> + tx_buffer = NULL ; <nl> + free ( rx_buffer ); <nl> + rx_buffer = NULL ; <nl> err_sys (" Compare TX and RX buffers failed "); <nl> } <nl> mmm wolfcrypt / test / test . c <nl> ppp wolfcrypt / test / test . c <nl> static int ClientBenchmarkThroughput ( WOLFSSL_CTX * ctx , char * host , word16 port , <nl>  <nl> /* Compare TX and RX buffers */ <nl> if ( XMEMCMP ( tx_buffer , rx_buffer , len ) != 0 ) { <nl> + free ( tx_buffer ); <nl> + tx_buffer = NULL ; <nl> + free ( rx_buffer ); <nl> + rx_buffer = NULL ; <nl> err_sys (" Compare TX and RX buffers failed "); <nl> } <nl>  <nl> static int ecc_test_curve ( WC_RNG * rng , int keySize ) <nl>  <nl> # if ! defined ( WOLFSSL_ATECC508A ) && defined ( HAVE_ECC_KEY_IMPORT ) && \ <nl> defined ( HAVE_ECC_KEY_EXPORT ) <nl> - static int ecc_point_test () <nl> + static int ecc_point_test ( void ) <nl> { <nl> int ret ; <nl> ecc_point * point ;
mmm wolfssl / error - ssl . h <nl> ppp wolfssl / error - ssl . h <nl> enum wolfSSL_ErrorCodes { <nl> SANITY_MSG_E = - 394 , /* Sanity check on msg order error */ <nl> DUPLICATE_MSG_E = - 395 , /* Duplicate message error */ <nl> SNI_UNSUPPORTED = - 396 , /* SSL 3 . 0 does not support SNI */ <nl> + SOCKET_PEER_CLOSED_E = - 397 , /* Underlying transport closed */ <nl>  <nl> /* add strings to SetErrorString !!!!! */ <nl> mmm src / internal . c <nl> ppp src / internal . c <nl> enum wolfSSL_ErrorCodes { <nl> SANITY_MSG_E = - 394 , /* Sanity check on msg order error */ <nl> DUPLICATE_MSG_E = - 395 , /* Duplicate message error */ <nl> SNI_UNSUPPORTED = - 396 , /* SSL 3 . 0 does not support SNI */ <nl> + SOCKET_PEER_CLOSED_E = - 397 , /* Underlying transport closed */ <nl>  <nl> /* add strings to SetErrorString !!!!! */ <nl>  <nl> startScr : <nl> if ( ssl -> error == SOCKET_ERROR_E ) { <nl> if ( ssl -> options . connReset || ssl -> options . isClosed ) { <nl> WOLFSSL_MSG (" Peer reset or closed , connection done "); <nl> + ssl -> error = SOCKET_PEER_CLOSED_E ; <nl> + WOLFSSL_ERROR ( ssl -> error ); <nl> return 0 ; /* peer reset or closed */ <nl> } <nl> } <nl> const char * wolfSSL_ERR_reason_error_string ( unsigned long e ) <nl> case DUPLICATE_MSG_E : <nl> return " Duplicate HandShake message Error "; <nl>  <nl> + case SNI_UNSUPPORTED : <nl> + return " Protocol version does not support SNI Error "; <nl> + <nl> + case SOCKET_PEER_CLOSED_E : <nl> + return " Peer closed underlying transport Error "; <nl> + <nl> default : <nl> return " unknown error number "; <nl> }
mmm ctaocrypt / src / aes . c <nl> ppp ctaocrypt / src / aes . c <nl> void AesCtrEncrypt ( Aes * aes , byte * out , const byte * in , word32 sz ) <nl> word32 blocks = sz / AES_BLOCK_SIZE ; <nl>  <nl> while ( blocks --) { <nl> - AesEncrypt ( aes , aes -> reg , out ); <nl> + AesEncrypt ( aes , ( byte *) aes -> reg , out ); <nl> IncrementAesCounter (( byte *) aes -> reg ); <nl> xorbuf ( out , in , AES_BLOCK_SIZE ); <nl> 
mmm src / tls . c <nl> ppp src / tls . c <nl> static int TLSX_KeyShare_Parse ( WOLFSSL * ssl , byte * input , word16 length , <nl> if ( TLSX_KeyShare_Find ( ssl , group )) <nl> return BAD_KEY_SHARE_DATA ; <nl>  <nl> + /* Clear out unusable key shares . */ <nl> + ret = TLSX_KeyShare_Empty ( ssl ); <nl> + if ( ret != 0 ) <nl> + return ret ; <nl> + <nl> /* Try to use the server ' s group . */ <nl> ret = TLSX_KeyShare_Use ( ssl , group , 0 , NULL , NULL ); <nl> }
mmm wolfcrypt / src / logging . c <nl> ppp wolfcrypt / src / logging . c <nl> int wc_AddErrorNode ( int error , int line , char * buf , char * file ) <nl> if ( wc_errors != NULL ) { <nl> /* check for unexpected case before over writing wc_errors */ <nl> WOLFSSL_MSG (" ERROR in adding new node to logging queue !!\ n "); <nl> + /* In the event both wc_last_node and wc_errors are NULL , err <nl> + * goes unassigned to external wc_errors , wc_last_node . Free <nl> + * err in this instance since wc_ClearErrorNodes will not <nl> + */ <nl> + XFREE ( err , wc_error_heap , DYNAMIC_TYPE_LOG ); <nl> } <nl> else { <nl> wc_errors = err ;
mmm src / ssl . c <nl> ppp src / ssl . c <nl> void wolfSSL_X509_STORE_CTX_set_time ( WOLFSSL_X509_STORE_CTX * ctx , <nl> { <nl> ( void ) flags ; <nl>  <nl> + if ( ctx == NULL ) <nl> + return ; <nl> + <nl> ctx -> param -> check_time = t ; <nl> ctx -> param -> flags |= WOLFSSL_USE_CHECK_TIME ; <nl> }
mmm src / internal . c <nl> ppp src / internal . c <nl> int DoSessionTicket ( WOLFSSL * ssl , <nl> # ifdef WOLFSSL_DTLS <nl> Hmac cookieHmac ; <nl> byte peerCookie [ MAX_COOKIE_LEN ]; <nl> - byte peerCookieSz ; <nl> + byte peerCookieSz = 0 ; <nl> byte cookieType ; <nl> byte cookieSz ; <nl> # endif /* WOLFSSL_DTLS */
mmm src / internal . c <nl> ppp src / internal . c <nl> static int BuildMessage ( CYASSL * ssl , byte * output , int outSz , <nl> ivSz = blockSz ; <nl> sz += ivSz ; <nl>  <nl> + if ( ivSz > ( word32 ) sizeof ( iv )) <nl> + return BUFFER_E ; <nl> + <nl> ret = RNG_GenerateBlock ( ssl -> rng , iv , ivSz ); <nl> if ( ret != 0 ) <nl> return ret ;
mmm src / ssl . c <nl> ppp src / ssl . c <nl> int AddCA ( WOLFSSL_CERT_MANAGER * cm , DerBuffer ** pDer , int type , int verify ) <nl> ret = MEMORY_ERROR ; <nl> else { <nl> signer -> keyOID = cert -> keyOID ; <nl> - signer -> publicKey = cert -> publicKey ; <nl> - signer -> pubKeySize = cert -> pubKeySize ; <nl> - signer -> nameLen = cert -> subjectCNLen ; <nl> - signer -> name = cert -> subjectCN ; <nl> + if ( cert -> pubKeyStored ) { <nl> + signer -> publicKey = cert -> publicKey ; <nl> + signer -> pubKeySize = cert -> pubKeySize ; <nl> + } <nl> + if ( cert -> subjectCNStored ) { <nl> + signer -> nameLen = cert -> subjectCNLen ; <nl> + signer -> name = cert -> subjectCN ; <nl> + } <nl> signer -> pathLength = cert -> pathLength ; <nl> signer -> pathLengthSet = cert -> pathLengthSet ; <nl> # ifndef IGNORE_NAME_CONSTRAINTS
mmm src / internal . c <nl> ppp src / internal . c <nl> void InitX509Name ( WOLFSSL_X509_NAME * name , int dynamicFlag ) <nl> # ifdef OPENSSL_EXTRA <nl> XMEMSET (& name -> fullName , 0 , sizeof ( DecodedName )); <nl> XMEMSET (& name -> cnEntry , 0 , sizeof ( WOLFSSL_X509_NAME_ENTRY )); <nl> + XMEMSET (& name -> extra , 0 , sizeof ( name -> extra )); <nl> name -> cnEntry . value = &( name -> cnEntry . data ); /* point to internal data */ <nl> name -> cnEntry . nid = ASN_COMMON_NAME ; <nl> name -> x509 = NULL ;
mmm src / internal . c <nl> ppp src / internal . c <nl> int SendCertificateRequest ( WOLFSSL * ssl ) <nl> /* write to output */ <nl> output [ i ++] = ( byte ) typeTotal ; /* # of types */ <nl> # ifdef HAVE_ECC <nl> - if ( ssl -> options . cipherSuite0 == ECC_BYTE && <nl> + if (( ssl -> options . cipherSuite0 == ECC_BYTE || <nl> + ssl -> options . cipherSuite0 == CHACHA_BYTE ) && <nl> ssl -> specs . sig_algo == ecc_dsa_sa_algo ) { <nl> output [ i ++] = ecdsa_sign ; <nl> } else
mmm wolfcrypt / src / integer . c <nl> ppp wolfcrypt / src / integer . c <nl> int mp_init ( mp_int * a ) <nl> { <nl> int i ; <nl>  <nl> + /* Safeguard against passing in a null pointer */ <nl> + if ( a == NULL ) <nl> + return MP_VAL ; <nl> + <nl> /* allocate memory required and clear it */ <nl> a -> dp = OPT_CAST ( mp_digit ) XMALLOC ( sizeof ( mp_digit ) * MP_PREC , 0 , <nl> DYNAMIC_TYPE_BIGINT ); <nl> mp_copy ( mp_int * a , mp_int * b ) <nl> { <nl> int res , n ; <nl>  <nl> + /* Safeguard against passing in a null pointer */ <nl> + if ( a == NULL || b == NULL ) <nl> + return MP_VAL ; <nl> + <nl> /* if dst == src do nothing */ <nl> if ( a == b ) { <nl> return MP_OKAY ;
mmm cyassl / ctaocrypt / settings . h <nl> ppp cyassl / ctaocrypt / settings . h <nl> # include " mutex . h " <nl> # endif <nl>  <nl> - # define XMALLOC ( s , h , type ) ( void *) _mem_alloc_system (( s )) <nl> - # define XFREE ( p , h , type ) _mem_free ( p ) <nl> + # define XMALLOC ( s , h , t ) ( void *) _mem_alloc_system (( s )) <nl> + # define XFREE ( p , h , t ) { void * xp = ( p ); if (( xp )) _mem_free (( xp ));} <nl> /* Note : MQX has no realloc , using fastmath above */ <nl> # endif <nl> 
mmm src / internal . c <nl> ppp src / internal . c <nl> int SetCipherList ( WOLFSSL_CTX * ctx , Suites * suites , const char * list ) <nl> } <nl> # endif /* WOLFSSL_DTLS */ <nl>  <nl> + if ( idx + 1 >= WOLFSSL_MAX_SUITE_SZ ) { <nl> + WOLFSSL_MSG (" WOLFSSL_MAX_SUITE_SZ set too low "); <nl> + return 0 ; /* suites buffer not large enough , error out */ <nl> + } <nl> + <nl> suites -> suites [ idx ++] = ( XSTRSTR ( name , " TLS13 ")) ? TLS13_BYTE <nl> : ( XSTRSTR ( name , " CHACHA ")) ? CHACHA_BYTE <nl> : ( XSTRSTR ( name , " QSH ")) ? QSH_BYTE
mmm examples / echoclient / echoclient . c <nl> ppp examples / echoclient / echoclient . c <nl> void echoclient_test ( void * args ) <nl> # if defined ( CYASSL_DTLS ) <nl> method = DTLSv1_client_method (); <nl> # elif ! defined ( NO_TLS ) <nl> - method = TLSv1_2_client_method (); <nl> + method = CyaSSLv23_client_method (); <nl> # else <nl> method = SSLv3_client_method (); <nl> # endif
mmm src / ssl . c <nl> ppp src / ssl . c <nl> long wolfSSL_set_options ( WOLFSSL * ssl , long op ) <nl> long wolfSSL_get_options ( const WOLFSSL * ssl ) <nl> { <nl> WOLFSSL_ENTER (" wolfSSL_get_options "); <nl> - if ( ssl == NULL ) return WOLFSSL_FAILURE ; <nl> + if ( ssl == NULL ) <nl> + return WOLFSSL_FAILURE ; <nl> return ssl -> options . mask ; <nl> } <nl>  <nl> long wolfSSL_clear_options ( WOLFSSL * ssl , long opt ) <nl> { <nl> WOLFSSL_ENTER (" SSL_clear_options "); <nl> + if ( ssl == NULL ) <nl> + return WOLFSSL_FAILURE ; <nl> ssl -> options . mask &= ~ opt ; <nl> return ssl -> options . mask ; <nl> }
mmm src / tls . c <nl> ppp src / tls . c <nl> int TLSX_ValidateEllipticCurves ( CYASSL * ssl , byte first , byte second ) { <nl> int sig = 0 ; /* valitade signature */ <nl> int key = 0 ; /* validate key */ <nl>  <nl> + ( void ) oid ; <nl> + ( void ) octets ; <nl> + <nl> if (! extension ) <nl> return 1 ; /* no suite restriction */ <nl> 
mmm src / tls . c <nl> ppp src / tls . c <nl> static int TLSX_SNI_Parse ( WOLFSSL * ssl , byte * input , word16 length , <nl> return BUFFER_ERROR ; <nl>  <nl> for ( size = 0 ; offset < length ; offset += size ) { <nl> - SNI * sni ; <nl> + SNI * sni = NULL ; <nl> byte type = input [ offset ++]; <nl>  <nl> if ( offset + OPAQUE16_LEN > length )
mmm src / internal . c <nl> ppp src / internal . c <nl> static int DoCertificate ( CYASSL * ssl , byte * input , word32 * inOutIdx , <nl> ret = KEYUSE_ENCIPHER_E ; <nl> } <nl> if (( ssl -> specs . sig_algo == rsa_sa_algo || <nl> - ssl -> specs . sig_algo == ecc_dsa_sa_algo ) && <nl> + ( ssl -> specs . sig_algo == ecc_dsa_sa_algo && <nl> + ! ssl -> specs . static_ecdh )) && <nl> ( dCert . extKeyUsage & KEYUSE_DIGITAL_SIG ) == 0 ) { <nl> CYASSL_MSG (" KeyUse Digital Sig not set "); <nl> ret = KEYUSE_SIGNATURE_E ;
mmm src / internal . c <nl> ppp src / internal . c <nl> static int DoHandShakeMsgType ( CYASSL * ssl , byte * input , word32 * inOutIdx , <nl> return OUT_OF_ORDER_E ; <nl> } <nl>  <nl> + if ( ssl -> options . side == CLIENT_END && ssl -> options . dtls == 0 && <nl> + ssl -> options . serverState == NULL_STATE && type != server_hello ) { <nl> + CYASSL_MSG (" First server message not server hello "); <nl> + return OUT_OF_ORDER_E ; <nl> + } <nl> + <nl> + if ( ssl -> options . side == SERVER_END && <nl> + ssl -> options . clientState == NULL_STATE && type != client_hello ) { <nl> + CYASSL_MSG (" First client message not client hello "); <nl> + return OUT_OF_ORDER_E ; <nl> + } <nl> + <nl> + <nl> switch ( type ) { <nl>  <nl> case hello_request :
mmm src / internal . c <nl> ppp src / internal . c <nl> int DoSessionTicket ( WOLFSSL * ssl , const byte * input , word32 * inOutIdx , <nl> { <nl> int i ; <nl>  <nl> - if ( suites == NULL ) { <nl> - WOLFSSL_MSG (" Suites pointer error "); <nl> + if ( suites == NULL || suites -> suiteSz == 0 ) { <nl> + WOLFSSL_MSG (" Suites pointer error or suiteSz 0 "); <nl> return SUITES_ERROR ; <nl> } <nl>  <nl> - for ( i = 0 ; i < suites -> suiteSz ; i += 2 ) { <nl> + for ( i = 0 ; i < suites -> suiteSz - 1 ; i += SUITE_LEN ) { <nl> if ( suites -> suites [ i ] == first && <nl> suites -> suites [ i + 1 ] == second ) <nl> return i ;
mmm src / internal . c <nl> ppp src / internal . c <nl> const char * wolfSSL_ERR_reason_error_string ( unsigned long e ) <nl> case MATCH_SUITE_ERROR : <nl> return " can ' t match cipher suite "; <nl>  <nl> + case COMPRESSION_ERROR : <nl> + return " compression mismatch error "; <nl> + <nl> case BUILD_MSG_ERROR : <nl> return " build message failure "; <nl>  <nl> static void PickHashSigAlgo ( WOLFSSL * ssl , <nl> ssl -> options . cipherSuite = cs1 ; <nl> compression = input [ i ++]; <nl>  <nl> + if ( compression != NO_COMPRESSION && ! ssl -> options . usingCompression ) { <nl> + WOLFSSL_MSG (" Server forcing compression w / o support "); <nl> + return COMPRESSION_ERROR ; <nl> + } <nl> + <nl> if ( compression != ZLIB_COMPRESSION && ssl -> options . usingCompression ) { <nl> WOLFSSL_MSG (" Server refused compression , turning off "); <nl> ssl -> options . usingCompression = 0 ; /* turn off if server refused */mmm wolfssl / error - ssl . h <nl> ppp wolfssl / error - ssl . h <nl> const char * wolfSSL_ERR_reason_error_string ( unsigned long e ) <nl> case MATCH_SUITE_ERROR : <nl> return " can ' t match cipher suite "; <nl>  <nl> + case COMPRESSION_ERROR : <nl> + return " compression mismatch error "; <nl> + <nl> case BUILD_MSG_ERROR : <nl> return " build message failure "; <nl>  <nl> static void PickHashSigAlgo ( WOLFSSL * ssl , <nl> ssl -> options . cipherSuite = cs1 ; <nl> compression = input [ i ++]; <nl>  <nl> + if ( compression != NO_COMPRESSION && ! ssl -> options . usingCompression ) { <nl> + WOLFSSL_MSG (" Server forcing compression w / o support "); <nl> + return COMPRESSION_ERROR ; <nl> + } <nl> + <nl> if ( compression != ZLIB_COMPRESSION && ssl -> options . usingCompression ) { <nl> WOLFSSL_MSG (" Server refused compression , turning off "); <nl> ssl -> options . usingCompression = 0 ; /* turn off if server refused */ <nl> enum wolfSSL_ErrorCodes { <nl>  <nl> /* begin negotiation parameter errors */ <nl> UNSUPPORTED_SUITE = - 500 , /* unsupported cipher suite */ <nl> - MATCH_SUITE_ERROR = - 501 /* can ' t match cipher suite */ <nl> + MATCH_SUITE_ERROR = - 501 , /* can ' t match cipher suite */ <nl> + COMPRESSION_ERROR = - 502 /* compression mismatch */ <nl> /* end negotiation parameter errors only 10 for now */ <nl> /* add strings to wolfSSL_ERR_reason_error_string in internal . c !!!!! */ <nl> 
mmm src / ssl . c <nl> ppp src / ssl . c <nl> int wolfSSL_EVP_MD_type ( const WOLFSSL_EVP_MD * md ) <nl> return WOLFSSL_FAILURE ; <nl> # endif <nl> } <nl> - <nl> +# ifdef SESSION_CERTS <nl> + ssl -> session . chain . count = 0 ; <nl> +# endif <nl> # ifdef KEEP_PEER_CERT <nl> FreeX509 (& ssl -> peerCert ); <nl> InitX509 (& ssl -> peerCert , 0 , ssl -> heap );
mmm wolfcrypt / benchmark / benchmark . c <nl> ppp wolfcrypt / benchmark / benchmark . c <nl> void bench_chacha ( void ) <nl> void bench_chacha20_poly1305_aead ( void ) <nl> { <nl> double start ; <nl> - int ret , i , count ; <nl> + int ret = 0 , i , count ; <nl>  <nl> byte authTag [ CHACHA20_POLY1305_AEAD_AUTHTAG_SIZE ]; <nl> XMEMSET ( authTag , 0 , sizeof ( authTag ));
mmm wolfcrypt / src / asn . c <nl> ppp wolfcrypt / src / asn . c <nl> static int DecodePolicyOID ( char * out , word32 outSz , byte * in , word32 inSz ) <nl> # endif <nl> } <nl> idx += policy_length ; <nl> - } while (( int ) idx < total_length && cert -> extCertPoliciesNb < MAX_CERTPOL_NB ); <nl> + } while (( int ) idx < total_length <nl> + # if defined ( WOLFSSL_CERT_EXT ) <nl> + && cert -> extCertPoliciesNb < MAX_CERTPOL_NB <nl> + # endif <nl> + ); <nl>  <nl> WOLFSSL_LEAVE (" DecodeCertPolicy ", 0 ); <nl> return 0 ;
mmm src / sniffer . c <nl> ppp src / sniffer . c <nl> static int ProcessServerHello ( const byte * input , int * sslBytes , <nl> XMEMCPY ( session -> sslServer -> arrays . sessionID , input , ID_LEN ); <nl> input += b ; <nl> * sslBytes -= b ; <nl> + if ( b ) <nl> + session -> sslServer -> options . haveSessionId = 1 ; <nl>  <nl> ( void )* input ++; /* eat first byte , always 0 */ <nl> b = * input ++; <nl> static int ProcessServerHello ( const byte * input , int * sslBytes , <nl> session -> sslClient -> options . cipherSuite = b ; <nl> * sslBytes -= SUITE_LEN ; <nl>  <nl> - if ( XMEMCMP ( session -> sslServer -> arrays . sessionID , <nl> - session -> sslClient -> arrays . sessionID , ID_LEN ) == 0 ) { <nl> + if ( session -> sslServer -> options . haveSessionId && <nl> + XMEMCMP ( session -> sslServer -> arrays . sessionID , <nl> + session -> sslClient -> arrays . sessionID , ID_LEN ) == 0 ) { <nl> /* resuming */ <nl> SSL_SESSION * resume = GetSession ( session -> sslServer , <nl> session -> sslServer -> arrays . masterSecret ); <nl> static int DoHandShake ( const byte * input , int * sslBytes , <nl> ret = DoFinished ( ssl , input , & inOutIdx , SNIFF ); <nl>  <nl> if ( ret == 0 && session -> flags . cached == 0 ) { <nl> + session -> sslServer -> options . haveSessionId = 1 ; <nl> AddSession ( session -> sslServer ); <nl> session -> flags . cached = 1 ; <nl> }
mmm src / internal . c <nl> ppp src / internal . c <nl> int SetCipherList ( Suites * s , const char * list ) <nl> byte b ; <nl> byte compression ; <nl> ProtocolVersion pv ; <nl> - word16 extSz ; <nl> word32 i = * inOutIdx ; <nl> word32 begin = i ; <nl> 
mmm ctaocrypt / src / asn . c <nl> ppp ctaocrypt / src / asn . c <nl> static int SetMyVersion ( word32 version , byte * output , int header ) <nl> } <nl>  <nl>  <nl> +/* convert der buffer to pem into output , can ' t do inplace , der and output <nl> + need to be different */ <nl> int DerToPem ( const byte * der , word32 derSz , byte * output , word32 outSz , <nl> int type ) <nl> { <nl> int DerToPem ( const byte * der , word32 derSz , byte * output , word32 outSz , <nl> int err ; <nl> int outLen ; /* return length or error */ <nl>  <nl> + if ( der == output ) /* no in place conversion */ <nl> + return BAD_FUNC_ARG ; <nl> + <nl> if ( type == CERT_TYPE ) { <nl> XSTRNCPY ( header , "----- BEGIN CERTIFICATE -----\ n ", sizeof ( header )); <nl> XSTRNCPY ( footer , "----- END CERTIFICATE -----\ n ", sizeof ( footer ));
mmm wolfcrypt / user - crypto / include / user_rsa . h <nl> ppp wolfcrypt / user - crypto / include / user_rsa . h <nl> WOLFSSL_API int wc_RsaPublicKeyDecodeRaw ( const byte * n , word32 nSz , <nl> # endif <nl> WOLFSSL_API int wc_RsaFlattenPublicKey ( RsaKey *, byte *, word32 *, byte *, <nl> word32 *); <nl> + WOLFSSL_API int wc_RsaSetRNG ( RsaKey * key , WC_RNG * rng ); <nl>  <nl>  <nl> # if defined ( WOLFSSL_CERT_GEN ) || defined ( WOLFSSL_KEY_GEN )mmm wolfcrypt / user - crypto / src / rsa . c <nl> ppp wolfcrypt / user - crypto / src / rsa . c <nl> WOLFSSL_API int wc_RsaPublicKeyDecodeRaw ( const byte * n , word32 nSz , <nl> # endif <nl> WOLFSSL_API int wc_RsaFlattenPublicKey ( RsaKey *, byte *, word32 *, byte *, <nl> word32 *); <nl> + WOLFSSL_API int wc_RsaSetRNG ( RsaKey * key , WC_RNG * rng ); <nl>  <nl>  <nl> # if defined ( WOLFSSL_CERT_GEN ) || defined ( WOLFSSL_KEY_GEN ) <nl> int wc_RsaKeyToPublicDer ( RsaKey * key , byte * output , word32 inLen ) <nl>  <nl> # endif /* WOLFSSL_KEY_GEN */ <nl>  <nl> +# ifdef WC_RSA_BLINDING <nl> + <nl> + int wc_RsaSetRNG ( RsaKey * key , WC_RNG * rng ) <nl> +{ <nl> + if ( key == NULL ) <nl> + return BAD_FUNC_ARG ; <nl> + <nl> + ( void ) rng ; <nl> + <nl> + return 0 ; <nl> +} <nl> + <nl> +# endif /* WC_RSA_BLINDING */ <nl> + <nl> # endif /* NO_RSA */ <nl> 
mmm src / ssl . c <nl> ppp src / ssl . c <nl> int wolfSSL_BN_hex2bn ( WOLFSSL_BIGNUM ** bn , const char * str ) <nl> # else <nl> byte decoded [ 1024 ]; <nl> # endif <nl> + int weOwn = 0 ; <nl>  <nl> WOLFSSL_MSG (" wolfSSL_BN_hex2bn "); <nl>  <nl> int wolfSSL_BN_hex2bn ( WOLFSSL_BIGNUM ** bn , const char * str ) <nl> else if ( bn == NULL ) <nl> ret = decSz ; <nl> else { <nl> - if (* bn == NULL ) <nl> + if (* bn == NULL ) { <nl> * bn = wolfSSL_BN_new (); <nl> + if (* bn != NULL ) { <nl> + weOwn = 1 ; <nl> + } <nl> + } <nl>  <nl> if (* bn == NULL ) <nl> WOLFSSL_MSG (" BN new failed "); <nl> else if ( wolfSSL_BN_bin2bn ( decoded , decSz , * bn ) == NULL ) { <nl> WOLFSSL_MSG (" Bad bin2bn error "); <nl> - wolfSSL_BN_free (* bn ); /* Free new BN */ <nl> + if ( weOwn == 1 ) { <nl> + wolfSSL_BN_free (* bn ); /* Free new BN */ <nl> + } <nl> } <nl> else <nl> ret = WOLFSSL_SUCCESS ;
mmm wolfcrypt / src / error . c <nl> ppp wolfcrypt / src / error . c <nl> const char * wc_GetErrorString ( int error ) <nl> case ASN_COUNTRY_SIZE_E : <nl> return " Country code size error , either too small or large "; <nl>  <nl> + case MISSING_RNG_E : <nl> + return " RNG required but not provided "; <nl> + <nl> default : <nl> return " unknown error number "; <nl> mmm wolfcrypt / src / rsa . c <nl> ppp wolfcrypt / src / rsa . c <nl> const char * wc_GetErrorString ( int error ) <nl> case ASN_COUNTRY_SIZE_E : <nl> return " Country code size error , either too small or large "; <nl>  <nl> + case MISSING_RNG_E : <nl> + return " RNG required but not provided "; <nl> + <nl> default : <nl> return " unknown error number "; <nl>  <nl> static int mp_rand ( mp_int * a , int digits , WC_RNG * rng ) <nl> int ret ; <nl> mp_digit d ; <nl>  <nl> - if ( a == NULL || rng == NULL ) <nl> + if ( rng == NULL ) <nl> + return MISSING_RNG_E ; <nl> + <nl> + if ( a == NULL ) <nl> return BAD_FUNC_ARG ; <nl>  <nl> mp_zero ( a );mmm wolfssl / wolfcrypt / error - crypt . h <nl> ppp wolfssl / wolfcrypt / error - crypt . h <nl> const char * wc_GetErrorString ( int error ) <nl> case ASN_COUNTRY_SIZE_E : <nl> return " Country code size error , either too small or large "; <nl>  <nl> + case MISSING_RNG_E : <nl> + return " RNG required but not provided "; <nl> + <nl> default : <nl> return " unknown error number "; <nl>  <nl> static int mp_rand ( mp_int * a , int digits , WC_RNG * rng ) <nl> int ret ; <nl> mp_digit d ; <nl>  <nl> - if ( a == NULL || rng == NULL ) <nl> + if ( rng == NULL ) <nl> + return MISSING_RNG_E ; <nl> + <nl> + if ( a == NULL ) <nl> return BAD_FUNC_ARG ; <nl>  <nl> mp_zero ( a ); <nl> enum { <nl>  <nl> WC_KEY_SIZE_E = - 234 , /* Key size error , either too small or large */ <nl> ASN_COUNTRY_SIZE_E = - 235 , /* ASN Cert Gen , invalid country code size */ <nl> + MISSING_RNG_E = - 236 , /* RNG required but not provided */ <nl>  <nl> MIN_CODE_E = - 300 /* errors - 101 - - 299 */ <nl> 
mmm wolfcrypt / src / asn . c <nl> ppp wolfcrypt / src / asn . c <nl> int ParseCertRelative ( DecodedCert * cert , int type , int verify , void * cm ) <nl> } <nl>  <nl> # ifdef HAVE_OCSP <nl> - /* trust for the lifetime of the responder ' s cert */ <nl> - if ( cert -> ocspNoCheckSet && verify == VERIFY_OCSP ) <nl> - verify = NO_VERIFY ; <nl> + if ( verify == VERIFY_OCSP_CERT ) { <nl> + /* trust for the lifetime of the responder ' s cert */ <nl> + if ( cert -> ocspNoCheckSet ) <nl> + verify = VERIFY ; <nl> + else <nl> + verify = VERIFY_OCSP ; <nl> + } <nl> # endif <nl> /* advance past extensions */ <nl> cert -> srcIdx = cert -> sigIndex ; <nl> static int DecodeBasicOcspResponse ( byte * source , word32 * ioIndex , <nl>  <nl> /* Don ' t verify if we don ' t have access to Cert Manager . */ <nl> ret = ParseCertRelative (& cert , CERT_TYPE , <nl> - noVerify ? NO_VERIFY : VERIFY_OCSP , cm ); <nl> + noVerify ? NO_VERIFY : VERIFY_OCSP_CERT , cm ); <nl> if ( ret < 0 ) { <nl> WOLFSSL_MSG ("\ tOCSP Responder certificate parsing failed "); <nl> FreeDecodedCert (& cert );mmm wolfssl / wolfcrypt / asn . h <nl> ppp wolfssl / wolfcrypt / asn . h <nl> int ParseCertRelative ( DecodedCert * cert , int type , int verify , void * cm ) <nl> } <nl>  <nl> # ifdef HAVE_OCSP <nl> - /* trust for the lifetime of the responder ' s cert */ <nl> - if ( cert -> ocspNoCheckSet && verify == VERIFY_OCSP ) <nl> - verify = NO_VERIFY ; <nl> + if ( verify == VERIFY_OCSP_CERT ) { <nl> + /* trust for the lifetime of the responder ' s cert */ <nl> + if ( cert -> ocspNoCheckSet ) <nl> + verify = VERIFY ; <nl> + else <nl> + verify = VERIFY_OCSP ; <nl> + } <nl> # endif <nl> /* advance past extensions */ <nl> cert -> srcIdx = cert -> sigIndex ; <nl> static int DecodeBasicOcspResponse ( byte * source , word32 * ioIndex , <nl>  <nl> /* Don ' t verify if we don ' t have access to Cert Manager . */ <nl> ret = ParseCertRelative (& cert , CERT_TYPE , <nl> - noVerify ? NO_VERIFY : VERIFY_OCSP , cm ); <nl> + noVerify ? NO_VERIFY : VERIFY_OCSP_CERT , cm ); <nl> if ( ret < 0 ) { <nl> WOLFSSL_MSG ("\ tOCSP Responder certificate parsing failed "); <nl> FreeDecodedCert (& cert ); <nl> enum VerifyType { <nl> VERIFY_OCSP = 3 , <nl> VERIFY_NAME = 4 , <nl> VERIFY_SKIP_DATE = 5 , <nl> + VERIFY_OCSP_CERT = 6 , <nl> }; <nl>  <nl> # ifdef WOLFSSL_CERT_EXT
mmm src / internal . c <nl> ppp src / internal . c <nl> void PickHashSigAlgo ( WOLFSSL * ssl , const byte * hashSigAlgo , <nl>  <nl> PickHashSigAlgo ( ssl , input + * inOutIdx , len ); <nl> * inOutIdx += len ; <nl> + # ifdef WC_RSA_PSS <nl> + ssl -> pssAlgo = 0 ; <nl> + if ( ssl -> suites -> sigAlgo == rsa_pss_sa_algo ) <nl> + ssl -> pssAlgo |= 1 << ssl -> suites -> hashAlgo ; <nl> + # endif <nl> } <nl>  <nl> /* authorities */ <nl> int SendCertificateVerify ( WOLFSSL * ssl ) <nl> if ( ssl -> hsType == DYNAMIC_TYPE_RSA ) { <nl> # ifdef WC_RSA_PSS <nl> if ( IsAtLeastTLSv1_2 ( ssl ) && <nl> - ( ssl -> pssAlgo | ( 1 << ssl -> suites -> hashAlgo ))) { <nl> + ( ssl -> pssAlgo & ( 1 << ssl -> suites -> hashAlgo ))) { <nl> args -> sigAlgo = rsa_pss_sa_algo ; <nl> } <nl> elsemmm src / tls13 . c <nl> ppp src / tls13 . c <nl> void PickHashSigAlgo ( WOLFSSL * ssl , const byte * hashSigAlgo , <nl>  <nl> PickHashSigAlgo ( ssl , input + * inOutIdx , len ); <nl> * inOutIdx += len ; <nl> + # ifdef WC_RSA_PSS <nl> + ssl -> pssAlgo = 0 ; <nl> + if ( ssl -> suites -> sigAlgo == rsa_pss_sa_algo ) <nl> + ssl -> pssAlgo |= 1 << ssl -> suites -> hashAlgo ; <nl> + # endif <nl> } <nl>  <nl> /* authorities */ <nl> int SendCertificateVerify ( WOLFSSL * ssl ) <nl> if ( ssl -> hsType == DYNAMIC_TYPE_RSA ) { <nl> # ifdef WC_RSA_PSS <nl> if ( IsAtLeastTLSv1_2 ( ssl ) && <nl> - ( ssl -> pssAlgo | ( 1 << ssl -> suites -> hashAlgo ))) { <nl> + ( ssl -> pssAlgo & ( 1 << ssl -> suites -> hashAlgo ))) { <nl> args -> sigAlgo = rsa_pss_sa_algo ; <nl> } <nl> else <nl> int SendTls13CertificateVerify ( WOLFSSL * ssl ) <nl> /* Add signature algorithm . */ <nl> if ( ssl -> hsType == DYNAMIC_TYPE_RSA ) { <nl> # ifdef WC_RSA_PSS <nl> - if ( ssl -> pssAlgo | ( 1 << ssl -> suites -> hashAlgo )) <nl> + if ( ssl -> pssAlgo & ( 1 << ssl -> suites -> hashAlgo )) <nl> args -> sigAlgo = rsa_pss_sa_algo ; <nl> else <nl> # endif
mmm xbmc / cores / paplayer / VideoPlayerCodec . cpp <nl> ppp xbmc / cores / paplayer / VideoPlayerCodec . cpp <nl> bool VideoPlayerCodec :: Init ( const CFileItem & file , unsigned int filecache ) <nl> // we have to decode initial data in order to get channels / samplerate <nl> // for sanity - we read no more than 10 packets <nl> int nErrors = 0 ; <nl> - for ( int nPacket = 0 ; nPacket < 10 && ( m_channels == 0 || m_format . m_sampleRate == 0 ); nPacket ++) <nl> + for ( int nPacket = 0 ; <nl> + nPacket < 10 && ( m_channels == 0 || m_format . m_sampleRate == 0 || m_format . m_frameSize == 0 ); <nl> + nPacket ++) <nl> { <nl> uint8_t dummy [ 256 ]; <nl> size_t nSize = 256 ; <nl> bool VideoPlayerCodec :: Init ( const CFileItem & file , unsigned int filecache ) <nl> if ( NeedConvert ( m_srcFormat . m_dataFormat )) <nl> { <nl> m_needConvert = true ; <nl> + // if we don ' t know the framesize yet , we will fail when converting <nl> + if ( m_srcFormat . m_frameSize == 0 ) <nl> + return false ; <nl> + <nl> m_pResampler = ActiveAE :: CAEResampleFactory :: Create (); <nl>  <nl> SampleConfig dstConfig , srcConfig ;
mmm thunar / thunar - transfer - job . c <nl> ppp thunar / thunar - transfer - job . c <nl> thunar_transfer_job_copy_node ( ThunarTransferJob * job , <nl> } <nl>  <nl> /* update progress information */ <nl> - exo_job_info_message ( EXO_JOB ( job ), g_file_info_get_display_name ( info )); <nl> + exo_job_info_message ( EXO_JOB ( job ), "% s ", g_file_info_get_display_name ( info )); <nl>  <nl> retry_copy : <nl> /* copy the item specified by this node ( not recursively ) */
mmm src / opusfile . c <nl> ppp src / opusfile . c <nl> static int op_get_data ( OggOpusFile * _of , int _nbytes ){ <nl> int nbytes ; <nl> OP_ASSERT ( _nbytes > 0 ); <nl> buffer =( unsigned char *) ogg_sync_buffer (& _of -> oy , _nbytes ); <nl> + if ( OP_UNLIKELY ( buffer == NULL )) return OP_EFAULT ; <nl> nbytes =( int )(* _of -> callbacks . read )( _of -> stream , buffer , _nbytes ); <nl> OP_ASSERT ( nbytes <= _nbytes ); <nl> if ( OP_LIKELY ( nbytes > 0 )) ogg_sync_wrote (& _of -> oy , nbytes ); <nl> static int op_open1 ( OggOpusFile * _of , <nl> if ( _initial_bytes > 0 ){ <nl> char * buffer ; <nl> buffer = ogg_sync_buffer (& _of -> oy ,( long ) _initial_bytes ); <nl> + if ( OP_UNLIKELY ( buffer == NULL )) return OP_EFAULT ; <nl> memcpy ( buffer , _initial_data , _initial_bytes * sizeof (* buffer )); <nl> ogg_sync_wrote (& _of -> oy ,( long ) _initial_bytes ); <nl> }
mmm src / maprules . c <nl> ppp src / maprules . c <nl> XkbRF_SubstituteVars ( char * name , XkbRF_MultiDefsPtr mdefs ) <nl> int len , ndx ; <nl>  <nl> orig = name ; <nl> - str = index ( name ,'%'); <nl> + str = strchr ( name ,'%'); <nl> if ( str == NULL ) <nl> return name ; <nl> len = strlen ( name ); <nl> XkbRF_SubstituteVars ( char * name , XkbRF_MultiDefsPtr mdefs ) <nl> var = str + 1 ; <nl> str = get_index ( var + 1 , & ndx ); <nl> if ( ndx == - 1 ) { <nl> - str = index ( str ,'%'); <nl> + str = strchr ( str ,'%'); <nl> continue ; <nl> } <nl> if ((* var ==' l ') && mdefs -> layout [ ndx ] && * mdefs -> layout [ ndx ]) <nl> XkbRF_SubstituteVars ( char * name , XkbRF_MultiDefsPtr mdefs ) <nl> if (( pfx =='(')&&(* str ==')')) { <nl> str ++; <nl> } <nl> - str = index (& str [ 0 ],'%'); <nl> + str = strchr (& str [ 0 ],'%'); <nl> } <nl> name = ( char *) malloc ( len + 1 ); <nl> str = orig ;
mmm src / xkbcomp / expr . c <nl> ppp src / xkbcomp / expr . c <nl> ExprResolveBoolean ( struct xkb_context * ctx , const ExprDef * expr , <nl>  <nl> case EXPR_INVERT : <nl> case EXPR_NOT : <nl> - ok = ExprResolveBoolean ( ctx , expr , set_rtrn ); <nl> + ok = ExprResolveBoolean ( ctx , expr -> unary . child , set_rtrn ); <nl> if ( ok ) <nl> * set_rtrn = !* set_rtrn ; <nl> return ok ;
mmm src / xkbcomp / expr . c <nl> ppp src / xkbcomp / expr . c <nl> ExprResolveLhs ( struct xkb_context * ctx , const ExprDef * expr , <nl> * elem_rtrn = NULL ; <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> ident . ident ); <nl> * index_rtrn = NULL ; <nl> - return true ; <nl> + return (* field_rtrn != NULL ); <nl> case EXPR_FIELD_REF : <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> field_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> field_ref . field );
mmm src / xkbcomp / expr . c <nl> ppp src / xkbcomp / expr . c <nl> LookupModMask ( struct xkb_context * ctx , const void * priv , xkb_atom_t field , <nl> return false ; <nl>  <nl> str = xkb_atom_text ( ctx , field ); <nl> + if (! str ) <nl> + return false ; <nl>  <nl> if ( istreq ( str , " all ")) { <nl> * val_rtrn = MOD_REAL_MASK_ALL ;
mmm src / utils . c <nl> ppp src / utils . c <nl> bool <nl> map_file ( FILE * file , char ** string_out , size_t * size_out ) <nl> { <nl> struct stat stat_buf ; <nl> - const int fd = fileno ( file ); <nl> + int fd ; <nl> char * string ; <nl>  <nl> /* Make sure to keep the errno on failure ! */ <nl> + fd = fileno ( file ); <nl> + if ( fd < 0 ) <nl> + return false ; <nl>  <nl> if ( fstat ( fd , & stat_buf ) != 0 ) <nl> return false ;
mmm src / compose / parser . c <nl> ppp src / compose / parser . c <nl> skip_more_whitespace_and_comments : <nl>  <nl> /* LHS Keysym . */ <nl> if ( chr ( s , '<')) { <nl> - while ( peek ( s ) != '>' && ! eol ( s )) <nl> + while ( peek ( s ) != '>' && ! eol ( s ) && ! eof ( s )) <nl> buf_append ( s , next ( s )); <nl> if (! chr ( s , '>')) { <nl> scanner_err ( s , " unterminated keysym literal ");
mmm src / xkbcomp / keymap . c <nl> ppp src / xkbcomp / keymap . c <nl> CompileKeymap ( XkbFile * file , struct xkb_keymap * keymap , enum merge_mode merge ) <nl> file = ( XkbFile *) file -> common . next ) { <nl> if ( file -> file_type < FIRST_KEYMAP_FILE_TYPE || <nl> file -> file_type > LAST_KEYMAP_FILE_TYPE ) { <nl> - log_err ( ctx , " Cannot define % s in a keymap file \ n ", <nl> - xkb_file_type_to_string ( file -> file_type )); <nl> + if ( file -> file_type == FILE_TYPE_GEOMETRY ) { <nl> + log_vrb ( ctx , 1 , <nl> + " Geometry sections are not supported ; ignoring \ n "); <nl> + } else { <nl> + log_err ( ctx , " Cannot define % s in a keymap file \ n ", <nl> + xkb_file_type_to_string ( file -> file_type )); <nl> + } <nl> continue ; <nl> } <nl> 
mmm src / xkbcomp / compat . c <nl> ppp src / xkbcomp / compat . c <nl> ResolveStateAndPredicate ( ExprDef * expr , enum xkb_match_operation * pred_rtrn , <nl> * pred_rtrn = MATCH_EXACTLY ; <nl> if ( expr -> expr . op == EXPR_ACTION_DECL ) { <nl> const char * pred_txt = xkb_atom_text ( info -> ctx , expr -> action . name ); <nl> - if (! LookupString ( symInterpretMatchMaskNames , pred_txt , pred_rtrn )) { <nl> + if (! LookupString ( symInterpretMatchMaskNames , pred_txt , pred_rtrn ) || <nl> + ! expr -> action . args ) { <nl> log_err ( info -> ctx , <nl> " Illegal modifier predicate \"% s \"; Ignored \ n ", pred_txt ); <nl> return false ;
mmm src / xkbcomp / keycodes . c <nl> ppp src / xkbcomp / keycodes . c <nl> CopyKeyAliasesToKeymap ( struct xkb_keymap * keymap , KeyNamesInfo * info ) <nl> key_aliases = calloc ( num_key_aliases , sizeof (* key_aliases )); <nl> if (! key_aliases ) <nl> return false ; <nl> - } <nl>  <nl> - i = 0 ; <nl> - darray_foreach ( alias , info -> aliases ) { <nl> - if ( alias -> real != XKB_ATOM_NONE ) { <nl> - key_aliases [ i ]. alias = alias -> alias ; <nl> - key_aliases [ i ]. real = alias -> real ; <nl> - i ++; <nl> + i = 0 ; <nl> + darray_foreach ( alias , info -> aliases ) { <nl> + if ( alias -> real != XKB_ATOM_NONE ) { <nl> + key_aliases [ i ]. alias = alias -> alias ; <nl> + key_aliases [ i ]. real = alias -> real ; <nl> + i ++; <nl> + } <nl> } <nl> } <nl> 
mmm src / xkbcomp / expr . c <nl> ppp src / xkbcomp / expr . c <nl> ExprResolveLhs ( struct xkb_context * ctx , const ExprDef * expr , <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> field_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> field_ref . field ); <nl> * index_rtrn = NULL ; <nl> - return true ; <nl> + return (* elem_rtrn != NULL && * field_rtrn != NULL ); <nl> case EXPR_ARRAY_REF : <nl> * elem_rtrn = xkb_atom_text ( ctx , expr -> array_ref . element ); <nl> * field_rtrn = xkb_atom_text ( ctx , expr -> array_ref . field ); <nl> * index_rtrn = expr -> array_ref . entry ; <nl> + if ( expr -> array_ref . element != XKB_ATOM_NONE && * elem_rtrn == NULL ) <nl> + return false ; <nl> + if (* field_rtrn == NULL ) <nl> + return false ; <nl> return true ; <nl> default : <nl> break ;
mmm src / xkbcomp / ast - build . c <nl> ppp src / xkbcomp / ast - build . c <nl> ExprAppendMultiKeysymList ( ExprDef * expr , ExprDef * append ) <nl> darray_append ( expr -> keysym_list . symsNumEntries , numEntries ); <nl> darray_concat ( expr -> keysym_list . syms , append -> keysym_list . syms ); <nl>  <nl> - FreeStmt (( ParseCommon *) & append ); <nl> + FreeStmt (( ParseCommon *) append ); <nl>  <nl> return expr ; <nl> }
mmm src / x11 / keymap . c <nl> ppp src / x11 / keymap . c <nl> get_controls ( struct xkb_keymap * keymap , xcb_connection_t * conn , <nl> xcb_xkb_get_controls_reply ( conn , cookie , NULL ); <nl>  <nl> FAIL_IF_BAD_REPLY ( reply , " XkbGetControls "); <nl> + FAIL_UNLESS ( reply -> numGroups > 0 && reply -> numGroups <= 4 ); <nl>  <nl> keymap -> enabled_ctrls = translate_controls_mask ( reply -> enabledControls ); <nl> keymap -> num_groups = reply -> numGroups ;
mmm src / xkbcomp / compat . c <nl> ppp src / xkbcomp / compat . c <nl> typedef struct _GroupCompatInfo <nl> { <nl> unsigned char fileID ; <nl> unsigned char merge ; <nl> + Bool defined ; <nl> unsigned char real_mods ; <nl> xkb_atom_t vmods ; <nl> } GroupCompatInfo ; <nl> AddGroupCompat ( CompatInfo * info , unsigned group , GroupCompatInfo * newGC ) <nl> ACTION (" Using % s definition \ n ", <nl> ( merge == MergeAugment ? " old " : " new ")); <nl> } <nl> - if ( merge != MergeAugment ) <nl> + if ( newGC -> defined && ( merge != MergeAugment || ! gc -> defined )) <nl> * gc = * newGC ; <nl> return True ; <nl> } <nl> HandleGroupCompatDef ( GroupCompatDef * def , <nl> } <nl> tmp . real_mods = val . uval & 0xff ; <nl> tmp . vmods = ( val . uval >> 8 ) & 0xffff ; <nl> + tmp . defined = True ; <nl> return AddGroupCompat ( info , def -> group - 1 , & tmp ); <nl> } <nl> 
mmm src / Xrd / XrdConfig . cc <nl> ppp src / Xrd / XrdConfig . cc <nl> int XrdConfig :: Configure ( int argc , char ** argv ) <nl> if (* dfltProt != '.' ) <nl> { char * p = dfltProt ; <nl> while (* p && * p != '.') p ++; <nl> - if (* p == '.') {* p = '\ 0 '; dfltProt = strdup ( dfltProt ); * p = ',';} <nl> + if (* p == '.') {* p = '\ 0 '; dfltProt = strdup ( dfltProt ); * p = '.';} <nl> } <nl>  <nl> // Process the options
mmm src / XrdCl / XrdClPlugInManager . cc <nl> ppp src / XrdCl / XrdClPlugInManager . cc <nl> namespace XrdCl <nl>  <nl> XrdSysPwd pwdHandler ; <nl> passwd * pwd = pwdHandler . Get ( getuid () ); <nl> + if ( ! pwd ) return ; <nl> std :: string userPlugIns = pwd -> pw_dir ; <nl> userPlugIns += "/. xrootd / client . plugins . d "; <nl> ProcessConfigDir ( userPlugIns );
mmm src / XrdOdc / XrdOdcFinder . cc <nl> ppp src / XrdOdc / XrdOdcFinder . cc <nl> int XrdOdcFinderRMT :: Locate ( XrdOucErrInfo & Resp , const char * path , int flags , <nl> { xmsg [ 1 ]. iov_base = ( char *)" select " ; xmsg [ 1 ]. iov_len = 7 ;} <nl> xmsg [ 2 ]. iov_base = ( char *) ptype ; xmsg [ 2 ]. iov_len = 2 ; <nl> if ( Avoid ) <nl> - { xmsg [ 3 ]. iov_base = ( char *)" -"; xmsg [ 3 ]. iov_len = 2 ; <nl> + { xmsg [ 3 ]. iov_base = ( char *)"-"; xmsg [ 3 ]. iov_len = 1 ; <nl> xmsg [ 4 ]. iov_base = Avoid ; xmsg [ 4 ]. iov_len = strlen ( Avoid ); <nl> xmsg [ 5 ]. iov_base = ( char *)" "; xmsg [ 5 ]. iov_len = 1 ; <nl> ioveol = 6 ;
mmm src / XrdCl / XrdClXRootDMsgHandler . cc <nl> ppp src / XrdCl / XrdClXRootDMsgHandler . cc <nl> namespace XrdCl <nl> XRDCL_SMART_PTR_T < Message > msgPtr ( pResponse ); <nl> pResponse = 0 ; <nl>  <nl> - if ( rsp -> hdr . dlen < 4 ) <nl> + if ( rsp -> hdr . dlen <= 4 ) <nl> { <nl> log -> Error ( XRootDMsg , "[% s ] Got invalid redirect response .", <nl> pUrl . GetHostId (). c_str () );
mmm src / XrdClient / XrdClientUrlSet . cc <nl> ppp src / XrdClient / XrdClientUrlSet . cc <nl> void XrdClientUrlSet :: ConvertDNSAlias ( UrlArray & urls , XrdClientString proto , <nl> // Notify <nl> Info ( XrdClientDebug :: kHIDEBUG , " ConvertDNSAlias ", <nl> " found host " << newurl -> Host << " with addr " << newurl -> HostAddr ); <nl> + <nl> + // Get a copy , if we need to store another <nl> + if ( i < ( naddr - 1 )) <nl> + newurl = new XrdClientUrlInfo (* newurl ); <nl> + <nl> } <nl> }
mmm src / XrdSut / XrdSutAux . cc <nl> ppp src / XrdSut / XrdSutAux . cc <nl> int XrdSutGetPass ( const char * prompt , XrdOucString & passwd ) <nl>  <nl> char * pw = getpass ( prompt ); <nl> if ( pw ) { <nl> - if ( pw [ strlen ( pw )- 1 ] == '\ n ') <nl> - pw [ strlen ( pw ) - 1 ] = 0 ; // get rid of \ n <nl> + // Get rid of special chars , if any <nl> + int k = 0 , i = 0 , len = strlen ( pw ); <nl> + for (; i < len ; i ++) <nl> + if ( pw [ i ] > 0x20 ) pw [ k ++] = pw [ i ]; <nl> + pw [ k ] = 0 ; <nl> passwd = pw ; <nl> - XrdSutMemSet (( volatile void *) pw , 0 , strlen ( pw )); <nl> + XrdSutMemSet (( volatile void *) pw , 0 , len ); <nl> } else { <nl> DEBUG (" error from getpass "); <nl> return - 1 ;
mmm src / XrdHttp / XrdHttpReq . cc <nl> ppp src / XrdHttp / XrdHttpReq . cc <nl> int XrdHttpReq :: ProcessHTTPReq () { <nl> } else { <nl>  <nl> // We lookup the requested path in a hash containing the preread files <nl> - XrdHttpProtocol :: StaticPreloadInfo * mydata = prot -> staticpreload -> Find ( resource . c_str ()); <nl> - if ( mydata ) { <nl> + if ( prot -> staticpreload ) { <nl> + XrdHttpProtocol :: StaticPreloadInfo * mydata = prot -> staticpreload -> Find ( resource . c_str ()); <nl> + if ( mydata ) { <nl> prot -> SendSimpleResp ( 200 , NULL , NULL , ( char *) mydata -> data , mydata -> len ); <nl> reset (); <nl> return 1 ; <nl> } <nl> + } <nl> + <nl> } <nl>  <nl> 
mmm src / XrdApps / XrdCpConfig . cc <nl> ppp src / XrdApps / XrdCpConfig . cc <nl> do { while ( optind < Argc && Legacy ( optind )) {} <nl> switch ( opC ) <nl> { case OpCksum : defCks ( optarg ); <nl> break ; <nl> + case OpCoerce : OpSpec |= DoCoerce ; <nl> + break ; <nl> case OpDebug : OpSpec |= DoDebug ; <nl> if (! a2i ( optarg , & Dlvl , 0 , 3 )) Usage ( 22 ); <nl> break ;
mmm src / XrdOuc / XrdOuca2x . cc <nl> ppp src / XrdOuc / XrdOuca2x . cc <nl> int XrdOuca2x :: a2sz ( XrdSysError & Eroute , const char * emsg , const char * item , <nl> else if (* fP == ' t ' || * fP == ' T ') qmult = 1024LL * 1024LL * 1024LL * 1024LL ; <nl> else { qmult = 1 ; fP ++;} <nl> errno = 0 ; <nl> - * val = strtoll ( item , & eP , 10 ) * qmult ; <nl> + double dval = strtod ( item , & eP ) * qmult ; <nl> if ( errno || eP != fP ) <nl> { Eroute . Emsg (" a2x ", emsg , item , " is not a number "); <nl> return - 1 ; <nl> } <nl> + * val = ( long long ) dval ; <nl> if (* val < minv ) <nl> return Emsg ( Eroute , emsg , item , " may not be less than % lld ", minv ); <nl> if ( maxv >= 0 && * val > maxv )
mmm src / XrdSsi / XrdSsiUtils . cc <nl> ppp src / XrdSsi / XrdSsiUtils . cc <nl> void DoIt () { myMutex . Lock (); <nl> virtual void Finished ( XrdSsiRequest & rqstR , <nl> const XrdSsiRespInfo & rInfo , <nl> bool cancel = false ) <nl> - { myMutex . Lock (); <nl> + { UnBindRequest (); <nl> + myMutex . Lock (); <nl> if (! isActive ) delete this ; <nl> else { isActive = false ; <nl> myMutex . UnLock ();
mmm src / configure . cpp <nl> ppp src / configure . cpp <nl> bool Handler :: xsecretkey ( XrdOucStream & config_obj , XrdSysError * log , std :: string <nl> return false ; <nl> } <nl>  <nl> - FILE * fp = fopen ( val , " r +"); <nl> + FILE * fp = fopen ( val , " rb "); <nl>  <nl> if ( fp == NULL ) { <nl> log -> Emsg (" Config ", " Cannot open shared secret key file '", val , "'");
mmm XrdClFileStateHandler . cc <nl> ppp XrdClFileStateHandler . cc <nl> namespace XrdCl <nl> pFileUrl ( 0 ), <nl> pDataServer ( 0 ), <nl> pLoadBalancer ( 0 ), <nl> + pStateRedirect ( 0 ), <nl> pFileHandle ( 0 ), <nl> pOpenMode ( 0 ), <nl> pOpenFlags ( 0 ),
mmm src / XrdClient / XrdClientConn . cc <nl> ppp src / XrdClient / XrdClientConn . cc <nl> XReqErrorType XrdClientConn :: GoBackToRedirector () { <nl> // redirections . Used typically for stat and similar functions <nl> Disconnect ( false ); <nl> if ( fGlobalRedirCnt ) fGlobalRedirCnt --; <nl> - return GoToAnotherServer (* fLBSUrl ); <nl> + return ( fLBSUrl ? GoToAnotherServer (* fLBSUrl ) : kOK ); <nl> } <nl>  <nl> // _____________________________________________________________________________
mmm src / XrdClient / XrdClient . cc <nl> ppp src / XrdClient / XrdClient . cc <nl> bool XrdClient :: TryOpen ( kXR_unt16 mode , kXR_unt16 options , bool doitparallel ) { <nl>  <nl> // If the open request failed for the error " file not found " proceed , <nl> // otherwise return FALSE <nl> - if ( fConnModule -> LastServerResp . status != kXR_NotFound ) { <nl> + if ( ( fConnModule -> LastServerResp . status != kXR_error ) || <nl> + (( fConnModule -> LastServerResp . status == kXR_error ) && <nl> + ( fConnModule -> LastServerError . errnum != kXR_NotFound )) ){ <nl> + <nl> TerminateOpenAttempt (); <nl>  <nl> return FALSE ; <nl> bool XrdClient :: TryOpen ( kXR_unt16 mode , kXR_unt16 options , bool doitparallel ) { <nl> // from the one we formerly connected , then we resend the request <nl> // specifyng the supposed failing server as opaque info <nl> if ( fConnModule -> GetLBSUrl () && <nl> - ( fConnModule -> GetCurrentUrl (). Host != fConnModule -> GetLBSUrl ()-> Host ) ) { <nl> + ( ( fConnModule -> GetCurrentUrl (). Host != fConnModule -> GetLBSUrl ()-> Host ) || <nl> + ( fConnModule -> GetCurrentUrl (). Port != fConnModule -> GetLBSUrl ()-> Port ) ) ) { <nl> XrdOucString opinfo ; <nl>  <nl> opinfo = "& tried =" + fConnModule -> GetCurrentUrl (). Host ;
mmm src / XrdClient / XrdClientReadV . cc <nl> ppp src / XrdClient / XrdClientReadV . cc <nl> kXR_int32 XrdClientReadV :: UnpackReadVResp ( char * destbuf , char * respdata , kXR_int <nl>  <nl> // I just rebuild the readahead_list element <nl> struct readahead_list header ; <nl> - kXR_int64 pos_from = 0 , pos_to = 0 ; <nl> + kXR_int32 pos_from = 0 , pos_to = 0 ; <nl> int i = 0 ; <nl> - <nl> - int cur_buf_len = 0 , cur_buf_offset = - 1 , cur_buf = 0 ; <nl> + kXR_int64 cur_buf_offset = - 1 ; <nl> + int cur_buf_len = 0 , cur_buf = 0 ; <nl>  <nl> while ( ( pos_from < respdatalen ) && ( i < nbuf ) ) { <nl> memcpy (& header , respdata + pos_from , sizeof ( struct readahead_list ));
mmm src / XrdCl / XrdClURL . cc <nl> ppp src / XrdCl / XrdClURL . cc <nl> namespace XrdCl <nl> // Check if we ' re IPv6 encoded IPv4 <nl> //---------------------------------------------------------------------- <nl> pos = pHostName . find ( "." ); <nl> - if ( pos != std :: string :: npos ) <nl> + size_t pos2 = pHostName . find ( "[:: ffff " ); <nl> + if ( pos != std :: string :: npos && pos2 == std :: string :: npos ) <nl> { <nl> pHostName . erase ( 0 , 3 ); <nl> pHostName . erase ( pHostName . length ()- 1 , 1 );
mmm src / XrdOlb / XrdOlbConfig . cc <nl> ppp src / XrdOlb / XrdOlbConfig . cc <nl> int XrdOlbConfig :: Configure2 () <nl> // <nl> Say . Say ( 0 , myInstance , " phase 2 initialization started ."); <nl>  <nl> +// Readjust the thread parameters as we know how many we will actually need <nl> +// <nl> + Sched -> setParms ( 16 , 256 , 8 , 0 ); <nl> + <nl> // Determine who we are . If we are a manager or supervisor start the file <nl> // location cache scrubber . <nl> //
mmm src / XrdFileCache / XrdFileCacheFactory . cc <nl> ppp src / XrdFileCache / XrdFileCacheFactory . cc <nl> bool Factory :: ConfigParameters ( std :: string part , XrdOucStream & config ) <nl> } <nl> else if ( part == " prefetch " ) <nl> { <nl> - printf (" prefetch enabled !!!!\ n "); <nl> - m_configuration . m_prefetch = true ; <nl> - config . GetWord (); <nl> + int p = :: atoi ( config . GetWord ()); <nl> + if ( p != 0 ) { <nl> + printf (" prefetch enabled !!!!\ n "); <nl> + m_configuration . m_prefetch = true ; <nl> + } <nl> + else { <nl> + m_configuration . m_prefetch = false ; <nl> + } <nl> } <nl> else if ( part == " nram " ) <nl> {
mmm src / XrdCl / XrdClCopyProcess . cc <nl> ppp src / XrdCl / XrdClCopyProcess . cc <nl> namespace XrdCl <nl>  <nl> props . Get ( " source ", tmp ); <nl> URL source = tmp ; <nl> + if ( ! source . IsValid () ) <nl> + return XRootDStatus ( stError , errInvalidArgs , 0 , " invalid source " ); <nl> + <nl> props . Get ( " target ", tmp ); <nl> URL target = tmp ; <nl> + if ( ! target . IsValid () ) <nl> + return XRootDStatus ( stError , errInvalidArgs , 0 , " invalid target " ); <nl>  <nl> bool tpc = false ; <nl> bool tpcFallBack = false ;
mmm src / XrdNet / XrdNet . cc <nl> ppp src / XrdNet / XrdNet . cc <nl> int XrdNet :: do_Accept_UDP ( XrdNetPeer & myPeer , int opts ) <nl>  <nl> // Read the message and get the host address <nl> // <nl> - do { dlen = recvfrom ( iofd , ( Sokdata_t ) bp -> data , BuffSize , 0 , & addr ,& addrlen ); <nl> + do { dlen = recvfrom ( iofd ,( Sokdata_t ) bp -> data , BuffSize - 1 , 0 ,& addr ,& addrlen ); <nl> } while ( dlen < 0 && errno == EINTR ); <nl>  <nl> if ( dlen < 0 ) <nl> { eDest -> Emsg (" Receive ", errno , " perform UDP recvfrom ()"); <nl> BuffQ -> Recycle ( bp ); <nl> return 0 ; <nl> - } <nl> + } else bp -> data [ dlen ] = '\ 0 '; <nl>  <nl> // Authorize this connection . We don ' t accept messages that set the <nl> // loopback address since this can be trivially spoofed in UDP packets .
mmm src / XrdClient / Xrdcp . cc <nl> ppp src / XrdClient / Xrdcp . cc <nl> int doCp_xrd2xrd ( XrdClient ** xrddest , const char * src , const char * dst ) { <nl> cout << endl ; <nl> } <nl>  <nl> - if (( unsigned ) cpnfo . len != bytesread ) retvalue = 13 ; <nl> + if ( cpnfo . len != bytesread ) retvalue = 13 ; <nl>  <nl> # ifdef HAVE_XRDCRYPTO <nl> if ( md5 ) MD_5 -> Final (); <nl> int doCp_xrd2loc ( const char * src , const char * dst ) { <nl> cout << endl ; <nl> } <nl>  <nl> - if (( unsigned ) cpnfo . len != bytesread ) retvalue = 13 ; <nl> + if ( cpnfo . len != bytesread ) retvalue = 13 ; <nl>  <nl> # ifdef HAVE_XRDCRYPTO <nl> if ( md5 ) MD_5 -> Final ();
mmm src / XrdXrootd / XrdXrootdMonitor . cc <nl> ppp src / XrdXrootd / XrdXrootdMonitor . cc <nl> XrdXrootdMonitor ::~ XrdXrootdMonitor () <nl>  <nl> void XrdXrootdMonitor :: appID ( char * id ) <nl> { <nl> + static const int apInfoSize = sizeof ( XrdXrootdMonTrace )- 4 ; <nl>  <nl> // Application ID ' s are only meaningful for io event recording <nl> // <nl> void XrdXrootdMonitor :: appID ( char * id ) <nl> if ( lastWindow != currWindow ) Mark (); <nl> else if ( nextEnt == lastEnt ) Flush (); <nl> monBuff -> info [ nextEnt ]. arg0 . id [ 0 ] = XROOTD_MON_APPID ; <nl> - strncpy (( char *)& monBuff -> info [ nextEnt ]. arg0 . id [ 4 ], id , <nl> - sizeof ( XrdXrootdMonTrace )- 4 ); <nl> + strncpy (( char *)(&( monBuff -> info [ nextEnt ])+ 4 ), id , apInfoSize ); <nl> } <nl>  <nl> /******************************************************************************/
mmm src / XrdOuc / XrdOuca2x . cc <nl> ppp src / XrdOuc / XrdOuca2x . cc <nl> int XrdOuca2x :: a2sp ( XrdSysError & Eroute , const char * emsg , const char * item , <nl> if ( maxv < 0 ) maxv = 100 ; <nl>  <nl> if (* val > maxv ) <nl> - { sprintf ( buff , " may not be greater than % d %%", maxv ); <nl> + { sprintf ( buff , " may not be greater than % lld %%", maxv ); <nl> Eroute . Emsg (" a2x ", emsg , item , buff ); <nl> return - 1 ; <nl> } <nl> int XrdOuca2x :: a2sp ( XrdSysError & Eroute , const char * emsg , const char * item , <nl> if ( minv < 0 ) minv = 0 ; <nl>  <nl> if (* val > maxv ) <nl> - { sprintf ( buff , " may not be less than % d %%", minv ); <nl> + { sprintf ( buff , " may not be less than % lld %%", minv ); <nl> Eroute . Emsg (" a2x ", emsg , item , buff ); <nl> return - 1 ; <nl> }mmm src / XrdOuc / XrdOucUtils . cc <nl> ppp src / XrdOuc / XrdOucUtils . cc <nl> int XrdOuca2x :: a2sp ( XrdSysError & Eroute , const char * emsg , const char * item , <nl> if ( maxv < 0 ) maxv = 100 ; <nl>  <nl> if (* val > maxv ) <nl> - { sprintf ( buff , " may not be greater than % d %%", maxv ); <nl> + { sprintf ( buff , " may not be greater than % lld %%", maxv ); <nl> Eroute . Emsg (" a2x ", emsg , item , buff ); <nl> return - 1 ; <nl> } <nl> int XrdOuca2x :: a2sp ( XrdSysError & Eroute , const char * emsg , const char * item , <nl> if ( minv < 0 ) minv = 0 ; <nl>  <nl> if (* val > maxv ) <nl> - { sprintf ( buff , " may not be less than % d %%", minv ); <nl> + { sprintf ( buff , " may not be less than % lld %%", minv ); <nl> Eroute . Emsg (" a2x ", emsg , item , buff ); <nl> return - 1 ; <nl> } <nl> int XrdOucUtils :: fmtBytes ( long long val , char * buff , int bsz ) <nl> static const long long Gval = 1024LL * 1024LL * 1024LL ; <nl> static const long long Tval = 1024LL * 1024LL * 1024LL * 1024LL ; <nl> char sName = ' '; <nl> - int n , resid ; <nl> + int resid ; <nl>  <nl> // Get correct scaling <nl> //
mmm src / XrdHttp / XrdHttpReq . cc <nl> ppp src / XrdHttp / XrdHttpReq . cc <nl> int XrdHttpReq :: PostProcessHTTPReq ( bool final_ ) { <nl>  <nl> } else <nl> for ( int i = 0 ; i < iovN ; i ++) { <nl> - prot -> SendData (( char *) iovP [ i ]. iov_base , iovP [ i ]. iov_len ); <nl> + if ( prot -> SendData (( char *) iovP [ i ]. iov_base , iovP [ i ]. iov_len )) return 1 ; <nl> writtenbytes += iovP [ i ]. iov_len ; <nl> } <nl> 
mmm src / XrdClient / XrdCommandLine . cc <nl> ppp src / XrdClient / XrdCommandLine . cc <nl> int main ( int argc , char ** argv ) { <nl>  <nl> if (! path . length ()) { <nl> cout << " The current path is empty ." << endl ; <nl> - retval = 1 ; <nl> + path = '/'; <nl> } <nl>  <nl> // Now try to issue the request
mmm src / XrdNet / XrdNet . cc <nl> ppp src / XrdNet / XrdNet . cc <nl> int XrdNet :: do_Accept_TCP ( XrdNetAddr & hAddr , int opts ) <nl>  <nl> // Initialize the address of the new connection <nl> // <nl> - hAddr . Set (& IP . Addr , newfd ); <nl> + const char * eMsg = hAddr . Set (& IP . Addr , newfd ); <nl> + if ( eMsg ) <nl> + { char buff [ 256 ]; <nl> + snprintf ( buff , sizeof ( buff ), "% d ;", newfd ); <nl> + eDest -> Emsg (" Accept ", " Failed to identify FD ", buff , eMsg ); <nl> + close ( newfd ); <nl> + return 0 ; <nl> + } <nl>  <nl> // Remove TCP_NODELAY option for unix domain sockets to avoid error message <nl> //
mmm src / XrdSecgsi / XrdSecProtocolgsi . cc <nl> ppp src / XrdSecgsi / XrdSecProtocolgsi . cc <nl> int XrdSecProtocolgsi :: Encrypt ( const char * inbuf , // Data to be encrypted <nl> return - EINVAL ; <nl>  <nl> // Get output buffer <nl> - char * buf = ( char *) malloc ( sessionKey -> EncOutLength ( inlen )); <nl> - if (! buf ) <nl> - return - ENOMEM ; <nl> + int sz = sessionKey -> EncOutLength ( inlen ); <nl> + char * buf = ( char *) malloc ( sz ); <nl> + if (! buf ) return - ENOMEM ; <nl> + memset ( buf , 0 , sz ); <nl>  <nl> // Encrypt <nl> int len = sessionKey -> Encrypt ( inbuf , inlen , buf ); <nl> int XrdSecProtocolgsi :: Decrypt ( const char * inbuf , // Data to be decrypted <nl> return - EINVAL ; <nl>  <nl> // Get output buffer <nl> - char * buf = ( char *) malloc ( sessionKey -> DecOutLength ( inlen )); <nl> - if (! buf ) <nl> - return - ENOMEM ; <nl> + int sz = sessionKey -> DecOutLength ( inlen ); <nl> + char * buf = ( char *) malloc ( sz ); <nl> + if (! buf ) return - ENOMEM ; <nl> + memset ( buf , 0 , sz ); <nl>  <nl> // Decrypt <nl> int len = sessionKey -> Decrypt ( inbuf , inlen , buf );
mmm src / XrdFileCache / XrdFileCachePrefetch . cc <nl> ppp src / XrdFileCache / XrdFileCachePrefetch . cc <nl> ssize_t Prefetch :: ReadInBlocks ( char * buff , off_t off , size_t size ) <nl>  <nl> int Prefetch :: ReadV ( const XrdOucIOVec * readV , int n ) <nl> { <nl> + { <nl> + XrdSysCondVarHelper monitor ( m_stateCond ); <nl> + <nl> + // AMT check if this can be done once during initalization <nl> + if ( m_failed ) return m_input . ReadV ( readV , n ); <nl> + <nl> + if ( ! m_started ) <nl> + { <nl> + m_stateCond . Wait (); <nl> + if ( m_failed ) return 0 ; <nl> + } <nl> + } <nl> + <nl> // check if read sizes are big enough to cache <nl>  <nl> XrdCl :: XRootDStatus Status ;
mmm src / XrdCl / XrdClCopy . cc <nl> ppp src / XrdCl / XrdClCopy . cc <nl> class ProgressDisplay : public XrdCl :: CopyProgressHandler <nl> uint64_t speed = 0 ; <nl> if ( now - d . started ) <nl> speed = d . bytesProcessed /( now - d . started ); <nl> + else <nl> + speed = d . bytesProcessed ; <nl>  <nl> std :: string bar ; <nl> int prog = 0 ;
mmm src / XrdClient / XrdClientAdmin_c . cc <nl> ppp src / XrdClient / XrdClientAdmin_c . cc <nl> extern " C " { <nl> char tok1 [ 256 ], tok2 [ 256 ]; <nl> long v ; <nl>  <nl> - if ( sscanf ((* env )[ it ]. c_str (), "% 256s % d ", tok1 , & v ) == 2 ) { <nl> + if ( sscanf ((* env )[ it ]. c_str (), "% 256s % ld ", tok1 , & v ) == 2 ) { <nl> // It ' s an integer value <nl> EnvPutInt ( tok1 , v ); <nl> // cout << " Env : " << tok1 << " Val =" << EnvGetLong ( tok1 ) << endl ;
mmm src / XrdSecgsi / XrdSecgsiGMAPFunLDAP . cc <nl> ppp src / XrdSecgsi / XrdSecgsiGMAPFunLDAP . cc <nl> XrdVERSIONINFO ( XrdSecgsiGMAPFun , secgsigmap ); <nl> /* */ <nl> /* GMAP function implementation querying a LDAP database */ <nl> /* */ <nl> +/* Warning : this plug - in is not build any longer because the external */ <nl> +/* LDAP query via the popen () represents a potential security threat */ <nl> +/* and it is believed that functionality provided is not actually used . */ <nl> +/* If this believe happens to be uncorrect please report at */ <nl> +/* */ <nl> +/* https :// github . com / xrootd */ <nl> +/* */ <nl> +/* a sanitized version of the plug - in can be provided using a proper library . */ <nl> +/* */ <nl> /* ************************************************************************** */ <nl>  <nl> # include < stdio . h >mmm src / XrdCl / XrdClDefaultEnv . cc <nl> ppp src / XrdCl / XrdClDefaultEnv . cc <nl> XrdVERSIONINFO ( XrdSecgsiGMAPFun , secgsigmap ); <nl> /* */ <nl> /* GMAP function implementation querying a LDAP database */ <nl> /* */ <nl> +/* Warning : this plug - in is not build any longer because the external */ <nl> +/* LDAP query via the popen () represents a potential security threat */ <nl> +/* and it is believed that functionality provided is not actually used . */ <nl> +/* If this believe happens to be uncorrect please report at */ <nl> +/* */ <nl> +/* https :// github . com / xrootd */ <nl> +/* */ <nl> +/* a sanitized version of the plug - in can be provided using a proper library . */ <nl> +/* */ <nl> /* ************************************************************************** */ <nl>  <nl> # include < stdio . h > <nl> namespace XrdCl <nl> " libXrdSecgsi . so ", <nl> " libXrdSecgsiAuthzVO . so ", <nl> " libXrdSecgsiGMAPDN . so ", <nl> - " libXrdSecgsiGMAPLDAP . so ", <nl> " libXrdSecpwd . so ", <nl> " libXrdSecsss . so ", <nl> " libXrdSecunix . so ",
mmm src / XrdNet / XrdNetConnect . cc <nl> ppp src / XrdNet / XrdNetConnect . cc <nl> int XrdNetConnect :: Connect ( int fd , <nl> new_flags = old_flags | O_NDELAY | O_NONBLOCK ; <nl> fcntl ( fd , F_SETFL , new_flags ); <nl> if (! connect ( fd , name , namelen )) myRC = 0 ; <nl> - else if ( EINPROGRESS != errno ) myRC = errno ; <nl> + else if ( EINPROGRESS != net_errno ) myRC = net_errno ; <nl> else { struct pollfd polltab = { fd , POLLOUT | POLLWRNORM , 0 }; <nl> do { myRC = poll (& polltab , 1 , tsec * 1000 );} <nl> while ( myRC < 0 && errno == EINTR );
mmm src / XrdCeph / XrdCephPosix . cc <nl> ppp src / XrdCeph / XrdCephPosix . cc <nl> int ceph_posix_open ( XrdOucEnv * env , const char * pathname , int flags , mode_t mode <nl> // in case of O_TRUNC , we should truncate the file <nl> if ( flags & O_TRUNC ) { <nl> int rc = ceph_posix_internal_truncate ( fr , 0 ); <nl> - if ( rc < 0 ) return rc ; <nl> + // fail only if file exists and cannot be truncated <nl> + if ( rc < 0 && rc != - ENOENT ) return rc ; <nl> } <nl> return g_nextCephFd - 1 ; <nl> }
mmm src / XrdOfs / XrdOfsConfig . cc <nl> ppp src / XrdOfs / XrdOfsConfig . cc <nl> int XrdOfs :: xred ( XrdOucStream & Config , XrdOucError & Eroute ) <nl> if (! ropt ) ropt = XrdOfsREDIRRMT ; <nl> else if ( val ) val = Config . GetWord (); <nl>  <nl> - if ( val && ! strcmp (" if ", val )) <nl> - if (( rc = XrdOucUtils :: doIf (& Eroute , Config , " redirect directive ", <nl> + if ( val ) <nl> + { if ( strcmp (" if ", val )) <nl> + { Config . RetToken (); <nl> + Eroute . Emsg (" Config ", " Warning ! Implied ' if ' on redirect is now deprecated ."); <nl> + } <nl> + if (( rc = XrdOucUtils :: doIf (& Eroute , Config , " redirect directive ", <nl> getenv (" XRDHOST "), getenv (" XRDNAME "))) <= 0 ) <nl> - return ( rc < 0 ); <nl> - <nl> + return ( rc < 0 ); <nl> + } <nl> Options |= ropt ; <nl> return 0 ; <nl> }
mmm src / XrdSys / XrdSysTimer . cc <nl> ppp src / XrdSys / XrdSysTimer . cc <nl> unsigned long XrdSysTimer :: Report ( double & Total_Time ) <nl>  <nl> // Add up the time as a double <nl> // <nl> - Total_Time += ( double ) LastReport . tv_sec + <nl> - (( double )( LastReport . tv_usec / 1000 ))/ 1000 . 0 ; <nl> + Total_Time += static_cast < double >( LastReport . tv_sec ) + <nl> + static_cast < double >( LastReport . tv_usec / 1000 )/ 1000 . 0 ; <nl>  <nl> // Return time <nl> // <nl> unsigned long XrdSysTimer :: Report ( unsigned long & Total_Time ) <nl> { <nl> unsigned long report_time = Report (); <nl>  <nl> -// Add up the time as a 32 - bit value to nearest microsecond ( max = 24 days ) <nl> +// Add up the time as a 32 - bit value to nearest milliseconds ( max = 24 days ) <nl> // <nl> Total_Time += ( unsigned long ) LastReport . tv_sec * 1000 + <nl> ( unsigned long )( LastReport . tv_usec / 1000 ); <nl> unsigned long XrdSysTimer :: Report ( unsigned long long & Total_Time ) <nl> { <nl> unsigned long report_time = Report (); <nl>  <nl> -// Add up the time as a 64 - bit value to nearest microsecond <nl> +// Add up the time as a 64 - bit value to nearest milliseconds <nl> // <nl> Total_Time += ( unsigned long long ) LastReport . tv_sec * 1000 + <nl> ( unsigned long long )( LastReport . tv_usec / 1000 );
mmm src / XrdXrootd / XrdXrootdResponse . cc <nl> ppp src / XrdXrootd / XrdXrootdResponse . cc <nl> int XrdXrootdResponse :: Send ( XResponseType rcode , int info , const char * data ) <nl> kXR_int32 xbuf = static_cast < kXR_int32 >( htonl ( info )); <nl> int dlen ; <nl>  <nl> - TRACES ( RSP , " sending " << dlen <<" data bytes ; status =" << rcode ); <nl> - <nl> RespIO [ 1 ]. iov_base = ( caddr_t )(& xbuf ); <nl> RespIO [ 1 ]. iov_len = sizeof ( xbuf ); <nl> RespIO [ 2 ]. iov_base = ( caddr_t ) data ; <nl> RespIO [ 2 ]. iov_len = dlen = strlen ( data ); <nl>  <nl> + TRACES ( RSP ," sending " <<( sizeof ( xbuf )+ dlen ) <<" data bytes ; status =" << rcode ); <nl> + <nl> if ( Bridge ) <nl> { if ( Bridge -> Send ( rcode , & RespIO [ 1 ], 1 , dlen ) >= 0 ) return 0 ; <nl> return Link -> setEtext (" send failure ");
mmm src / XrdOuc / XrdOucPup . cc <nl> ppp src / XrdOuc / XrdOucPup . cc <nl> int XrdOucPup :: Pack ( struct iovec * iovP , struct iovec * iovE , XrdOucPupArgs * pup , <nl> break ; <nl>  <nl> case PT_int : <nl> - n32 = htons (* Base . B32 ); <nl> + n32 = htonl (* Base . B32 ); <nl> * wP = PT_int ; memcpy ( wP + 1 , & n32 , sizeof ( n32 )); <nl> vP -> iov_base = wP ; vP -> iov_len = Sz32 ; vP ++; <nl> wP += Sz32 ; TotLen += Sz32 ; dlen = sizeof ( n32 ); <nl> break ; <nl>  <nl> case PT_longlong : <nl> - n64 = htons (* Base . B64 ); <nl> + h2nll (* Base . B64 , n64 ); <nl> * wP = PT_longlong ; memcpy ( wP + 1 , & n64 , sizeof ( n64 )); <nl> vP -> iov_base = wP ; vP -> iov_len = Sz64 ; vP ++; <nl> wP += Sz64 ; TotLen += Sz64 ; dlen = sizeof ( n64 );
mmm src / XrdXrootd / XrdXrootdXeq . cc <nl> ppp src / XrdXrootd / XrdXrootdXeq . cc <nl> int XrdXrootdProtocol :: do_Qconf () <nl> // Now determine what the user wants to query <nl> // <nl> if (! strcmp (" readv_ior_max ", val )) <nl> - { n = sprintf ( bp , "% d \ n ", maxTransz - sizeof ( readahead_list )); <nl> + { n = sprintf ( bp , "% d \ n ", maxTransz - ( int ) sizeof ( readahead_list )); <nl> bp += n ; bleft -= n ; <nl> } <nl> else if (! strcmp (" readv_iov_max ", val ))
mmm viostor / virtio_stor_hw_helper . c <nl> ppp viostor / virtio_stor_hw_helper . c <nl> RhelShutDown ( <nl>  <nl> virtio_device_reset (& adaptExt -> vdev ); <nl> virtio_delete_queues (& adaptExt -> vdev ); <nl> + virtio_device_shutdown (& adaptExt -> vdev ); <nl> adaptExt -> vq = NULL ; <nl> } <nl> 
mmm VirtIO / VirtIOPCICommon . c <nl> ppp VirtIO / VirtIOPCICommon . c <nl> void virtio_delete_queues ( VirtIODevice * vdev ) <nl> struct virtqueue * vq ; <nl> unsigned i ; <nl>  <nl> + if ( vdev -> info == NULL ) <nl> + return ; <nl> + <nl> for ( i = 0 ; i < vdev -> maxQueues ; i ++) { <nl> vq = vdev -> info [ i ]. vq ; <nl> if ( vq != NULL ) {
mmm NetKVM / Common / ParaNdis - Common . cpp <nl> ppp NetKVM / Common / ParaNdis - Common . cpp <nl> static NDIS_STATUS ParaNdis_VirtIONetInit ( PARANDIS_ADAPTER * pContext ) <nl> for ( i = 0 ; i < pContext -> nPathBundles ; i ++) <nl> { <nl> new ( pContext -> pPathBundles + i , PLACEMENT_NEW ) CPUPathesBundle (); <nl> + } <nl>  <nl> + for ( i = 0 ; i < pContext -> nPathBundles ; i ++) <nl> + { <nl> if (! pContext -> pPathBundles [ i ]. rxPath . Create ( pContext , i * 2 )) <nl> { <nl> DPrintf ( 0 , ("% s : CParaNdisRX creation failed \ n ", __FUNCTION__ ));
mmm lib / ytnef . c <nl> ppp lib / ytnef . c <nl> BYTE * DecompressRTF ( variableLength * p , int * size ) { <nl> ALLOCCHECK_CHAR ( dst ); <nl> memcpy ( dst , comp_Prebuf . data , comp_Prebuf . size ); <nl> out = comp_Prebuf . size ; <nl> - while ( out < ( comp_Prebuf . size + uncompressedSize )) { <nl> + while (( out < ( comp_Prebuf . size + uncompressedSize )) && ( in < p -> size )) { <nl> // each flag byte flags 8 literals / references , 1 per bit <nl> flags = ( flagCount ++ % 8 == 0 ) ? src [ in ++] : flags >> 1 ; <nl> if (( flags & 1 ) == 1 ) { // each flag bit is 1 for reference , 0 for literal
mmm lib / ytnef . c <nl> ppp lib / ytnef . c <nl> int TNEFParse ( TNEFStruct * TNEF ) { <nl> while ( TNEFGetHeader ( TNEF , & type , & size ) == 0 ) { <nl> DEBUG2 ( TNEF -> Debug , 2 , " Header says type = 0x % X , size =% u ", type , size ); <nl> DEBUG2 ( TNEF -> Debug , 2 , " Header says type =% u , size =% u ", type , size ); <nl> + if ( size == 0 ) { <nl> + printf (" ERROR : Field with size of 0 \ n "); <nl> + return YTNEF_ERROR_READING_DATA ; <nl> + } <nl> data = calloc ( size , sizeof ( BYTE )); <nl> ALLOCCHECK ( data ); <nl> if ( TNEFRawRead ( TNEF , data , size , & header_checksum ) < 0 ) {
mmm ytnef / src / ytnef / vcal . c <nl> ppp ytnef / src / ytnef / vcal . c <nl> void SaveVCalendar ( TNEFStruct TNEF , int isMtgReq ) { <nl> if ( isMtgReq ) { <nl> CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " MtgReq ", " ics ", filepath ); <nl> } else { <nl> - CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " calendar ", " vcf ", filepath ); <nl> + CreateUniqueFilename ( ifilename , MAX_FILENAME_SIZE , " calendar ", " ics ", filepath ); <nl> } <nl>  <nl> printf ("% s \ n ", ifilename );
mmm lib / ytnef . c <nl> ppp lib / ytnef . c <nl> void MAPIPrint ( MAPIProps * p ) { <nl> printf ("] (% llu )\ n ", ddword_tmp ); <nl> break ; <nl> case PT_LONG : <nl> - printf (" Value : % li \ n ", *(( long *) mapidata -> data )); <nl> + printf (" Value : % i \ n ", *(( int *) mapidata -> data )); <nl> break ; <nl> case PT_I2 : <nl> printf (" Value : % hi \ n ", *(( short int *) mapidata -> data ));
mmm probe . c <nl> ppp probe . c <nl> void hexdump ( msg_info msg_info , const char * mem , unsigned int len ) <nl> } <nl> str [ c ++] = '\ n '; <nl> str [ c ++] = 0 ; <nl> - print_message ( msg_info , str ); <nl> + print_message ( msg_info , "% s ", str ); <nl> c = 0 ; <nl> } <nl> }
mmm pam_yubico . c <nl> ppp pam_yubico . c <nl> display_error ( pam_handle_t * pamh , const char * message ) { <nl> } <nl>  <nl> D ((" conv returned : '% s '", resp -> resp )); <nl> + free ( resp ); <nl> return retval ; <nl> } <nl> # endif /* HAVE_CR */ <nl> pam_sm_authenticate ( pam_handle_t * pamh , <nl> struct pam_conv * conv ; <nl> const struct pam_message * pmsg [ 1 ]; <nl> struct pam_message msg [ 1 ]; <nl> - struct pam_response * resp ; <nl> + struct pam_response * resp = NULL ; <nl> int nargs = 1 ; <nl> ykclient_t * ykc = NULL ; <nl> struct cfg cfg_st ; <nl> pam_sm_authenticate ( pam_handle_t * pamh , <nl> } <nl> } <nl> msg [ 0 ]. msg_style = cfg -> verbose_otp ? PAM_PROMPT_ECHO_ON : PAM_PROMPT_ECHO_OFF ; <nl> - resp = NULL ; <nl>  <nl> retval = conv -> conv ( nargs , pmsg , & resp , conv -> appdata_ptr ); <nl>  <nl> done : <nl> DBG ((" done . [% s ]", pam_strerror ( pamh , retval ))); <nl> pam_set_data ( pamh , " yubico_setcred_return ", ( void *) ( intptr_t ) retval , NULL ); <nl>  <nl> + if ( resp ) <nl> + free ( resp ); <nl> + <nl> return retval ; <nl> } <nl> 
mmm util . c <nl> ppp util . c <nl> check_user_token ( const char * authfile , <nl> { <nl> if ( verbose ) <nl> D ( debug_file , " Match user / token as % s /% s ", username , otp_id ); <nl> + <nl> + fclose ( opwfile ); <nl> return AUTH_FOUND ; <nl> } <nl> }
mmm CoinSpend . cpp <nl> ppp CoinSpend . cpp <nl> CoinSpend :: CoinSpend ( const Params * p , const PrivateCoin & coin , <nl> throw ZerocoinException (" Accumulator witness does not verify "); <nl> } <nl>  <nl> + // The serial # needs to be within the specified range our else it can be incremented by the modulus and create another valid proof <nl> + if (! HasValidSerial ()) { <nl> + throw ZerocoinException (" Invalid serial # range "); <nl> + } <nl> + <nl> // 1 : Generate two separate commitments to the public coin ( C ), each under <nl> // a different set of public parameters . We do this because the RSA accumulator <nl> // has specific requirements for the commitment parameters that are not <nl> const uint256 CoinSpend :: signatureHash ( const SpendMetaData & m ) const { <nl> return h . GetHash (); <nl> } <nl>  <nl> + bool CoinSpend :: HasValidSerial () const <nl> +{ <nl> + return coinSerialNumber > 0 && coinSerialNumber < params -> coinCommitmentGroup . groupOrder ; <nl> +} <nl> + <nl> } /* namespace libzerocoin */mmm CoinSpend . h <nl> ppp CoinSpend . h <nl> CoinSpend :: CoinSpend ( const Params * p , const PrivateCoin & coin , <nl> throw ZerocoinException (" Accumulator witness does not verify "); <nl> } <nl>  <nl> + // The serial # needs to be within the specified range our else it can be incremented by the modulus and create another valid proof <nl> + if (! HasValidSerial ()) { <nl> + throw ZerocoinException (" Invalid serial # range "); <nl> + } <nl> + <nl> // 1 : Generate two separate commitments to the public coin ( C ), each under <nl> // a different set of public parameters . We do this because the RSA accumulator <nl> // has specific requirements for the commitment parameters that are not <nl> const uint256 CoinSpend :: signatureHash ( const SpendMetaData & m ) const { <nl> return h . GetHash (); <nl> } <nl>  <nl> + bool CoinSpend :: HasValidSerial () const <nl> +{ <nl> + return coinSerialNumber > 0 && coinSerialNumber < params -> coinCommitmentGroup . groupOrder ; <nl> +} <nl> + <nl> } /* namespace libzerocoin */ <nl> public : <nl> */ <nl> const CoinDenomination getDenomination (); <nl>  <nl> + bool HasValidSerial () const ; <nl> bool Verify ( const Accumulator & a , const SpendMetaData & metaData ) const ; <nl>  <nl> IMPLEMENT_SERIALIZE
mmm src / decoder_allocators . hpp <nl> ppp src / decoder_allocators . hpp <nl> class c_single_allocator <nl>  <nl> std :: size_t size () const { return _buf_size ; } <nl>  <nl> - void resize ( std :: size_t new_size_ ) { _buf_size = new_size_ ; } <nl> + // This buffer is fixed , size must not be changed <nl> + void resize ( std :: size_t new_size_ ) { LIBZMQ_UNUSED ( new_size_ ); } <nl>  <nl> private : <nl> std :: size_t _buf_size ;
mmm jerry - core / vm / opcodes - ecma - relational . c <nl> ppp jerry - core / vm / opcodes - ecma - relational . c <nl> opfunc_instanceof ( ecma_value_t left_value , /**< left value */ <nl>  <nl> if (! ecma_is_value_object ( right_value )) <nl> { <nl> - ret_value = ecma_raise_type_error ( ECMA_ERR_MSG ("")); <nl> + ret_value = ecma_raise_type_error ( ECMA_ERR_MSG (" Expected an object in ' instanceof ' check .")); <nl> } <nl> else <nl> { <nl> opfunc_in ( ecma_value_t left_value , /**< left value */ <nl>  <nl> if (! ecma_is_value_object ( right_value )) <nl> { <nl> - ret_value = ecma_raise_type_error ( ECMA_ERR_MSG ("")); <nl> + ret_value = ecma_raise_type_error ( ECMA_ERR_MSG (" Expected an object in ' in ' check .")); <nl> } <nl> else <nl> {mmm jerry - core / vm / vm . c <nl> ppp jerry - core / vm / vm . c <nl> opfunc_instanceof ( ecma_value_t left_value , /**< left value */ <nl>  <nl> if (! ecma_is_value_object ( right_value )) <nl> { <nl> - ret_value = ecma_raise_type_error ( ECMA_ERR_MSG ("")); <nl> + ret_value = ecma_raise_type_error ( ECMA_ERR_MSG (" Expected an object in ' instanceof ' check .")); <nl> } <nl> else <nl> { <nl> opfunc_in ( ecma_value_t left_value , /**< left value */ <nl>  <nl> if (! ecma_is_value_object ( right_value )) <nl> { <nl> - ret_value = ecma_raise_type_error ( ECMA_ERR_MSG ("")); <nl> + ret_value = ecma_raise_type_error ( ECMA_ERR_MSG (" Expected an object in ' in ' check .")); <nl> } <nl> else <nl> { <nl> vm_op_get_value ( ecma_value_t object , /**< base object */ <nl>  <nl> if ( unlikely ( ecma_is_value_undefined ( object ) || ecma_is_value_null ( object ))) <nl> { <nl> - return ecma_raise_type_error ( ECMA_ERR_MSG ("")); <nl> + return ecma_raise_type_error ( ECMA_ERR_MSG (" Base object cannot be null or undefined .")); <nl> } <nl>  <nl> ecma_value_t prop_to_string_result = ecma_op_to_string ( property ); <nl> opfunc_call ( vm_frame_ctx_t * frame_ctx_p ) /**< frame context */ <nl>  <nl> if (! ecma_op_is_callable ( func_value )) <nl> { <nl> - completion_value = ecma_raise_type_error ( ECMA_ERR_MSG ("")); <nl> + completion_value = ecma_raise_type_error ( ECMA_ERR_MSG (" Expected a function .")); <nl> } <nl> else <nl> { <nl> opfunc_construct ( vm_frame_ctx_t * frame_ctx_p ) /**< frame context */ <nl>  <nl> if (! ecma_is_constructor ( constructor_value )) <nl> { <nl> - completion_value = ecma_raise_type_error ( ECMA_ERR_MSG ("")); <nl> + completion_value = ecma_raise_type_error ( ECMA_ERR_MSG (" Expected a constructor .")); <nl> } <nl> else <nl> { <nl> vm_loop ( vm_frame_ctx_t * frame_ctx_p ) /**< frame context */ <nl> } <nl> else <nl> { <nl> - result = ecma_raise_reference_error ( ECMA_ERR_MSG ("")); <nl> + result = ecma_raise_reference_error ( ECMA_ERR_MSG (" Cannot resolve reference .")); <nl> } <nl>  <nl> if ( ECMA_IS_VALUE_ERROR ( result )) <nl> vm_loop ( vm_frame_ctx_t * frame_ctx_p ) /**< frame context */ <nl> } <nl> case VM_OC_THROW_REFERENCE_ERROR : <nl> { <nl> - result = ecma_raise_reference_error ( ECMA_ERR_MSG ("")); <nl> + result = ecma_raise_reference_error ( ECMA_ERR_MSG (" Undefined reference .")); <nl> goto error ; <nl> } <nl> case VM_OC_EVAL :
mmm src / libcoreint / opcodes - ecma - relational . c <nl> ppp src / libcoreint / opcodes - ecma - relational . c <nl> opfunc_in ( opcode_t opdata __unused , /**< operation data */ <nl> const idx_t left_var_idx = opdata . data . in . var_left ; <nl> const idx_t right_var_idx = opdata . data . in . var_right ; <nl>  <nl> + int_data -> pos ++; <nl> + <nl> ecma_completion_value_t ret_value ; <nl>  <nl> ECMA_TRY_CATCH ( left_value , get_variable_value ( int_data , left_var_idx , false ), ret_value );
mmm jerry - main / main - unix - minimal . c <nl> ppp jerry - main / main - unix - minimal . c <nl> main ( int argc , <nl> { <nl> break ; <nl> } <nl> + <nl> + jerry_release_value ( ret_value ); <nl> + ret_value = jerry_create_undefined (); <nl> } <nl>  <nl> int ret_code = JERRY_STANDALONE_EXIT_CODE_OK ;mmm jerry - main / main - unix . c <nl> ppp jerry - main / main - unix . c <nl> main ( int argc , <nl> { <nl> break ; <nl> } <nl> + <nl> + jerry_release_value ( ret_value ); <nl> + ret_value = jerry_create_undefined (); <nl> } <nl>  <nl> int ret_code = JERRY_STANDALONE_EXIT_CODE_OK ; <nl> main ( int argc , <nl> { <nl> break ; <nl> } <nl> + <nl> + jerry_release_value ( ret_value ); <nl> + ret_value = jerry_create_undefined (); <nl> } <nl> } <nl> 
mmm src / mod_auth_openidc . c <nl> ppp src / mod_auth_openidc . c <nl> static int oidc_request_post_preserved_restore ( request_rec * r , <nl> " input . type = \" hidden \";\ n " <nl> " document . forms [ 0 ]. appendChild ( input );\ n " <nl> " }\ n " <nl> - " document . forms [ 0 ]. action = '% s ';\ n " <nl> + " document . forms [ 0 ]. action = \"% s \";\ n " <nl> " document . forms [ 0 ]. submit ();\ n " <nl> " }\ n " <nl> " </ script >\ n ", method , original_url );
mmm src / mod_auth_openidc . c <nl> ppp src / mod_auth_openidc . c <nl> static int oidc_handle_session_management_iframe_rp ( request_rec * r , oidc_cfg * c , <nl> "\ n " <nl> " function setTimer () {\ n " <nl> " checkSession ();\ n " <nl> - " timerID = setInterval (' checkSession ()', % s );\ n " <nl> + " timerID = setInterval (' checkSession ()', % d );\ n " <nl> " }\ n " <nl> "\ n " <nl> " function receiveMessage ( e ) {\ n " <nl> static int oidc_handle_session_management_iframe_rp ( request_rec * r , oidc_cfg * c , <nl>  <nl> char * s_poll_interval = NULL ; <nl> oidc_util_get_request_parameter ( r , " poll ", & s_poll_interval ); <nl> - if ( s_poll_interval == NULL ) <nl> - s_poll_interval = " 3000 "; <nl> + int poll_interval = s_poll_interval ? strtol ( s_poll_interval , NULL , 10 ) : 0 ; <nl> + if (( poll_interval <= 0 ) || ( poll_interval > 3600 * 24 )) <nl> + poll_interval = 3000 ; <nl>  <nl> const char * redirect_uri = oidc_get_redirect_uri ( r , c ); <nl> java_script = apr_psprintf ( r -> pool , java_script , origin , client_id , <nl> - session_state , op_iframe_id , s_poll_interval , redirect_uri , <nl> + session_state , op_iframe_id , poll_interval , redirect_uri , <nl> redirect_uri ); <nl>  <nl> return oidc_util_html_send ( r , NULL , java_script , " setTimer ", NULL , DONE );
mmm lib / Components / ExtensionClass / src / ExtensionClass . c <nl> ppp lib / Components / ExtensionClass / src / ExtensionClass . c <nl> static char ExtensionClass_module_documentation [] = <nl> " - They provide access to unbound methods ,\ n " <nl> " - They can be called to create instances .\ n " <nl> "\ n " <nl> -"$ Id : ExtensionClass . c , v 1 . 56 2002 / 06 / 18 23 : 19 : 02 jeremy Exp $\ n " <nl> +"$ Id : ExtensionClass . c , v 1 . 57 2002 / 08 / 22 16 : 55 : 53 shane Exp $\ n " <nl> ; <nl>  <nl> # include < stdio . h > <nl> PMethod_repr ( PMethod * self ) <nl> char * func_name , buf [ 8192 ]; <nl> int n ; <nl>  <nl> - func_name = PyString_AS_STRING ((( PyFunctionObject *) self -> meth )-> func_name ); <nl> + if ( PyFunction_Check ( self -> meth )) { <nl> + func_name = PyString_AS_STRING ( <nl> + (( PyFunctionObject *) self -> meth )-> func_name ); <nl> + } <nl> + else { <nl> + /* self -> meth is some other kind of object */ <nl> + func_name = "(?)"; <nl> + } <nl> + <nl> if ( self -> self ) { <nl> PyObject * repr = PyObject_Repr ( self -> self ); <nl> if (! repr )